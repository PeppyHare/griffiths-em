{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Griffiths - Introduction to Electrodynamics This is basically just a web-friendly version of David Griffiths' Introduction to Electrodynamics, 4th Ed. . These are my class notes for the University of Washington's PHYS 543. This is mostly an exercise for myself in learning about Mkdocs, MathJax, and physics! Click around in the sidebar to find a chapter to read, or follow the links at the bottom of the page to read in order! Don't forget to try out the interface on mobile, it's very slick ;) Table of Contents 1 - Vector Analysis 1.1 - Vector Algebra 1.2 - Differential Calculus 1.3 - Integral Calculus 1.4 - Curvilinear Coordinates 1.5 - The Dirac Delta Function 1.6 - The Theory of Vector Fields 2 - Electrostatics 2.1 - The Electric Field 2.2 - Divergence and Curl of Electrostatic Fields 2.3 - Electric Potential 2.4 - Work and Energy in Electrostatics 2.5 - Conductors 3 - Potentials 3.1 - Laplace's Equation 3.2 - The Method of Images 3.3 - Separation of Variables 3.4 - Multipole Expansion Chapter 3 Problems 4 - Electric Fields in Matter 4.1 - Polarization 4.2 - The Field of a Polarized Object 4.3 - The Electric Displacement 4.4 - Linear Dielectrics 5 - Magnetostatics 5.1 - The Lorentz Force Law Internals All content is written in Markdown and rendered to a static site using MkDocs . The theme for the site is Material for MkDocs . I use python-markdown-math to turn any LaTeX in my source into full-blown MathJax to be rendered in the browser (and in a mobile-friendly format!).","title":"Home"},{"location":"#griffiths-introduction-to-electrodynamics","text":"This is basically just a web-friendly version of David Griffiths' Introduction to Electrodynamics, 4th Ed. . These are my class notes for the University of Washington's PHYS 543. This is mostly an exercise for myself in learning about Mkdocs, MathJax, and physics! Click around in the sidebar to find a chapter to read, or follow the links at the bottom of the page to read in order! Don't forget to try out the interface on mobile, it's very slick ;)","title":"Griffiths - Introduction to Electrodynamics"},{"location":"#table-of-contents","text":"1 - Vector Analysis 1.1 - Vector Algebra 1.2 - Differential Calculus 1.3 - Integral Calculus 1.4 - Curvilinear Coordinates 1.5 - The Dirac Delta Function 1.6 - The Theory of Vector Fields 2 - Electrostatics 2.1 - The Electric Field 2.2 - Divergence and Curl of Electrostatic Fields 2.3 - Electric Potential 2.4 - Work and Energy in Electrostatics 2.5 - Conductors 3 - Potentials 3.1 - Laplace's Equation 3.2 - The Method of Images 3.3 - Separation of Variables 3.4 - Multipole Expansion Chapter 3 Problems 4 - Electric Fields in Matter 4.1 - Polarization 4.2 - The Field of a Polarized Object 4.3 - The Electric Displacement 4.4 - Linear Dielectrics 5 - Magnetostatics 5.1 - The Lorentz Force Law","title":"Table of Contents"},{"location":"#internals","text":"All content is written in Markdown and rendered to a static site using MkDocs . The theme for the site is Material for MkDocs . I use python-markdown-math to turn any LaTeX in my source into full-blown MathJax to be rendered in the browser (and in a mobile-friendly format!).","title":"Internals"},{"location":"ch1-1/","text":"1.1 Vector Algebra 1.1.1 Vector Operations If you walk 4 miles due north and then 3 miles due east (Fig. 1.1), you will have gone a total of 7 miles, but you're not 7 miles from where you set out-you're only 5. We need an arithmetic to describe quantities like this, which evidently do not add in the ordinary way. The reason they don't, of course, is that displacements (straight line segments going from one point to another) have direction as well as magnitude (length), and it is essential to take both into account when you combine them. Such objects are called vectors: velocity, acceleration, force and momentum are other examples. By contrast, quantities that have magnitude but no direction are called scalars: examples include mass, charge, density, and temperature. I shall use boldface ( \\vec{A} , \\vec{B} , and so on) for vectors and ordinary type for scalars. The magnitude of a vector \\vec{A} is written |\\vec{A}| or, more simply, A . In diagrams, vectors are denoted by arrows: the length of the arrow is proportional to the magnitude of the vector, and the arrowhead indicates its direction. Minus \\vec{A} ( - \\vec{A} ) is a vector with the same magnitude as A but of opposite direction (Fig. 1.2). Note that vectors have magnitude and direction but not location: a displacement of 4 miles due north from Washington is represented by the same vector as a displacement 4 miles north from Baltimore (neglecting, of course, the curvature of the earth). On a diagram, therefore, you can slide the arrow around at will, as long as you don't change its length or direction. We define four vector operations: addition and three kinds of multiplication. (i) Addition of two vectors. . Place the tail of \\vec{B} at the head of \\vec{A} ; the sum, \\vec{A} + \\vec{B} , is the vector from the tail of \\vec{A} to the head of \\vec{B} (Fig 1.3). This rule generalizes the obvious procedure for combining two displacements. Addition is commutative : \\vec{A} + \\vec{B} = \\vec{B} + \\vec{A} 3 miles east followed by 4 miles north gets you to the same place as 4 miles north followed by 3 miles east. Addition is also associative: (\\vec{A} + \\vec{B}) + \\vec{C} = \\vec{A} + (\\vec{B} + \\vec{C}) To subtract a vector, add its opposite (Fig. 1.4): \\vec{A} - \\vec{B} = \\vec{A} + (- \\vec{B}) (ii) Multiplication by a scalar. Multiplication of a vector by a positive scalar a multiplies the magnitude but leaves the direction unchanged (Fig. 1.5). (If a is negative, the direction is reversed.) Scalar multiplication is distributive: a(\\vec{A} + \\vec{B}) = a \\vec{A} + a \\vec{B} (iii) Dot product of two vectors. The dot product of two vectors is defined by \\vec{A} \\cdot \\vec{B} = A B \\cos \\theta \\tag{1.1} where \\theta is the angle they form when placed tail-to-tail (Fig. 1.6). Note that \\vec{A} \\cdot \\vec{B} is itself a scalar (hence the alternative name scalar product ). The dot product is commutative, \\vec{A} \\cdot \\vec{B} = \\vec{B} \\cdot \\vec{A} and distributive \\vec{A} \\cdot (\\vec{B} + \\vec{C}) = \\vec{A} \\cdot \\vec{B} + \\vec{A} \\cdot \\vec{C} \\tag{1.2} Geometrically, \\vec{A} \\cdot \\vec{B} is the product of A times the projection of B along A (or the product of B times the projection of A along B). If the two vectors are parallel, then \\vec{A} \\cdot \\vec{B} = AB . In particular, for any vector A \\vec{A} \\cdot \\vec{A} = A^2 \\tag{1.3} If A and B are perpendicular, then \\vec{A} \\cdot \\vec{B} = 0 Example 1.1 Let \\vec{C} = \\vec{A} - \\vec{B} (Fig 1.7), and calculate the dot product of \\vec{C} with itself. Solution \\vec{C} \\cdot \\vec{C} = ( \\vec{A} - \\vec{B} ) \\cdot (\\vec{A} - \\vec{B}) = \\vec{A} \\cdot \\vec{A} - \\vec{A} \\cdot \\vec{B} - \\vec{B} \\cdot \\vec{A} + \\vec{B} \\cdot \\vec{B} or C^2 = A^2 + B^2 - 2AB\\cos \\theta This is the law of cosines. (iv) Cross product of two vectors. The cross product of two vectors is defined by \\vec{A} \\cross \\vec{B} = AB \\sin \\theta \\vu{n} \\tag{1.4} where \\vu{n} is a unit vector (vector of magnitude 1) pointing perpendicular to the plane of A and B. (I shall use a hat \\vu{} to denote unit vectors.) Of course, there are two directions perpendicular to any plane: \"in\" and \"out.\" The ambiguity is resolved by the right-hand rule: let your fingers point in the direction of the first vector and curl around (via the smaller angle) toward the second; then your thumb indicates the direction of \\vu{n} . (In Fig. 1.8, \\vec{A} \\cross \\vec{B} points into the page; \\vec{B} \\cross \\vec{A} points out of the page.) Note that \\vec{A} \\cross \\vec{B} is itself a vector (hence the alternative name vector product). The cross product is distributive \\vec{A} \\cross ( \\vec{B} + \\vec{C}) = ( \\vec{A} \\cross \\vec{B}) + (\\vec{A} \\cross \\vec{C}) but not commutative . In fact, (\\vec{B} \\cross \\vec{A}) = - (\\vec{A} \\cross \\vec{B}) \\tagl{1.6} Geometrically, | \\vec{A} \\cross \\vec{B} | is the area of the parallelogram generated by \\vec{A} and \\vec{B} (Fig 1.8). If two vectors are parallel, their cross product is zero. In particular, \\vec{A} \\cross \\vec{A} = 0 for any vector A. 1.1.2: Vector Algebra: Component Form In the previous section, I defined the four vector operations (addition, scalar multiplication, dot product, and cross product) in \"abstract\" form-that is, without reference to any particular coordinate system. In practice, it is often easier to set up Cartesian coordinates x, y, z and work with vector components. Let \\vu{x} , \\vu{y} , and \\vu{z} be unit vectors parallel to the x, y, and z axes, respectively (Fig. 1.9(a)). An arbitrary vector A can be expanded in terms ofthese basis vectors (Fig. 1.9(b)): \\vec{A} = A_x \\vu{x} + A_y \\vu{y} + A_z \\vu{z} The numbers A_x , A_y , and A_z are the \"components\" of A; geometrically, they are the projections of A along the three coordinate axes ( A_x = \\vec{A} \\cdot \\vu{x}, A_y = \\vec{A} \\cdot \\vu{y}, A_z = \\vec{A} \\cdot \\vu{z} ). We can now reformulate each of the four vector operations as a rule for manipulating components: \\vec{A} + \\vec{B} = (A_x \\vu{x} + A_y \\vu{y} + A_z \\vu{z}) + (B_x \\vu{x} + B_y \\vu{y} + B_z \\vu{z}) \\\\ = (A_x + B_x) \\vu{x} + (A_y + B_y) \\vu{y} + (A_z + B_z) \\vu{z} \\tag{1.7} Rule (i): To add vectors, add like components. a\\vec{A} = (a A_x) \\vu{x} + (a A_y) \\vu{y} + (a A_z)\\vu{z} \\tag{1.8} Rule (ii): To multiply by a scalar, multiply each component. Because \\vu{x}, \\vu{y} , and \\vu{z} are mutually perpendicular unit vectors \\vu{x} \\cdot \\vu{x} = \\vu{y} \\cdot \\vu{y} = \\vu{z} \\cdot \\vu{z} = 1; \\qquad \\vu{x} \\cdot \\vu{y} = \\vu{x} \\cdot \\vu{z} = \\vu{y} \\cdot \\vu{z} = 0 \\tag{1.9} Accordingly, \\vec{A} \\cdot \\vec{B} = (A_x \\vu{x} + A_y \\vu{y} + A_z \\vu{z}) \\cdot (B_x \\vu{x} + B_y \\vu{y} + B_z \\vu{z}) \\\\ = A_x B_x + A_y B_y + A_z B_z \\tag{1.10} Rule (iii): To calculate the dot product, multiply like components and add. In particular, \\vec{A} \\cdot \\vec{A} = A_x ^2 + A_y ^2 + A_z ^2 so A = \\sqrt{A_x ^2 + A_y ^2 + A_z ^2} \\tag{1.11} Similarly, \\begin{align} \\vu{x} \\cross \\vu{x} & = & \\vu{y} \\cross \\vu{y} & = & \\vu{z} \\cross \\vu{z} = 0 \\\\ \\vu{x} \\cross \\vu{y} & = & - \\vu{y} \\cross \\vu{x} & = & \\vu{z} \\\\ \\vu{y} \\cross \\vu{z} & = & - \\vu{z} \\cross \\vu{y} & = & \\vu{x} \\\\ \\vu{z} \\cross \\vu{x} & = & - \\vu{x} \\cross \\vu{z} & = & \\vu{y} \\end{align} Therefore, \\vec{A} \\cross \\vec{B} = (A_x \\vu{x} + A_y \\vu{y} + A_z \\vu{z}) \\cross (B_x \\vu{x} + B_y \\vu{y} + B_z \\vu{z}) \\\\ = (A_y B_z - A_z B_y) \\vu{x} + (A_z B_x - A_x B_z)\\vu{y} + (A_x B_y - A_y B_x) \\vu{z} \\tag{1.13} This cumbersome expression can be written more neatly as a determinant: \\vec{A} \\cross \\vec{B} = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ A_x & A_y & A_z \\\\ B_x & B_y & B_z \\end{vmatrix} Rule (iv): To calculate the cross product, form the determinant whose first row is \\vu{x}, \\vu{y}, \\vu{z} , whose second row is A, and whose third row is B. Example 1.2 Find the angle between the face diagonals of a cube. Solution We might as well use a cube of side 1, and place it as shown in Fig 1.10, with one corner at the origin. The face diagonals \\vec{A} and \\vec{B} are \\vec{A} = 1 \\vu{x} + 0 \\vu{y} + 1 \\vu{z}; \\qquad \\vec{B} = 0 \\vu{x} + 1 \\vu{y} + 1 \\vu{z} So, in component form, \\vec{A} \\cdot \\vec{B} = 1 \\cdot 0 + 0 \\cdot 1 + 1 \\cdot 1 = 1 On the other hand, in \"abstract\" form, \\vec{A} \\cdot \\vec{B} = AB \\cos \\theta = \\sqrt{2} \\sqrt{2} \\cos \\theta = 2 \\cos \\theta Therefore, \\cos \\theta = 1/2 \\quad \\text{ or } \\quad \\theta = 60^{\\circ} Of course, you can get the answer more easily by drawing in a diagonal across the top of the cube, completing the equilateral triangle. But in cases where the geometry is not so simple, this device of comparing the abstract and component forms of the dot product can be a very efficient means of finding angles. 1.1.3: Triple Products Since the cross product of two vectors is itself a vector, it can be dotted or crossed with a third vector to form a triple product. (i) Scalar triple product: \\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) . Geometrically, |\\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) | is the volume of the parallelpiped generated by A , B , and C , since |\\vec{B} \\cross \\vec{C}| is the area of the base, and | \\vec{A} \\cos \\theta | is the altitude (Fig. 1.12). Evidently, \\vec{A} \\cdot(\\vec{B} \\cross \\vec{C}) = \\vec{B} \\cdot (\\vec{C} \\cross \\vec{A}) = \\vec{C} \\cdot (\\vec{A} \\cross \\vec{B}) \\tagl{1.15} for they all correspond to the same figure. Note that \"alphabetical\" order is preserved - in view of \\eqref{1.6} , the \"nonalphabetical\" triple products \\vec{A} \\cdot(\\vec{C} \\cross \\vec{B}) = \\vec{B} \\cdot (\\vec{A} \\cross \\vec{C}) = \\vec{C} \\cdot (\\vec{B} \\cross \\vec{A}) have the opposite sign. In component form, \\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) = \\begin{vmatrix} A_x & A_y & A_z \\\\ B_x & B_y & B_z \\\\ C_x & C_y & C_z \\end{vmatrix} \\tagl{1.16} Note that the dot and cross can be interchanged: \\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) = (\\vec{A} \\cross \\vec{B}) \\cdot \\vec{C} (this follows immediately from Eq. 1.15); however, the placement of the parentheses is critical: (\\vec{A} \\cdot \\vec{B}) \\cdot \\vec{C} is a meaningless expression - you can't make a cross product from a scalar and a vector. (ii) Vector triple product: \\vec{A} \\cross (\\vec{B} \\cross \\vec{C}) . The vector triple product can be simplified by the so-called BAC-CAB rule: \\vec{A} \\cross (\\vec{B} \\cross \\vec{C}) = \\vec{B}(\\vec{A} \\cdot \\vec{C}) - \\vec{C} (\\vec{A} \\cdot \\vec{B}) \\tagl{1.17} Notice that (\\vec{A} \\cross \\vec{B}) \\cross \\vec{C} = - \\vec{C} \\cross (\\vec{A} \\cross \\vec{B}) = -\\vec{A}(\\vec{B} \\cdot \\vec{C}) + \\vec{B}(\\vec{A} \\cdot \\vec{C}) is an entirely different vector (cross-products are not associative). All higher vector products can be similarly reduced, often by repeated application of \\eqref{1.17} , so it is never necessary for an expression to contain more than one cross product in any term. For instance, (\\vec{A} \\cross \\vec{B}) \\cdot (\\vec{C} \\cross \\vec{D}) = (\\vec{A} \\cdot \\vec{C})(\\vec{B} \\cdot \\vec{D}) - (\\vec{A} \\cdot \\vec{D}) (\\vec{B} \\cdot \\vec{C}) \\vec{A} \\cross [\\vec{B} \\cross (\\vec{C} \\cross \\vec{D})] = \\vec{B}[ \\vec{A} \\cdot (\\vec{C} \\cross \\vec{D})] - (\\vec{A} \\cdot \\vec{B})(\\vec{C} \\cross \\vec{D}) \\tagl{1.18} 1.1.4: Position, Displacement, and Separation Vectors The location of a point in three dimensions can be described by listing its Cartesian coordinates (x, y, z). The vector to that point from the origin ( \\mathscr{O} ) is called the position vector (Fig 1.13): \\vec{r} \\equiv x \\vu{x} + y \\vu{y} + z \\vu{z} \\tagl{1.19} I will reserve the letter \\vec{r} for this purpose. Its magnitude, r = \\sqrt{x^2 + y^2 + z^2} \\tagl{1.20} is the distance from the origin, and \\vu{r} = \\frac{\\vec{r}}{r} = \\frac{x \\vu{x} + y \\vu{y} + z \\vu{z}}{ \\sqrt{x^2 + y^2 + z^2}} \\tagl{1.21} is a unit vector pointing radially outward. The infinitesimal displacement vector from (x, y, z) to x + \\dd{x}, y + \\dd{y}, z + \\dd{z} is \\dd{\\vec{l}} = \\dd{x} \\vu{x} + \\dd{y} \\vu{y} + \\dd{z} \\vu{z} \\tagl{1.22} (We could call this \\dd{\\vec{r}} , since that's what it is, but it is useful to have a special notation for infinitesimal displacements.) In electrodynamics, one frequently encounters problems involving two points - typically a source point , \\vec{r'} , where an electric charge is located, and a field point \\vec{r} at which you are calculating the electric or magnetic field (Fig 1.14). It pays to adopt right from the start some short-hand notation for the separation vector from the source point to the field point. I shall use for this purpose the letter \\gr : \\vec{\\gr} \\equiv \\vec{r} - \\vec{r'} \\tagl{1.23} Its magnitude is |\\gr| = | \\vu{r} - \\vu{r'} | \\tagl{1.24} and a unit vector in the direction from \\vec{r'} to \\vec{r} is \\vu{\\gr} = \\frac{\\gr}{|\\gr|} = \\frac{\\vec{r} - \\vec{r'}}{|\\vec{r} - \\vec{r'}|} \\tagl{1.25} In Cartesian coordinates, \\gr = (x - x') \\vu{x} + (y-y') \\vu{y} + (z-z') \\vu{z} \\tagl{1.26} |\\gr| = \\sqrt{(x - x')^2 + (y-y')^2 + (z-z')^2 } \\tagl{1.27} \\vu{\\gr} = \\frac{(x - x') \\vu{x} + (y-y') \\vu{y} + (z-z') \\vu{z}}{\\sqrt{(x - x')^2 + (y-y')^2 + (z-z')^2 }} (from which you can appreciate the economy of the \\gr notation). 1.1.5: How Vectors Transform The definition of a vector as \"a quantity with a magnitude and direction\" is not altogether satisfactory: What precisely does \"direction\" mean? This may seem a pedantic question, but we shall soon encounter a species of derivative that looks rather like a vector, and we'll want to know for sure whether it is one. You might be inclined to say that a vector is anything that has three components that combine properly under addition. Well, how about this: We have a barrel of fruit that contains N_x pears, N_y apples, and N_z bananas. Is \\vec{N} = N_x \\vu{x} + N_y \\vu{y} + N_z \\vu{z} a vector? It has three components, and when you add another barrel with M_x pears, M_y apples, and M_z bananas the result is N_x + M_x pears, N_y + M_y apples, N_z + M_z bananas. So it does add like a vector. Yet it's obviously not a vector, in the physicist's sense of the word, because it doesn't really have a direction. What exactly is wrong with it? The answer is that \\vec{N} does not transform properly when you change coordinates . The coordinate frame we use to describe positions in space is of course entirely arbitrary, but there is a specific geometrical transformation law for converting vector components from one frame to another. Suppose, for instance, the \\overline{x}, \\overline{y}, \\overline{z} system is rotated by angle \\phi , relative to x, y, z , about the common x = \\overline{x} axes. From Fig. 1.15, A_y = A \\cos \\theta, \\qquad A_z = A \\sin \\theta while \\begin{align*} \\overline{A_y} & = A \\cos \\overline{\\theta} = A \\cos (\\theta - \\phi) = A (\\cos \\theta \\cos \\phi + \\sin \\theta \\sin \\phi) \\\\ & = \\cos \\phi A_y + \\sin \\phi A_z \\\\ \\overline{A_z} & = A \\sin \\overline{\\theta} = A \\sin (\\theta - \\phi) = A (\\sin \\theta \\cos \\phi - \\cos \\theta \\sin \\phi) \\\\ & = - \\sin \\phi A_y + \\cos \\phi A_z \\end{align*} We might express this conclusion in matrix notation: \\begin{pmatrix} \\overline{A_y} \\\\ \\overline{A_z} \\end{pmatrix} = \\begin{pmatrix} \\cos \\phi & \\sin \\phi \\\\ - \\sin \\phi & \\cos \\phi \\end{pmatrix} \\begin{pmatrix} A_y \\\\ A_z \\end{pmatrix} \\tagl{1.29} More generally, for rotation about an arbitrary axis in three dimensions, the transformation law takes the form \\begin{pmatrix} \\overline{A_x} \\\\ \\overline{A_y} \\\\ \\overline{A_z} \\end{pmatrix} = \\begin{pmatrix} R_{xx} & R_{xy} & R_{xz} \\\\ R_{yx} & R_{yy} & R_{yz} \\\\ R_{zx} & R_{zy} & R_{zz} \\end{pmatrix} \\begin{pmatrix} A_x \\\\ A_y \\\\ A_z \\end{pmatrix} \\tagl{1.30} or, more compactly, \\overline{A_i} = \\sum_{j=1}^3 R_{ij} A_j \\tagl{1.31} where index 1 stands for x, 2 for y, and 3 for z. The elements of the matrix R can be ascertained, for a given rotation, by the same sort of trigonometric arguments as we used for a rotation about the x axis. Now: Do the components of \\vec{N} transform this way? Of course not - it doesn't matter what coordinates you use to represent positions in space; there are still just as many apples in the barrel. You can't convert a pear into a banana by choosing a different set of axes, but you can turn in A_x into \\overline{A_y} . Formally, then, a vector is any set of three components that transforms in the same manner as a displacement when you change coordinates . As always, displacement is the model for the behavior of vectors. By the way, a (second-rank) tensor is a quantity with nine components, T_{xx}, T_{xy}, T_{xz}, T_{yx}, \\ldots T_{zz} which transform with two factors of R : \\begin{align*} \\overline{T}_{xx} & = R_{xx}(R_{xx} T_{xx} + R_{xy} T_{xy} + R_{xz} T_{xz}) \\\\ & + R_{xy}(R_{xx} T_{yx} + R_{xy} T_{yy} + R_{xz} T_{yz}) \\\\ & + R_{xz}(R_{xx} T_{zx} + R_{xy} T_{zy} + R_{xz} T_{zz}), \\ldots \\end{align*} or, more compactly, \\overline{T}_{ij} = \\sum_{k=1}^3 \\sum_{l=1} ^3 R_{ik} R_{jl} T_{kl} \\tagl{1.32} In general, an n-th rank tensor has n indices and 3^n components, and transforms with n factors of R . In this hierarchy, a vector is a tensor of rank 1, and a scalar is a tensor of rank zero.","title":"1.1 - Vector Algebra"},{"location":"ch1-1/#11-vector-algebra","text":"","title":"1.1 Vector Algebra"},{"location":"ch1-1/#111-vector-operations","text":"If you walk 4 miles due north and then 3 miles due east (Fig. 1.1), you will have gone a total of 7 miles, but you're not 7 miles from where you set out-you're only 5. We need an arithmetic to describe quantities like this, which evidently do not add in the ordinary way. The reason they don't, of course, is that displacements (straight line segments going from one point to another) have direction as well as magnitude (length), and it is essential to take both into account when you combine them. Such objects are called vectors: velocity, acceleration, force and momentum are other examples. By contrast, quantities that have magnitude but no direction are called scalars: examples include mass, charge, density, and temperature. I shall use boldface ( \\vec{A} , \\vec{B} , and so on) for vectors and ordinary type for scalars. The magnitude of a vector \\vec{A} is written |\\vec{A}| or, more simply, A . In diagrams, vectors are denoted by arrows: the length of the arrow is proportional to the magnitude of the vector, and the arrowhead indicates its direction. Minus \\vec{A} ( - \\vec{A} ) is a vector with the same magnitude as A but of opposite direction (Fig. 1.2). Note that vectors have magnitude and direction but not location: a displacement of 4 miles due north from Washington is represented by the same vector as a displacement 4 miles north from Baltimore (neglecting, of course, the curvature of the earth). On a diagram, therefore, you can slide the arrow around at will, as long as you don't change its length or direction. We define four vector operations: addition and three kinds of multiplication. (i) Addition of two vectors. . Place the tail of \\vec{B} at the head of \\vec{A} ; the sum, \\vec{A} + \\vec{B} , is the vector from the tail of \\vec{A} to the head of \\vec{B} (Fig 1.3). This rule generalizes the obvious procedure for combining two displacements. Addition is commutative : \\vec{A} + \\vec{B} = \\vec{B} + \\vec{A} 3 miles east followed by 4 miles north gets you to the same place as 4 miles north followed by 3 miles east. Addition is also associative: (\\vec{A} + \\vec{B}) + \\vec{C} = \\vec{A} + (\\vec{B} + \\vec{C}) To subtract a vector, add its opposite (Fig. 1.4): \\vec{A} - \\vec{B} = \\vec{A} + (- \\vec{B}) (ii) Multiplication by a scalar. Multiplication of a vector by a positive scalar a multiplies the magnitude but leaves the direction unchanged (Fig. 1.5). (If a is negative, the direction is reversed.) Scalar multiplication is distributive: a(\\vec{A} + \\vec{B}) = a \\vec{A} + a \\vec{B} (iii) Dot product of two vectors. The dot product of two vectors is defined by \\vec{A} \\cdot \\vec{B} = A B \\cos \\theta \\tag{1.1} where \\theta is the angle they form when placed tail-to-tail (Fig. 1.6). Note that \\vec{A} \\cdot \\vec{B} is itself a scalar (hence the alternative name scalar product ). The dot product is commutative, \\vec{A} \\cdot \\vec{B} = \\vec{B} \\cdot \\vec{A} and distributive \\vec{A} \\cdot (\\vec{B} + \\vec{C}) = \\vec{A} \\cdot \\vec{B} + \\vec{A} \\cdot \\vec{C} \\tag{1.2} Geometrically, \\vec{A} \\cdot \\vec{B} is the product of A times the projection of B along A (or the product of B times the projection of A along B). If the two vectors are parallel, then \\vec{A} \\cdot \\vec{B} = AB . In particular, for any vector A \\vec{A} \\cdot \\vec{A} = A^2 \\tag{1.3} If A and B are perpendicular, then \\vec{A} \\cdot \\vec{B} = 0","title":"1.1.1 Vector Operations"},{"location":"ch1-1/#example-11","text":"Let \\vec{C} = \\vec{A} - \\vec{B} (Fig 1.7), and calculate the dot product of \\vec{C} with itself. Solution \\vec{C} \\cdot \\vec{C} = ( \\vec{A} - \\vec{B} ) \\cdot (\\vec{A} - \\vec{B}) = \\vec{A} \\cdot \\vec{A} - \\vec{A} \\cdot \\vec{B} - \\vec{B} \\cdot \\vec{A} + \\vec{B} \\cdot \\vec{B} or C^2 = A^2 + B^2 - 2AB\\cos \\theta This is the law of cosines. (iv) Cross product of two vectors. The cross product of two vectors is defined by \\vec{A} \\cross \\vec{B} = AB \\sin \\theta \\vu{n} \\tag{1.4} where \\vu{n} is a unit vector (vector of magnitude 1) pointing perpendicular to the plane of A and B. (I shall use a hat \\vu{} to denote unit vectors.) Of course, there are two directions perpendicular to any plane: \"in\" and \"out.\" The ambiguity is resolved by the right-hand rule: let your fingers point in the direction of the first vector and curl around (via the smaller angle) toward the second; then your thumb indicates the direction of \\vu{n} . (In Fig. 1.8, \\vec{A} \\cross \\vec{B} points into the page; \\vec{B} \\cross \\vec{A} points out of the page.) Note that \\vec{A} \\cross \\vec{B} is itself a vector (hence the alternative name vector product). The cross product is distributive \\vec{A} \\cross ( \\vec{B} + \\vec{C}) = ( \\vec{A} \\cross \\vec{B}) + (\\vec{A} \\cross \\vec{C}) but not commutative . In fact, (\\vec{B} \\cross \\vec{A}) = - (\\vec{A} \\cross \\vec{B}) \\tagl{1.6} Geometrically, | \\vec{A} \\cross \\vec{B} | is the area of the parallelogram generated by \\vec{A} and \\vec{B} (Fig 1.8). If two vectors are parallel, their cross product is zero. In particular, \\vec{A} \\cross \\vec{A} = 0 for any vector A.","title":"Example 1.1"},{"location":"ch1-1/#112-vector-algebra-component-form","text":"In the previous section, I defined the four vector operations (addition, scalar multiplication, dot product, and cross product) in \"abstract\" form-that is, without reference to any particular coordinate system. In practice, it is often easier to set up Cartesian coordinates x, y, z and work with vector components. Let \\vu{x} , \\vu{y} , and \\vu{z} be unit vectors parallel to the x, y, and z axes, respectively (Fig. 1.9(a)). An arbitrary vector A can be expanded in terms ofthese basis vectors (Fig. 1.9(b)): \\vec{A} = A_x \\vu{x} + A_y \\vu{y} + A_z \\vu{z} The numbers A_x , A_y , and A_z are the \"components\" of A; geometrically, they are the projections of A along the three coordinate axes ( A_x = \\vec{A} \\cdot \\vu{x}, A_y = \\vec{A} \\cdot \\vu{y}, A_z = \\vec{A} \\cdot \\vu{z} ). We can now reformulate each of the four vector operations as a rule for manipulating components: \\vec{A} + \\vec{B} = (A_x \\vu{x} + A_y \\vu{y} + A_z \\vu{z}) + (B_x \\vu{x} + B_y \\vu{y} + B_z \\vu{z}) \\\\ = (A_x + B_x) \\vu{x} + (A_y + B_y) \\vu{y} + (A_z + B_z) \\vu{z} \\tag{1.7} Rule (i): To add vectors, add like components. a\\vec{A} = (a A_x) \\vu{x} + (a A_y) \\vu{y} + (a A_z)\\vu{z} \\tag{1.8} Rule (ii): To multiply by a scalar, multiply each component. Because \\vu{x}, \\vu{y} , and \\vu{z} are mutually perpendicular unit vectors \\vu{x} \\cdot \\vu{x} = \\vu{y} \\cdot \\vu{y} = \\vu{z} \\cdot \\vu{z} = 1; \\qquad \\vu{x} \\cdot \\vu{y} = \\vu{x} \\cdot \\vu{z} = \\vu{y} \\cdot \\vu{z} = 0 \\tag{1.9} Accordingly, \\vec{A} \\cdot \\vec{B} = (A_x \\vu{x} + A_y \\vu{y} + A_z \\vu{z}) \\cdot (B_x \\vu{x} + B_y \\vu{y} + B_z \\vu{z}) \\\\ = A_x B_x + A_y B_y + A_z B_z \\tag{1.10} Rule (iii): To calculate the dot product, multiply like components and add. In particular, \\vec{A} \\cdot \\vec{A} = A_x ^2 + A_y ^2 + A_z ^2 so A = \\sqrt{A_x ^2 + A_y ^2 + A_z ^2} \\tag{1.11} Similarly, \\begin{align} \\vu{x} \\cross \\vu{x} & = & \\vu{y} \\cross \\vu{y} & = & \\vu{z} \\cross \\vu{z} = 0 \\\\ \\vu{x} \\cross \\vu{y} & = & - \\vu{y} \\cross \\vu{x} & = & \\vu{z} \\\\ \\vu{y} \\cross \\vu{z} & = & - \\vu{z} \\cross \\vu{y} & = & \\vu{x} \\\\ \\vu{z} \\cross \\vu{x} & = & - \\vu{x} \\cross \\vu{z} & = & \\vu{y} \\end{align} Therefore, \\vec{A} \\cross \\vec{B} = (A_x \\vu{x} + A_y \\vu{y} + A_z \\vu{z}) \\cross (B_x \\vu{x} + B_y \\vu{y} + B_z \\vu{z}) \\\\ = (A_y B_z - A_z B_y) \\vu{x} + (A_z B_x - A_x B_z)\\vu{y} + (A_x B_y - A_y B_x) \\vu{z} \\tag{1.13} This cumbersome expression can be written more neatly as a determinant: \\vec{A} \\cross \\vec{B} = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ A_x & A_y & A_z \\\\ B_x & B_y & B_z \\end{vmatrix} Rule (iv): To calculate the cross product, form the determinant whose first row is \\vu{x}, \\vu{y}, \\vu{z} , whose second row is A, and whose third row is B.","title":"1.1.2: Vector Algebra: Component Form"},{"location":"ch1-1/#example-12","text":"Find the angle between the face diagonals of a cube. Solution We might as well use a cube of side 1, and place it as shown in Fig 1.10, with one corner at the origin. The face diagonals \\vec{A} and \\vec{B} are \\vec{A} = 1 \\vu{x} + 0 \\vu{y} + 1 \\vu{z}; \\qquad \\vec{B} = 0 \\vu{x} + 1 \\vu{y} + 1 \\vu{z} So, in component form, \\vec{A} \\cdot \\vec{B} = 1 \\cdot 0 + 0 \\cdot 1 + 1 \\cdot 1 = 1 On the other hand, in \"abstract\" form, \\vec{A} \\cdot \\vec{B} = AB \\cos \\theta = \\sqrt{2} \\sqrt{2} \\cos \\theta = 2 \\cos \\theta Therefore, \\cos \\theta = 1/2 \\quad \\text{ or } \\quad \\theta = 60^{\\circ} Of course, you can get the answer more easily by drawing in a diagonal across the top of the cube, completing the equilateral triangle. But in cases where the geometry is not so simple, this device of comparing the abstract and component forms of the dot product can be a very efficient means of finding angles.","title":"Example 1.2"},{"location":"ch1-1/#113-triple-products","text":"Since the cross product of two vectors is itself a vector, it can be dotted or crossed with a third vector to form a triple product. (i) Scalar triple product: \\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) . Geometrically, |\\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) | is the volume of the parallelpiped generated by A , B , and C , since |\\vec{B} \\cross \\vec{C}| is the area of the base, and | \\vec{A} \\cos \\theta | is the altitude (Fig. 1.12). Evidently, \\vec{A} \\cdot(\\vec{B} \\cross \\vec{C}) = \\vec{B} \\cdot (\\vec{C} \\cross \\vec{A}) = \\vec{C} \\cdot (\\vec{A} \\cross \\vec{B}) \\tagl{1.15} for they all correspond to the same figure. Note that \"alphabetical\" order is preserved - in view of \\eqref{1.6} , the \"nonalphabetical\" triple products \\vec{A} \\cdot(\\vec{C} \\cross \\vec{B}) = \\vec{B} \\cdot (\\vec{A} \\cross \\vec{C}) = \\vec{C} \\cdot (\\vec{B} \\cross \\vec{A}) have the opposite sign. In component form, \\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) = \\begin{vmatrix} A_x & A_y & A_z \\\\ B_x & B_y & B_z \\\\ C_x & C_y & C_z \\end{vmatrix} \\tagl{1.16} Note that the dot and cross can be interchanged: \\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) = (\\vec{A} \\cross \\vec{B}) \\cdot \\vec{C} (this follows immediately from Eq. 1.15); however, the placement of the parentheses is critical: (\\vec{A} \\cdot \\vec{B}) \\cdot \\vec{C} is a meaningless expression - you can't make a cross product from a scalar and a vector. (ii) Vector triple product: \\vec{A} \\cross (\\vec{B} \\cross \\vec{C}) . The vector triple product can be simplified by the so-called BAC-CAB rule: \\vec{A} \\cross (\\vec{B} \\cross \\vec{C}) = \\vec{B}(\\vec{A} \\cdot \\vec{C}) - \\vec{C} (\\vec{A} \\cdot \\vec{B}) \\tagl{1.17} Notice that (\\vec{A} \\cross \\vec{B}) \\cross \\vec{C} = - \\vec{C} \\cross (\\vec{A} \\cross \\vec{B}) = -\\vec{A}(\\vec{B} \\cdot \\vec{C}) + \\vec{B}(\\vec{A} \\cdot \\vec{C}) is an entirely different vector (cross-products are not associative). All higher vector products can be similarly reduced, often by repeated application of \\eqref{1.17} , so it is never necessary for an expression to contain more than one cross product in any term. For instance, (\\vec{A} \\cross \\vec{B}) \\cdot (\\vec{C} \\cross \\vec{D}) = (\\vec{A} \\cdot \\vec{C})(\\vec{B} \\cdot \\vec{D}) - (\\vec{A} \\cdot \\vec{D}) (\\vec{B} \\cdot \\vec{C}) \\vec{A} \\cross [\\vec{B} \\cross (\\vec{C} \\cross \\vec{D})] = \\vec{B}[ \\vec{A} \\cdot (\\vec{C} \\cross \\vec{D})] - (\\vec{A} \\cdot \\vec{B})(\\vec{C} \\cross \\vec{D}) \\tagl{1.18}","title":"1.1.3: Triple Products"},{"location":"ch1-1/#114-position-displacement-and-separation-vectors","text":"The location of a point in three dimensions can be described by listing its Cartesian coordinates (x, y, z). The vector to that point from the origin ( \\mathscr{O} ) is called the position vector (Fig 1.13): \\vec{r} \\equiv x \\vu{x} + y \\vu{y} + z \\vu{z} \\tagl{1.19} I will reserve the letter \\vec{r} for this purpose. Its magnitude, r = \\sqrt{x^2 + y^2 + z^2} \\tagl{1.20} is the distance from the origin, and \\vu{r} = \\frac{\\vec{r}}{r} = \\frac{x \\vu{x} + y \\vu{y} + z \\vu{z}}{ \\sqrt{x^2 + y^2 + z^2}} \\tagl{1.21} is a unit vector pointing radially outward. The infinitesimal displacement vector from (x, y, z) to x + \\dd{x}, y + \\dd{y}, z + \\dd{z} is \\dd{\\vec{l}} = \\dd{x} \\vu{x} + \\dd{y} \\vu{y} + \\dd{z} \\vu{z} \\tagl{1.22} (We could call this \\dd{\\vec{r}} , since that's what it is, but it is useful to have a special notation for infinitesimal displacements.) In electrodynamics, one frequently encounters problems involving two points - typically a source point , \\vec{r'} , where an electric charge is located, and a field point \\vec{r} at which you are calculating the electric or magnetic field (Fig 1.14). It pays to adopt right from the start some short-hand notation for the separation vector from the source point to the field point. I shall use for this purpose the letter \\gr : \\vec{\\gr} \\equiv \\vec{r} - \\vec{r'} \\tagl{1.23} Its magnitude is |\\gr| = | \\vu{r} - \\vu{r'} | \\tagl{1.24} and a unit vector in the direction from \\vec{r'} to \\vec{r} is \\vu{\\gr} = \\frac{\\gr}{|\\gr|} = \\frac{\\vec{r} - \\vec{r'}}{|\\vec{r} - \\vec{r'}|} \\tagl{1.25} In Cartesian coordinates, \\gr = (x - x') \\vu{x} + (y-y') \\vu{y} + (z-z') \\vu{z} \\tagl{1.26} |\\gr| = \\sqrt{(x - x')^2 + (y-y')^2 + (z-z')^2 } \\tagl{1.27} \\vu{\\gr} = \\frac{(x - x') \\vu{x} + (y-y') \\vu{y} + (z-z') \\vu{z}}{\\sqrt{(x - x')^2 + (y-y')^2 + (z-z')^2 }} (from which you can appreciate the economy of the \\gr notation).","title":"1.1.4: Position, Displacement, and Separation Vectors"},{"location":"ch1-1/#115-how-vectors-transform","text":"The definition of a vector as \"a quantity with a magnitude and direction\" is not altogether satisfactory: What precisely does \"direction\" mean? This may seem a pedantic question, but we shall soon encounter a species of derivative that looks rather like a vector, and we'll want to know for sure whether it is one. You might be inclined to say that a vector is anything that has three components that combine properly under addition. Well, how about this: We have a barrel of fruit that contains N_x pears, N_y apples, and N_z bananas. Is \\vec{N} = N_x \\vu{x} + N_y \\vu{y} + N_z \\vu{z} a vector? It has three components, and when you add another barrel with M_x pears, M_y apples, and M_z bananas the result is N_x + M_x pears, N_y + M_y apples, N_z + M_z bananas. So it does add like a vector. Yet it's obviously not a vector, in the physicist's sense of the word, because it doesn't really have a direction. What exactly is wrong with it? The answer is that \\vec{N} does not transform properly when you change coordinates . The coordinate frame we use to describe positions in space is of course entirely arbitrary, but there is a specific geometrical transformation law for converting vector components from one frame to another. Suppose, for instance, the \\overline{x}, \\overline{y}, \\overline{z} system is rotated by angle \\phi , relative to x, y, z , about the common x = \\overline{x} axes. From Fig. 1.15, A_y = A \\cos \\theta, \\qquad A_z = A \\sin \\theta while \\begin{align*} \\overline{A_y} & = A \\cos \\overline{\\theta} = A \\cos (\\theta - \\phi) = A (\\cos \\theta \\cos \\phi + \\sin \\theta \\sin \\phi) \\\\ & = \\cos \\phi A_y + \\sin \\phi A_z \\\\ \\overline{A_z} & = A \\sin \\overline{\\theta} = A \\sin (\\theta - \\phi) = A (\\sin \\theta \\cos \\phi - \\cos \\theta \\sin \\phi) \\\\ & = - \\sin \\phi A_y + \\cos \\phi A_z \\end{align*} We might express this conclusion in matrix notation: \\begin{pmatrix} \\overline{A_y} \\\\ \\overline{A_z} \\end{pmatrix} = \\begin{pmatrix} \\cos \\phi & \\sin \\phi \\\\ - \\sin \\phi & \\cos \\phi \\end{pmatrix} \\begin{pmatrix} A_y \\\\ A_z \\end{pmatrix} \\tagl{1.29} More generally, for rotation about an arbitrary axis in three dimensions, the transformation law takes the form \\begin{pmatrix} \\overline{A_x} \\\\ \\overline{A_y} \\\\ \\overline{A_z} \\end{pmatrix} = \\begin{pmatrix} R_{xx} & R_{xy} & R_{xz} \\\\ R_{yx} & R_{yy} & R_{yz} \\\\ R_{zx} & R_{zy} & R_{zz} \\end{pmatrix} \\begin{pmatrix} A_x \\\\ A_y \\\\ A_z \\end{pmatrix} \\tagl{1.30} or, more compactly, \\overline{A_i} = \\sum_{j=1}^3 R_{ij} A_j \\tagl{1.31} where index 1 stands for x, 2 for y, and 3 for z. The elements of the matrix R can be ascertained, for a given rotation, by the same sort of trigonometric arguments as we used for a rotation about the x axis. Now: Do the components of \\vec{N} transform this way? Of course not - it doesn't matter what coordinates you use to represent positions in space; there are still just as many apples in the barrel. You can't convert a pear into a banana by choosing a different set of axes, but you can turn in A_x into \\overline{A_y} . Formally, then, a vector is any set of three components that transforms in the same manner as a displacement when you change coordinates . As always, displacement is the model for the behavior of vectors. By the way, a (second-rank) tensor is a quantity with nine components, T_{xx}, T_{xy}, T_{xz}, T_{yx}, \\ldots T_{zz} which transform with two factors of R : \\begin{align*} \\overline{T}_{xx} & = R_{xx}(R_{xx} T_{xx} + R_{xy} T_{xy} + R_{xz} T_{xz}) \\\\ & + R_{xy}(R_{xx} T_{yx} + R_{xy} T_{yy} + R_{xz} T_{yz}) \\\\ & + R_{xz}(R_{xx} T_{zx} + R_{xy} T_{zy} + R_{xz} T_{zz}), \\ldots \\end{align*} or, more compactly, \\overline{T}_{ij} = \\sum_{k=1}^3 \\sum_{l=1} ^3 R_{ik} R_{jl} T_{kl} \\tagl{1.32} In general, an n-th rank tensor has n indices and 3^n components, and transforms with n factors of R . In this hierarchy, a vector is a tensor of rank 1, and a scalar is a tensor of rank zero.","title":"1.1.5: How Vectors Transform"},{"location":"ch1-2/","text":"1.2: Differential Calculus 1.2.1: \"Ordinary\" Derivatives Suppose we have a function of one variable, f(x) . Question : what does the derivative \\dv{f}{x} do for us? Answer : It tells us how rapidly the function f(x) varies when we change the argument x by a tiny amount, \\dd{x} \\dd{f} = \\left( \\dv{f}{x} \\right) \\dd{x} \\tagl{1.33} In words: If we increment x by an infinitesimal amount \\dd{x} , then f changes by an amount \\dd{f} ; the derivative is the proportionality factor. Foe example, in Fig. 1.17(a), the function varies slowly with x , and the derivative is correspondingly small. In Fig 1.17(b), f increases rapidly with x , and the derivative is large as you move away from x = 0 . Geometrical interpretation : The derivative \\dv{f}{x} is the slope of the graph of f versus x . 1.2.2: Gradient Suppose, now, that we have a function of three variables-say, the temperature T (x, y, z) in this room. (Start out in one comer, and set up a system of axes; then for each point (x, y, z) in the room, T gives the temperature at that spot.) We want to generalize the notion of \"derivative\" to functions like T, which depend not on one but on three variables. A derivative is supposed to tell us how fast the function varies, if we move a little distance. But this time the situation is more complicated, because it depends on what direction we move: If we go straight up, then the temperature will prob- ably increase fairly rapidly, but if we move horizontally, it may not change much at all. In fact, the question \"How fast does T vary?\" has an infinite number of answers, one for each direction we might choose to explore. Fortunately, the problem is not as bad as it looks. A theorem on partial derivatives states that \\dd{T} = \\left( \\pdv{T}{x} \\right)\\dd{x} + \\left( \\pdv{T}{y} \\right) \\dd{y} + \\left( \\pdv{T}{z} \\right) \\dd{z} \\tagl{1.34} This tells us how T changes when we alter all three variables by the infinitesimal amounts dx, dy, dz. Notice that we do not require an infinite number of derivatives - three will suffice: the partial derivatives along each of the three coordinate directions. \\eqref{1.34} is reminiscent of a dot product: \\dd{T} = \\left( \\pdv{T}{x} \\vu{x} + \\pdv{T}{y} \\vu{y} + \\pdv{T}{z} \\vu{z} \\right)\\cdot ( \\dd{x} \\vu{x} + \\dd{y} \\vu{y} + \\dd{z} \\vu{z} \\\\ = (\\grad{T}) \\cdot (\\dd{\\vec{l}}) \\tagl{1.35} where \\grad{T} \\equiv \\pdv{T}{x} \\vu{x} + \\pdv{T}{y} \\vu{y} + \\pdv{T}{z} \\vu{z} \\tagl{1.36} is the gradient of T. Note that \\grad{T} is a vector quantity, with three components; it is the generalized derivative we have been looking for. \\eqref{1.35} is the three-dimensional version of \\eqref{1.33} . Geometrical interpretation of the Gradient : Like any vector, the gradient has magnitude and direction . To determine its geometrical meaning, let's rewrite the dot product using Eq. 1.1 \\dd{T} = \\grad{T} \\cdot \\dd{\\vec{l}} = |\\grad{T}| |\\dd{\\vec{l}}| \\cos \\theta \\tagl{1.37} where \\theta is the angle between \\grad{T} and \\dd{\\vec{l}} . Now if we fix the magnitude |\\dd{\\vec{l}}| and search around in various directions, the maximum change in T evidently occurs when \\theta = 0 (for then \\cos \\theta = 1 ). That is, for a fixed distance, dT is greatest when I move in the same direction as \\grad{T} . Thus: The gradient \\grad{T} points in the direction of maximum increase of the function T. Moreover: The magnitude | \\grad{T} | gives the slope (rate of increase) along this maximal direction Imagine you are standing on a hillside. Look all around you, and find the direction of steepest ascent. That is the direction of the gradient. Now measure the slope in that direction (rise over run). That is the magnitude of the gradient. (Here the function we're talking about is the height of the hill, and the coordinates it depends on are positions-latitude and longitude, say. This function depends on only two variables, not three, but the geometrical meaning of the gradient is easier to grasp in two dimensions.) Notice from Eq. 1.37 that the direction of maximum descent is opposite to the direction of maximum ascent, while at right angles (\\theta = 90^{\\circ}) the slope is zero (the gradient is perpendicular to the contour lines). You can conceive of surfaces that do not have these properties, but they always have \"kinks\" in them, and correspond to non-differentiable functions. What would it mean for the gradient to vanish? If \\grad{T} = 0 at (x, y, z), then \\dd{T} = 0 for small displacements about the point (x, y, z). This is, then, a stationary point of the function T(x, y, z). It could be a maximum (a summit), a minimum (a valley), a saddle point (a pass), or a \"shoulder.\" This is analogous to the situation for functions of one variable, where a vanishing derivative signals a maximum, a minimum, or an inflection. In particular, if you want to locate the extrema of a function of three variables, set its gradient equal to zero. Example 1.3 Find the gradient of r = \\sqrt{x^2 + y^2 + z^2} (the magnitude of the position vector) Solution \\begin{align*} \\grad{r} & = \\pdv{r}{x} \\vu{x} + \\pdv{r}{y} \\vu{y} + \\pdv{r}{z} \\vu{z} \\\\ & = \\frac{1}{2} \\frac{2x}{\\sqrt{x^2 + y^2 + z^2}}\\vu{x} + \\frac{1}{2} \\frac{2y}{\\sqrt{x^2 + y^2 + z^2}}\\vu{y} + \\frac{1}{2} \\frac{2z}{\\sqrt{x^2 + y^2 + z^2}}\\vu{z} \\\\ & = \\frac{x \\vu{x} + y \\vu{y} + z \\vu{z}}{\\sqrt{x^2 + y^2 + z^2}} = \\frac{\\vec{r}}{r} = \\vu{r} \\end{align*} Does this makes sense? Well, it says that the distance from the origin increases most rapidly in the radial direction, and that its rate of increase in that direction is 1... just what you'd expect. 1.2.3: The Del Operator The gradient has the formal appearance of a vector, \\nabla , \"multiplying\" a scalar T: \\grad{T} = \\left( \\vu{x} \\pdv{}{x} + \\vu{y} \\pdv{}{y} + \\vu{z} \\pdv{}{z} \\right) T \\tagl{1.38} (For once, I write the unit vectors to the left, just so no one will think that this means \\pdv{\\vu{x}}{x} and so on, which would be zero since the coordinate directions are constant.) The term in parentheses is called del : \\grad{} = \\vu{x} \\pdv{}{x} + \\vu{y} \\pdv{}{y} + \\vu{z} \\pdv{}{z} \\tagl{1.39} Of course, del is not a vector, in the usual sense. Indeed, it doesn't mean much until we provide it with a function to act upon. Furthermore, it does not \"multiply\" T; rather, it is an instruction to differentiate what follows. To be precise, then, we say that \\grad{} is a vector operator that acts upon T, not a vector that multiplies T. With this qualification, though, \\grad{} mimics the behavior of an ordinary vector in virtually every way; almost anything that can be done with other vectors can also be done with \\grad{} , if we merely translate \"multiply\" by \"act upon.\" So by all means take the vector appearance of \\grad{} seriously: it is a marvelous piece of notational simplification, as you will appreciate if you ever consult Maxwell's original work on electromagnetism, written without the benefit of \\grad{} . Now, an ordinary vector \\vec{A} can multiply in three ways: By a scalar a : \\vec{A}a By a vector \\vec{B} , via the dot product: \\vec{A} \\cdot \\vec{B} By a vector \\vec{B} , via the cross product: \\vec{A} \\cross \\vec{B} Correspondingly, there are three ways the operator \\grad{} can act: On a scalar function T: \\grad{T} (the gradient) On a vector function \\vec{v} , via the dot product: \\div{\\vec{v}} (the divergence ) On a vector function \\vec{v} , via the cross product: \\curl{\\vec{v}} (the curl ) We have already discussed the gradient. In the following sections we examine the other two vector derivatives: divergence and curl. 1.2.4: The Divergence From the definition of \\grad{} we construct the divergence: \\begin{align*} \\div{\\vec{v}} & = \\left( \\vu{x} \\pdv{}{x} + \\vu{y} \\pdv{}{y} + \\vu{z} \\pdv{}{z} \\right) \\cdot (v_x \\vu{x} + v_y \\vu{y} + v_z \\vu{z}) \\\\ & = \\pdv{v_x}{x} + \\pdv{v_y}{y} + \\pdv{v_z}{z} \\tagl{1.40} \\end{align*} Observe that the divergence of a vector function is itself a scalar. Geometrical interpretation : The name divergence is well chosen, for \\div{\\vec{v}} is a measure of how much the vector \\vec{v} spreads out (diverges) from the point in question. For example, the vector function in Fig. 1.18a has a large (positive) divergence (if the arrows pointed in, it would be a negative divergence), the function in Fig. 1.18b has zero divergence, and the function in Fig. 1.18c again has a positive divergence. (Please understand that \\vec{v} here is a function - there's a different vector associated with every point in space. In the diagrams, of course, I can only draw the arrows at a few representative locations.) Imagine standing at the edge of a pond. Sprinkle some sawdust or pine needles on the surface. If the material spreads out, then you dropped it at a point of positive divergence; if it collects together, you dropped it at a point of negative divergence. (The vector function \\vec{v} in this model is the velocity of the water at the surface - this is a two-dimensional example, but it helps give one a \"feel\" for what the divergence means. A point of positive divergence is a source, or \"faucet\"; a point of negative divergence is a sink, or \"drain.\") Example 1.4 Suppose the functions in Fig 1.18 are \\vec{v_a} = \\vec{r} = x \\vu{x} + y \\vu{y} + z \\vu{z} , \\vec{v_b} = \\vu{z} , and \\vec{v_c} = z\\vu{z} . Calculate their divergences. Solution \\div{\\vec{v_a}} = \\pdv{}{x} (x) + \\pdv{}{y} (x) + \\pdv{}{z} (z) = 1 + 1 + 1 = 3 As anticipated, this function has a positive divergence. \\div{\\vec{v_b}} = \\pdv{}{x} (0) + \\pdv{}{0} (x) + \\pdv{}{z} (1) = 0 + 0 + 0 = 0 as expected. \\div{\\vec{v_b}} = \\pdv{}{x} (0) + \\pdv{}{0} (x) + \\pdv{}{z} (z) = 0 + 0 + 1 = 1 1.2.5: The Curl From the definition of \\grad{} we construct the curl: \\begin{align*} \\curl{\\vec{v}} & = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ \\pdv{}{x} & \\pdv{}{y} & \\pdv{}{z} \\\\ v_x & v_y & v_z \\end{vmatrix} \\\\ & + \\vu{x} \\left( \\pdv{v_z}{y} - \\pdv{v_y}{z} \\right) + \\vu{y} \\left( \\pdv{v_x}{z} - \\pdv{v_z}{x} \\right) + \\vu{z} \\left( \\pdv{v_y}{x} - \\pdv{v_x}{y} \\right) \\tagl{1.41} \\end{align*} Notice that the curl of a vector function is, like any cross product, a vector. Geometrical Interpretation : The name curl is also well chosen, for \\curl{\\vec{v}} is a measure of how much the vector \\vec{v} swirls around the point in question. Thus the three functions in Fig. 1.18 all have zero curl (as you can easily check for yourself), whereas the functions in Fig. 1.19 have a substantial curl, pointing in the z direction, as the natural right-hand rule would suggest. Imagine (again) you are standing at the edge of a pond. Float a small paddlewheel (a cork with toothpicks pointing out radially would do); if it starts to rotate, then you placed it at a point of nonzero curl. A whirlpool would be a region of large curl. Example 1.5 Suppose the function sketched in Fig 1.19a is \\vec{v_a} = -y \\vu{x} + x \\vu{y} , and that in Fig 1.19b is \\vec{v_b} = x \\vu{y} . Calculate their curls. Solution \\curl{\\vec{v_a}} = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ \\pdv{}{x} & \\pdv{}{y} & \\pdv{}{z} \\\\ -y & x & 0 \\end{vmatrix} = 2 \\vu{z} and \\curl{\\vec{v_b}} = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ \\pdv{}{x} & \\pdv{}{y} & \\pdv{}{z} \\\\ 0 & x & 0 \\end{vmatrix} = \\vu{z} As expected, these curls point in the +z direction. (Incidentally, they both have zero divergence, as you might guess from the pictures: nothing is \"spreading out\"... it just \"swirls around.\") 1.2.6: Product Rules The calculation of ordinary derivatives is facilitated by a number of rules, such as the sum rule \\dv{}{x} (f + g) = \\dv{f}{x} + \\dv{g}{x} the rule for multiplying a constant: \\dv{}{x} (kf) = k \\dv{f}{x} the product rule: \\dv{}{x}(fg) = f \\dv{g}{x} + g \\dv{f}{x} and the quotient rule \\dv{}{x} \\left( \\frac{f}{g} \\right) = \\frac{g \\dv{f}{x} - f \\dv{g}{x}}{g^2} Similar relations hold for the vector derivatives. Thus, \\grad{(f + g)} = \\grad{f} + \\grad{g}, \\qquad \\div{(\\vec{A} + \\vec{B})} = (\\div{\\vec{A}}) + (\\div{\\vec{B}}) \\curl{(\\vec{A} + \\vec{B})} = (\\curl{\\vec{A}}) + (\\curl{\\vec{B}}) and \\grad{(kf)} = k \\grad f, \\quad \\div{(k\\vec{A})} = k (\\div{\\vec{A}}), \\quad \\curl{(k\\vec{A})} = k(\\curl{\\vec{A}}) as you can check for yourself. The product rules are not quite so simple. There are two ways to construct a scalar as the product of two functions: fg \\quad \\text{(product of two scalar functions),} \\\\ \\vec{A} \\cdot \\vec{B} \\quad \\text{(dot product of two vector functions),} and two ways to make a vector: f \\vec{A} \\quad \\text{(scalar times vector),} \\\\ \\vec{A} \\cross \\vec{B} \\quad \\text{(cross product of two vectors).} Accordingly, there are six product rules, two for gradients: \\grad{(fg)} = f \\grad{g} + g \\grad{f} \\tag{i} \\grad( \\vec{A} \\cdot \\vec{B}) = \\vec{A} \\cross (\\curl{\\vec{B}}) + \\vec{B} \\cross (\\curl{\\vec{A}}) + (\\vec{A} \\cdot \\grad{})\\vec{B} + (\\vec{B} \\cdot \\grad{}) \\vec{A} \\tag{ii} two for divergences: \\div{(f\\vec{A})} = f(\\div{\\vec{A}}) + \\vec{A} \\cdot (\\grad{f}) \\tag{iii} \\div{(\\vec{A} \\cross \\vec{B})} = \\vec{B} \\cdot (\\curl{\\vec{A}}) - \\vec{A} \\cdot (\\curl{\\vec{B}}) \\tag{iv} and two for curls: \\curl{(f\\vec{A})} = f(\\curl{\\vec{A}}) - \\vec{A} \\cross (\\grad{f}) \\tag{v} \\curl{(\\vec{A} \\cross \\vec{B})} = (\\vec{B} \\cdot \\grad{})\\vec{A} - (\\vec{A} \\cdot \\grad{}) \\vec{B} + \\vec{A}(\\div{\\vec{B}}) - \\vec{B}(\\div{\\vec{A}}) \\tag{vi} If there's anything in this chapter that's worth memorizing, it is this set of identities. The proofs come straight from the product rule for ordinary derivatives. For instance, \\begin{align*} \\div{(f\\vec{A})} & = \\pdv{}{x} (f A_x) + \\pdv{}{y} (fA_y) + \\pdv{}{z}(f A_z) \\\\ & = \\left( \\pdv{f}{x} A_x + f \\pdv{A_x}{x} \\right) + \\left( \\pdv{f}{y} A_y + f \\pdv{A_y}{y} \\right) + \\left( \\pdv{f}{z}A_z + f \\pdv{A_z}{z} \\right) \\\\ & = (\\grad{f}) \\cdot \\vec{A} + f(\\div{\\vec{A}}) \\end{align*} It is also possible to formulate three quotient rules: \\grad \\left( \\frac{f}{g} \\right) = \\frac{g \\grad{f} - f \\grad{g}}{g^2} \\div{\\left( \\frac{\\vec{A}}{g} \\right)} = \\frac{g(\\div{\\vec{A}}) - \\vec{A} \\cdot (\\grad{g})}{g^2} \\curl \\left( \\frac{\\vec{A}}{g} \\right) = \\frac{g(\\curl{\\vec{A}}) + \\vec{A} \\cross (\\grad{g})}{g^2} However, since these can be obtained quickly from the corresponding product rules, there is no point in listing them separately. 1.2.7: Second Derivatives The gradient, the divergence, and the curl are the only first derivatives we can make with \\grad ; by applying \\grad twice, we can construct five species of second derivatives. The gradient \\grad{T} is a vector, so we can take the divergence and curl of it: Divergence of gradient: \\div (\\grad{T}) Curl of gradient: \\curl (\\grad{T}) The divergence \\div{\\vec{v}} is a scalar - all we can do is take its gradient: Gradient of divergence: \\grad (\\div{\\vec{v}}) The curl \\curl \\vec{v} is a vector, so we can take its divergence and curl: Divergence of curl: \\div (\\curl \\vec{v}) Curl of curl: \\curl (\\curl \\vec{v}) This exhausts the possibilities, and in fact not all of them give anything new. Let's consider them one at a time: \\begin{align*} \\div (\\grad{T}) & = \\left( \\vu{x} \\pdv{}{x} + \\vu{y} \\pdv{}{y} + \\vu{z} \\pdv{}{z} \\right) \\cdot \\left( \\pdv{T}{x} \\vu{x} + \\pdv{T}{y} \\vu{y} + \\pdv{T}{z} \\vu{z} \\right) \\\\ & = \\frac{\\partial ^2 T}{\\partial x^2} + \\frac{\\partial ^2 T}{\\partial y^2} + \\frac{\\partial ^2 T}{\\partial z^2} \\end{align*} \\tagl{1.42} This object, which we write as \\laplacian T for short, is called the Laplacian of T; we shall be studying it in great detail later on. Notice that the Laplacian of a scalar T is a scalar. Occasionally we shall speak of the laplacian of a vector, \\laplacian \\vec{v} . By this we mean a vector quantity whose x-component is the Laplacian of v_x , and so in: \\laplacian \\vec{v} \\equiv (\\laplacian v_x) \\vu{x} + (\\laplacian v_y) \\vu{y} + (\\laplacian v_z) \\vu{z} \\tagl{1.43} This is nothing more than a convenient extension of the meaning of \\laplacian . The curl of a gradient is always zero : \\curl (\\grad{T}) = 0 \\tagl{1.44} This is an important fact, which we shall use repeatedly; you can easily prove it from the definition of \\grad , hinging on the equality of cross-derivatives: \\pdv{}{x} \\left( \\pdv{T}{y} \\right) = \\pdv{}{y} \\left( \\pdv{T}{x} \\right) \\tagl{1.45} \\grad(\\div{\\vec{v}}) seldom occurs in physical applications, and it has not been given any special name of its own - it's just the gradient of the divergence . Notice that \\grad (\\div \\vec{v}) is not the same as the Laplacian of a vector: \\laplacian \\vec{v} = (\\div \\grad) \\vec{v} \\neq \\grad( \\div \\vec{v}) . The divergence of a curl, like the curl of a gradient, is always zero: \\div (\\curl \\vec{v}) = 0 \\tagl{1.46} You can prove this for yourself. As you can check from the definition of \\grad : \\curl (\\curl \\vec{v}) = \\grad(\\div \\vec{v}) - \\laplacian \\vec{v} \\tagl{1.47} So curl-of-curl gives nothing new; the first term is just gradient of divergence, and the second is the Laplacian (of a vector). (In fact, this is often used to define the Laplacian of a vector, in preference to \\eqref{1.43} which makes explicit reference to Cartesian coordinates.) Really, then, there are just two kinds of second derivatives: the Laplacian (which is of fundamental importance) and the gradient-of-divergence (which we seldom encounter). We could go through a similar ritual to work out third derivatives, but fortunately second derivatives suffice for practically all physical applications. A final word on vector differential calculus: It all flows from the operator \\grad , and from taking seriously its vectorial character. Even if you remembered only the definition of \\grad , you could easily reconstruct all the rest.","title":"1.2 - Differential Calculus"},{"location":"ch1-2/#12-differential-calculus","text":"","title":"1.2: Differential Calculus"},{"location":"ch1-2/#121-ordinary-derivatives","text":"Suppose we have a function of one variable, f(x) . Question : what does the derivative \\dv{f}{x} do for us? Answer : It tells us how rapidly the function f(x) varies when we change the argument x by a tiny amount, \\dd{x} \\dd{f} = \\left( \\dv{f}{x} \\right) \\dd{x} \\tagl{1.33} In words: If we increment x by an infinitesimal amount \\dd{x} , then f changes by an amount \\dd{f} ; the derivative is the proportionality factor. Foe example, in Fig. 1.17(a), the function varies slowly with x , and the derivative is correspondingly small. In Fig 1.17(b), f increases rapidly with x , and the derivative is large as you move away from x = 0 . Geometrical interpretation : The derivative \\dv{f}{x} is the slope of the graph of f versus x .","title":"1.2.1: \"Ordinary\" Derivatives"},{"location":"ch1-2/#122-gradient","text":"Suppose, now, that we have a function of three variables-say, the temperature T (x, y, z) in this room. (Start out in one comer, and set up a system of axes; then for each point (x, y, z) in the room, T gives the temperature at that spot.) We want to generalize the notion of \"derivative\" to functions like T, which depend not on one but on three variables. A derivative is supposed to tell us how fast the function varies, if we move a little distance. But this time the situation is more complicated, because it depends on what direction we move: If we go straight up, then the temperature will prob- ably increase fairly rapidly, but if we move horizontally, it may not change much at all. In fact, the question \"How fast does T vary?\" has an infinite number of answers, one for each direction we might choose to explore. Fortunately, the problem is not as bad as it looks. A theorem on partial derivatives states that \\dd{T} = \\left( \\pdv{T}{x} \\right)\\dd{x} + \\left( \\pdv{T}{y} \\right) \\dd{y} + \\left( \\pdv{T}{z} \\right) \\dd{z} \\tagl{1.34} This tells us how T changes when we alter all three variables by the infinitesimal amounts dx, dy, dz. Notice that we do not require an infinite number of derivatives - three will suffice: the partial derivatives along each of the three coordinate directions. \\eqref{1.34} is reminiscent of a dot product: \\dd{T} = \\left( \\pdv{T}{x} \\vu{x} + \\pdv{T}{y} \\vu{y} + \\pdv{T}{z} \\vu{z} \\right)\\cdot ( \\dd{x} \\vu{x} + \\dd{y} \\vu{y} + \\dd{z} \\vu{z} \\\\ = (\\grad{T}) \\cdot (\\dd{\\vec{l}}) \\tagl{1.35} where \\grad{T} \\equiv \\pdv{T}{x} \\vu{x} + \\pdv{T}{y} \\vu{y} + \\pdv{T}{z} \\vu{z} \\tagl{1.36} is the gradient of T. Note that \\grad{T} is a vector quantity, with three components; it is the generalized derivative we have been looking for. \\eqref{1.35} is the three-dimensional version of \\eqref{1.33} . Geometrical interpretation of the Gradient : Like any vector, the gradient has magnitude and direction . To determine its geometrical meaning, let's rewrite the dot product using Eq. 1.1 \\dd{T} = \\grad{T} \\cdot \\dd{\\vec{l}} = |\\grad{T}| |\\dd{\\vec{l}}| \\cos \\theta \\tagl{1.37} where \\theta is the angle between \\grad{T} and \\dd{\\vec{l}} . Now if we fix the magnitude |\\dd{\\vec{l}}| and search around in various directions, the maximum change in T evidently occurs when \\theta = 0 (for then \\cos \\theta = 1 ). That is, for a fixed distance, dT is greatest when I move in the same direction as \\grad{T} . Thus: The gradient \\grad{T} points in the direction of maximum increase of the function T. Moreover: The magnitude | \\grad{T} | gives the slope (rate of increase) along this maximal direction Imagine you are standing on a hillside. Look all around you, and find the direction of steepest ascent. That is the direction of the gradient. Now measure the slope in that direction (rise over run). That is the magnitude of the gradient. (Here the function we're talking about is the height of the hill, and the coordinates it depends on are positions-latitude and longitude, say. This function depends on only two variables, not three, but the geometrical meaning of the gradient is easier to grasp in two dimensions.) Notice from Eq. 1.37 that the direction of maximum descent is opposite to the direction of maximum ascent, while at right angles (\\theta = 90^{\\circ}) the slope is zero (the gradient is perpendicular to the contour lines). You can conceive of surfaces that do not have these properties, but they always have \"kinks\" in them, and correspond to non-differentiable functions. What would it mean for the gradient to vanish? If \\grad{T} = 0 at (x, y, z), then \\dd{T} = 0 for small displacements about the point (x, y, z). This is, then, a stationary point of the function T(x, y, z). It could be a maximum (a summit), a minimum (a valley), a saddle point (a pass), or a \"shoulder.\" This is analogous to the situation for functions of one variable, where a vanishing derivative signals a maximum, a minimum, or an inflection. In particular, if you want to locate the extrema of a function of three variables, set its gradient equal to zero.","title":"1.2.2: Gradient"},{"location":"ch1-2/#example-13","text":"Find the gradient of r = \\sqrt{x^2 + y^2 + z^2} (the magnitude of the position vector) Solution \\begin{align*} \\grad{r} & = \\pdv{r}{x} \\vu{x} + \\pdv{r}{y} \\vu{y} + \\pdv{r}{z} \\vu{z} \\\\ & = \\frac{1}{2} \\frac{2x}{\\sqrt{x^2 + y^2 + z^2}}\\vu{x} + \\frac{1}{2} \\frac{2y}{\\sqrt{x^2 + y^2 + z^2}}\\vu{y} + \\frac{1}{2} \\frac{2z}{\\sqrt{x^2 + y^2 + z^2}}\\vu{z} \\\\ & = \\frac{x \\vu{x} + y \\vu{y} + z \\vu{z}}{\\sqrt{x^2 + y^2 + z^2}} = \\frac{\\vec{r}}{r} = \\vu{r} \\end{align*} Does this makes sense? Well, it says that the distance from the origin increases most rapidly in the radial direction, and that its rate of increase in that direction is 1... just what you'd expect.","title":"Example 1.3"},{"location":"ch1-2/#123-the-del-operator","text":"The gradient has the formal appearance of a vector, \\nabla , \"multiplying\" a scalar T: \\grad{T} = \\left( \\vu{x} \\pdv{}{x} + \\vu{y} \\pdv{}{y} + \\vu{z} \\pdv{}{z} \\right) T \\tagl{1.38} (For once, I write the unit vectors to the left, just so no one will think that this means \\pdv{\\vu{x}}{x} and so on, which would be zero since the coordinate directions are constant.) The term in parentheses is called del : \\grad{} = \\vu{x} \\pdv{}{x} + \\vu{y} \\pdv{}{y} + \\vu{z} \\pdv{}{z} \\tagl{1.39} Of course, del is not a vector, in the usual sense. Indeed, it doesn't mean much until we provide it with a function to act upon. Furthermore, it does not \"multiply\" T; rather, it is an instruction to differentiate what follows. To be precise, then, we say that \\grad{} is a vector operator that acts upon T, not a vector that multiplies T. With this qualification, though, \\grad{} mimics the behavior of an ordinary vector in virtually every way; almost anything that can be done with other vectors can also be done with \\grad{} , if we merely translate \"multiply\" by \"act upon.\" So by all means take the vector appearance of \\grad{} seriously: it is a marvelous piece of notational simplification, as you will appreciate if you ever consult Maxwell's original work on electromagnetism, written without the benefit of \\grad{} . Now, an ordinary vector \\vec{A} can multiply in three ways: By a scalar a : \\vec{A}a By a vector \\vec{B} , via the dot product: \\vec{A} \\cdot \\vec{B} By a vector \\vec{B} , via the cross product: \\vec{A} \\cross \\vec{B} Correspondingly, there are three ways the operator \\grad{} can act: On a scalar function T: \\grad{T} (the gradient) On a vector function \\vec{v} , via the dot product: \\div{\\vec{v}} (the divergence ) On a vector function \\vec{v} , via the cross product: \\curl{\\vec{v}} (the curl ) We have already discussed the gradient. In the following sections we examine the other two vector derivatives: divergence and curl.","title":"1.2.3: The Del Operator"},{"location":"ch1-2/#124-the-divergence","text":"From the definition of \\grad{} we construct the divergence: \\begin{align*} \\div{\\vec{v}} & = \\left( \\vu{x} \\pdv{}{x} + \\vu{y} \\pdv{}{y} + \\vu{z} \\pdv{}{z} \\right) \\cdot (v_x \\vu{x} + v_y \\vu{y} + v_z \\vu{z}) \\\\ & = \\pdv{v_x}{x} + \\pdv{v_y}{y} + \\pdv{v_z}{z} \\tagl{1.40} \\end{align*} Observe that the divergence of a vector function is itself a scalar. Geometrical interpretation : The name divergence is well chosen, for \\div{\\vec{v}} is a measure of how much the vector \\vec{v} spreads out (diverges) from the point in question. For example, the vector function in Fig. 1.18a has a large (positive) divergence (if the arrows pointed in, it would be a negative divergence), the function in Fig. 1.18b has zero divergence, and the function in Fig. 1.18c again has a positive divergence. (Please understand that \\vec{v} here is a function - there's a different vector associated with every point in space. In the diagrams, of course, I can only draw the arrows at a few representative locations.) Imagine standing at the edge of a pond. Sprinkle some sawdust or pine needles on the surface. If the material spreads out, then you dropped it at a point of positive divergence; if it collects together, you dropped it at a point of negative divergence. (The vector function \\vec{v} in this model is the velocity of the water at the surface - this is a two-dimensional example, but it helps give one a \"feel\" for what the divergence means. A point of positive divergence is a source, or \"faucet\"; a point of negative divergence is a sink, or \"drain.\")","title":"1.2.4: The Divergence"},{"location":"ch1-2/#example-14","text":"Suppose the functions in Fig 1.18 are \\vec{v_a} = \\vec{r} = x \\vu{x} + y \\vu{y} + z \\vu{z} , \\vec{v_b} = \\vu{z} , and \\vec{v_c} = z\\vu{z} . Calculate their divergences. Solution \\div{\\vec{v_a}} = \\pdv{}{x} (x) + \\pdv{}{y} (x) + \\pdv{}{z} (z) = 1 + 1 + 1 = 3 As anticipated, this function has a positive divergence. \\div{\\vec{v_b}} = \\pdv{}{x} (0) + \\pdv{}{0} (x) + \\pdv{}{z} (1) = 0 + 0 + 0 = 0 as expected. \\div{\\vec{v_b}} = \\pdv{}{x} (0) + \\pdv{}{0} (x) + \\pdv{}{z} (z) = 0 + 0 + 1 = 1","title":"Example 1.4"},{"location":"ch1-2/#125-the-curl","text":"From the definition of \\grad{} we construct the curl: \\begin{align*} \\curl{\\vec{v}} & = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ \\pdv{}{x} & \\pdv{}{y} & \\pdv{}{z} \\\\ v_x & v_y & v_z \\end{vmatrix} \\\\ & + \\vu{x} \\left( \\pdv{v_z}{y} - \\pdv{v_y}{z} \\right) + \\vu{y} \\left( \\pdv{v_x}{z} - \\pdv{v_z}{x} \\right) + \\vu{z} \\left( \\pdv{v_y}{x} - \\pdv{v_x}{y} \\right) \\tagl{1.41} \\end{align*} Notice that the curl of a vector function is, like any cross product, a vector. Geometrical Interpretation : The name curl is also well chosen, for \\curl{\\vec{v}} is a measure of how much the vector \\vec{v} swirls around the point in question. Thus the three functions in Fig. 1.18 all have zero curl (as you can easily check for yourself), whereas the functions in Fig. 1.19 have a substantial curl, pointing in the z direction, as the natural right-hand rule would suggest. Imagine (again) you are standing at the edge of a pond. Float a small paddlewheel (a cork with toothpicks pointing out radially would do); if it starts to rotate, then you placed it at a point of nonzero curl. A whirlpool would be a region of large curl.","title":"1.2.5: The Curl"},{"location":"ch1-2/#example-15","text":"Suppose the function sketched in Fig 1.19a is \\vec{v_a} = -y \\vu{x} + x \\vu{y} , and that in Fig 1.19b is \\vec{v_b} = x \\vu{y} . Calculate their curls. Solution \\curl{\\vec{v_a}} = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ \\pdv{}{x} & \\pdv{}{y} & \\pdv{}{z} \\\\ -y & x & 0 \\end{vmatrix} = 2 \\vu{z} and \\curl{\\vec{v_b}} = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ \\pdv{}{x} & \\pdv{}{y} & \\pdv{}{z} \\\\ 0 & x & 0 \\end{vmatrix} = \\vu{z} As expected, these curls point in the +z direction. (Incidentally, they both have zero divergence, as you might guess from the pictures: nothing is \"spreading out\"... it just \"swirls around.\")","title":"Example 1.5"},{"location":"ch1-2/#126-product-rules","text":"The calculation of ordinary derivatives is facilitated by a number of rules, such as the sum rule \\dv{}{x} (f + g) = \\dv{f}{x} + \\dv{g}{x} the rule for multiplying a constant: \\dv{}{x} (kf) = k \\dv{f}{x} the product rule: \\dv{}{x}(fg) = f \\dv{g}{x} + g \\dv{f}{x} and the quotient rule \\dv{}{x} \\left( \\frac{f}{g} \\right) = \\frac{g \\dv{f}{x} - f \\dv{g}{x}}{g^2} Similar relations hold for the vector derivatives. Thus, \\grad{(f + g)} = \\grad{f} + \\grad{g}, \\qquad \\div{(\\vec{A} + \\vec{B})} = (\\div{\\vec{A}}) + (\\div{\\vec{B}}) \\curl{(\\vec{A} + \\vec{B})} = (\\curl{\\vec{A}}) + (\\curl{\\vec{B}}) and \\grad{(kf)} = k \\grad f, \\quad \\div{(k\\vec{A})} = k (\\div{\\vec{A}}), \\quad \\curl{(k\\vec{A})} = k(\\curl{\\vec{A}}) as you can check for yourself. The product rules are not quite so simple. There are two ways to construct a scalar as the product of two functions: fg \\quad \\text{(product of two scalar functions),} \\\\ \\vec{A} \\cdot \\vec{B} \\quad \\text{(dot product of two vector functions),} and two ways to make a vector: f \\vec{A} \\quad \\text{(scalar times vector),} \\\\ \\vec{A} \\cross \\vec{B} \\quad \\text{(cross product of two vectors).} Accordingly, there are six product rules, two for gradients: \\grad{(fg)} = f \\grad{g} + g \\grad{f} \\tag{i} \\grad( \\vec{A} \\cdot \\vec{B}) = \\vec{A} \\cross (\\curl{\\vec{B}}) + \\vec{B} \\cross (\\curl{\\vec{A}}) + (\\vec{A} \\cdot \\grad{})\\vec{B} + (\\vec{B} \\cdot \\grad{}) \\vec{A} \\tag{ii} two for divergences: \\div{(f\\vec{A})} = f(\\div{\\vec{A}}) + \\vec{A} \\cdot (\\grad{f}) \\tag{iii} \\div{(\\vec{A} \\cross \\vec{B})} = \\vec{B} \\cdot (\\curl{\\vec{A}}) - \\vec{A} \\cdot (\\curl{\\vec{B}}) \\tag{iv} and two for curls: \\curl{(f\\vec{A})} = f(\\curl{\\vec{A}}) - \\vec{A} \\cross (\\grad{f}) \\tag{v} \\curl{(\\vec{A} \\cross \\vec{B})} = (\\vec{B} \\cdot \\grad{})\\vec{A} - (\\vec{A} \\cdot \\grad{}) \\vec{B} + \\vec{A}(\\div{\\vec{B}}) - \\vec{B}(\\div{\\vec{A}}) \\tag{vi} If there's anything in this chapter that's worth memorizing, it is this set of identities. The proofs come straight from the product rule for ordinary derivatives. For instance, \\begin{align*} \\div{(f\\vec{A})} & = \\pdv{}{x} (f A_x) + \\pdv{}{y} (fA_y) + \\pdv{}{z}(f A_z) \\\\ & = \\left( \\pdv{f}{x} A_x + f \\pdv{A_x}{x} \\right) + \\left( \\pdv{f}{y} A_y + f \\pdv{A_y}{y} \\right) + \\left( \\pdv{f}{z}A_z + f \\pdv{A_z}{z} \\right) \\\\ & = (\\grad{f}) \\cdot \\vec{A} + f(\\div{\\vec{A}}) \\end{align*} It is also possible to formulate three quotient rules: \\grad \\left( \\frac{f}{g} \\right) = \\frac{g \\grad{f} - f \\grad{g}}{g^2} \\div{\\left( \\frac{\\vec{A}}{g} \\right)} = \\frac{g(\\div{\\vec{A}}) - \\vec{A} \\cdot (\\grad{g})}{g^2} \\curl \\left( \\frac{\\vec{A}}{g} \\right) = \\frac{g(\\curl{\\vec{A}}) + \\vec{A} \\cross (\\grad{g})}{g^2} However, since these can be obtained quickly from the corresponding product rules, there is no point in listing them separately.","title":"1.2.6: Product Rules"},{"location":"ch1-2/#127-second-derivatives","text":"The gradient, the divergence, and the curl are the only first derivatives we can make with \\grad ; by applying \\grad twice, we can construct five species of second derivatives. The gradient \\grad{T} is a vector, so we can take the divergence and curl of it: Divergence of gradient: \\div (\\grad{T}) Curl of gradient: \\curl (\\grad{T}) The divergence \\div{\\vec{v}} is a scalar - all we can do is take its gradient: Gradient of divergence: \\grad (\\div{\\vec{v}}) The curl \\curl \\vec{v} is a vector, so we can take its divergence and curl: Divergence of curl: \\div (\\curl \\vec{v}) Curl of curl: \\curl (\\curl \\vec{v}) This exhausts the possibilities, and in fact not all of them give anything new. Let's consider them one at a time: \\begin{align*} \\div (\\grad{T}) & = \\left( \\vu{x} \\pdv{}{x} + \\vu{y} \\pdv{}{y} + \\vu{z} \\pdv{}{z} \\right) \\cdot \\left( \\pdv{T}{x} \\vu{x} + \\pdv{T}{y} \\vu{y} + \\pdv{T}{z} \\vu{z} \\right) \\\\ & = \\frac{\\partial ^2 T}{\\partial x^2} + \\frac{\\partial ^2 T}{\\partial y^2} + \\frac{\\partial ^2 T}{\\partial z^2} \\end{align*} \\tagl{1.42} This object, which we write as \\laplacian T for short, is called the Laplacian of T; we shall be studying it in great detail later on. Notice that the Laplacian of a scalar T is a scalar. Occasionally we shall speak of the laplacian of a vector, \\laplacian \\vec{v} . By this we mean a vector quantity whose x-component is the Laplacian of v_x , and so in: \\laplacian \\vec{v} \\equiv (\\laplacian v_x) \\vu{x} + (\\laplacian v_y) \\vu{y} + (\\laplacian v_z) \\vu{z} \\tagl{1.43} This is nothing more than a convenient extension of the meaning of \\laplacian . The curl of a gradient is always zero : \\curl (\\grad{T}) = 0 \\tagl{1.44} This is an important fact, which we shall use repeatedly; you can easily prove it from the definition of \\grad , hinging on the equality of cross-derivatives: \\pdv{}{x} \\left( \\pdv{T}{y} \\right) = \\pdv{}{y} \\left( \\pdv{T}{x} \\right) \\tagl{1.45} \\grad(\\div{\\vec{v}}) seldom occurs in physical applications, and it has not been given any special name of its own - it's just the gradient of the divergence . Notice that \\grad (\\div \\vec{v}) is not the same as the Laplacian of a vector: \\laplacian \\vec{v} = (\\div \\grad) \\vec{v} \\neq \\grad( \\div \\vec{v}) . The divergence of a curl, like the curl of a gradient, is always zero: \\div (\\curl \\vec{v}) = 0 \\tagl{1.46} You can prove this for yourself. As you can check from the definition of \\grad : \\curl (\\curl \\vec{v}) = \\grad(\\div \\vec{v}) - \\laplacian \\vec{v} \\tagl{1.47} So curl-of-curl gives nothing new; the first term is just gradient of divergence, and the second is the Laplacian (of a vector). (In fact, this is often used to define the Laplacian of a vector, in preference to \\eqref{1.43} which makes explicit reference to Cartesian coordinates.) Really, then, there are just two kinds of second derivatives: the Laplacian (which is of fundamental importance) and the gradient-of-divergence (which we seldom encounter). We could go through a similar ritual to work out third derivatives, but fortunately second derivatives suffice for practically all physical applications. A final word on vector differential calculus: It all flows from the operator \\grad , and from taking seriously its vectorial character. Even if you remembered only the definition of \\grad , you could easily reconstruct all the rest.","title":"1.2.7: Second Derivatives"},{"location":"ch1-3/","text":"1.3: Integral Calculus 1.3.1: Line, Surface, and Volume Integrals In electrodynamics, we encounter several different kinds of integrals, among which the most important are line (or path) integrals , surface integrals , and volume integrals . Line Integrals A line integral is an expression of the form \\int_a ^b \\vec{v} \\cdot \\dd{\\vec{l}} \\tagl{1.48} where v is a vector function, \\dd \\vec{l} is the infinitesimal displacement vector and the integral is to be carried out along a prescribed path P from point a to point b . If the path in question forms a closed loop (that is, if \\vec{b} = \\vec{a} , then put a circle on the integral sign: \\oint \\vec{v} \\cdot \\dd \\vec{l} \\tagl{1.49} At each point on the path, we take the dot product of v (evaluated at that point) with the displacement to the next point on the pat. To a physicist, the most familiar example of a line integral is the work done by a force \\vec{F} : W = \\int \\vec{F} \\cdot \\dd \\vec{l} Ordinarily, the value of a line integral depends critically on the path taken from a to b, but there is an important special class of vector functions for which the line integral is independent of path and is determined entirely by the end points. It will be our business in due course to characterize this special class of vectors. (A force that has this property is called conservative. ) Example 1.6 Calculate the line integral of the function \\vec{v} = y^2 \\vu{x} + 2x (y+1) \\vu{y} from the point a = (1, 1, 0) to the point b = (2, 2, 0), along the paths (1) and (2) in Fig 1.21. What is \\oint \\vec{v} \\cdot \\dd \\vec{l} for the loop that goes from a to b along (1) and returns to a along (2)? Solution As always, \\dd \\vec{l} = \\dd x \\vu{x} + \\dd y \\vu{y} + \\dd z \\vu{z} . Path (1) consists of two parts. Along the \"horizontal\" segment, dy = dz = 0 so \\dd \\vec{l} = \\dd x \\vu{x} , y = 1, \\vec{v} \\cdot \\dd{\\vec{l}} = y^2 \\dd x = \\dd x, \\text{ so } \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int_1 ^2 \\dd x = 1 \\tag{i} On the \"vertical\" stretch, dx = dz = 0, so \\dd \\vec{l} = \\dd y \\vu{y}, x = 2, \\vec{v} \\cdot \\dd \\vec{l} = 2x(y+1) \\dd y = 4(y+1) \\dd y, \\text{ so } \\tag{ii} \\int \\vec{v} \\dd \\vec{l} = 4 \\int_1 ^2 (y+1) \\dd y = 10 By path (1), then \\int _{\\vec{a}} ^{\\vec{b}} \\vec{v} \\cdot \\dd \\vec{l} = 1 + 10 = 11 Meanwhile on path (2), x = y, \\dd x = \\dd y, and \\dd z = 0 , so \\dd \\vec{l} = \\dd x \\vu{x} + \\dd x \\vu{y}, \\vec{v} \\cdot \\dd \\vec{l} = x^2 \\dd x + 2x(x+1) \\dd x = (3x^2 + 2x) \\dd x and \\int_{\\vec{a}} ^{\\vec{b}} \\vec{v} \\cdot \\dd \\vec{l} = \\int_1 ^2 (3x^2 + 2x) \\dd x = \\left. (x^3 + x^2)\\right|_{1} ^2 = 10 (The strategy here is to get everything in terms of one variable; I could just as well have eliminated x in favor of y.) For the loop that goes out (1) and back (2), then \\oint \\vec{v} \\cdot \\dd \\vec{l} = 11 - 10 = 1 Surface Integrals A surface integral is an expression of the form \\int_{\\mathscr{S}} \\vec{v} \\cdot \\dd \\vec{a} \\tagl{1.50} where v is again some vector function, and the integral is over a specified surface \\mathscr{S} . Here \\dd \\vec{a} is an infinitesimal patch of area, with direction perpendicular to the surface (Fig 1.22). There are, of course, two directions perpendicular to any surface, so the sign of a surface integral is intrinsically ambiguous. If the surface is closed (forming a \"balloon\"), in which case I again put a circle on the integral sign \\oint \\vec{v} \\cdot \\dd \\vec{a} then tradition dictates that \"outward\" is positive, but for open surfaces it's arbitrary. If v describes the flow of a fluid (mass per unit area per unit time), then \\int \\vec{v} \\cdot \\dd \\vec{a} represents the total mass per unit time passing through the surface - hence the alternative name, \"flux.\" Ordinarily, the value of a surface integral depends on the particular surface chosen, but there is a special class of vector functions for which it is independent of the surface and is determined entirely by the boundary line. An important task will be to characterize this special class of functions. Example 1.7 Calculate the surface integral of \\vec{v} = 2xz \\vu{x} + (x+2) \\vu{y} + y(z^2 -3) \\vu{z} over five sides (excluding the bottom) of the cubical box (side 2) in Fig 1.23. Let 'upward and outward' be the positive direction, as indicated by the arrows. Solution Taking the sides one at a time (i) x = 2, \\dd \\vec{a} = \\dd y \\dd z \\, \\vu{x}, \\vec{v} \\cdot \\dd \\vec{a} = 2xyz \\, \\dd y \\dd z = 4z \\dd y \\dd z , so \\int \\vec{v} \\cdot \\dd \\vec{a} = 4 \\int_0 ^2 \\dd y \\int_0 ^2 z \\dd z = 16 (ii) x = 0, \\dd \\vec{a} = -\\dd y \\dd z \\, \\vu{x}, \\vec{v} \\cdot \\dd \\vec{a} = -2xyz \\, \\dd y \\dd z = 4z \\dd y \\dd z = 0 , so \\int \\vec{v} \\cdot \\dd \\vec{a} = 0 (iii) y = 2, \\dd \\vec{a} = \\dd x \\dd z \\, \\vu{y}, \\vec{v} \\cdot \\dd \\vec{a} = (x+2) \\, \\dd x \\dd z , so \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int _0 ^2 (x + 2) \\dd x \\int _0 ^2 \\dd z = 12 (iv) y = 0, \\dd \\vec{a} = - \\dd x \\dd z \\, \\vu{y}, \\vec{v} \\cdot \\dd \\vec{a} = -(x+2) \\, \\dd x \\dd z , so \\int \\vec{v} \\cdot \\dd \\vec{a} = -\\int _0 ^2 (x + 2) \\dd x \\int _0 ^2 \\dd z = -12 (v) z = 2, \\dd \\vec{a} = \\dd x \\dd y \\, \\vu{z}, \\vec{v} \\cdot \\dd \\vec{a} = y(z^2 -3) \\, \\dd x \\dd y = y \\, \\dd x \\dd y , so \\int \\vec{v} \\cdot \\dd{a} = \\int_0 ^2 \\dd x \\int_0 ^2 y \\dd y = 4 The total flux is \\int _{surface} \\vec{v} \\cdot \\dd \\vec{a} = 16 + 0 + 12 - 12 + 4 = 20 Volume Integrals A volume integral is an expression of the form \\int_{V} T \\dd \\tau \\tagl{1.51} where T is a scalar function and \\dd \\tau is an infinitesimal volume element. In Cartesian coordinates, \\dd \\tau = \\dd x \\, \\dd y \\, \\dd z \\tagl{1.52} For example, if T is the density of a substance (which might vary from point to point), then the volume integral would give the total mass. Occasionally we shall encounter volume integrals of vector functions: \\int \\vec{v} \\dd \\tau = \\int (v_x \\vu{x} + v_y \\vu{y} + v_z \\vu{z}) \\dd \\tau = \\vu{x} \\int v_x \\dd \\tau + \\vu{y} \\int v_y \\dd \\tau + \\vu{z} \\int v_z \\dd \\tau \\tagl{1.53} Because the unit vectors are constants, they come outside the integral. Example 1.8 Calculate the volume integral of T = xyz^2 over the prism in Fig 1.24. Solution \\begin{align*} \\int T \\dd \\tau & = \\int _0 ^3 z^2 \\left( \\int _0 ^1 y \\left[ \\int_0 ^{1-y} x \\, \\dd x \\right] \\dd y \\right) \\dd z \\\\ & = \\frac{1}{2} \\int_0 ^3 z^2 \\, \\dd z \\int_0 ^1 (1-y)^2 y \\, \\dd \\y = \\frac{1}{2} (9) \\left( \\frac{1}{12} \\right) = \\frac{3}{8} \\end{align*} 1.3.2: The Fundamental Theorem of Calculus Suppose f(x) is a function in one variable. The fundamental theorem of calculus says \\int_a ^b \\left( \\dv{f}{x} \\right) \\dd x = f(b) - f(a) \\tagl{1.54} In case this doesn't look familiar, I'll write it another way: \\int_a ^b F(x) \\dd x = f(b) - f(a) where df / dx = F(x) . The fundamental theorem tells you how to integrate F(x) : you think up a function f(x) whose derivative is equal to F. Geometrical interpretation : According to Eq. 1.33, df = (df / dx) dx is the infinitesimal change in f when you go from (x) to (x + dx). The fundamental theorem (Eq. 1.54) says that if you chop the interval from a to b (Fig. 1.25) into many tiny pieces, dx, and add up the increments df from each little piece, the result is (not surprisingly) equal to the total change in f: f(b) - f(a) . In other words, there are two ways to determine the total change in the function: either subtract the values at the ends or go step-by-step, adding up all the tiny increments as you go. You'll get the same answer either way. Notice the basic format of the fundamental theorem: the integral of a derivative over some region is given by the value of the function at the end points (boundaries). In vector calculus there are three species of derivative (gradient, divergence, and curl), and each has its own \"fundamental theorem,\" with essentially the same format. I don't plan to prove these theorems here; rather, I will explain what they mean, and try to make them plausible. 1.3.3: The Fundamental Theorem for Gradients Suppose we have a scalar function of three variables T(x, y, z). Starting at point a , we move a small distance \\dd \\vec{l}_1 (Fig 1.26). According to Eq. 1.37, the function T will change by an amount \\dd T = (\\grad T) \\cdot \\dd \\vec{l}_1 Now we move a little further, by an additional small displacement \\dd \\vec{l}_2 ; the incremental change in T will be (\\grad T) \\cdot \\dd \\vec{l}_2 . In this manner, proceeding by infinitesimal steps, we make the journey to point b. At each step we compute the gradient of T (at that point) and dot it into the displacement dl... this gives us the change in T. Evidently the total change in Tin going from a to b (along the path selected) is \\int_{\\vec{a}} ^{\\vec{b}} (\\grad T) \\cdot \\dd \\vec{l} = T(\\vec{b}) - T(\\vec{a}) \\tagl{1.55} This is the fundamental theorem for gradients; like the \"ordinary\" fundamental theorem, it says that the integral (here a line integral) of a derivative (here the gradient) is given by the value of the function at the boundaries (a and b). Geometrical Interpretation: Suppose you wanted to determine the height of the Eiffel Tower. You could climb the stairs, using a ruler to measure the rise at each step, and adding them all up (that's the left side of Eq. 1.55), or you could place altimeters at the top and the bottom, and subtract the two readings (that's the right side); you should get the same answer either way (that's the fundamental theorem). Incidentally, as we found in Ex. 1.6, line integrals ordinarily depend on the path taken from a to b. But the right side of Eq. 1.55 makes no reference to the path - only to the end points. Evidently, gradients have the special property that their line integrals are path independent: Corollary 1: \\int_a ^b (\\grad T) \\cdot \\dd \\vec{l} is independent of the path taken from a to b. Corollary 2: \\oint (\\grad T) \\cdot \\dd \\vec{l} = 0 , since the beginning and end points are identical, and hence T(\\vec{b}) - T(\\vec{a}) = 0 . Example 1.9 Let T = xy^2 , and take point a to be the origin (0, 0, 0) and b to be the point (2, 1, 0). Check the fundamental theorem for gradients. Solution We know a priori that the integral should be independent of the path, but we must sill pick a specific path in order to evaluate it. Let's go out along the x axis, then up (Fig 1.27). As always, \\dd \\vec{l} = \\dd x \\vu{x} + \\dd y \\vu{y} + \\dd z \\vu{z}; \\grad T = y^2 \\vu{x} + 2xy \\vu{y} y = 0; \\dd \\vec{l} = \\dd x \\vu{x}; \\grad T \\cdot \\dd \\vec{l} = y^2 \\dd x = 0 \\rightarrow \\int_{i} \\grad T \\cdot \\dd \\vec{l} = 0 x = 2; \\dd \\vec{l} = \\dd y \\vu{y}; \\grad T \\cdot \\dd \\vec{l} = 4y \\dd y \\rightarrow \\int_{ii} \\grad T \\cdot \\dd \\vec{l} = \\left. 2y^2 \\right| _0 ^1 = 2 The total line integral is 2. So is this consistent with what we expect from the fundamental theorem? Well, T(b) - T(a) = 2 - 0 = 2 , so yes! 1.3.4: The Fundamental Theorem for Divergences The fundamental theorem for divergences states that \\int _{\\mathscr{V}} (\\div \\vec{v} ) \\dd \\tau = \\oint_{\\mathscr{S}} \\vec{v} \\cdot \\dd \\vec{a} \\tagl{1.56} In honor, I suppose, of its great importance, this theorem has at least three special names: Gauss's theorem , Green's theorem , or simply the divergence theorem . Like the other \"fundamental theorems,\" it says that the integral of a derivative (in this case the divergence) over a region (in this case a volume, V) is equal to the value of the function at the boundary (in this case the surface S that bounds the volume). Notice that the boundary term is itself an integral (specifically, a surface integral). This is reasonable: the \"boundary\" of a line is just two end points, but the boundary of a volume is a (closed) surface. Geometrical Interpretation: If v represents the flow of an incompressible fluid, then the flux of v (the right side of Eq. 1.56) is the total amount of fluid passing out through the surface, per unit time. Now, the divergence measures the \"spreading out\" of the vectors from a point-a place of high divergence is like a \"faucet,\" pouring out liquid. If we have a bunch of faucets in a region filled with incompressible fluid, an equal amount of liquid will be forced out through the boundaries of the region. In fact, there are two ways we could determine how much is being produced: (a) we could count up all the faucets, recording how much each puts out, or (b) we could go around the boundary, measuring the flow at each point, and add it all up. You get the same answer either way: \\int (\\text{faucets within the volume}) = \\oint (\\text{flow out through the surface}) This, in essence, is what the divergence theorem says. Example 1.10 Check the divergence theorem using the function \\vec{v} = y^2 \\vu{x} + (2xy + z^2) \\vu{y} + (2yz) \\vu{z} using a unit cube at the origin as the surface boundary (Fig 1.29). Solution In this case \\div \\vec{v} = 2(x + y) and \\int_V 2(x+y) \\dd \\tau = 2 \\int_0 ^1 \\dd x \\int _0 ^1 \\dd y \\int _0 ^1 \\dd z (x+y) \\int _0 ^1 \\dd x (x+y) = \\frac{1}{2} + y, \\quad \\int _0 ^1 \\dd y (\\frac{1}{2} + y) \\dd y = 1, \\quad \\int_0 ^1 \\dd z (1) = 1 Thus, \\int_{V} \\div \\vec{v} \\dd \\tau = 2 That takes care of the volume integral part of Gauss' Law, now how about the surface integral? We have to it in six parts, for each face of the cube: \\tag{i} \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd y \\int_0 ^1 \\dd z y^2 = \\frac{1}{3} \\tag{ii} \\int \\vec{v} \\cdot \\dd \\vec{a} = - \\int _0 ^1 \\dd y \\int _0 ^1 \\dd z y^2 = - \\frac{1}{3} \\tag{iii} \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd x \\int_0 ^1 \\dd z (2x + z^2) = \\frac{4}{3} \\tag{iv} \\int \\vec{v} \\cdot \\dd \\vec{a} = - \\int _0 ^1 \\dd x \\int _0 ^1 \\dd z (z^2) = - \\frac{1}{3} \\tag{v} \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd x \\int_0 ^1 \\dd y (2y) = 1 \\tag{vi} \\int \\vec{v} \\cdot \\dd \\vec{a} = - \\int _0 ^1 \\dd x \\int _0 ^1 \\dd y (0) = 0 So the total flux is \\oint _S \\vec{v} \\cdot \\dd \\vec{a} = \\frac{1}{3} - \\frac{1}{3} + \\frac{4}{3} - \\frac{1}{3} + 1 + 0 = 2 as we should expect. 1.3.5: The Fundamental Theorem for Curls The fundamental theorem for curls, which goes by the name Stokes' Theorem , states that \\int _S(\\curl \\vec{V}) \\cdot \\dd \\vec{a} = \\oint _P \\vec{v} \\cdot \\dd \\vec{l} \\tagl{1.57} As always, the integral of a derivative (here, the curl) over a region (here, a patch of surface, S) is equal to the value of the function at the boundary (here, the perimeter of the patch, P). As in the case of the divergence theorem, the boundary term is itself an integral-specifically, a closed line integral. Geometrical Interpretation: Recall that the curl measures the \"twist\" of the vectors v ; a region of high curl is a whirlpool - if you put a tiny paddle wheel there, it will rotate. Now, the integral of the curl over some surface (or, more precisely, the flux of the curl through that surface) represents the \"total amount of swirl,\" and we can determine that just as well by going around the edge and finding how much the flow is following the boundary (Fig. 1.31). Indeed, \\oint \\vec{v} \\cdot \\dd \\vec{l} is sometimes called the circulation of v. You may have noticed an apparent ambiguity in Stokes' theorem: concerning the boundary line integral, which way are we supposed to go around (clockwise or counterclockwise)? If we go the \"wrong\" way, we'll pick up an overall sign error. The answer is that it doesn't matter which way you go as long as you are consistent, for there is a compensating sign ambiguity in the surface integral: Which way does \\dd \\vec{a} point? For a closed surface (as in the divergence theorem), \\dd \\vec{a} points in the direction of the outward normal; but for an open surface, which way is \"out\"? Consistency in Stokes' theorem (as in all such matters) is given by the right-hand rule: if your fingers point in the direction of the line integral, then your thumb fixes the direction of \\dd \\vec{a} (Fig. 1.32). Now, there are plenty of surfaces (infinitely many) that share any given boundary line. Twist a paper clip into a loop, and dip it in soapy water. The soap film constitutes a surface, with the wire loop as its boundary. If you blow on it, the soap film will expand, making a larger surface, with the same boundary. Ordinarily, a flux integral depends critically on what surface you integrate over, but evidently this is not the case with curls. For Stokes' theorem says that \\int (\\curl \\vec{v}) \\cdot \\dd \\vec{a} is equal to the line integral of \\vec{v}\\ around the boundary, and the latter makes no reference to the specific surface you choose. Corollary 1: \\int (\\curl \\vec{v}) \\cdot \\dd \\vec{a} depends only on the boundary line, not on the particular surface used. Corollary 2: \\oint (\\curl \\vec{v}) \\cdot \\dd \\vec{a} = 0 for any closed surface, since the boundary line, like the mouth of a balloon, shrinks down to a point, and hence the right side of \\eqref{1.57} vanishes. Example 1.11 Suppose \\vec{v} = (2xz + 3y^2) \\vu{y} + (4yz^2) \\vu{z} . Check Stokes' theorem for the square surface shown in Fig 1.33. Solution Here \\curl \\vec{v} = (4z^2 - 2x) \\vu{x} + 2z \\vu{z} \\quad \\text{and} \\quad \\dd \\vec{a} = \\dd y \\, \\dd z \\, \\vu{x} (In saying that \\dd \\vec{a} points in the x direction, we are committing ourselves to a counterclockwise integral. We could as well write \\dd \\vec{a} pointing in the other direction ( \\dd \\vec{a} = - \\dd y \\, \\dd z\\, \\vu{x} ) and perform the integral in the clockwise direction.) Since x = 0 for this surface, \\int (\\curl \\vec{v}) \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd y \\int_0 ^1 \\dd z (4z^2) = \\frac{4}{3} Now for the line integral, which we of course break into 4 pieces: \\tag{i} x = 0 \\quad z = 0 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 3y^2 \\dd y, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _0 ^1 3y^2 \\dd y = 1 \\tag{ii} x = 0 \\quad y = 1 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 4z^2 \\dd z, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _0 ^1 4z^2 \\dd z = \\frac{4}{3} \\tag{iii} x = 0 \\quad z = 1 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 3y^2 \\dd y, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _1 ^0 3y^2 \\dd y = -1 \\tag{iv} x = 0 \\quad y = 0 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 0 \\dd z, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _1 ^0 0 \\dd z = 0 So \\oint \\vec{v} \\cdot \\dd \\vec{l} = 1 + \\frac{4}{3} - 1 + 0 = \\frac{4}{3} It all checks out! 1.3.6: Integration by Parts The technique known (awkwardly) as integration by parts exploits the product rule for derivatives: \\dv{}{x} (fg) = f \\left( \\dv{g}{x} \\right) + g \\left( \\dv{f}{x} \\right) Integrating both sides, and invoking the fundamental theorem, \\int_a ^b \\dv{}{x} (fg) \\dd x = \\left. fg \\right| ^b _a = \\int _a ^b f \\left( \\dv{g}{x} \\right) \\dd x + \\int_a ^b g \\left( \\dv{f}{x} \\right) \\dd x or \\int_a ^b f \\left( \\dv{g}{x} \\right) \\dd x = - \\int_a ^b \\left( \\dv{f}{x} \\right) \\dd x + \\left. fg \\right| ^b _a \\tagl{1.58} That's \"integration by parts.\" It applies to the situation in which you are called upon to integrate the product of one function (f) and the derivative of another (g); it says you can transfer the derivative from g to f, at the cost of a minus sign and a boundary term. Example 1.12 Evaluate the integral $$ \\int _0 ^\\infty x e^{-x} \\dd x Solution The exponential can be expressed as a derivative: e^{-x} = \\dv{}{x} \\left( - e^{-x} \\right) in this case, then, f(x) = x , g(x) = - e^{-x} , and df /dx = 1 , so \\int_0 ^\\infty x e^{-x} \\dd x = \\int _0 ^{\\infty} e^{-x} \\dd x - \\left. x e^{-x} \\right| _{0} ^{\\infty} = - \\left. e^{-x} \\right| _0 ^{\\infty} = 1 We can exploit the product rules of vector calculus, together with the appropriate fundamental theorems, in exactly the same way. For example, integrating \\div (f\\vec{A}) = f(\\div \\vec{A}) + \\vec{A} \\cdot (\\grad f) over a volume, and invoking the divergence theorem, yileds \\int \\div (f \\vec{A}) \\dd \\tau = \\int f(\\div \\vec{A}) \\dd \\tau + \\int \\vec{A} \\cdot (\\grad f) \\dd \\tau \\ \\oint f \\vec{A} \\cdot \\dd \\vec{a} or \\int _V f(\\div \\vec{A}) \\dd \\tau = - \\int _V \\vec{A} \\cdot (\\grad f) \\dd \\tau + \\oint _S f \\vec{A} \\cdot \\dd \\vec{a} \\tagl{1.59} Here again the integrand is the product of one function (f) and the derivative (in this case the divergence) of another ( A ), and integration by parts licenses us to transfer the derivative from A to f (where it becomes a gradient), at the cost of a minus sign and a boundary term (in this case a surface integral). In practice, this turns out to be one of the most useful tools at our disposal in vector calculus. Though you might wonder how often you're really likely to encounter an integral involving the product of one function and the derivative of another, the answer is surprisingly often .","title":"1.3 - Integral Calculus"},{"location":"ch1-3/#13-integral-calculus","text":"","title":"1.3: Integral Calculus"},{"location":"ch1-3/#131-line-surface-and-volume-integrals","text":"In electrodynamics, we encounter several different kinds of integrals, among which the most important are line (or path) integrals , surface integrals , and volume integrals .","title":"1.3.1: Line, Surface, and Volume Integrals"},{"location":"ch1-3/#line-integrals","text":"A line integral is an expression of the form \\int_a ^b \\vec{v} \\cdot \\dd{\\vec{l}} \\tagl{1.48} where v is a vector function, \\dd \\vec{l} is the infinitesimal displacement vector and the integral is to be carried out along a prescribed path P from point a to point b . If the path in question forms a closed loop (that is, if \\vec{b} = \\vec{a} , then put a circle on the integral sign: \\oint \\vec{v} \\cdot \\dd \\vec{l} \\tagl{1.49} At each point on the path, we take the dot product of v (evaluated at that point) with the displacement to the next point on the pat. To a physicist, the most familiar example of a line integral is the work done by a force \\vec{F} : W = \\int \\vec{F} \\cdot \\dd \\vec{l} Ordinarily, the value of a line integral depends critically on the path taken from a to b, but there is an important special class of vector functions for which the line integral is independent of path and is determined entirely by the end points. It will be our business in due course to characterize this special class of vectors. (A force that has this property is called conservative. )","title":"Line Integrals"},{"location":"ch1-3/#example-16","text":"Calculate the line integral of the function \\vec{v} = y^2 \\vu{x} + 2x (y+1) \\vu{y} from the point a = (1, 1, 0) to the point b = (2, 2, 0), along the paths (1) and (2) in Fig 1.21. What is \\oint \\vec{v} \\cdot \\dd \\vec{l} for the loop that goes from a to b along (1) and returns to a along (2)? Solution As always, \\dd \\vec{l} = \\dd x \\vu{x} + \\dd y \\vu{y} + \\dd z \\vu{z} . Path (1) consists of two parts. Along the \"horizontal\" segment, dy = dz = 0 so \\dd \\vec{l} = \\dd x \\vu{x} , y = 1, \\vec{v} \\cdot \\dd{\\vec{l}} = y^2 \\dd x = \\dd x, \\text{ so } \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int_1 ^2 \\dd x = 1 \\tag{i} On the \"vertical\" stretch, dx = dz = 0, so \\dd \\vec{l} = \\dd y \\vu{y}, x = 2, \\vec{v} \\cdot \\dd \\vec{l} = 2x(y+1) \\dd y = 4(y+1) \\dd y, \\text{ so } \\tag{ii} \\int \\vec{v} \\dd \\vec{l} = 4 \\int_1 ^2 (y+1) \\dd y = 10 By path (1), then \\int _{\\vec{a}} ^{\\vec{b}} \\vec{v} \\cdot \\dd \\vec{l} = 1 + 10 = 11 Meanwhile on path (2), x = y, \\dd x = \\dd y, and \\dd z = 0 , so \\dd \\vec{l} = \\dd x \\vu{x} + \\dd x \\vu{y}, \\vec{v} \\cdot \\dd \\vec{l} = x^2 \\dd x + 2x(x+1) \\dd x = (3x^2 + 2x) \\dd x and \\int_{\\vec{a}} ^{\\vec{b}} \\vec{v} \\cdot \\dd \\vec{l} = \\int_1 ^2 (3x^2 + 2x) \\dd x = \\left. (x^3 + x^2)\\right|_{1} ^2 = 10 (The strategy here is to get everything in terms of one variable; I could just as well have eliminated x in favor of y.) For the loop that goes out (1) and back (2), then \\oint \\vec{v} \\cdot \\dd \\vec{l} = 11 - 10 = 1","title":"Example 1.6"},{"location":"ch1-3/#surface-integrals","text":"A surface integral is an expression of the form \\int_{\\mathscr{S}} \\vec{v} \\cdot \\dd \\vec{a} \\tagl{1.50} where v is again some vector function, and the integral is over a specified surface \\mathscr{S} . Here \\dd \\vec{a} is an infinitesimal patch of area, with direction perpendicular to the surface (Fig 1.22). There are, of course, two directions perpendicular to any surface, so the sign of a surface integral is intrinsically ambiguous. If the surface is closed (forming a \"balloon\"), in which case I again put a circle on the integral sign \\oint \\vec{v} \\cdot \\dd \\vec{a} then tradition dictates that \"outward\" is positive, but for open surfaces it's arbitrary. If v describes the flow of a fluid (mass per unit area per unit time), then \\int \\vec{v} \\cdot \\dd \\vec{a} represents the total mass per unit time passing through the surface - hence the alternative name, \"flux.\" Ordinarily, the value of a surface integral depends on the particular surface chosen, but there is a special class of vector functions for which it is independent of the surface and is determined entirely by the boundary line. An important task will be to characterize this special class of functions.","title":"Surface Integrals"},{"location":"ch1-3/#example-17","text":"Calculate the surface integral of \\vec{v} = 2xz \\vu{x} + (x+2) \\vu{y} + y(z^2 -3) \\vu{z} over five sides (excluding the bottom) of the cubical box (side 2) in Fig 1.23. Let 'upward and outward' be the positive direction, as indicated by the arrows. Solution Taking the sides one at a time (i) x = 2, \\dd \\vec{a} = \\dd y \\dd z \\, \\vu{x}, \\vec{v} \\cdot \\dd \\vec{a} = 2xyz \\, \\dd y \\dd z = 4z \\dd y \\dd z , so \\int \\vec{v} \\cdot \\dd \\vec{a} = 4 \\int_0 ^2 \\dd y \\int_0 ^2 z \\dd z = 16 (ii) x = 0, \\dd \\vec{a} = -\\dd y \\dd z \\, \\vu{x}, \\vec{v} \\cdot \\dd \\vec{a} = -2xyz \\, \\dd y \\dd z = 4z \\dd y \\dd z = 0 , so \\int \\vec{v} \\cdot \\dd \\vec{a} = 0 (iii) y = 2, \\dd \\vec{a} = \\dd x \\dd z \\, \\vu{y}, \\vec{v} \\cdot \\dd \\vec{a} = (x+2) \\, \\dd x \\dd z , so \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int _0 ^2 (x + 2) \\dd x \\int _0 ^2 \\dd z = 12 (iv) y = 0, \\dd \\vec{a} = - \\dd x \\dd z \\, \\vu{y}, \\vec{v} \\cdot \\dd \\vec{a} = -(x+2) \\, \\dd x \\dd z , so \\int \\vec{v} \\cdot \\dd \\vec{a} = -\\int _0 ^2 (x + 2) \\dd x \\int _0 ^2 \\dd z = -12 (v) z = 2, \\dd \\vec{a} = \\dd x \\dd y \\, \\vu{z}, \\vec{v} \\cdot \\dd \\vec{a} = y(z^2 -3) \\, \\dd x \\dd y = y \\, \\dd x \\dd y , so \\int \\vec{v} \\cdot \\dd{a} = \\int_0 ^2 \\dd x \\int_0 ^2 y \\dd y = 4 The total flux is \\int _{surface} \\vec{v} \\cdot \\dd \\vec{a} = 16 + 0 + 12 - 12 + 4 = 20","title":"Example 1.7"},{"location":"ch1-3/#volume-integrals","text":"A volume integral is an expression of the form \\int_{V} T \\dd \\tau \\tagl{1.51} where T is a scalar function and \\dd \\tau is an infinitesimal volume element. In Cartesian coordinates, \\dd \\tau = \\dd x \\, \\dd y \\, \\dd z \\tagl{1.52} For example, if T is the density of a substance (which might vary from point to point), then the volume integral would give the total mass. Occasionally we shall encounter volume integrals of vector functions: \\int \\vec{v} \\dd \\tau = \\int (v_x \\vu{x} + v_y \\vu{y} + v_z \\vu{z}) \\dd \\tau = \\vu{x} \\int v_x \\dd \\tau + \\vu{y} \\int v_y \\dd \\tau + \\vu{z} \\int v_z \\dd \\tau \\tagl{1.53} Because the unit vectors are constants, they come outside the integral.","title":"Volume Integrals"},{"location":"ch1-3/#example-18","text":"Calculate the volume integral of T = xyz^2 over the prism in Fig 1.24. Solution \\begin{align*} \\int T \\dd \\tau & = \\int _0 ^3 z^2 \\left( \\int _0 ^1 y \\left[ \\int_0 ^{1-y} x \\, \\dd x \\right] \\dd y \\right) \\dd z \\\\ & = \\frac{1}{2} \\int_0 ^3 z^2 \\, \\dd z \\int_0 ^1 (1-y)^2 y \\, \\dd \\y = \\frac{1}{2} (9) \\left( \\frac{1}{12} \\right) = \\frac{3}{8} \\end{align*}","title":"Example 1.8"},{"location":"ch1-3/#132-the-fundamental-theorem-of-calculus","text":"Suppose f(x) is a function in one variable. The fundamental theorem of calculus says \\int_a ^b \\left( \\dv{f}{x} \\right) \\dd x = f(b) - f(a) \\tagl{1.54} In case this doesn't look familiar, I'll write it another way: \\int_a ^b F(x) \\dd x = f(b) - f(a) where df / dx = F(x) . The fundamental theorem tells you how to integrate F(x) : you think up a function f(x) whose derivative is equal to F. Geometrical interpretation : According to Eq. 1.33, df = (df / dx) dx is the infinitesimal change in f when you go from (x) to (x + dx). The fundamental theorem (Eq. 1.54) says that if you chop the interval from a to b (Fig. 1.25) into many tiny pieces, dx, and add up the increments df from each little piece, the result is (not surprisingly) equal to the total change in f: f(b) - f(a) . In other words, there are two ways to determine the total change in the function: either subtract the values at the ends or go step-by-step, adding up all the tiny increments as you go. You'll get the same answer either way. Notice the basic format of the fundamental theorem: the integral of a derivative over some region is given by the value of the function at the end points (boundaries). In vector calculus there are three species of derivative (gradient, divergence, and curl), and each has its own \"fundamental theorem,\" with essentially the same format. I don't plan to prove these theorems here; rather, I will explain what they mean, and try to make them plausible.","title":"1.3.2: The Fundamental Theorem of Calculus"},{"location":"ch1-3/#133-the-fundamental-theorem-for-gradients","text":"Suppose we have a scalar function of three variables T(x, y, z). Starting at point a , we move a small distance \\dd \\vec{l}_1 (Fig 1.26). According to Eq. 1.37, the function T will change by an amount \\dd T = (\\grad T) \\cdot \\dd \\vec{l}_1 Now we move a little further, by an additional small displacement \\dd \\vec{l}_2 ; the incremental change in T will be (\\grad T) \\cdot \\dd \\vec{l}_2 . In this manner, proceeding by infinitesimal steps, we make the journey to point b. At each step we compute the gradient of T (at that point) and dot it into the displacement dl... this gives us the change in T. Evidently the total change in Tin going from a to b (along the path selected) is \\int_{\\vec{a}} ^{\\vec{b}} (\\grad T) \\cdot \\dd \\vec{l} = T(\\vec{b}) - T(\\vec{a}) \\tagl{1.55} This is the fundamental theorem for gradients; like the \"ordinary\" fundamental theorem, it says that the integral (here a line integral) of a derivative (here the gradient) is given by the value of the function at the boundaries (a and b). Geometrical Interpretation: Suppose you wanted to determine the height of the Eiffel Tower. You could climb the stairs, using a ruler to measure the rise at each step, and adding them all up (that's the left side of Eq. 1.55), or you could place altimeters at the top and the bottom, and subtract the two readings (that's the right side); you should get the same answer either way (that's the fundamental theorem). Incidentally, as we found in Ex. 1.6, line integrals ordinarily depend on the path taken from a to b. But the right side of Eq. 1.55 makes no reference to the path - only to the end points. Evidently, gradients have the special property that their line integrals are path independent: Corollary 1: \\int_a ^b (\\grad T) \\cdot \\dd \\vec{l} is independent of the path taken from a to b. Corollary 2: \\oint (\\grad T) \\cdot \\dd \\vec{l} = 0 , since the beginning and end points are identical, and hence T(\\vec{b}) - T(\\vec{a}) = 0 .","title":"1.3.3: The Fundamental Theorem for Gradients"},{"location":"ch1-3/#example-19","text":"Let T = xy^2 , and take point a to be the origin (0, 0, 0) and b to be the point (2, 1, 0). Check the fundamental theorem for gradients. Solution We know a priori that the integral should be independent of the path, but we must sill pick a specific path in order to evaluate it. Let's go out along the x axis, then up (Fig 1.27). As always, \\dd \\vec{l} = \\dd x \\vu{x} + \\dd y \\vu{y} + \\dd z \\vu{z}; \\grad T = y^2 \\vu{x} + 2xy \\vu{y} y = 0; \\dd \\vec{l} = \\dd x \\vu{x}; \\grad T \\cdot \\dd \\vec{l} = y^2 \\dd x = 0 \\rightarrow \\int_{i} \\grad T \\cdot \\dd \\vec{l} = 0 x = 2; \\dd \\vec{l} = \\dd y \\vu{y}; \\grad T \\cdot \\dd \\vec{l} = 4y \\dd y \\rightarrow \\int_{ii} \\grad T \\cdot \\dd \\vec{l} = \\left. 2y^2 \\right| _0 ^1 = 2 The total line integral is 2. So is this consistent with what we expect from the fundamental theorem? Well, T(b) - T(a) = 2 - 0 = 2 , so yes!","title":"Example 1.9"},{"location":"ch1-3/#134-the-fundamental-theorem-for-divergences","text":"The fundamental theorem for divergences states that \\int _{\\mathscr{V}} (\\div \\vec{v} ) \\dd \\tau = \\oint_{\\mathscr{S}} \\vec{v} \\cdot \\dd \\vec{a} \\tagl{1.56} In honor, I suppose, of its great importance, this theorem has at least three special names: Gauss's theorem , Green's theorem , or simply the divergence theorem . Like the other \"fundamental theorems,\" it says that the integral of a derivative (in this case the divergence) over a region (in this case a volume, V) is equal to the value of the function at the boundary (in this case the surface S that bounds the volume). Notice that the boundary term is itself an integral (specifically, a surface integral). This is reasonable: the \"boundary\" of a line is just two end points, but the boundary of a volume is a (closed) surface. Geometrical Interpretation: If v represents the flow of an incompressible fluid, then the flux of v (the right side of Eq. 1.56) is the total amount of fluid passing out through the surface, per unit time. Now, the divergence measures the \"spreading out\" of the vectors from a point-a place of high divergence is like a \"faucet,\" pouring out liquid. If we have a bunch of faucets in a region filled with incompressible fluid, an equal amount of liquid will be forced out through the boundaries of the region. In fact, there are two ways we could determine how much is being produced: (a) we could count up all the faucets, recording how much each puts out, or (b) we could go around the boundary, measuring the flow at each point, and add it all up. You get the same answer either way: \\int (\\text{faucets within the volume}) = \\oint (\\text{flow out through the surface}) This, in essence, is what the divergence theorem says.","title":"1.3.4: The Fundamental Theorem for Divergences"},{"location":"ch1-3/#example-110","text":"Check the divergence theorem using the function \\vec{v} = y^2 \\vu{x} + (2xy + z^2) \\vu{y} + (2yz) \\vu{z} using a unit cube at the origin as the surface boundary (Fig 1.29). Solution In this case \\div \\vec{v} = 2(x + y) and \\int_V 2(x+y) \\dd \\tau = 2 \\int_0 ^1 \\dd x \\int _0 ^1 \\dd y \\int _0 ^1 \\dd z (x+y) \\int _0 ^1 \\dd x (x+y) = \\frac{1}{2} + y, \\quad \\int _0 ^1 \\dd y (\\frac{1}{2} + y) \\dd y = 1, \\quad \\int_0 ^1 \\dd z (1) = 1 Thus, \\int_{V} \\div \\vec{v} \\dd \\tau = 2 That takes care of the volume integral part of Gauss' Law, now how about the surface integral? We have to it in six parts, for each face of the cube: \\tag{i} \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd y \\int_0 ^1 \\dd z y^2 = \\frac{1}{3} \\tag{ii} \\int \\vec{v} \\cdot \\dd \\vec{a} = - \\int _0 ^1 \\dd y \\int _0 ^1 \\dd z y^2 = - \\frac{1}{3} \\tag{iii} \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd x \\int_0 ^1 \\dd z (2x + z^2) = \\frac{4}{3} \\tag{iv} \\int \\vec{v} \\cdot \\dd \\vec{a} = - \\int _0 ^1 \\dd x \\int _0 ^1 \\dd z (z^2) = - \\frac{1}{3} \\tag{v} \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd x \\int_0 ^1 \\dd y (2y) = 1 \\tag{vi} \\int \\vec{v} \\cdot \\dd \\vec{a} = - \\int _0 ^1 \\dd x \\int _0 ^1 \\dd y (0) = 0 So the total flux is \\oint _S \\vec{v} \\cdot \\dd \\vec{a} = \\frac{1}{3} - \\frac{1}{3} + \\frac{4}{3} - \\frac{1}{3} + 1 + 0 = 2 as we should expect.","title":"Example 1.10"},{"location":"ch1-3/#135-the-fundamental-theorem-for-curls","text":"The fundamental theorem for curls, which goes by the name Stokes' Theorem , states that \\int _S(\\curl \\vec{V}) \\cdot \\dd \\vec{a} = \\oint _P \\vec{v} \\cdot \\dd \\vec{l} \\tagl{1.57} As always, the integral of a derivative (here, the curl) over a region (here, a patch of surface, S) is equal to the value of the function at the boundary (here, the perimeter of the patch, P). As in the case of the divergence theorem, the boundary term is itself an integral-specifically, a closed line integral. Geometrical Interpretation: Recall that the curl measures the \"twist\" of the vectors v ; a region of high curl is a whirlpool - if you put a tiny paddle wheel there, it will rotate. Now, the integral of the curl over some surface (or, more precisely, the flux of the curl through that surface) represents the \"total amount of swirl,\" and we can determine that just as well by going around the edge and finding how much the flow is following the boundary (Fig. 1.31). Indeed, \\oint \\vec{v} \\cdot \\dd \\vec{l} is sometimes called the circulation of v. You may have noticed an apparent ambiguity in Stokes' theorem: concerning the boundary line integral, which way are we supposed to go around (clockwise or counterclockwise)? If we go the \"wrong\" way, we'll pick up an overall sign error. The answer is that it doesn't matter which way you go as long as you are consistent, for there is a compensating sign ambiguity in the surface integral: Which way does \\dd \\vec{a} point? For a closed surface (as in the divergence theorem), \\dd \\vec{a} points in the direction of the outward normal; but for an open surface, which way is \"out\"? Consistency in Stokes' theorem (as in all such matters) is given by the right-hand rule: if your fingers point in the direction of the line integral, then your thumb fixes the direction of \\dd \\vec{a} (Fig. 1.32). Now, there are plenty of surfaces (infinitely many) that share any given boundary line. Twist a paper clip into a loop, and dip it in soapy water. The soap film constitutes a surface, with the wire loop as its boundary. If you blow on it, the soap film will expand, making a larger surface, with the same boundary. Ordinarily, a flux integral depends critically on what surface you integrate over, but evidently this is not the case with curls. For Stokes' theorem says that \\int (\\curl \\vec{v}) \\cdot \\dd \\vec{a} is equal to the line integral of \\vec{v}\\ around the boundary, and the latter makes no reference to the specific surface you choose. Corollary 1: \\int (\\curl \\vec{v}) \\cdot \\dd \\vec{a} depends only on the boundary line, not on the particular surface used. Corollary 2: \\oint (\\curl \\vec{v}) \\cdot \\dd \\vec{a} = 0 for any closed surface, since the boundary line, like the mouth of a balloon, shrinks down to a point, and hence the right side of \\eqref{1.57} vanishes.","title":"1.3.5: The Fundamental Theorem for Curls"},{"location":"ch1-3/#example-111","text":"Suppose \\vec{v} = (2xz + 3y^2) \\vu{y} + (4yz^2) \\vu{z} . Check Stokes' theorem for the square surface shown in Fig 1.33. Solution Here \\curl \\vec{v} = (4z^2 - 2x) \\vu{x} + 2z \\vu{z} \\quad \\text{and} \\quad \\dd \\vec{a} = \\dd y \\, \\dd z \\, \\vu{x} (In saying that \\dd \\vec{a} points in the x direction, we are committing ourselves to a counterclockwise integral. We could as well write \\dd \\vec{a} pointing in the other direction ( \\dd \\vec{a} = - \\dd y \\, \\dd z\\, \\vu{x} ) and perform the integral in the clockwise direction.) Since x = 0 for this surface, \\int (\\curl \\vec{v}) \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd y \\int_0 ^1 \\dd z (4z^2) = \\frac{4}{3} Now for the line integral, which we of course break into 4 pieces: \\tag{i} x = 0 \\quad z = 0 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 3y^2 \\dd y, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _0 ^1 3y^2 \\dd y = 1 \\tag{ii} x = 0 \\quad y = 1 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 4z^2 \\dd z, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _0 ^1 4z^2 \\dd z = \\frac{4}{3} \\tag{iii} x = 0 \\quad z = 1 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 3y^2 \\dd y, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _1 ^0 3y^2 \\dd y = -1 \\tag{iv} x = 0 \\quad y = 0 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 0 \\dd z, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _1 ^0 0 \\dd z = 0 So \\oint \\vec{v} \\cdot \\dd \\vec{l} = 1 + \\frac{4}{3} - 1 + 0 = \\frac{4}{3} It all checks out!","title":"Example 1.11"},{"location":"ch1-3/#136-integration-by-parts","text":"The technique known (awkwardly) as integration by parts exploits the product rule for derivatives: \\dv{}{x} (fg) = f \\left( \\dv{g}{x} \\right) + g \\left( \\dv{f}{x} \\right) Integrating both sides, and invoking the fundamental theorem, \\int_a ^b \\dv{}{x} (fg) \\dd x = \\left. fg \\right| ^b _a = \\int _a ^b f \\left( \\dv{g}{x} \\right) \\dd x + \\int_a ^b g \\left( \\dv{f}{x} \\right) \\dd x or \\int_a ^b f \\left( \\dv{g}{x} \\right) \\dd x = - \\int_a ^b \\left( \\dv{f}{x} \\right) \\dd x + \\left. fg \\right| ^b _a \\tagl{1.58} That's \"integration by parts.\" It applies to the situation in which you are called upon to integrate the product of one function (f) and the derivative of another (g); it says you can transfer the derivative from g to f, at the cost of a minus sign and a boundary term.","title":"1.3.6: Integration by Parts"},{"location":"ch1-3/#example-112","text":"Evaluate the integral $$ \\int _0 ^\\infty x e^{-x} \\dd x Solution The exponential can be expressed as a derivative: e^{-x} = \\dv{}{x} \\left( - e^{-x} \\right) in this case, then, f(x) = x , g(x) = - e^{-x} , and df /dx = 1 , so \\int_0 ^\\infty x e^{-x} \\dd x = \\int _0 ^{\\infty} e^{-x} \\dd x - \\left. x e^{-x} \\right| _{0} ^{\\infty} = - \\left. e^{-x} \\right| _0 ^{\\infty} = 1 We can exploit the product rules of vector calculus, together with the appropriate fundamental theorems, in exactly the same way. For example, integrating \\div (f\\vec{A}) = f(\\div \\vec{A}) + \\vec{A} \\cdot (\\grad f) over a volume, and invoking the divergence theorem, yileds \\int \\div (f \\vec{A}) \\dd \\tau = \\int f(\\div \\vec{A}) \\dd \\tau + \\int \\vec{A} \\cdot (\\grad f) \\dd \\tau \\ \\oint f \\vec{A} \\cdot \\dd \\vec{a} or \\int _V f(\\div \\vec{A}) \\dd \\tau = - \\int _V \\vec{A} \\cdot (\\grad f) \\dd \\tau + \\oint _S f \\vec{A} \\cdot \\dd \\vec{a} \\tagl{1.59} Here again the integrand is the product of one function (f) and the derivative (in this case the divergence) of another ( A ), and integration by parts licenses us to transfer the derivative from A to f (where it becomes a gradient), at the cost of a minus sign and a boundary term (in this case a surface integral). In practice, this turns out to be one of the most useful tools at our disposal in vector calculus. Though you might wonder how often you're really likely to encounter an integral involving the product of one function and the derivative of another, the answer is surprisingly often .","title":"Example 1.12"},{"location":"ch1-4/","text":"1.4: Curvilinear Coordinates 1.4.1: Spherical Coordinates You can label a point P by its Cartesian coordinates (x, y, z), but sometimes it is more convenient to use spherical coordinates (r, \\theta, \\phi) ; r is the distance from the origin (the magnitude of the position vector r ), \\theta (the angle down from the z axis) is called the polar angle , and \\phi (the angle around from the x axis) is the azimuthal angle . Their relation to Cartesian coordinates can be read trigonometrically from Fig 1.36: x = r \\sin \\theta \\cos \\phi, \\qquad y = r \\sin \\theta \\sin \\phi, \\qquad z = r \\cos \\theta \\tagl{1.62} Figure 1.36 also shows three unit vectors, \\vu{r}, \\vu{\\theta}, \\vu{\\phi} , pointing in the direction of increase of the corresponding coordinates. They constitute an orthogonal (mutually perpendicular) basis set (just like \\vu{x}, \\vu{y}, \\vu{z} ), and any vector A can be expressed in terms of them, in the usual way: \\vec{A} = A_r \\vu{r} + A_\\theta \\vu{\\theta} + A_\\phi \\vu{\\phi} \\tagl{1.63} A_r, A_{\\theta}, A_{\\phi} are the radial, polar, and azimuthal components of A . In terms of the Cartesian unit vectors, \\begin{align*} \\vu{r} & = \\sin \\theta \\cos \\phi \\vu{x} + \\sin \\theta \\sin \\phi \\vu{y} + \\cos \\theta \\vu{z} \\\\ \\vu{\\theta} & = \\cos \\theta \\cos \\phi \\vu{x} + \\cos \\theta \\sin \\phi \\vu{y} - \\sin \\theta \\vu{z} \\\\ \\vu{\\phi} & = - \\sin \\phi \\vu{x} + \\cos \\phi \\vu{y} \\end{align*} \\tagl{1.64} as you can check for yourself (Prob 1.38). There is a poisonous snake lurking here that I'd better warn you about: \\vu{r} , \\vu{\\theta} , and \\vu{\\phi} are associated with a particular point P, and they change direction as P moves around. For example, \\vu{r} always points radially outward, but \"radially outward\" can be in the x direction, the y direction, or any other direction, depending on where you are. In Fig. 1.37, \\vec{A} = \\vu{y} and \\vec{B} = - \\vu{y} , and yet both of them would be written as \\vu{r} in spherical coordinates. One could take account of this by explicitly indicating the point of reference: \\vu{r}(\\theta, \\phi), \\vu{\\theta}(\\theta, \\phi), \\vu{\\phi}(\\theta, \\phi) , but this would be cumbersome, and as long as you are alert to the problem, I don't think it will cause difficulties. In particular, do not naively combine the spherical components of vectors associated with different points (in Fig. 1.37, \\vec{A} + \\vec{B} = 0 , not 2 \\vu{r} , and \\vec{A} \\cdot \\vec{B} = -1 , not +1 ). Beware of differentiating a vector that is expressed in spherical coordinates, since the unit vectors themselves are functions of position ( \\partial \\vu{r} / \\partial \\theta = \\vu{\\theta} , for example). And do not take \\vu{r}, \\vu{\\theta}, \\vu{\\phi} outside an integral, as I did with the Cartesian unit vectors. In general, if you're uncertain about the validity of an operation, rewrite the problem using Cartesian coordinates, for which this difficulty does not arise. An infinitesimal displacement in the \\vu{r} direction is simply dr (Fig. 1.38a), just as an infinitesimal element of length in the x direction is dx: \\dd l_r = dr \\tagl{1.65} On the other hand, an infinitesimal element of length in the \\vu{\\theta} direction (Fig 1.38b) is not just \\dd \\theta - that doesn't even have the right units for a length! Rather, \\dd l_{\\theta} = r \\dd \\theta \\tagl{1.66} Similarly, an infinitesimal element of length in the \\vu{\\phi} direction (Fig 1.38c) is \\dd l_{\\phi} = r \\sin \\theta \\dd \\phi \\tagl{1.67} so that we can write the general infinitesimal displacement as \\dd \\vec{l} = \\dd r \\vu{r} + r \\dd \\theta \\vu{\\theta} + r \\sin \\theta \\dd \\phi \\vu{\\phi} \\tagl{1.68} This plays the role that \\dd x \\vu{x} + \\dd y \\vu{y} + \\dd z \\vu{z} played in Cartesian coordinates. The infinitesimal volume element \\dd \\tau in spherical coordinates, is the product of the three infinitesimal displacements: \\dd \\tau = \\dd l_r \\dd l_{\\theta} \\dd l_{\\phi} = r^2 \\sin \\theta \\dd r \\dd \\theta \\dd \\phi \\tagl{1.69} I cannot give you a general expression for surface elements \\dd \\vec{a} , since these depend on the orientation of the surface. You simply have to analyze the geometry for any given case (this goes for Cartesian and curvilinear coordinates alike). If you are integrating over the surface of a sphere, for instance, then r is constant, whereas \\theta and \\phi change (Fig. 1.39), so \\dd \\vec{a}_1 = \\dd l_\\theta \\dd l_\\phi \\vu{r} = r^2 \\sin \\theta \\dd \\theta \\dd \\phi \\vu{r} On the other hand, if the surface lies in the xy plane, say, so that \\theta is constant ( \\pi / 2 ) while r and \\phi may vary, then \\dd \\vec{a}_2 = \\dd l_r \\dd l_{\\phi} \\vu{\\theta} = r \\dd r \\dd \\phi \\vu{\\theta} Notice, finally, that r ranges from 0 to \\infty , \\phi from 0 to 2 \\pi , and \\theta from 0 to \\pi . Example 1.13 Find the volume of a sphere of radius R Solution Well, we know that we should get \\frac{4}{3} \\pi R^3 . Let's see what happens... \\begin{align*} V & = \\int \\dd \\tau = \\int_{r = 0} ^R \\dd r \\int_{\\theta = 0} ^{\\pi} r \\dd \\theta \\int_{\\phi = 0} ^{2 \\pi} r \\sin \\theta \\dd \\phi \\\\ & = \\left( \\int_{0} ^R r^2 \\dd r \\right) \\left( \\int_0 ^\\pi \\sin \\theta \\dd \\theta \\right) \\left( \\int_0 ^{2 \\pi} \\dd \\phi \\right) \\\\ & = \\left( \\frac{R^3}{3} \\right)(2)(2 \\pi) = \\frac{4}{3} \\pi R^3 \\end{align*} Great! So far we have talked only about the geometry of spherical coordinates. Now I would like to \"translate\" the vector derivatives (gradient, divergence, curl, and Laplacian) into r, \\theta, \\phi notation. In principle, this is entirely straightforward: in the case of the gradient, \\grad T = \\pdv{T}{x} \\vu{x} + \\pdv{T}{y} \\vu{y} + \\pdv{T}{z} \\vu{z} for instance, we would first use the chain rule to expand the partials \\pdv{T}{x} = \\pdv{T}{r} \\left( \\pdv{r}{x} \\right) + \\pdv{T}{\\theta} \\left( \\pdv{\\theta}{x} \\right) + \\pdv{T}{\\phi} \\left( \\pdv{\\phi}{x} \\right) The terms in parentheses could be worked out from \\eqref{1.62} - or rather, their inverse. Then we'd do the same for y and z, and then substitute in the formulas for \\vu{x}, \\vu{y}, \\vu{z} in terms of \\vu{r}, \\vu{\\theta}, \\vu{\\phi} . It would take an hour to carry out this very brute-force approach, and I suppose this is how it was originally done, but there is a much more efficient indirect approach, which has the extra advantage of treating all coordinate systems at once. I describe the \"straightforward\" method only to show you that there is nothing subtle or mysterious about transforming to spherical coordinates: you're expressing the same quantity in different notation, that's all. The indirect method is relegated to one of the appendices, which I may add later. Here, then, are the vector derivatives in spherical coordinates: Gradient : \\grad T = \\pdv{T}{r} \\vu{r} + \\frac{1}{r} \\pdv{T}{\\theta} \\vu{\\theta} + \\frac{1}{r \\sin \\theta} \\pdv{T}{\\phi} \\vu{\\phi} \\tagl{1.70} Divergence : \\div \\vec{v} = \\frac{1}{r^2} \\pdv{}{r} (r^2 v_r) + \\frac{1}{r\\sin \\theta} \\pdv{}{\\theta} (\\sin \\theta v_{\\theta}) + \\frac{1}{r \\sin \\theta} \\pdv{v_{\\phi}}{\\phi} \\tagl{1.71} Curl : \\begin{align*} \\curl \\vec{v} = & \\frac{1}{r \\sin \\theta} \\left[ \\pdv{}{\\theta} (\\sin \\theta V_{\\phi}) - \\pdv{v_{\\theta}}{\\phi} \\right] \\vu{r} \\\\ & \\quad + \\frac{1}{r} \\left[ \\frac{1}{\\sin \\theta} \\pdv{v_r}{\\phi} - \\pdv{}{r} (r v_{\\phi}) \\right] \\vu{\\theta} \\\\ & \\quad + \\frac{1}{r} \\left[ \\pdv{}{r} (r v_{\\theta}) - \\pdv{v_r}{\\theta} \\right] \\vu{\\phi} \\end{align*} \\tagl{1.72} Laplacian : \\laplacian T = \\frac{1}{r^2} \\pdv{}{r} \\left( r^2 \\pdv{T}{r} \\right) + \\frac{1}{r^2 \\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{T}{\\theta} \\right) + \\frac{1}{r^2 \\sin ^2 \\theta} \\frac{\\partial ^2 T}{\\partial \\phi ^2} \\tagl{1.73} 1.4.2: Cylindrical Coordinates The cylindrical coordinates (s, \\phi, z) of a point P are defined in Fig 1.42. Notice that \\phi has the same meaning as in spherical coordinates, and z is the same as Cartesian. s is the distance to P from the z axis , whereas the spherical coordinate r is the distance from the origin . The relation to Cartesian coordinates is somewhat cleaner than the spherical sort x = s \\cos \\phi, \\qquad y = s \\sin \\phi, \\qquad z = z \\tagl{1.74} The unit vectors (Prob 1.42) are \\begin{align*} \\vu{s} & = \\cos \\phi \\vu{x} + \\sin \\phi \\vu{y} \\\\ \\vu{\\phi} & = - \\sin \\phi \\vu{x} + \\cos \\phi \\vu{y} \\\\ \\vu{z} & = \\vu{z} \\end{align*} \\tagl{1.75} The infinitesimal displacements are dl_s = ds, \\qquad dl_{\\phi} = s \\dd \\phi, \\qquad dl_z = dz \\tagl{1.76} so \\dd \\vec{l} = ds \\vu{s} + s \\dd \\phi \\vu{\\phi} + dz \\vu{z} \\tagl{1.77} and the volume element is \\dd \\tau = s \\, \\dd s \\, \\dd \\phi \\, \\dd z \\tagl{1.78} The range of s is 0 \\rightarrow \\infty , \\phi goes from 0 \\rightarrow 2\\pi , and z from -\\infty \\rightarrow \\infty . The vector derivatives in cylindrical coordinates are: Gradient : \\grad T = \\pdv{T}{s} \\vu{s} + \\frac{1}{s} \\pdv{T}{\\phi} \\vu{\\phi} + \\pdv{T}{z} \\vu{z} \\tagl{1.79} Divergence : \\grad \\vec{v} = \\frac{1}{s} \\pdv{}{s} (s v_s) + \\frac{1}{s} \\pdv{v_\\phi}{\\phi} + \\pdv{v_z}{z} \\tagl{1.80} Curl : \\begin{align*} \\curl \\vec{v} = & \\left( \\frac{1}{s} \\pdv{v_z}{\\phi} - \\pdv{v_{\\phi}}{z} \\right) \\vu{s} \\\\ & + \\left( \\pdv{v_s}{z} - \\pdv{v_z}{s} \\right) \\vu{\\phi} \\\\ & + \\frac{1}{s} \\left[ \\pdv{}{s} (s v_{\\phi}) - \\pdv{v_s}{\\phi} \\right] \\vu{z} \\end{align*} \\tagl{1.81} Laplacian : \\laplacian T = \\frac{1}{s} \\pdv{}{s} \\left( s \\pdv{T}{s} \\right) + \\frac{1}{s^2} \\frac{\\partial ^2 T}{\\partial \\phi ^2} + \\frac{\\partial ^2 T}{\\partial z^2} \\tagl{1.82}","title":"1.4 - Curvilinear Coordinates"},{"location":"ch1-4/#14-curvilinear-coordinates","text":"","title":"1.4: Curvilinear Coordinates"},{"location":"ch1-4/#141-spherical-coordinates","text":"You can label a point P by its Cartesian coordinates (x, y, z), but sometimes it is more convenient to use spherical coordinates (r, \\theta, \\phi) ; r is the distance from the origin (the magnitude of the position vector r ), \\theta (the angle down from the z axis) is called the polar angle , and \\phi (the angle around from the x axis) is the azimuthal angle . Their relation to Cartesian coordinates can be read trigonometrically from Fig 1.36: x = r \\sin \\theta \\cos \\phi, \\qquad y = r \\sin \\theta \\sin \\phi, \\qquad z = r \\cos \\theta \\tagl{1.62} Figure 1.36 also shows three unit vectors, \\vu{r}, \\vu{\\theta}, \\vu{\\phi} , pointing in the direction of increase of the corresponding coordinates. They constitute an orthogonal (mutually perpendicular) basis set (just like \\vu{x}, \\vu{y}, \\vu{z} ), and any vector A can be expressed in terms of them, in the usual way: \\vec{A} = A_r \\vu{r} + A_\\theta \\vu{\\theta} + A_\\phi \\vu{\\phi} \\tagl{1.63} A_r, A_{\\theta}, A_{\\phi} are the radial, polar, and azimuthal components of A . In terms of the Cartesian unit vectors, \\begin{align*} \\vu{r} & = \\sin \\theta \\cos \\phi \\vu{x} + \\sin \\theta \\sin \\phi \\vu{y} + \\cos \\theta \\vu{z} \\\\ \\vu{\\theta} & = \\cos \\theta \\cos \\phi \\vu{x} + \\cos \\theta \\sin \\phi \\vu{y} - \\sin \\theta \\vu{z} \\\\ \\vu{\\phi} & = - \\sin \\phi \\vu{x} + \\cos \\phi \\vu{y} \\end{align*} \\tagl{1.64} as you can check for yourself (Prob 1.38). There is a poisonous snake lurking here that I'd better warn you about: \\vu{r} , \\vu{\\theta} , and \\vu{\\phi} are associated with a particular point P, and they change direction as P moves around. For example, \\vu{r} always points radially outward, but \"radially outward\" can be in the x direction, the y direction, or any other direction, depending on where you are. In Fig. 1.37, \\vec{A} = \\vu{y} and \\vec{B} = - \\vu{y} , and yet both of them would be written as \\vu{r} in spherical coordinates. One could take account of this by explicitly indicating the point of reference: \\vu{r}(\\theta, \\phi), \\vu{\\theta}(\\theta, \\phi), \\vu{\\phi}(\\theta, \\phi) , but this would be cumbersome, and as long as you are alert to the problem, I don't think it will cause difficulties. In particular, do not naively combine the spherical components of vectors associated with different points (in Fig. 1.37, \\vec{A} + \\vec{B} = 0 , not 2 \\vu{r} , and \\vec{A} \\cdot \\vec{B} = -1 , not +1 ). Beware of differentiating a vector that is expressed in spherical coordinates, since the unit vectors themselves are functions of position ( \\partial \\vu{r} / \\partial \\theta = \\vu{\\theta} , for example). And do not take \\vu{r}, \\vu{\\theta}, \\vu{\\phi} outside an integral, as I did with the Cartesian unit vectors. In general, if you're uncertain about the validity of an operation, rewrite the problem using Cartesian coordinates, for which this difficulty does not arise. An infinitesimal displacement in the \\vu{r} direction is simply dr (Fig. 1.38a), just as an infinitesimal element of length in the x direction is dx: \\dd l_r = dr \\tagl{1.65} On the other hand, an infinitesimal element of length in the \\vu{\\theta} direction (Fig 1.38b) is not just \\dd \\theta - that doesn't even have the right units for a length! Rather, \\dd l_{\\theta} = r \\dd \\theta \\tagl{1.66} Similarly, an infinitesimal element of length in the \\vu{\\phi} direction (Fig 1.38c) is \\dd l_{\\phi} = r \\sin \\theta \\dd \\phi \\tagl{1.67} so that we can write the general infinitesimal displacement as \\dd \\vec{l} = \\dd r \\vu{r} + r \\dd \\theta \\vu{\\theta} + r \\sin \\theta \\dd \\phi \\vu{\\phi} \\tagl{1.68} This plays the role that \\dd x \\vu{x} + \\dd y \\vu{y} + \\dd z \\vu{z} played in Cartesian coordinates. The infinitesimal volume element \\dd \\tau in spherical coordinates, is the product of the three infinitesimal displacements: \\dd \\tau = \\dd l_r \\dd l_{\\theta} \\dd l_{\\phi} = r^2 \\sin \\theta \\dd r \\dd \\theta \\dd \\phi \\tagl{1.69} I cannot give you a general expression for surface elements \\dd \\vec{a} , since these depend on the orientation of the surface. You simply have to analyze the geometry for any given case (this goes for Cartesian and curvilinear coordinates alike). If you are integrating over the surface of a sphere, for instance, then r is constant, whereas \\theta and \\phi change (Fig. 1.39), so \\dd \\vec{a}_1 = \\dd l_\\theta \\dd l_\\phi \\vu{r} = r^2 \\sin \\theta \\dd \\theta \\dd \\phi \\vu{r} On the other hand, if the surface lies in the xy plane, say, so that \\theta is constant ( \\pi / 2 ) while r and \\phi may vary, then \\dd \\vec{a}_2 = \\dd l_r \\dd l_{\\phi} \\vu{\\theta} = r \\dd r \\dd \\phi \\vu{\\theta} Notice, finally, that r ranges from 0 to \\infty , \\phi from 0 to 2 \\pi , and \\theta from 0 to \\pi .","title":"1.4.1: Spherical Coordinates"},{"location":"ch1-4/#example-113","text":"Find the volume of a sphere of radius R Solution Well, we know that we should get \\frac{4}{3} \\pi R^3 . Let's see what happens... \\begin{align*} V & = \\int \\dd \\tau = \\int_{r = 0} ^R \\dd r \\int_{\\theta = 0} ^{\\pi} r \\dd \\theta \\int_{\\phi = 0} ^{2 \\pi} r \\sin \\theta \\dd \\phi \\\\ & = \\left( \\int_{0} ^R r^2 \\dd r \\right) \\left( \\int_0 ^\\pi \\sin \\theta \\dd \\theta \\right) \\left( \\int_0 ^{2 \\pi} \\dd \\phi \\right) \\\\ & = \\left( \\frac{R^3}{3} \\right)(2)(2 \\pi) = \\frac{4}{3} \\pi R^3 \\end{align*} Great! So far we have talked only about the geometry of spherical coordinates. Now I would like to \"translate\" the vector derivatives (gradient, divergence, curl, and Laplacian) into r, \\theta, \\phi notation. In principle, this is entirely straightforward: in the case of the gradient, \\grad T = \\pdv{T}{x} \\vu{x} + \\pdv{T}{y} \\vu{y} + \\pdv{T}{z} \\vu{z} for instance, we would first use the chain rule to expand the partials \\pdv{T}{x} = \\pdv{T}{r} \\left( \\pdv{r}{x} \\right) + \\pdv{T}{\\theta} \\left( \\pdv{\\theta}{x} \\right) + \\pdv{T}{\\phi} \\left( \\pdv{\\phi}{x} \\right) The terms in parentheses could be worked out from \\eqref{1.62} - or rather, their inverse. Then we'd do the same for y and z, and then substitute in the formulas for \\vu{x}, \\vu{y}, \\vu{z} in terms of \\vu{r}, \\vu{\\theta}, \\vu{\\phi} . It would take an hour to carry out this very brute-force approach, and I suppose this is how it was originally done, but there is a much more efficient indirect approach, which has the extra advantage of treating all coordinate systems at once. I describe the \"straightforward\" method only to show you that there is nothing subtle or mysterious about transforming to spherical coordinates: you're expressing the same quantity in different notation, that's all. The indirect method is relegated to one of the appendices, which I may add later. Here, then, are the vector derivatives in spherical coordinates: Gradient : \\grad T = \\pdv{T}{r} \\vu{r} + \\frac{1}{r} \\pdv{T}{\\theta} \\vu{\\theta} + \\frac{1}{r \\sin \\theta} \\pdv{T}{\\phi} \\vu{\\phi} \\tagl{1.70} Divergence : \\div \\vec{v} = \\frac{1}{r^2} \\pdv{}{r} (r^2 v_r) + \\frac{1}{r\\sin \\theta} \\pdv{}{\\theta} (\\sin \\theta v_{\\theta}) + \\frac{1}{r \\sin \\theta} \\pdv{v_{\\phi}}{\\phi} \\tagl{1.71} Curl : \\begin{align*} \\curl \\vec{v} = & \\frac{1}{r \\sin \\theta} \\left[ \\pdv{}{\\theta} (\\sin \\theta V_{\\phi}) - \\pdv{v_{\\theta}}{\\phi} \\right] \\vu{r} \\\\ & \\quad + \\frac{1}{r} \\left[ \\frac{1}{\\sin \\theta} \\pdv{v_r}{\\phi} - \\pdv{}{r} (r v_{\\phi}) \\right] \\vu{\\theta} \\\\ & \\quad + \\frac{1}{r} \\left[ \\pdv{}{r} (r v_{\\theta}) - \\pdv{v_r}{\\theta} \\right] \\vu{\\phi} \\end{align*} \\tagl{1.72} Laplacian : \\laplacian T = \\frac{1}{r^2} \\pdv{}{r} \\left( r^2 \\pdv{T}{r} \\right) + \\frac{1}{r^2 \\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{T}{\\theta} \\right) + \\frac{1}{r^2 \\sin ^2 \\theta} \\frac{\\partial ^2 T}{\\partial \\phi ^2} \\tagl{1.73}","title":"Example 1.13"},{"location":"ch1-4/#142-cylindrical-coordinates","text":"The cylindrical coordinates (s, \\phi, z) of a point P are defined in Fig 1.42. Notice that \\phi has the same meaning as in spherical coordinates, and z is the same as Cartesian. s is the distance to P from the z axis , whereas the spherical coordinate r is the distance from the origin . The relation to Cartesian coordinates is somewhat cleaner than the spherical sort x = s \\cos \\phi, \\qquad y = s \\sin \\phi, \\qquad z = z \\tagl{1.74} The unit vectors (Prob 1.42) are \\begin{align*} \\vu{s} & = \\cos \\phi \\vu{x} + \\sin \\phi \\vu{y} \\\\ \\vu{\\phi} & = - \\sin \\phi \\vu{x} + \\cos \\phi \\vu{y} \\\\ \\vu{z} & = \\vu{z} \\end{align*} \\tagl{1.75} The infinitesimal displacements are dl_s = ds, \\qquad dl_{\\phi} = s \\dd \\phi, \\qquad dl_z = dz \\tagl{1.76} so \\dd \\vec{l} = ds \\vu{s} + s \\dd \\phi \\vu{\\phi} + dz \\vu{z} \\tagl{1.77} and the volume element is \\dd \\tau = s \\, \\dd s \\, \\dd \\phi \\, \\dd z \\tagl{1.78} The range of s is 0 \\rightarrow \\infty , \\phi goes from 0 \\rightarrow 2\\pi , and z from -\\infty \\rightarrow \\infty . The vector derivatives in cylindrical coordinates are: Gradient : \\grad T = \\pdv{T}{s} \\vu{s} + \\frac{1}{s} \\pdv{T}{\\phi} \\vu{\\phi} + \\pdv{T}{z} \\vu{z} \\tagl{1.79} Divergence : \\grad \\vec{v} = \\frac{1}{s} \\pdv{}{s} (s v_s) + \\frac{1}{s} \\pdv{v_\\phi}{\\phi} + \\pdv{v_z}{z} \\tagl{1.80} Curl : \\begin{align*} \\curl \\vec{v} = & \\left( \\frac{1}{s} \\pdv{v_z}{\\phi} - \\pdv{v_{\\phi}}{z} \\right) \\vu{s} \\\\ & + \\left( \\pdv{v_s}{z} - \\pdv{v_z}{s} \\right) \\vu{\\phi} \\\\ & + \\frac{1}{s} \\left[ \\pdv{}{s} (s v_{\\phi}) - \\pdv{v_s}{\\phi} \\right] \\vu{z} \\end{align*} \\tagl{1.81} Laplacian : \\laplacian T = \\frac{1}{s} \\pdv{}{s} \\left( s \\pdv{T}{s} \\right) + \\frac{1}{s^2} \\frac{\\partial ^2 T}{\\partial \\phi ^2} + \\frac{\\partial ^2 T}{\\partial z^2} \\tagl{1.82}","title":"1.4.2: Cylindrical Coordinates"},{"location":"ch1-5/","text":"1.5: The Dirac Delta Function 1.5.1: The Divergence of \\vu{r} / r^2 Consider the vector function \\vec{v} = \\frac{1}{r^2} \\vu{r} \\tagl{1.83} At every location, v is directed radially outward (Fig. 1.44); if ever there was a function that ought to have a large positive divergence, this is it. And yet, when you actually calculate the divergence (using Eq. 1.71), you get precisely zero: \\div \\vec{v} = \\frac{1}{r^2} \\pdv{}{r} \\left( r^2 \\frac{1}{r^2} \\right) = \\frac{1}{r^2} \\pdv{}{r} (1) = 0 \\tagl{1.84} (You will have encountered this paradox already, if you worked Prob. 1.16.) The plot thickens when we apply the divergence theorem to this function. Suppose we integrate over a sphere of radius R, centered at the origin (Prob. 1.38b); the surface integral is \\begin{align*} \\oint \\vec{v} \\cdot \\dd \\vec{a} & = \\int \\left( \\frac{1}{R^2} \\vu{r} \\right) \\cdot (R^2 \\sin \\theta \\dd \\theta \\dd \\phi \\vu{r}) \\\\ & = \\left( \\int_0 ^\\pi \\sin \\theta \\dd \\theta \\right) \\left( \\int_0 ^{2 \\pi} \\dd \\phi \\right) = 4 \\pi \\end{align*} \\tagl{1.85} But if we really believe \\eqref{1.84} , then the volume integral \\int \\div \\vec{v} \\dd \\tau must be zero. What the heck is going on here?? The source of the problem is obviously the point r = 0 , where v blows up (and where, in Eq. 1.84, we have unwittingly divided by zero). It is quite true that \\div \\vec{v} = 0 everywhere except the origin, but right at the origin the situation is more complicated. Notice that the surface integral (Eq. 1.85) is independent of R; if the divergence theorem is right (and it is), we should get \\int (\\div \\vec{v}) \\dd \\tau = 4 \\pi for any sphere centered at the origin, no matter how small. Evidently the entire contribution must be coming from the point r = 0 ! Thus, \\div \\vec{v} has the bizarre property that it vanishes everywhere except at one point, and yet its integral (over any volume containing that point) is 4 \\pi . No ordinary function behaves like that. (On the other hand, a physical example does come to mind: the density (mass per unit volume) of a point particle. It's zero except at the exact location of the particle, and yet its integral is finite-namely, the mass of the particle.) What we have stumbled on is a mathematical object known to physicists as the Dirac delta function. It arises in many branches of theoretical physics. Moreover, the specific problem at hand (the divergence of the function \\vu{r} / r^2 ) is not just some arcane curiosity - it is, in fact, central to the whole theory of electrodynamics. So it is worthwhile to pause here and study the Dirac delta function with some care. 1.5.2: The One-Dimensional Dirac Delta Function The one-dimensional Dirac delta function, \\delta(x) , can be pictured as an infinitely high, infinitesimally narrow \"spike,\" with area 1 (Fig 1.45). That is to say: \\delta(x) = \\begin{cases} 0, & \\qquad \\text{ if } x \\neq 0 \\\\ \\infty , & \\qquad \\text{ if } x = 0 \\end{cases} \\tagl{1.86} and \\int_{-\\infty} ^{\\infty} \\delta(x) \\dd x = 1 \\tagl{1.87} Technically, \\delta(x) is not a function at all, since its value is not finite at x = 0; in the mathematical literature it is known as a generalized function , or distribution . It is, if you like, the limit of a sequence of functions , such as rectangles R_n(x) of height n and width 1/n , or isosceles triangles T_n(x) of height n and base 2/n (Fig 1.46) If f(x) is some \"ordinary\" function (let's just say continuous, just to be safe), then the product f(x) \\delta(x) is zero everywhere except at x = 0. It follows that f(x) \\delta(x) = f(0) \\delta(x) \\tagl{1.88} This is probably the most important fact about the delta function! Since the product is zero anyway except at x = 0, we may as well replace f(x) by the value it assumes at the origin. In particular, \\int_{-\\infty} ^{\\infty} f(x) \\delta(x) \\dd x = f(0) \\int_{-\\infty} ^{\\infty} \\delta(x) \\dd x = f(0) \\tagl{1.89} Under an integral, then, the delta function \"picks out\" the value of f(x) at a particular point. Of course, the limits of integration need not be all space, as long as the origin is included. We can also shift the spike from x = 0 to some other point, x = a, as well (Fig 1.47) \\delta(x - a) = \\begin{cases} 0, & \\text{ if } x \\neq a \\\\ \\infty , & \\text{ if } x = a \\end{cases} \\quad \\text{ with } \\quad \\int_{-\\infty} ^{\\infty} \\delta(x - a) \\dd x = 1 \\tagl{1.90} Equation \\eqref{1.88} becomes f(x) \\delta(x - a) = f(a) \\delta(x - a) and \\eqref{1.89} generalizes to \\int_{-\\infty} ^{\\infty} f(x) \\delta(x - a) \\dd x = f(a) \\tagl{1.92} Example 1.14 Evaluate the integral \\int_0 ^3 x^3 \\delta(x-2) \\dd x Solution Easy peasy. The delta function picks out the value of x^3 at the point x = 2, so the integral is 2^3 = 8 . Notice that if the limits of integration had not included x = 2, then the answer would be 0. Although \\delta(x) itself is not a legitimate function, integrals over \\delta are perfectly acceptable. In fact, it's best to think of the delta function as something that is always intended for use under an integral sign . In particular, two expressions involving delta functions are considered equal if \\int_{-\\infty} ^{\\infty} f(x) D_1 (x) \\dd x = \\int_{-\\infty} ^{\\infty} f(x) D_2 (x) \\dd x \\tagl{1.93} for all (\"ordinary\") functions f(x). Example 1.15 Show that \\delta(kx) = \\frac{1}{|k||}\\delta(x) where k is any (nonzero) constant. (In particular, \\delta(-x) = \\delta(x). ) Solution For an arbitrary test function f(x), consider the integral \\int_{-\\infty} ^{\\infty} f(x) \\delta(kx) \\dd x Changing variables, we let y \\equiv kx so that x = y/k and \\dd x = 1 / k \\dd y . If k is positive, the integration still runs from -\\infty to \\infty , but if k is negative, then x = \\infty implies y = -\\infty , and vice versa, so the order of the limits is reversed. Restoring the \"proper\" order costs a minus sign. Thus \\int_{-\\infty} ^{\\infty} f(x) \\delta(kx) \\dd x + \\pm \\int_{-\\infty} ^{\\infty} f(y/k) \\delta(y) \\frac{dy}{k} = \\pm \\frac{1}{k} f(0) = \\frac{1}{|k|} f(0) (where here the lower signs apply when k is negative, and we account for this neatly by putting absolute value bars around the final k.) Under the integral sign, then, \\delta(kx) serves the same purpose as (1/|k|)\\delta(x) : \\int_{-\\infty} ^{\\infty} f(x) \\delta(kx) \\dd x = \\int_{-\\infty} ^{\\infty} f(x) \\left[ \\frac{1}{|k|} \\delta(x) \\right] \\dd x According to our criterion \\eqref{1.93} , therefore, \\delta(kx) and (1/|k|)\\delta(x) are equal. The Three-Dimensional Delta Function It is easy to generalize the delta function to three dimensions: \\delta ^3 (\\vec{r}) = \\delta(x) \\delta(y) \\delta(z) \\tagl{1.96} This three-dimensional delta function is zero everywhere except at (0, 0, 0), where it blows up. Its volume integral is 1. \\int_{\\text{all space}} \\delta ^3 (\\vec{r}) \\dd \\tau = \\int_{-\\infty} ^{\\infty} \\dd x \\int_{-\\infty} ^{\\infty} \\dd y \\int_{-\\infty} ^{\\infty} \\dd z \\delta(x) \\delta(y) \\delta(z) = 1 \\tagl{1.97} And, generalizing \\eqref{1.92} \\int_{\\text{all space}} f(\\vec{r}) \\delta^3(\\vec{r} - \\vec{a}) \\dd \\tau = f(\\vec{a}) \\tagl{1.98} As in the one-dimensional case, integration with \\delta picks out the value of the function f at the location of the spike. We are now in a position to resolve the paradox introduced in Sect. 1.5.1. As you will recall, we found that the divergence of \\vu{r}/r^2 is zero everywhere except at the origin, and yet its integral over any volume containing the origin is a constant (to wit: 4\\pi ). These are precisely the defining conditions for the Dirac delta function; evidently \\div \\left( \\frac{\\vu{r}}{r^2} \\right) = 4 \\pi \\delta^3(\\vec{r}) \\tagl{1.99} More generally, \\div \\left( \\frac{\\vu{\\gr}}{\\gr^2} \\right) = 4 \\pi \\delta^3(\\gr) \\tagl{1.100} where, as always, \\vec{\\gr} is the separation vector \\vec{\\gr} \\equiv \\vec{r} - \\vec{r'} . Note that differentiation here is with respect to \\vec{r} , while \\vec{r'} is held constant. Incidentally, since \\grad \\left( \\frac{1}{\\gr} \\right) = - \\frac{\\vu{\\gr}}{\\gr ^2} (from Problem 1.13), it follows that \\laplacian \\frac{1}{\\gr} = - 4 \\pi \\delta^3 (\\vec{\\gr}) \\tagl{1.102} Example 1.16 Evaluate the integral J = \\int_V (r^2 + 2) \\div \\left( \\frac{\\vu{r}}{r^2} \\right) \\dd \\tau where V is a sphere of radius R centered at the origin. Solution 1 Use \\eqref{1.99} to rewrite the divergence, and \\eqref{1.98} to do the integral: J = \\int_V (r^2 + 2) 4 \\pi \\delta ^3(\\vec{r}) \\dd \\tau = 4 \\pi (0 + 2) = 8 \\pi This one-line solution demonstrates something of the power and beauty of the delta function, but I would like to show you a second method, which is much more cumbersome but serves to illustrate the method of integration by parts (Sect. 1.3.6). Solution 2 Using Eq 1.59, we transfer the derivative from \\vu{r}/r^2 to (r^2 + 2) J = - \\int_V \\frac{\\vu{r}}{r^2} \\cdot [\\grad(r^2 + 2)] \\dd \\tau + \\oint_S (r^2 + 2) \\frac{\\vu{r}}{r^2} \\cdot \\dd \\vec{a} The gradient is \\grad(r^2 + 2) = 2 r \\vu{r} so the volume integral becomes \\int \\frac{2}{r} \\dd \\tau = \\int \\frac{2}{r} r^2 \\sin \\theta \\dd r \\dd \\theta \\dd \\phi = 8 \\pi \\int_0 ^R r \\dd r = 4 \\pi R^2 Meanwhile on the boundary of the sphere (where r = R) \\dd \\vec{a} = R^2 \\sin \\theta \\dd \\theta \\dd \\phi \\vu{r} so the surface integral is \\int(R^2 + 2) \\sin \\theta \\dd \\theta \\dd \\phi = 4 \\pi (R^2 + 2) which, all together makes J = -4 \\pi R^2 + r \\pi (R^2 + 2) = 8 \\pi In proper mathematical jargon, \"sphere\" denotes the surface, and \"ball\" the volume it encloses. But physicists are (as usual) sloppy about this sort of thing, and I use the word \"sphere\" for both the surface and the volume. Where the meaning is not clear from the context, I will write \"spherical surface\" or \"spherical volume.\" The language police tell me that the former is redundant and the latter an oxymoron, but a poll of my physics colleagues reveals that this is (for us) the standard usage.","title":"1.5 - The Dirac Delta Function"},{"location":"ch1-5/#15-the-dirac-delta-function","text":"","title":"1.5: The Dirac Delta Function"},{"location":"ch1-5/#151-the-divergence-of-vur-r2","text":"Consider the vector function \\vec{v} = \\frac{1}{r^2} \\vu{r} \\tagl{1.83} At every location, v is directed radially outward (Fig. 1.44); if ever there was a function that ought to have a large positive divergence, this is it. And yet, when you actually calculate the divergence (using Eq. 1.71), you get precisely zero: \\div \\vec{v} = \\frac{1}{r^2} \\pdv{}{r} \\left( r^2 \\frac{1}{r^2} \\right) = \\frac{1}{r^2} \\pdv{}{r} (1) = 0 \\tagl{1.84} (You will have encountered this paradox already, if you worked Prob. 1.16.) The plot thickens when we apply the divergence theorem to this function. Suppose we integrate over a sphere of radius R, centered at the origin (Prob. 1.38b); the surface integral is \\begin{align*} \\oint \\vec{v} \\cdot \\dd \\vec{a} & = \\int \\left( \\frac{1}{R^2} \\vu{r} \\right) \\cdot (R^2 \\sin \\theta \\dd \\theta \\dd \\phi \\vu{r}) \\\\ & = \\left( \\int_0 ^\\pi \\sin \\theta \\dd \\theta \\right) \\left( \\int_0 ^{2 \\pi} \\dd \\phi \\right) = 4 \\pi \\end{align*} \\tagl{1.85} But if we really believe \\eqref{1.84} , then the volume integral \\int \\div \\vec{v} \\dd \\tau must be zero. What the heck is going on here?? The source of the problem is obviously the point r = 0 , where v blows up (and where, in Eq. 1.84, we have unwittingly divided by zero). It is quite true that \\div \\vec{v} = 0 everywhere except the origin, but right at the origin the situation is more complicated. Notice that the surface integral (Eq. 1.85) is independent of R; if the divergence theorem is right (and it is), we should get \\int (\\div \\vec{v}) \\dd \\tau = 4 \\pi for any sphere centered at the origin, no matter how small. Evidently the entire contribution must be coming from the point r = 0 ! Thus, \\div \\vec{v} has the bizarre property that it vanishes everywhere except at one point, and yet its integral (over any volume containing that point) is 4 \\pi . No ordinary function behaves like that. (On the other hand, a physical example does come to mind: the density (mass per unit volume) of a point particle. It's zero except at the exact location of the particle, and yet its integral is finite-namely, the mass of the particle.) What we have stumbled on is a mathematical object known to physicists as the Dirac delta function. It arises in many branches of theoretical physics. Moreover, the specific problem at hand (the divergence of the function \\vu{r} / r^2 ) is not just some arcane curiosity - it is, in fact, central to the whole theory of electrodynamics. So it is worthwhile to pause here and study the Dirac delta function with some care.","title":"1.5.1: The Divergence of  \\vu{r} / r^2"},{"location":"ch1-5/#152-the-one-dimensional-dirac-delta-function","text":"The one-dimensional Dirac delta function, \\delta(x) , can be pictured as an infinitely high, infinitesimally narrow \"spike,\" with area 1 (Fig 1.45). That is to say: \\delta(x) = \\begin{cases} 0, & \\qquad \\text{ if } x \\neq 0 \\\\ \\infty , & \\qquad \\text{ if } x = 0 \\end{cases} \\tagl{1.86} and \\int_{-\\infty} ^{\\infty} \\delta(x) \\dd x = 1 \\tagl{1.87} Technically, \\delta(x) is not a function at all, since its value is not finite at x = 0; in the mathematical literature it is known as a generalized function , or distribution . It is, if you like, the limit of a sequence of functions , such as rectangles R_n(x) of height n and width 1/n , or isosceles triangles T_n(x) of height n and base 2/n (Fig 1.46) If f(x) is some \"ordinary\" function (let's just say continuous, just to be safe), then the product f(x) \\delta(x) is zero everywhere except at x = 0. It follows that f(x) \\delta(x) = f(0) \\delta(x) \\tagl{1.88} This is probably the most important fact about the delta function! Since the product is zero anyway except at x = 0, we may as well replace f(x) by the value it assumes at the origin. In particular, \\int_{-\\infty} ^{\\infty} f(x) \\delta(x) \\dd x = f(0) \\int_{-\\infty} ^{\\infty} \\delta(x) \\dd x = f(0) \\tagl{1.89} Under an integral, then, the delta function \"picks out\" the value of f(x) at a particular point. Of course, the limits of integration need not be all space, as long as the origin is included. We can also shift the spike from x = 0 to some other point, x = a, as well (Fig 1.47) \\delta(x - a) = \\begin{cases} 0, & \\text{ if } x \\neq a \\\\ \\infty , & \\text{ if } x = a \\end{cases} \\quad \\text{ with } \\quad \\int_{-\\infty} ^{\\infty} \\delta(x - a) \\dd x = 1 \\tagl{1.90} Equation \\eqref{1.88} becomes f(x) \\delta(x - a) = f(a) \\delta(x - a) and \\eqref{1.89} generalizes to \\int_{-\\infty} ^{\\infty} f(x) \\delta(x - a) \\dd x = f(a) \\tagl{1.92}","title":"1.5.2: The One-Dimensional Dirac Delta Function"},{"location":"ch1-5/#example-114","text":"Evaluate the integral \\int_0 ^3 x^3 \\delta(x-2) \\dd x Solution Easy peasy. The delta function picks out the value of x^3 at the point x = 2, so the integral is 2^3 = 8 . Notice that if the limits of integration had not included x = 2, then the answer would be 0. Although \\delta(x) itself is not a legitimate function, integrals over \\delta are perfectly acceptable. In fact, it's best to think of the delta function as something that is always intended for use under an integral sign . In particular, two expressions involving delta functions are considered equal if \\int_{-\\infty} ^{\\infty} f(x) D_1 (x) \\dd x = \\int_{-\\infty} ^{\\infty} f(x) D_2 (x) \\dd x \\tagl{1.93} for all (\"ordinary\") functions f(x).","title":"Example 1.14"},{"location":"ch1-5/#example-115","text":"Show that \\delta(kx) = \\frac{1}{|k||}\\delta(x) where k is any (nonzero) constant. (In particular, \\delta(-x) = \\delta(x). ) Solution For an arbitrary test function f(x), consider the integral \\int_{-\\infty} ^{\\infty} f(x) \\delta(kx) \\dd x Changing variables, we let y \\equiv kx so that x = y/k and \\dd x = 1 / k \\dd y . If k is positive, the integration still runs from -\\infty to \\infty , but if k is negative, then x = \\infty implies y = -\\infty , and vice versa, so the order of the limits is reversed. Restoring the \"proper\" order costs a minus sign. Thus \\int_{-\\infty} ^{\\infty} f(x) \\delta(kx) \\dd x + \\pm \\int_{-\\infty} ^{\\infty} f(y/k) \\delta(y) \\frac{dy}{k} = \\pm \\frac{1}{k} f(0) = \\frac{1}{|k|} f(0) (where here the lower signs apply when k is negative, and we account for this neatly by putting absolute value bars around the final k.) Under the integral sign, then, \\delta(kx) serves the same purpose as (1/|k|)\\delta(x) : \\int_{-\\infty} ^{\\infty} f(x) \\delta(kx) \\dd x = \\int_{-\\infty} ^{\\infty} f(x) \\left[ \\frac{1}{|k|} \\delta(x) \\right] \\dd x According to our criterion \\eqref{1.93} , therefore, \\delta(kx) and (1/|k|)\\delta(x) are equal.","title":"Example 1.15"},{"location":"ch1-5/#the-three-dimensional-delta-function","text":"It is easy to generalize the delta function to three dimensions: \\delta ^3 (\\vec{r}) = \\delta(x) \\delta(y) \\delta(z) \\tagl{1.96} This three-dimensional delta function is zero everywhere except at (0, 0, 0), where it blows up. Its volume integral is 1. \\int_{\\text{all space}} \\delta ^3 (\\vec{r}) \\dd \\tau = \\int_{-\\infty} ^{\\infty} \\dd x \\int_{-\\infty} ^{\\infty} \\dd y \\int_{-\\infty} ^{\\infty} \\dd z \\delta(x) \\delta(y) \\delta(z) = 1 \\tagl{1.97} And, generalizing \\eqref{1.92} \\int_{\\text{all space}} f(\\vec{r}) \\delta^3(\\vec{r} - \\vec{a}) \\dd \\tau = f(\\vec{a}) \\tagl{1.98} As in the one-dimensional case, integration with \\delta picks out the value of the function f at the location of the spike. We are now in a position to resolve the paradox introduced in Sect. 1.5.1. As you will recall, we found that the divergence of \\vu{r}/r^2 is zero everywhere except at the origin, and yet its integral over any volume containing the origin is a constant (to wit: 4\\pi ). These are precisely the defining conditions for the Dirac delta function; evidently \\div \\left( \\frac{\\vu{r}}{r^2} \\right) = 4 \\pi \\delta^3(\\vec{r}) \\tagl{1.99} More generally, \\div \\left( \\frac{\\vu{\\gr}}{\\gr^2} \\right) = 4 \\pi \\delta^3(\\gr) \\tagl{1.100} where, as always, \\vec{\\gr} is the separation vector \\vec{\\gr} \\equiv \\vec{r} - \\vec{r'} . Note that differentiation here is with respect to \\vec{r} , while \\vec{r'} is held constant. Incidentally, since \\grad \\left( \\frac{1}{\\gr} \\right) = - \\frac{\\vu{\\gr}}{\\gr ^2} (from Problem 1.13), it follows that \\laplacian \\frac{1}{\\gr} = - 4 \\pi \\delta^3 (\\vec{\\gr}) \\tagl{1.102}","title":"The Three-Dimensional Delta Function"},{"location":"ch1-5/#example-116","text":"Evaluate the integral J = \\int_V (r^2 + 2) \\div \\left( \\frac{\\vu{r}}{r^2} \\right) \\dd \\tau where V is a sphere of radius R centered at the origin. Solution 1 Use \\eqref{1.99} to rewrite the divergence, and \\eqref{1.98} to do the integral: J = \\int_V (r^2 + 2) 4 \\pi \\delta ^3(\\vec{r}) \\dd \\tau = 4 \\pi (0 + 2) = 8 \\pi This one-line solution demonstrates something of the power and beauty of the delta function, but I would like to show you a second method, which is much more cumbersome but serves to illustrate the method of integration by parts (Sect. 1.3.6). Solution 2 Using Eq 1.59, we transfer the derivative from \\vu{r}/r^2 to (r^2 + 2) J = - \\int_V \\frac{\\vu{r}}{r^2} \\cdot [\\grad(r^2 + 2)] \\dd \\tau + \\oint_S (r^2 + 2) \\frac{\\vu{r}}{r^2} \\cdot \\dd \\vec{a} The gradient is \\grad(r^2 + 2) = 2 r \\vu{r} so the volume integral becomes \\int \\frac{2}{r} \\dd \\tau = \\int \\frac{2}{r} r^2 \\sin \\theta \\dd r \\dd \\theta \\dd \\phi = 8 \\pi \\int_0 ^R r \\dd r = 4 \\pi R^2 Meanwhile on the boundary of the sphere (where r = R) \\dd \\vec{a} = R^2 \\sin \\theta \\dd \\theta \\dd \\phi \\vu{r} so the surface integral is \\int(R^2 + 2) \\sin \\theta \\dd \\theta \\dd \\phi = 4 \\pi (R^2 + 2) which, all together makes J = -4 \\pi R^2 + r \\pi (R^2 + 2) = 8 \\pi In proper mathematical jargon, \"sphere\" denotes the surface, and \"ball\" the volume it encloses. But physicists are (as usual) sloppy about this sort of thing, and I use the word \"sphere\" for both the surface and the volume. Where the meaning is not clear from the context, I will write \"spherical surface\" or \"spherical volume.\" The language police tell me that the former is redundant and the latter an oxymoron, but a poll of my physics colleagues reveals that this is (for us) the standard usage.","title":"Example 1.16"},{"location":"ch1-6/","text":"1.6: The Theory of Vector Fields 1.6.1: The Helmholtz Theorem Ever since Faraday, the laws of electricity and magnetism have been expressed in terms of electric and magnetic fields, E and B . Like many physical laws, these are most compactly expressed as differential equations. Since E and B are vectors, the differential equations naturally involve vector derivatives: divergence and curl. Indeed, Maxwell reduced the entire theory to four equations, specifying respectively the divergence and the curl of E and B . Maxwell's formulation raises an important mathematical question: To what extent is a vector function determined by its divergence and curl? In other words, if I tell you that the divergence of F (which stands for E or B , as the case may be) is a specified (scalar) function D, \\div \\vec{F} = D and the curl of F is a specified (vector) function C , \\curl \\vec{F} = \\vec{C} can you then determine the function F ? Well... not quite. For example, as you may have discovered in Prob. 1.20, there are many functions whose divergence and curl are both zero everywhere - the trivial case \\vec{F} = 0 , of course, but also \\vec{F} = yz \\vu{x} + zx \\vu{y} + xy \\vu{z}, \\vec{F} = \\sin x \\cosh y \\vu{x} - \\cos x \\sinh y \\vu{y} , etc. To solve a differential equation you must also be supplied with appropriate boundary conditions . In electrodynamics we typically require that the fields go to zero \"at infinity\" (far away from all charges). With that extra information, the Helmholtz theorem guarantees that the field is uniquely determined by its divergence and curl. (The Helmholtz theorem is discussed in Appendix B.) 1.6.2: Potentials If the curl of a vector field ( F ) vanishes (everywhere), then F can be written as the gradient of a scalar potential (V): \\curl \\vec{F} = 0 \\Longleftrightarrow \\vec{F} = - \\grad V \\tagl{1.103} (The minus sign is purely conventional.) That's the essential burden of the following theorem: Theorem 1 . Curl-less (or 'irrotational') fields. The following conditions are equivalent (that is, F satisfies one if and only if it satisfies all the others): \\tag{a} \\curl \\vec{F} = 0 \\text{ everywhere } \\tag{b} \\int_a ^b \\vec{F} \\cdot \\dd \\vec{l} \\text{ is independent of path, for any given end points} \\tag{c} \\oint \\vec{F} \\cdot \\dd \\vec{l} = 0 \\text{ for any closed loop} \\tag{d} \\vec{F} \\text{ is the gradient of some scalar function: } \\vec{F} = - \\grad V The potential is not unique, and any constant can be added to V with impunity, since this will not affect its gradient. If the divergence of a vector field ( F ) vanishes (everywhere), then F can be expressed as the curl of a vector potential ( A ): \\div \\vec{F} = 0 \\Longleftrightarrow \\vec{F} = \\curl \\vec{A} \\tagl{1.104} That's the main conclusion of the following theorem: Theorem 2 . Divergence-less (or 'solenoidal') fields. The following conditions are equivalent: \\tag{a} \\div \\vec{F} = 0 \\text{ everywhere} \\tag{b} \\int \\vec{F} \\cdot \\dd \\vec{a} \\text{ is independent of surface, for any given boundary line} \\tag{c} \\oint \\vec{F} \\cdot \\dd \\vec{a} = 0 \\text{ for any closed surface.} \\tag{d} \\vec{F} \\text{ is the curl of some vector function: } \\vec{F} = \\curl \\vec{A} The vector potential is not unique - the gradient of any scalar function can be added to A without affecting the curl, since the curl of a gradient is zero. Incidentally, in all cases (whatever its curl and divergence may be), a vector field F can be written as the gradient of a scalar plus the curl of a vector: \\vec{F} = - \\grad V + \\curl \\vec{A} \\quad \\text{(always)} \\tagl{1.50} In physics, the word field denotes generically any function of position (x, y, z) and time (t). But in electrodynamics two particular fields ( E and B ) are of such paramount importance as to preempt the term. Thus technically the potentials are also \"fields,\" but we never call them that.","title":"1.6 - The Theory of Vector Fields"},{"location":"ch1-6/#16-the-theory-of-vector-fields","text":"","title":"1.6: The Theory of Vector Fields"},{"location":"ch1-6/#161-the-helmholtz-theorem","text":"Ever since Faraday, the laws of electricity and magnetism have been expressed in terms of electric and magnetic fields, E and B . Like many physical laws, these are most compactly expressed as differential equations. Since E and B are vectors, the differential equations naturally involve vector derivatives: divergence and curl. Indeed, Maxwell reduced the entire theory to four equations, specifying respectively the divergence and the curl of E and B . Maxwell's formulation raises an important mathematical question: To what extent is a vector function determined by its divergence and curl? In other words, if I tell you that the divergence of F (which stands for E or B , as the case may be) is a specified (scalar) function D, \\div \\vec{F} = D and the curl of F is a specified (vector) function C , \\curl \\vec{F} = \\vec{C} can you then determine the function F ? Well... not quite. For example, as you may have discovered in Prob. 1.20, there are many functions whose divergence and curl are both zero everywhere - the trivial case \\vec{F} = 0 , of course, but also \\vec{F} = yz \\vu{x} + zx \\vu{y} + xy \\vu{z}, \\vec{F} = \\sin x \\cosh y \\vu{x} - \\cos x \\sinh y \\vu{y} , etc. To solve a differential equation you must also be supplied with appropriate boundary conditions . In electrodynamics we typically require that the fields go to zero \"at infinity\" (far away from all charges). With that extra information, the Helmholtz theorem guarantees that the field is uniquely determined by its divergence and curl. (The Helmholtz theorem is discussed in Appendix B.)","title":"1.6.1: The Helmholtz Theorem"},{"location":"ch1-6/#162-potentials","text":"If the curl of a vector field ( F ) vanishes (everywhere), then F can be written as the gradient of a scalar potential (V): \\curl \\vec{F} = 0 \\Longleftrightarrow \\vec{F} = - \\grad V \\tagl{1.103} (The minus sign is purely conventional.) That's the essential burden of the following theorem: Theorem 1 . Curl-less (or 'irrotational') fields. The following conditions are equivalent (that is, F satisfies one if and only if it satisfies all the others): \\tag{a} \\curl \\vec{F} = 0 \\text{ everywhere } \\tag{b} \\int_a ^b \\vec{F} \\cdot \\dd \\vec{l} \\text{ is independent of path, for any given end points} \\tag{c} \\oint \\vec{F} \\cdot \\dd \\vec{l} = 0 \\text{ for any closed loop} \\tag{d} \\vec{F} \\text{ is the gradient of some scalar function: } \\vec{F} = - \\grad V The potential is not unique, and any constant can be added to V with impunity, since this will not affect its gradient. If the divergence of a vector field ( F ) vanishes (everywhere), then F can be expressed as the curl of a vector potential ( A ): \\div \\vec{F} = 0 \\Longleftrightarrow \\vec{F} = \\curl \\vec{A} \\tagl{1.104} That's the main conclusion of the following theorem: Theorem 2 . Divergence-less (or 'solenoidal') fields. The following conditions are equivalent: \\tag{a} \\div \\vec{F} = 0 \\text{ everywhere} \\tag{b} \\int \\vec{F} \\cdot \\dd \\vec{a} \\text{ is independent of surface, for any given boundary line} \\tag{c} \\oint \\vec{F} \\cdot \\dd \\vec{a} = 0 \\text{ for any closed surface.} \\tag{d} \\vec{F} \\text{ is the curl of some vector function: } \\vec{F} = \\curl \\vec{A} The vector potential is not unique - the gradient of any scalar function can be added to A without affecting the curl, since the curl of a gradient is zero. Incidentally, in all cases (whatever its curl and divergence may be), a vector field F can be written as the gradient of a scalar plus the curl of a vector: \\vec{F} = - \\grad V + \\curl \\vec{A} \\quad \\text{(always)} \\tagl{1.50} In physics, the word field denotes generically any function of position (x, y, z) and time (t). But in electrodynamics two particular fields ( E and B ) are of such paramount importance as to preempt the term. Thus technically the potentials are also \"fields,\" but we never call them that.","title":"1.6.2: Potentials"},{"location":"ch2-1/","text":"2.1: The Electric Field 2.1.1: Introduction The fundamental problem electrodynamics hopes to solve is this (Fig 2.1): We have some electric charges q_1, q_2, q_3, \\ldots (call them source charges ); what force do they exert on another charge, Q (call it the test charge )? The positions of the source charges are given (as functions of time); the trajectory of the test particle is to be calculated. In general, both the source charges and the test charge are in motion. The solution to this problem is facilitated by the principle of superposition, which states that the interaction between any two charges is completely unaffected by the presence of others. This means that to determine the force on Q, we can first compute the force \\vec{F_1} , due to q_1 alone (ignoring all the others); then we compute the force \\vec{F_2} , due to q_2 alone, and so in. Finally, we take the vector sum of all these individual forces: \\vec{F} = \\vec{F_1} + \\vec{F_2} + \\vec{F_3} + \\ldots Thus, if we can find the force on Q due to a single source charge q , we are, in principle, done (the rest is just a question of repeating the same operation over and over, and adding it all up) The principle of superposition may seem \"obvious\" to you, but it did not have to be so simple: if the electromagnetic force were proportional to the square of the total source charge, for instance, the principle of superposition would not hold, since (q_1 + q_2)^2 \\neq q_1 ^2 + q_2 ^2 (there would be \"cross terms\" to consider). Superposition is not a logical necessity, but an experimental fact. Well, at first sight this looks very easy: Why don't I just write down the formula for the force on Q due to q, and be done with it? I could , and in Chapter 10 I shall, but you would be shocked to see it at this stage, for not only does the force on Q depend on the separation distance \\gr between the charges (Fig 2.2), it also depends on both their velocities and on the acceleration of q . Moreover, it is not the position, velocity, and acceleration of q right now that matter: electromagnetic \"news\" travels at the speed of light, so what concerns Q is the position, velocity, and acceleration q had at some earlier time, when the message left. Therefore, in spite of the fact that the basic question (\"What is the force on Q due to q?\") is easy to state, it does not pay to confront it head on; rather, we shall go at it by stages. In the meantime, the theory we develop will allow for the solution of more subtle electromagnetic problems that do not present themselves in quite this simple format. To begin with, we shall consider the special case of electrostatics in which all the source charges are stationary (though the test charge may be moving). 2.1.2: Coulomb's Law What is the force on a test charge Q due to a single point charge q, that is at rest a distance \\gr away? The answer (based on experiments) is given by Coulomb's Law : \\vec{F} = \\frac{1}{4 \\pi \\epsilon_0}\\frac{q Q}{\\gr^2} \\vu{\\vec{\\gr}} \\label{2.1} \\tag{2.1} The constant \\epsilon_0 is called (ludicrously) the permittivity of free space . In SI units, where force is in newtons (N), distance in meters (m), and charge in coulombs (C), \\epsilon_0 = 8.85 \\times 10^{-12} \\frac{C^2}{N \\cdot m ^2} In words, the force is proportional to the product of the charges and inversely proportional to the square of the separation distance. As always (Sect 1.1.4), \\vec{\\gr} is the separation vector from \\vec{r'} (the location of q) to \\vec{r} (the location of Q): \\vec{\\gr} = \\vec{r} - \\vec{r}' \\gr is its magnitude, and \\vu{\\gr} is its direction. The force points along the line from q to Q; it is repulsive if q and Q have the same sign, and attractive if their signs are opposite. Coulomb's law and the principle of superposition constitute the physical input for electrostatics - the rest, except for some special properties of matter, is mathematical elaboration of these fundamental rules. 2.1.3: The Electric Field If we have several point charges q_1, q_2, \\ldots , q_n at distances \\gr_1 \\gr_2 \\ldots, \\gr_n from Q , the total force on Q is evidently \\begin{align} \\vec{F} & = & \\vec{F_1} + \\vec{F_2} + \\ldots \\\\ & = & \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q_1 Q}{\\gr_1 ^2} \\vu{\\gr}_1 + \\frac{q_2 Q}{\\gr_2 ^2} \\vu{\\gr}_2 + \\ldots \\right) \\\\ & = & \\frac{Q}{4 \\pi \\epsilon _0} \\left( \\frac{q_1}{\\gr ^2 _1} \\vu{\\gr_1} + \\frac{q_2}{\\gr _2 ^2}\\vu{\\gr_2} + \\ldots \\right) \\end{align} or \\vec{F} = Q \\vec{E} \\label{2.3} \\tag{2.3} where \\vec{E}(\\vec{r}) \\equiv \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i = 1}^n \\frac{q_i}{\\gr_{i}^2} \\vu{\\gr_i} \\label{2.4} \\tag{2.4} E is called the electric field of the source charges. Notice that it is a function of position ( r ), because the separation vectors \\gr_i depend on the location of the field point P (Fig 2.3). But it makes no reference to the test charge Q. The electric field is a vector quantity that varies from point to point and is determined by the configuration of source charges; physically, \\vec{E}(\\vec{r}) is the force per unit charge that would be exerted on a test charge, if you were to place one at P. What exactly is an electric field? I have deliberately begun with what you might call the \"minimal\" interpretation of E , as an intermediate step in the calculation of electric forces. But I encourage you to think of the field as a \"real\" physical entity, filling the space around electric charges. Maxwell himself came to believe that electric and magnetic fields are stresses and strains in an invisible primordial jellylike \"ether.\" Special relativity has forced us to abandon the notion of either, and with it Maxwell's mechanical interpretation of electromagnetic fields. (It is even possible, although cumbersome, to formulate classical electrodynamics as an \"action-at-a-distance\" theory, and dispense with the field concept altogether.) I can't tell you, then, what a field is -- only how to calculate it and what it can do for you once you've got it. Example 2.1 Find the electric field a distance z above the midpoint between two equal charges (q), a distance d apart (Fig. 2.4a) Solution Let \\vec{E_1} be the field of the left charge alone, and \\vec{E_2} that of the right charge alone (Fig. 2.4b). Adding them (vectorially), the horizontal components cancel and the vertical components conspire E_z = 2 \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{\\gr ^2} \\cos \\theta Here \\gr = \\sqrt{z^2 + (d/2)^2} and \\cos \\theta = z / \\gr , so \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{2qz}{\\left[ z^2 + (d/2)^2 \\right]^{3/2}} \\vu{z} Check : When z \\gg d you're so far away that it just looks like a single charge 2q , so the field should reduce to \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{2q}{z^2} \\vu{z} . And it does, just set d \\rightarrow 0 in the formula). 2.1.4: Continuous Charge Distributions Our definition of the electric field (Eq. \\eqref{2.4} ) assumes that the source of the field is a collection of discrete point charges q_i . If, instead, the charge is distributed continuously over some region, the sum becomes an integral (Fig 2.5a): \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{1}{\\gr ^2} \\vu{\\gr} \\dd{q} If the charge is spread out along a line (Fig. 2.5b), with charge-per-unit-length \\lambda then \\dd{q} = \\lambda \\dd{l}' (where \\dd{l}' ) is an element of length along the line); if the charge is smeared out over a surface (Fig. 2.5c) with charge-per-unit-area \\sigma , then \\dd{q} = \\sigma \\dd{a}' (where \\dd{a'} ) is an element of area on the surface); and if the charge fills a volume (Fig 2.5d), with charge-per-unit-volume \\rho , then \\dd{q} = \\rho\\dd{\\tau'} (where \\dd{\\tau'} is an element of volume): dq \\rightarrow \\lambda \\dd{l'} \\sim \\sigma \\dd{a'} \\sim \\rho \\dd{\\tau'} Thus the electric field of a line charge is \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\lambda(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{l'} for a surface charge, \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{a'} and for a volume charge, \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{\\tau'} \\label{2.8} \\tag{2.8} Equation \\eqref{2.8} itself is often referred to as \"Coulomb's law,\" because it is such a short step from the original, and because a volume charge is in a sense the most general and realistic case. Please note carefully the meaning of \\gr in these formulas. Originally, in \\eqref{2.4} , \\gr_i stood for the vector from the source charge q_i to the field point r . Correspondingly, in Eq.s 9-11, \\gr is the vector from \\dd{q} to the field point \\vec{r} . Warning: the unit vector \\vu{\\gr} is not constant: its direction depends on the source point \\vec{r'} , and hence it cannot be taken outside the integrals (9-11). In practice, you must work with Cartesian components ( \\vu{x}, \\vu{y}, \\vu{z} are constant, and do come out) , even if you use curvilinear coordinates to perform the integration. Example 2.2 Find the electric field a distance z above the midpoint of a straight line segment of length 2L that carries a uniform line charge \\lambda (Fig. 2.6). TODO!","title":"2.1 - The Electric Field"},{"location":"ch2-1/#21-the-electric-field","text":"","title":"2.1: The Electric Field"},{"location":"ch2-1/#211-introduction","text":"The fundamental problem electrodynamics hopes to solve is this (Fig 2.1): We have some electric charges q_1, q_2, q_3, \\ldots (call them source charges ); what force do they exert on another charge, Q (call it the test charge )? The positions of the source charges are given (as functions of time); the trajectory of the test particle is to be calculated. In general, both the source charges and the test charge are in motion. The solution to this problem is facilitated by the principle of superposition, which states that the interaction between any two charges is completely unaffected by the presence of others. This means that to determine the force on Q, we can first compute the force \\vec{F_1} , due to q_1 alone (ignoring all the others); then we compute the force \\vec{F_2} , due to q_2 alone, and so in. Finally, we take the vector sum of all these individual forces: \\vec{F} = \\vec{F_1} + \\vec{F_2} + \\vec{F_3} + \\ldots Thus, if we can find the force on Q due to a single source charge q , we are, in principle, done (the rest is just a question of repeating the same operation over and over, and adding it all up) The principle of superposition may seem \"obvious\" to you, but it did not have to be so simple: if the electromagnetic force were proportional to the square of the total source charge, for instance, the principle of superposition would not hold, since (q_1 + q_2)^2 \\neq q_1 ^2 + q_2 ^2 (there would be \"cross terms\" to consider). Superposition is not a logical necessity, but an experimental fact. Well, at first sight this looks very easy: Why don't I just write down the formula for the force on Q due to q, and be done with it? I could , and in Chapter 10 I shall, but you would be shocked to see it at this stage, for not only does the force on Q depend on the separation distance \\gr between the charges (Fig 2.2), it also depends on both their velocities and on the acceleration of q . Moreover, it is not the position, velocity, and acceleration of q right now that matter: electromagnetic \"news\" travels at the speed of light, so what concerns Q is the position, velocity, and acceleration q had at some earlier time, when the message left. Therefore, in spite of the fact that the basic question (\"What is the force on Q due to q?\") is easy to state, it does not pay to confront it head on; rather, we shall go at it by stages. In the meantime, the theory we develop will allow for the solution of more subtle electromagnetic problems that do not present themselves in quite this simple format. To begin with, we shall consider the special case of electrostatics in which all the source charges are stationary (though the test charge may be moving).","title":"2.1.1: Introduction"},{"location":"ch2-1/#212-coulombs-law","text":"What is the force on a test charge Q due to a single point charge q, that is at rest a distance \\gr away? The answer (based on experiments) is given by Coulomb's Law : \\vec{F} = \\frac{1}{4 \\pi \\epsilon_0}\\frac{q Q}{\\gr^2} \\vu{\\vec{\\gr}} \\label{2.1} \\tag{2.1} The constant \\epsilon_0 is called (ludicrously) the permittivity of free space . In SI units, where force is in newtons (N), distance in meters (m), and charge in coulombs (C), \\epsilon_0 = 8.85 \\times 10^{-12} \\frac{C^2}{N \\cdot m ^2} In words, the force is proportional to the product of the charges and inversely proportional to the square of the separation distance. As always (Sect 1.1.4), \\vec{\\gr} is the separation vector from \\vec{r'} (the location of q) to \\vec{r} (the location of Q): \\vec{\\gr} = \\vec{r} - \\vec{r}' \\gr is its magnitude, and \\vu{\\gr} is its direction. The force points along the line from q to Q; it is repulsive if q and Q have the same sign, and attractive if their signs are opposite. Coulomb's law and the principle of superposition constitute the physical input for electrostatics - the rest, except for some special properties of matter, is mathematical elaboration of these fundamental rules.","title":"2.1.2: Coulomb's Law"},{"location":"ch2-1/#213-the-electric-field","text":"If we have several point charges q_1, q_2, \\ldots , q_n at distances \\gr_1 \\gr_2 \\ldots, \\gr_n from Q , the total force on Q is evidently \\begin{align} \\vec{F} & = & \\vec{F_1} + \\vec{F_2} + \\ldots \\\\ & = & \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q_1 Q}{\\gr_1 ^2} \\vu{\\gr}_1 + \\frac{q_2 Q}{\\gr_2 ^2} \\vu{\\gr}_2 + \\ldots \\right) \\\\ & = & \\frac{Q}{4 \\pi \\epsilon _0} \\left( \\frac{q_1}{\\gr ^2 _1} \\vu{\\gr_1} + \\frac{q_2}{\\gr _2 ^2}\\vu{\\gr_2} + \\ldots \\right) \\end{align} or \\vec{F} = Q \\vec{E} \\label{2.3} \\tag{2.3} where \\vec{E}(\\vec{r}) \\equiv \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i = 1}^n \\frac{q_i}{\\gr_{i}^2} \\vu{\\gr_i} \\label{2.4} \\tag{2.4} E is called the electric field of the source charges. Notice that it is a function of position ( r ), because the separation vectors \\gr_i depend on the location of the field point P (Fig 2.3). But it makes no reference to the test charge Q. The electric field is a vector quantity that varies from point to point and is determined by the configuration of source charges; physically, \\vec{E}(\\vec{r}) is the force per unit charge that would be exerted on a test charge, if you were to place one at P. What exactly is an electric field? I have deliberately begun with what you might call the \"minimal\" interpretation of E , as an intermediate step in the calculation of electric forces. But I encourage you to think of the field as a \"real\" physical entity, filling the space around electric charges. Maxwell himself came to believe that electric and magnetic fields are stresses and strains in an invisible primordial jellylike \"ether.\" Special relativity has forced us to abandon the notion of either, and with it Maxwell's mechanical interpretation of electromagnetic fields. (It is even possible, although cumbersome, to formulate classical electrodynamics as an \"action-at-a-distance\" theory, and dispense with the field concept altogether.) I can't tell you, then, what a field is -- only how to calculate it and what it can do for you once you've got it.","title":"2.1.3: The Electric Field"},{"location":"ch2-1/#example-21","text":"Find the electric field a distance z above the midpoint between two equal charges (q), a distance d apart (Fig. 2.4a) Solution Let \\vec{E_1} be the field of the left charge alone, and \\vec{E_2} that of the right charge alone (Fig. 2.4b). Adding them (vectorially), the horizontal components cancel and the vertical components conspire E_z = 2 \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{\\gr ^2} \\cos \\theta Here \\gr = \\sqrt{z^2 + (d/2)^2} and \\cos \\theta = z / \\gr , so \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{2qz}{\\left[ z^2 + (d/2)^2 \\right]^{3/2}} \\vu{z} Check : When z \\gg d you're so far away that it just looks like a single charge 2q , so the field should reduce to \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{2q}{z^2} \\vu{z} . And it does, just set d \\rightarrow 0 in the formula).","title":"Example 2.1"},{"location":"ch2-1/#214-continuous-charge-distributions","text":"Our definition of the electric field (Eq. \\eqref{2.4} ) assumes that the source of the field is a collection of discrete point charges q_i . If, instead, the charge is distributed continuously over some region, the sum becomes an integral (Fig 2.5a): \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{1}{\\gr ^2} \\vu{\\gr} \\dd{q} If the charge is spread out along a line (Fig. 2.5b), with charge-per-unit-length \\lambda then \\dd{q} = \\lambda \\dd{l}' (where \\dd{l}' ) is an element of length along the line); if the charge is smeared out over a surface (Fig. 2.5c) with charge-per-unit-area \\sigma , then \\dd{q} = \\sigma \\dd{a}' (where \\dd{a'} ) is an element of area on the surface); and if the charge fills a volume (Fig 2.5d), with charge-per-unit-volume \\rho , then \\dd{q} = \\rho\\dd{\\tau'} (where \\dd{\\tau'} is an element of volume): dq \\rightarrow \\lambda \\dd{l'} \\sim \\sigma \\dd{a'} \\sim \\rho \\dd{\\tau'} Thus the electric field of a line charge is \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\lambda(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{l'} for a surface charge, \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{a'} and for a volume charge, \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{\\tau'} \\label{2.8} \\tag{2.8} Equation \\eqref{2.8} itself is often referred to as \"Coulomb's law,\" because it is such a short step from the original, and because a volume charge is in a sense the most general and realistic case. Please note carefully the meaning of \\gr in these formulas. Originally, in \\eqref{2.4} , \\gr_i stood for the vector from the source charge q_i to the field point r . Correspondingly, in Eq.s 9-11, \\gr is the vector from \\dd{q} to the field point \\vec{r} . Warning: the unit vector \\vu{\\gr} is not constant: its direction depends on the source point \\vec{r'} , and hence it cannot be taken outside the integrals (9-11). In practice, you must work with Cartesian components ( \\vu{x}, \\vu{y}, \\vu{z} are constant, and do come out) , even if you use curvilinear coordinates to perform the integration.","title":"2.1.4: Continuous Charge Distributions"},{"location":"ch2-1/#example-22","text":"Find the electric field a distance z above the midpoint of a straight line segment of length 2L that carries a uniform line charge \\lambda (Fig. 2.6). TODO!","title":"Example 2.2"},{"location":"ch2-2/","text":"2.2: Divergence and Curl of Electrostatic Fields 2.2.1 Field Lines, Flux, and Gauss' Law In principle, we are done with the subject of electrostatics. Eq. 2.8 tells us how to compute the field of a charge distribution, and Eq. 2.3 tells us what the force on a charge Q placed in this field will be. Unfortunately, as you may have discovered, the integrals involved in computing E can be formidable, even for reasonably simple charge distributions. Much of the rest of electrostatics is devoted to assembling a bag of tools and tricks for avoiding these integrals. It all begins with the divergence and curl of E . I shall calculate the divergence of E directly from Eq. 2.8 in section 2.2.2, but first I want to show you a more qualitative, and perhaps more illuminating, intuitive approach. Let's begin with the simplest possible case: a single point charge q , situated at the origin: \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\vu{\\vec{r}} \\tag{2.10} \\label{2.10} To get a \"feel\" for this field, I might sketch a few representative vectors, as in Fig. 2.12a. Because the field falls off like 1/r^2 , the vectors get shorter as you go farther away from the origin; they always point radially outward. But there is a nicer way to represent this field, and that's to connect up the arrows, to form field lines (Fig. 2.12b). You might think that I have thereby thrown away information about the strength of the field, which was contained in the length of the arrows. But actually I have not. The magnitude of the field is indicated by the density of the field lines: it's strong near the center where the field lines are close together, and weak farther out, where they are relatively far apart. In truth, the field-line diagram is deceptive, when I draw it on a two-dimensional surface, for the density of lines passing through a circle of radius r is the total number divided by the circumference ( n / 2 \\pi r ), which goes like (1/r) , not (1/r^2) . But if you imagine the model in three dimensions (a pincushion with needles sticking out in all directions), then the density of lines is the total number divided by the area of the sphere (n/4 \\pi r^2) , which does go like (1/r^2) . Such diagrams are also convenient for representing more complicated fields. Of course, the number of lines you draw depends on how lazy you are (and how sharp your pencil is), though you ought to include enough to get an accurate sense of the field, and you must be consistent: if q gets 8 lines, then 2q deserves 16. And you must space them fairly - they emanate from a point charge symmetrically in all directions. Field lines begin on positive charges and end on negative ones; they cannot simply terminate in midair, though they may extend out to infinity. Moreover, field lines can never cross - at the intersection the field would have two different directions at once! With all this in mind, it is easy to sketch the field of any simple configuration of point charges: Begin by drawing the lines in the neighborhood of each charge, and then connect them up or extend them to infinity (Figs. 2.13 and 2.14) In this model, the flux of E through a surface S, \\Phi_E \\equiv \\int _S \\vec{E} \\cdot \\dd{\\vec{a}} \\label{2.11} \\tag{2.11} is a measure of the \"number of lines\" passing through S. I put this in quotes because of course we can only draw a representative sample of field lines - the total number would be infinite. But for a given sampling rate the flux is proportional to the number of lines drawn, because the field strength, remember, is proportional to the density of field lines (the number per unit area), and hence \\vec{E} \\cdot \\dd{\\vec{a}} is proportional to the number of lines passing through the infinitesimal area \\dd{\\vec{a}} . (The dot product picks out the component of \\dd{\\vec{a}} along the direction of E , as indicated in Fig 2.15. It is the area in the plane perpendicular to E that we have in mind when we say that the density of field lines is the number per unit area). This suggests that the flux through any closed surface is a measure of the total charge inside. For the field lines that originate on a positive charge must either pass out through the surface or else terminate on a negative charge inside (Fig 2.16a). On the other hand, a charge outside the surface will contribute nothing to the total flux, since its field lines pass in one side and out the other (Fig 2.16b). This is the essence of Gauss's law. Now let's make it quantitative. In the case of a point charge q at the origin, the flux of E through a spherical surface or radius r is \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\int \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{r^2} \\vu{r} \\right) \\cdot \\left( r^2 \\sin \\theta \\dd{\\theta} \\dd{\\phi} \\vu{r} \\right) = \\frac{1}{\\epsilon_0} q \\label{2.12} \\tag{2.12} Notice that the radius of the sphere cancels out, for while the surface area goes up as r^2 , the field goes down as 1/r^2 , so the product is constant. In terms of the field-line picture, this makes good sense, since the same number of field lines pass through any sphere centered at the origin, regardless of its size. In fact, it didn't have to be a sphere - any closed surface, whatever its shape, would be pierced by the same number of field lines. Evidently, the flux through any surface enclosing the charge is q / \\epsilon_0 . Now suppose that instead of a single charge at the origin, we have a bunch of charges scattered about. According to the principle of superposition, the total field is the (vector) sum of all the individual fields: \\vec{E} = \\sum _{i = 1} ^\\nu \\vec{E}_i The flux through a surface that encloses them all is \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = \\sum _{i = 1}^n \\left( \\oint \\vec{E_i} \\cdot \\dd{\\vec{a}} \\right) = \\sum_{i = 1}^n \\left( \\frac{1}{\\epsilon_0} q_i \\right) For any closed surface, then \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} \\label{2.13} \\tag{2.13} where Q_{enc} is the total charge enclosed within the surface. This is the quantitative statement of Gauss's law. Although it contains no information that was not already present in Coulomb's law plus the principle of superposition, it is of almost magical power, as you will see in Sect. 2.2.3. Notice that it all hinges on the 1/r^2 character of Coulomb's law; without that, the crucial cancellation of the r 's in \\eqref{2.12} would not take place, and the total flux of E would depend on the surface chosen, not merely on the total charge enclosed. Other 1/r^2 forces (I am thinking particularly of Newton's law of universal gravitation) will obey \"Gauss's laws\" of their own, and the applications we develop here carry over directly. As it stands, Gauss's law is an integral equation, but we can easily turn it into a differential one by applying the divergence theorem: \\oint_{S} \\vec{E} \\cdot \\dd{\\vec{a}} = \\int_{\\mathscr{V}} (\\div{\\vec{E}}) \\dd{\\tau} Rewriting Q_{enc} in terms of the charge density \\rho we have Q_{enc} = \\int_{\\mathscr{V}} \\rho \\dd{\\tau} So Gauss's law becomes \\int_{\\mathscr{V}} (\\div{\\vec{E}}) \\dd{\\tau} = \\int_{\\mathscr{V}} \\left( \\frac{\\rho}{\\epsilon_0} \\dd{\\tau} \\right) And since this holds for any volume, the integrands must be equal: \\nabla \\cdot \\vec{E} = \\frac{1}{\\epsilon_0} \\rho \\label{2.14} \\tag{2.14} Equation \\eqref{2.14} carries the same message as \\eqref{2.13} ; it is Gauss's law in differential form . The differential version is tidier, but the integral form has the advantage that it accommodates point, line, and surface charges more naturally. 2.2.2: The Divergence of E Let's go back now, and calculate the divergence of \\vec{E} directly from Eq. 2.8: \\vec{E}(\\vec{r}) = \\frac{1}{4\\pi\\epsilon_0} \\int_{\\text{all space}} \\frac{\\vu{\\gr}}{\\gr ^2} \\rho(\\vec{r}') \\dd{\\tau'} \\label{2.15} (Originally the integration was over the volume occupied by the charge, but I may as well extend it to all space, since \\rho = 0 in the exterior region anyway.) Noting that the r-dependence is contained in \\gr = r - r' , we have \\div{\\vec{E}} = \\frac{1}{4\\pi\\epsilon_0} \\int \\vec{\\nabla} \\cdot \\left( \\frac{\\vu{\\gr}}{\\gr^2} \\right) \\rho(\\vec{r'}) \\dd{\\tau'} We calculated this divergence in Section 1.5: \\div{\\left( \\frac{\\vu{\\gr}}{\\gr^2} \\right)} = 4 \\pi \\delta ^3(\\gr) Thus \\div{\\vec{E}} = \\frac{1}{4\\pi\\epsilon_0} \\int 4 \\pi \\delta^3(\\vec{r} - \\vec{r'}) \\rho(\\vec{r'}) \\dd{\\tau'} = \\frac{1}{\\epsilon_0} \\rho(\\vec{r}) \\label{2.16} \\tag{2.16} which is Gauss's law in differential form \\eqref{2.14} . To recover the integral form \\eqref{2.13} we run the previous argument in reverse - integrate over a volume and apply the divergence theorem: \\int_{\\mathscr{V}} \\div{\\vec{E}} \\dd{\\tau} = \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} \\int_{\\mathscr{V}} \\rho \\dd{\\tau} = \\frac{1}{\\epsilon_0} Q_{enc} 2.2.3: Applications of Gauss's Law I must interrupt the theoretical development at this point to show you the extraordinary power of Gauss's law, in integral form. When symmetry permits, it affords by far the quickest and easiest way of computing electric fields. I'll illustrate the method with a series of examples. Example 2.3 Find the field outside a uniformly charged solid sphere of radius R and total charge q Solution Imagine a spherical surface at radius r > R (Fig. 2.18). This is called a Gaussian surface in the trade. Gauss's law says that \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} and in this case Q_{enc} = q . At first glance this doesn't seem to get us very far, because the quantity we want (E) is buried inside the surface integral. Luckily, symmetry allows us to extract E from under the integral sign: E certainly points radially outward, as does \\dd{\\vec{a}} , so we can drop the dot product \\int_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\int_{\\mathscr{S}} | \\vec{E} | da and the magnitude of E is constant over the Gaussian surface, so it comes outside the integral: \\int_{S} | E | da = |E| \\int_{S} da = E 4 \\pi r^2 Thus |\\vec{E}|4\\pi r^2 = \\frac{1}{\\epsilon_0} q or \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r} Notice a remarkable feature of this result: the field outside the sphere is exactly the same as it would have been if all the charge had been concentrated at the center. Gauss's law is always true , but not always useful . If \\rho had not been uniform (or at any rate, not spherically symmetrical), or if I had chosen some other shape for my Gaussian surface, it would have still been true that the flux of \\vec{E} is q / \\epsilon_0 , but \\vec{E} would not have pointed in the same direction as \\dd{\\vec{a}} , and its magnitude would not have been constant over the surface, and without that I cannot get |\\vec{E}| outside the integral. Symmetry is crucial to this application of Gauss's law. As far as I know, there are only three kinds of symmetry that work: Spherical symmetry. Make your Gaussian survace a concentric sphere. Cylindrical symmetry. Make your Gaussian surface a coaxial cylinder. Plane symmetry. Use a Gaussian \"pillbox\" that straddles the surface. Although 2 and 3 technically require infinitely long cylinders, and planes extending to infinity, we shall often use them to get approximate answers for \"long\" cylinders or \"large\" planes, at points far from the edges. Example 2.4 A long cylinder (Fig 2.21) carries a charge density that is proportional to the distance from the axis: \\rho = ks for some constant k . Find the electric field inside this cylinder. Solution : Draw a Gaussian cylinder of length l and radius s. For this surface, Gauss's law states \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} The enclosed charge is \\begin{align} Q_{enc} & = & \\int \\rho \\dd{\\tau} \\\\ & = & \\int(ks')(s' \\dd{s'} \\dd{\\phi} \\dd{z}) \\\\ & = & 2 \\pi k l \\int_{0}^{s} s'^2 \\dd{s'} \\\\ & = & \\frac{2}{3} \\pi k l s^3 \\end{align} (I used the volume element appropriate to cylindrical coordinates, and integrated \\phi from 0 to 2\\pi , \\dd{z} from 0 to l . I put a prime on the integration variable s' to distinguish it from the radius s of the Gaussian surface.) Now, symmetry dictates that \\vec{E} must point radially outward, so for the curved portion of the Gaussian cylinder we have: \\int \\vec{E} \\cdot \\dd{\\vec{a}} = \\int | \\vec{E}| da = | \\vec{E}| \\int da = |\\vec{E} 2 \\pi s l while the two ends contribute nothing (here \\vec{E} is perpendicular to \\dd{\\vec{a}} ). Thus, |\\vec{E} | 2 \\pi s l = \\frac{1}{\\epsilon_0} \\frac{2}{3} \\pi k l s^3 or, finally, \\vec{E} = \\frac{1}{3\\epsilon_0} k s^2 \\vu{s} Example 2.5 An infinite plane carries a uniform surface charge \\sigma . Find its electric field. Solution Draw a Gaussian pillbox, extending equal distances above and below the plane (Fig. 2.22). Apply Gauss's law to this surface: \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} In this case, Q = \\sigma A , where A is the area of the lid of the pillbox. By symmetry, \\vec{E} points away from the plane (upward for points above, downward for points below). So the top and bottom surfaces yield \\int \\vec{E} \\cdot \\dd{\\vec{a}} = 2 A |\\vec{E}|, whereas the sides contribute nothing. Thus 2 A | \\vec{E} | = \\frac{1}{\\epsilon_0} \\sigma A or \\vec{E} = \\frac{\\sigma}{2 \\epsilon_0} \\vu{n} where \\vu{n} is a unit vector pointing away from the surface. In Prob 2.6, you obtained this same result by a much more laborious method. It seems surprising, at first, that the field of an infinite plane is independent of how fara away you are . What about the 1/r^2 in Coulomb's law? The point is that as you move farther and farther away from the plane, more and more charge comes into your \"field of view,\" and this compensates for the diminishing influence of any particular piece. The electric field of a sphere falls off like 1/r^2 ; the electric field of an infinite line falls off like 1/r ; and the electric field of an infinite plane does not fall off at all (you cannot escape from an infinite plane). Although the direct use of Gauss's law to compute fields is limited to cases of spherical, cylindrical, and planar symmetry, we can put together combinations of objects posessing such symmetry, even though the arrangement as a whole is not symmetrical. For example, invoking the principle of superposition, we could find the field in the vicinity of two uniformly charged parallel cylinders, or a sphere near an infinite charged plane. Example 2.6 Two infinite parallel planes carry equal but opposite uniform charge densities \\pm \\sigma (Fig 2.23). Find the field in each of the three regions: (i) to the left of both, (ii) between them, (iii) to the right of both. Solution The left plate produces a field (1/2 \\epsilon_0)\\sigma , which points away from it (Fig. 2.24) to the left in region in (i) and to the right in regions (ii) and (iii). The right plate, being negatively charged, produces a field (1/2 \\epsilon_0)\\sigma which points toward it - to the right in regions (i) and (ii) and to the left in region (iii). The two fields cancel in regions (i) and (iii); they conspire in region (ii). Conclusion: The field between the plates is \\sigma / \\epsilon_0 , and points to the right; elsewhere it is zero. 2.2.4: The Curl of E I'll calculate the curl of \\vec{E} as I did the divergence in Sect 2.2.1, by studying first the simplest possible configuration: a point charge at the origin. In this case \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r} Now, a glance at Fig 2.12 should convince you that the curl of this field has to be zero, but I suppose we ought to come up with something a little more rigorous than that. What if we calculate the line integral of this field from some point \\vec{a} to some other point \\vec{b} (Fig 2.29): \\int_{\\vec{a}}^{\\vec{b}} \\vec{E} \\cdot \\dd{\\vec{l}} In spherical coordinates, \\dd{\\vec{l}} = \\dd{r} \\vu{r} + r \\dd{\\theta} \\vu{\\theta} + r \\sin \\theta \\dd{\\phi} \\vu{\\phi} , so \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\dd{r} Therefore, \\int_{\\vec{a}}^{\\vec{b}} \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{a}^{b} \\frac{q}{r^2} \\dd{r} \\\\ = \\left.\\frac{-1}{4 \\pi \\epsilon_0} \\frac{q}{r} \\right|_{r_a} ^{r_b} \\\\ = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{r_a} - \\frac{q}{r_b} \\right) The integral around a closed path is evidently zero (for then r_a = r_b ): \\oint \\vec{E} \\cdot \\dd\\vec{l} = 0 \\label{2.19} \\tag{2.19} and hence, applying Stokes' theorem \\curl{\\vec{E}} = 0 \\label{2.20} \\tag{2.20} Now, I proved eqs. \\eqref{2.19} and \\eqref{2.20} only for the field of a single point charge at the origin, but these results make no reference to what is, after all, a perfectly arbitrary choice of coordinates; they hold no matter where the charge is located. Moreover, if we have many charges, the principle of superposition states that the total field is a vector sum of their individual fields: \\vec{E} = \\vec{E_1} + \\vec{E_2} + \\ldots so \\curl{\\vec{E}} = \\curl{(\\vec{E_1} + \\vec{E_2} + \\ldots)} = (\\curl{\\vec{E_1}}) + (\\curl{\\vec{E_2}}) + \\ldots = 0 Thus, Eqs. \\eqref{2.19} and \\eqref{2.20} hold for any static charge distribution whatever.","title":"2.2 - Divergence and Curl of Electrostatic Fields"},{"location":"ch2-2/#22-divergence-and-curl-of-electrostatic-fields","text":"","title":"2.2: Divergence and Curl of Electrostatic Fields"},{"location":"ch2-2/#221-field-lines-flux-and-gauss-law","text":"In principle, we are done with the subject of electrostatics. Eq. 2.8 tells us how to compute the field of a charge distribution, and Eq. 2.3 tells us what the force on a charge Q placed in this field will be. Unfortunately, as you may have discovered, the integrals involved in computing E can be formidable, even for reasonably simple charge distributions. Much of the rest of electrostatics is devoted to assembling a bag of tools and tricks for avoiding these integrals. It all begins with the divergence and curl of E . I shall calculate the divergence of E directly from Eq. 2.8 in section 2.2.2, but first I want to show you a more qualitative, and perhaps more illuminating, intuitive approach. Let's begin with the simplest possible case: a single point charge q , situated at the origin: \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\vu{\\vec{r}} \\tag{2.10} \\label{2.10} To get a \"feel\" for this field, I might sketch a few representative vectors, as in Fig. 2.12a. Because the field falls off like 1/r^2 , the vectors get shorter as you go farther away from the origin; they always point radially outward. But there is a nicer way to represent this field, and that's to connect up the arrows, to form field lines (Fig. 2.12b). You might think that I have thereby thrown away information about the strength of the field, which was contained in the length of the arrows. But actually I have not. The magnitude of the field is indicated by the density of the field lines: it's strong near the center where the field lines are close together, and weak farther out, where they are relatively far apart. In truth, the field-line diagram is deceptive, when I draw it on a two-dimensional surface, for the density of lines passing through a circle of radius r is the total number divided by the circumference ( n / 2 \\pi r ), which goes like (1/r) , not (1/r^2) . But if you imagine the model in three dimensions (a pincushion with needles sticking out in all directions), then the density of lines is the total number divided by the area of the sphere (n/4 \\pi r^2) , which does go like (1/r^2) . Such diagrams are also convenient for representing more complicated fields. Of course, the number of lines you draw depends on how lazy you are (and how sharp your pencil is), though you ought to include enough to get an accurate sense of the field, and you must be consistent: if q gets 8 lines, then 2q deserves 16. And you must space them fairly - they emanate from a point charge symmetrically in all directions. Field lines begin on positive charges and end on negative ones; they cannot simply terminate in midair, though they may extend out to infinity. Moreover, field lines can never cross - at the intersection the field would have two different directions at once! With all this in mind, it is easy to sketch the field of any simple configuration of point charges: Begin by drawing the lines in the neighborhood of each charge, and then connect them up or extend them to infinity (Figs. 2.13 and 2.14) In this model, the flux of E through a surface S, \\Phi_E \\equiv \\int _S \\vec{E} \\cdot \\dd{\\vec{a}} \\label{2.11} \\tag{2.11} is a measure of the \"number of lines\" passing through S. I put this in quotes because of course we can only draw a representative sample of field lines - the total number would be infinite. But for a given sampling rate the flux is proportional to the number of lines drawn, because the field strength, remember, is proportional to the density of field lines (the number per unit area), and hence \\vec{E} \\cdot \\dd{\\vec{a}} is proportional to the number of lines passing through the infinitesimal area \\dd{\\vec{a}} . (The dot product picks out the component of \\dd{\\vec{a}} along the direction of E , as indicated in Fig 2.15. It is the area in the plane perpendicular to E that we have in mind when we say that the density of field lines is the number per unit area). This suggests that the flux through any closed surface is a measure of the total charge inside. For the field lines that originate on a positive charge must either pass out through the surface or else terminate on a negative charge inside (Fig 2.16a). On the other hand, a charge outside the surface will contribute nothing to the total flux, since its field lines pass in one side and out the other (Fig 2.16b). This is the essence of Gauss's law. Now let's make it quantitative. In the case of a point charge q at the origin, the flux of E through a spherical surface or radius r is \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\int \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{r^2} \\vu{r} \\right) \\cdot \\left( r^2 \\sin \\theta \\dd{\\theta} \\dd{\\phi} \\vu{r} \\right) = \\frac{1}{\\epsilon_0} q \\label{2.12} \\tag{2.12} Notice that the radius of the sphere cancels out, for while the surface area goes up as r^2 , the field goes down as 1/r^2 , so the product is constant. In terms of the field-line picture, this makes good sense, since the same number of field lines pass through any sphere centered at the origin, regardless of its size. In fact, it didn't have to be a sphere - any closed surface, whatever its shape, would be pierced by the same number of field lines. Evidently, the flux through any surface enclosing the charge is q / \\epsilon_0 . Now suppose that instead of a single charge at the origin, we have a bunch of charges scattered about. According to the principle of superposition, the total field is the (vector) sum of all the individual fields: \\vec{E} = \\sum _{i = 1} ^\\nu \\vec{E}_i The flux through a surface that encloses them all is \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = \\sum _{i = 1}^n \\left( \\oint \\vec{E_i} \\cdot \\dd{\\vec{a}} \\right) = \\sum_{i = 1}^n \\left( \\frac{1}{\\epsilon_0} q_i \\right) For any closed surface, then \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} \\label{2.13} \\tag{2.13} where Q_{enc} is the total charge enclosed within the surface. This is the quantitative statement of Gauss's law. Although it contains no information that was not already present in Coulomb's law plus the principle of superposition, it is of almost magical power, as you will see in Sect. 2.2.3. Notice that it all hinges on the 1/r^2 character of Coulomb's law; without that, the crucial cancellation of the r 's in \\eqref{2.12} would not take place, and the total flux of E would depend on the surface chosen, not merely on the total charge enclosed. Other 1/r^2 forces (I am thinking particularly of Newton's law of universal gravitation) will obey \"Gauss's laws\" of their own, and the applications we develop here carry over directly. As it stands, Gauss's law is an integral equation, but we can easily turn it into a differential one by applying the divergence theorem: \\oint_{S} \\vec{E} \\cdot \\dd{\\vec{a}} = \\int_{\\mathscr{V}} (\\div{\\vec{E}}) \\dd{\\tau} Rewriting Q_{enc} in terms of the charge density \\rho we have Q_{enc} = \\int_{\\mathscr{V}} \\rho \\dd{\\tau} So Gauss's law becomes \\int_{\\mathscr{V}} (\\div{\\vec{E}}) \\dd{\\tau} = \\int_{\\mathscr{V}} \\left( \\frac{\\rho}{\\epsilon_0} \\dd{\\tau} \\right) And since this holds for any volume, the integrands must be equal: \\nabla \\cdot \\vec{E} = \\frac{1}{\\epsilon_0} \\rho \\label{2.14} \\tag{2.14} Equation \\eqref{2.14} carries the same message as \\eqref{2.13} ; it is Gauss's law in differential form . The differential version is tidier, but the integral form has the advantage that it accommodates point, line, and surface charges more naturally.","title":"2.2.1 Field Lines, Flux, and Gauss' Law"},{"location":"ch2-2/#222-the-divergence-of-e","text":"Let's go back now, and calculate the divergence of \\vec{E} directly from Eq. 2.8: \\vec{E}(\\vec{r}) = \\frac{1}{4\\pi\\epsilon_0} \\int_{\\text{all space}} \\frac{\\vu{\\gr}}{\\gr ^2} \\rho(\\vec{r}') \\dd{\\tau'} \\label{2.15} (Originally the integration was over the volume occupied by the charge, but I may as well extend it to all space, since \\rho = 0 in the exterior region anyway.) Noting that the r-dependence is contained in \\gr = r - r' , we have \\div{\\vec{E}} = \\frac{1}{4\\pi\\epsilon_0} \\int \\vec{\\nabla} \\cdot \\left( \\frac{\\vu{\\gr}}{\\gr^2} \\right) \\rho(\\vec{r'}) \\dd{\\tau'} We calculated this divergence in Section 1.5: \\div{\\left( \\frac{\\vu{\\gr}}{\\gr^2} \\right)} = 4 \\pi \\delta ^3(\\gr) Thus \\div{\\vec{E}} = \\frac{1}{4\\pi\\epsilon_0} \\int 4 \\pi \\delta^3(\\vec{r} - \\vec{r'}) \\rho(\\vec{r'}) \\dd{\\tau'} = \\frac{1}{\\epsilon_0} \\rho(\\vec{r}) \\label{2.16} \\tag{2.16} which is Gauss's law in differential form \\eqref{2.14} . To recover the integral form \\eqref{2.13} we run the previous argument in reverse - integrate over a volume and apply the divergence theorem: \\int_{\\mathscr{V}} \\div{\\vec{E}} \\dd{\\tau} = \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} \\int_{\\mathscr{V}} \\rho \\dd{\\tau} = \\frac{1}{\\epsilon_0} Q_{enc}","title":"2.2.2: The Divergence of E"},{"location":"ch2-2/#223-applications-of-gausss-law","text":"I must interrupt the theoretical development at this point to show you the extraordinary power of Gauss's law, in integral form. When symmetry permits, it affords by far the quickest and easiest way of computing electric fields. I'll illustrate the method with a series of examples.","title":"2.2.3: Applications of Gauss's Law"},{"location":"ch2-2/#example-23","text":"Find the field outside a uniformly charged solid sphere of radius R and total charge q Solution Imagine a spherical surface at radius r > R (Fig. 2.18). This is called a Gaussian surface in the trade. Gauss's law says that \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} and in this case Q_{enc} = q . At first glance this doesn't seem to get us very far, because the quantity we want (E) is buried inside the surface integral. Luckily, symmetry allows us to extract E from under the integral sign: E certainly points radially outward, as does \\dd{\\vec{a}} , so we can drop the dot product \\int_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\int_{\\mathscr{S}} | \\vec{E} | da and the magnitude of E is constant over the Gaussian surface, so it comes outside the integral: \\int_{S} | E | da = |E| \\int_{S} da = E 4 \\pi r^2 Thus |\\vec{E}|4\\pi r^2 = \\frac{1}{\\epsilon_0} q or \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r} Notice a remarkable feature of this result: the field outside the sphere is exactly the same as it would have been if all the charge had been concentrated at the center. Gauss's law is always true , but not always useful . If \\rho had not been uniform (or at any rate, not spherically symmetrical), or if I had chosen some other shape for my Gaussian surface, it would have still been true that the flux of \\vec{E} is q / \\epsilon_0 , but \\vec{E} would not have pointed in the same direction as \\dd{\\vec{a}} , and its magnitude would not have been constant over the surface, and without that I cannot get |\\vec{E}| outside the integral. Symmetry is crucial to this application of Gauss's law. As far as I know, there are only three kinds of symmetry that work: Spherical symmetry. Make your Gaussian survace a concentric sphere. Cylindrical symmetry. Make your Gaussian surface a coaxial cylinder. Plane symmetry. Use a Gaussian \"pillbox\" that straddles the surface. Although 2 and 3 technically require infinitely long cylinders, and planes extending to infinity, we shall often use them to get approximate answers for \"long\" cylinders or \"large\" planes, at points far from the edges.","title":"Example 2.3"},{"location":"ch2-2/#example-24","text":"A long cylinder (Fig 2.21) carries a charge density that is proportional to the distance from the axis: \\rho = ks for some constant k . Find the electric field inside this cylinder. Solution : Draw a Gaussian cylinder of length l and radius s. For this surface, Gauss's law states \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} The enclosed charge is \\begin{align} Q_{enc} & = & \\int \\rho \\dd{\\tau} \\\\ & = & \\int(ks')(s' \\dd{s'} \\dd{\\phi} \\dd{z}) \\\\ & = & 2 \\pi k l \\int_{0}^{s} s'^2 \\dd{s'} \\\\ & = & \\frac{2}{3} \\pi k l s^3 \\end{align} (I used the volume element appropriate to cylindrical coordinates, and integrated \\phi from 0 to 2\\pi , \\dd{z} from 0 to l . I put a prime on the integration variable s' to distinguish it from the radius s of the Gaussian surface.) Now, symmetry dictates that \\vec{E} must point radially outward, so for the curved portion of the Gaussian cylinder we have: \\int \\vec{E} \\cdot \\dd{\\vec{a}} = \\int | \\vec{E}| da = | \\vec{E}| \\int da = |\\vec{E} 2 \\pi s l while the two ends contribute nothing (here \\vec{E} is perpendicular to \\dd{\\vec{a}} ). Thus, |\\vec{E} | 2 \\pi s l = \\frac{1}{\\epsilon_0} \\frac{2}{3} \\pi k l s^3 or, finally, \\vec{E} = \\frac{1}{3\\epsilon_0} k s^2 \\vu{s}","title":"Example 2.4"},{"location":"ch2-2/#example-25","text":"An infinite plane carries a uniform surface charge \\sigma . Find its electric field. Solution Draw a Gaussian pillbox, extending equal distances above and below the plane (Fig. 2.22). Apply Gauss's law to this surface: \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} In this case, Q = \\sigma A , where A is the area of the lid of the pillbox. By symmetry, \\vec{E} points away from the plane (upward for points above, downward for points below). So the top and bottom surfaces yield \\int \\vec{E} \\cdot \\dd{\\vec{a}} = 2 A |\\vec{E}|, whereas the sides contribute nothing. Thus 2 A | \\vec{E} | = \\frac{1}{\\epsilon_0} \\sigma A or \\vec{E} = \\frac{\\sigma}{2 \\epsilon_0} \\vu{n} where \\vu{n} is a unit vector pointing away from the surface. In Prob 2.6, you obtained this same result by a much more laborious method. It seems surprising, at first, that the field of an infinite plane is independent of how fara away you are . What about the 1/r^2 in Coulomb's law? The point is that as you move farther and farther away from the plane, more and more charge comes into your \"field of view,\" and this compensates for the diminishing influence of any particular piece. The electric field of a sphere falls off like 1/r^2 ; the electric field of an infinite line falls off like 1/r ; and the electric field of an infinite plane does not fall off at all (you cannot escape from an infinite plane). Although the direct use of Gauss's law to compute fields is limited to cases of spherical, cylindrical, and planar symmetry, we can put together combinations of objects posessing such symmetry, even though the arrangement as a whole is not symmetrical. For example, invoking the principle of superposition, we could find the field in the vicinity of two uniformly charged parallel cylinders, or a sphere near an infinite charged plane.","title":"Example 2.5"},{"location":"ch2-2/#example-26","text":"Two infinite parallel planes carry equal but opposite uniform charge densities \\pm \\sigma (Fig 2.23). Find the field in each of the three regions: (i) to the left of both, (ii) between them, (iii) to the right of both. Solution The left plate produces a field (1/2 \\epsilon_0)\\sigma , which points away from it (Fig. 2.24) to the left in region in (i) and to the right in regions (ii) and (iii). The right plate, being negatively charged, produces a field (1/2 \\epsilon_0)\\sigma which points toward it - to the right in regions (i) and (ii) and to the left in region (iii). The two fields cancel in regions (i) and (iii); they conspire in region (ii). Conclusion: The field between the plates is \\sigma / \\epsilon_0 , and points to the right; elsewhere it is zero.","title":"Example 2.6"},{"location":"ch2-2/#224-the-curl-of-e","text":"I'll calculate the curl of \\vec{E} as I did the divergence in Sect 2.2.1, by studying first the simplest possible configuration: a point charge at the origin. In this case \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r} Now, a glance at Fig 2.12 should convince you that the curl of this field has to be zero, but I suppose we ought to come up with something a little more rigorous than that. What if we calculate the line integral of this field from some point \\vec{a} to some other point \\vec{b} (Fig 2.29): \\int_{\\vec{a}}^{\\vec{b}} \\vec{E} \\cdot \\dd{\\vec{l}} In spherical coordinates, \\dd{\\vec{l}} = \\dd{r} \\vu{r} + r \\dd{\\theta} \\vu{\\theta} + r \\sin \\theta \\dd{\\phi} \\vu{\\phi} , so \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\dd{r} Therefore, \\int_{\\vec{a}}^{\\vec{b}} \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{a}^{b} \\frac{q}{r^2} \\dd{r} \\\\ = \\left.\\frac{-1}{4 \\pi \\epsilon_0} \\frac{q}{r} \\right|_{r_a} ^{r_b} \\\\ = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{r_a} - \\frac{q}{r_b} \\right) The integral around a closed path is evidently zero (for then r_a = r_b ): \\oint \\vec{E} \\cdot \\dd\\vec{l} = 0 \\label{2.19} \\tag{2.19} and hence, applying Stokes' theorem \\curl{\\vec{E}} = 0 \\label{2.20} \\tag{2.20} Now, I proved eqs. \\eqref{2.19} and \\eqref{2.20} only for the field of a single point charge at the origin, but these results make no reference to what is, after all, a perfectly arbitrary choice of coordinates; they hold no matter where the charge is located. Moreover, if we have many charges, the principle of superposition states that the total field is a vector sum of their individual fields: \\vec{E} = \\vec{E_1} + \\vec{E_2} + \\ldots so \\curl{\\vec{E}} = \\curl{(\\vec{E_1} + \\vec{E_2} + \\ldots)} = (\\curl{\\vec{E_1}}) + (\\curl{\\vec{E_2}}) + \\ldots = 0 Thus, Eqs. \\eqref{2.19} and \\eqref{2.20} hold for any static charge distribution whatever.","title":"2.2.4: The Curl of E"},{"location":"ch2-3/","text":"2.3: Electric Potential 2.3.1: Introduction to Potential The electric field E is not just any old vector function. It is a very special kind of vector function: one whose curl id zero. \\vec{E} = y \\vu{x} , for example, could not possibly be an electrostatic field; no set of charges, regardless of their sizes and positions, could ever produce such a field. We're going to exploit this special property of electric fields to reduce a vector problem (finding E ) to a much simpler scalar problem. The first theorem in Sect 1.6.2 asserts that any vector whose curl is zero is equal to the gradient of some scalar. What I'm going to do now amounts to a proof of that claim, in the context of electrostatics. Because \\nabla \\cross \\vec{E} = 0 , the line integral of E around any closed loop is zero (that follows from Stokes' theorem). Because \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = 0 , the line integral of E from point a to point b is the same for all paths (otherwise you could go out along path (i) and return along path (ii) - Fig 2.30 - and obtain \\oint \\vec{E} \\cdot \\dd{\\vec{l}} \\neq 0 ). Because the line integral is independent of path, we can define a function V(\\vec{r}) \\equiv - \\int _{O} ^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\label{2.21} \\tag{2.21} Here O is some standard reference point on which we have agreed beforehand; V then depends only on the point \\vec{r} . It is called the electric potential . The potential difference between two points a and b is \\begin{align} V(\\vec{b}) - V(\\vec{a}) & = & -\\int_{O}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} + \\int_{O}^{\\vec{a}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ & = & -\\int_{O}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} - \\int_{\\vec{a}}^{O} \\vec{E}\\cdot \\dd{\\vec{l}} \\\\ & = & - \\int_{\\vec{a}} ^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} \\end{align} \\label{2.22} \\tag{2.22} Now, the fundamental theorem for gradients states that V(\\vec{b}) - V(\\vec{a}) = \\int_{\\vec{a}} ^{\\vec{b}} (\\grad{V}) \\cdot \\dd{\\vec{l}} so \\int_{\\vec{a}}^{\\vec{b}} (\\grad{V})\\cdot \\dd{\\vec{l}} = - \\int_{\\vec{a}}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} Since, finally, this is true for any points a and b , the integrands must be equal: \\vec{E} = - \\grad{V} \\label{2.23} \\tag{2.23} Equation \\eqref{2.23} is the differential version of \\eqref{2.21} ; it says that the electric field is the gradient of a scalar potential, which is what we set out to prove. Notice the subtle but crucial role played by path independence (or, equivalently, the fact that \\nabla \\times \\vec{E} = 0 ) in this argument. If the line integral of E depended on the path taken, then the \"definition\" of V \\eqref{2.21} would be nonsense. It simply would not define a function, since changing the path would alter the value of V(\\vec{r}) . By the way, don't let the minus sign in \\eqref{2.23} distract you; it carries over from \\eqref{2.21} and is largely a matter of convention. 2.3.2: Comments on Potential The name . The word \"potential\" is a hideous misnomer because it inevitably reminds you of potential energy . This is particularly insidious, because there is a connection between \"potential\" and \"potential energy,\" as you will see in Sect 2.4. I'm sorry that it is impossible to escape this word. The best I can do is to insist once and for all that \"potential\" and \"potential energy\" are completely different terms and should, by all rights, have different names. Incidentially, a surface over which the potential is constant is called an equipotential . Advantage of the potential formulation . If you know V, you can easily get E - just take the gradient: \\vec{E} =- \\grad{V} . This is quite extraordinary when you stop to think about it, for E is a vector quantity (three components), but V is a scalar (one component). How can one function possibly contain all the information that three independent functions carry? The answer is that the three components of E are not really as independent as they look; in fact, they are explicitly interrelated by the very condition we started with, \\nabla \\times \\vec{E} = 0 . In terms of components, \\pdv{E_x}{y} = \\pdv{E_y}{x}, \\qquad \\pdv{E_z}{y} = \\pdv{E_y}{z}, \\qquad \\pdv{E_x}{z} = \\pdv{E_z}{x} This brings us back to my observation at the beginning of Sect 2.3.1: E is a very special kind of vector.What the potential formulation does is to exploit this feature to maximum advantage, reducing a vector problem to a scalar one, in which there is no need to fuss with components. The reference point \\mathscr{O} . There is an essential ambiguity in the definition of potential, since the choice of reference point \\mathscr{O} was arbitrary. Changing reference points amounts to adding a constant K to the potential: V'(r) = -\\int_{\\mathscr{O}'}^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = - \\int_{\\mathscr{O}'} ^{\\mathscr{O}} \\vec{E} \\cdot \\dd{\\vec{l}} - \\int_{\\mathscr{O}}^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = K + V(\\vec{r}) where K is the line integral of E from the old reference point \\mathscr{O} to the new one \\mathscr{O}' . Of course, adding a constant to V will not affect the potential difference between two points, since the K's cancel out. Nor does the ambiguity affect the gradient of V: \\grad{V'} = \\grad{V} since the derivative of a constant is zero. That's why all such V's, differing only in their choice of reference point, correspond to the same field E Potential as such carries no real physical significance, for at any given point we can adjust its value at will by suitable relocation of \\mathscr{O} . In this sense, it is rather like altitude: if I ask you how high Denver is, you will probably tell me its height above sea level, because that is a convenient and traditional reference point. But we could as well agree to measure altitude above Washington, DC, or Greenwich, or wherever. That would add (or rather, subtract) a fixed amount from all our sea-level readings, but it wouldn't change anything about the real world. The only quantity of interest is the difference in altitude between two points, and that is the same whatever your reference level. Having said this, however, there is a \"natural\" spot to use for \\mathscr{O} in electrostatics - analogous to sea level for altitude - and that is a point infinitely far from the charge. Ordinarily, then, we s\"set the zero of potential at infinity.\" (Since V(\\mathscr{O}) = 0 , choosing a reference point is equivalent to selecting a place where V is to be zero.) But I must warn you that there is one special circumstance in which this convention fails: when the charge distribution itself extends to infinity. The symptom of trouble, in such cases, is that the potential blows up. For instance, the field of a uniformly charged plane is (\\sigma / 2 \\epsilon_0) \\vu{n} , as we found in Ex 2.5; if we naively put \\mathscr{O} = \\infty , then the potential at height z above the plane becomes V(z) = - \\int_{\\infty}^{z}\\frac{1}{2\\epsilon_0} \\sigma \\dd{z} = - \\frac{1}{2\\epsilon_0} \\sigma(z - \\infty) The remedy is simply to choose some other reference point (in this example you might use a point on the plane). Notice that the difficulty occurs only in textbook problems; in \"real life\" there is no such thing as a charge distribution that goes on forever, and we can always use infinity as our reference point. Potential obeys the superposition principle . The original superposition principle pertains to the force on a test charge Q. It says that the total force on Q is the vector sum of the forces attributable to the source charges individually: \\vec{F} = \\vec{F_1} + \\vec{F_2} + \\ldots Dividing through by Q, we see that the electric field, too, obeys the superposition principle: \\vec{E} = \\vec{E_1} + \\vec{E_2} + \\ldots \\label{2.38} Integrating from the common reference point to \\vec{r} , it follows that the potential also satisfies such a principle: V = V_1 + V_2 + \\ldots That is, the potential at any given point is the sum of the potentials due to all the source charges separately. Only this time it is an ordinary sum, not a vector sum, which makes it a lot easier to work with. Units of Potential . In our units, force is measured in newtons and charge in coulombs, so electric fields are in newtons per coulomb. Accordingly, potential is newton-meters per coulomb, or joules per coulomb. A joule per coulomb is a volt . 2.3.3: Poisson's Equation and Laplace's Equation We found in Sect 2.3.1 that the electric field can be written as the gradient of a scalar potential \\vec{E} = - \\grad{V} The question arises, what do the divergence and curl of E , \\div{\\vec{E}} = \\frac{\\rho}{\\epsilon_0} \\qquad \\text{ and } \\qquad \\curl{\\vec{E}} = 0 look like, in terms of V? Well, \\div{\\vec{E}} = \\div(-\\grad{V}) = -\\laplacian{V} , so, apart from that persistent minus sign, the divergence of E is the Laplacian of V. Gauss's law, then, says \\laplacian{V} = -\\frac{\\rho}{\\epsilon_0} \\label{2.24} This is known as Poisson's equation . In regions where there is no charge, so \\rho = 0 , Poisson's equation reduces to Laplace's equation, \\laplacian{V} = 0 \\label{2.25} We'll explore this equation more fully in Chapter 3. So much for Gauss's law. What about the curl law? This says that \\curl{\\vec{E}} = \\curl(-\\grad{V}) = 0 But that's no condition on V - curl of gradient is always zero. Of course, we used the curl law to show that E could be expressed as the gradient of a scalar, so it's not really surprising that this works out: \\curl{\\vec{E}} = 0 permits our definition of V; in return, \\vec{E} = - \\grad{V} guarantees \\curl{\\vec{E}} = 0 . It only takes one differential equation (Poisson's) to determine V, because V is a scalar. For \\vec{E} we needed two, the divergence and the curl. 2.3.4: The potential of a Localized Charge Distribution I defined V in terms of \\vec{E} \\eqref{2.21} . Ordinarily, though, it's E that we're looking for (if we already knew E , there wouldn't be much point in calculating V). The idea is that it might be easier to get V first, and then calculate E by taking the gradient. Typically, then, we know where the charge is (that is, we know \\rho ), and we want to find V. Now, Poisson's equation relates V and \\rho , but unfortunately it's \"the wrong way round\": it would give us \\rho if we knew V, whereas we want V, knowing \\rho . What we must do, then, is \"invert\" Poisson's equation. That's the program for this section, although I shall do it by roundabout means, beginning, as always, with a point charge at the origin. The electric field is \\vec{E} = (1 / 4 \\pi \\epsilon_0)(1 / r^2) \\vu{r} , and \\dd{\\vec{l}} = \\dd{r} \\vu{r} , and \\dd{\\vec{l}} = \\dd{r} \\vu{r} + r \\dd{\\theta} \\vu{\\theta} + r \\sin \\theta \\dd{\\theta} \\vu{\\phi} , so \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\dd{r} Setting the reference point at infinity, the potential of a point charge q at the origin is V(r) = - \\int_{\\mathscr{O}} ^r \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = \\frac{-1}{4 \\pi \\epsilon_0} \\int_{\\infty}^r \\frac{q}{r' ^2} \\dd{r'} \\\\ = \\left.\\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r'} \\right| ^r _{\\infty} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r} (You see here the advantage of using infinity for the reference point: it kills the lower limit on the integral.) Notice the sign of V; presumably the conventional minus sign in the definition was chosen in order to make the potential of a positive charge come out positive. It is useful to remember that regions of positive charge are potential \"hills,\" and electric field points \"downhill\" from plus toward minus. In general, the potential of a point charge q is V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{\\gr} where \\gr , as always, is the distance from q to \\vec{r} (Fig 2.32). Invoking the superposition principle, then, the potential of a collection of charges is V(r) = \\frac{1}{4\\pi \\epsilon_0} \\sum_{i=1} ^n \\frac{q_i}{\\gr _i} or, for a continuous distribution, V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r}')}{\\gr} \\dd{\\tau'} \\label{2.29} \\tag{2.29} This is the equation we were looking for, telling us how to compute V when we know \\rho ; it is, if you like, the \"solution\" to Poisson's equation, for a localized charge distribution. Compare \\eqref{2.29} with the corresponding formula for the electric field in terms of \\rho : \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{\\tau'} The main point is that the pesky unit vector \\vu{\\gr} is gone, so there is no need to fuss with components. The potentials of line and surface charges are V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\lambda(\\vec{r'})}{\\gr} \\dd{l'} \\qquad \\text{ and } \\qquad V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma(\\vec{r'})}{\\gr} \\dd{a'} I should warn you that everything in this section is predicated on the assumption that the reference point is at infinity. This is hardly apparent in \\eqref{2.29} , but remember that we got the equation from the potential of a point charge at the origin, (1/4 \\pi \\epsilon_0) (q / r) , which is valid only when \\mathscr{O} = \\infty . If you try to apply these formulas to one of those artificial problems in which the charge itself extends to infinity, the integral will diverge. 2.3.5: Boundary Conditions In the typical electrostatic problem you are given a source charge distribution \\rho , and you want to find the electric field \\vec{E} it produces. Unless the symmetry of the problem allows a solution by Gauss's law, it is generally to your advantage to calculate the potential first, as an intermediate step. These are the three fundamental quantities of electrostatics: \\rho , \\vec{E} , and V . We have, in the course of our discussion, derived all six formulas interrelating them. These equations are neatly summarized in Fig. 2.35. We began with just two experimental observations: (1) the principle of superposition - a broad general rule applying to all electromagnetic forces, and (2) Coulomb's law - the fundamental law of electrostatics. From these, all else followed. You may have noticed, in studying the exercises in this chapter, that the electric field always undergoes a discontinuity when you cross a surface charge \\sigma . In fact, it is a simple matter to find the amount by which E changes at such a boundary. Suppose we draw a wafer-thin Gaussian pillbox, extending just barely over the edge in each direction (Fig. 2.36). Gauss's law says that \\oint _{S} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} = \\frac{1}{\\epsilon_0} \\sigma A where A is the area of the pillbox lid. If \\sigma varies from point to point or the surface is curved, we can simply pick A to be extremely small. Now, the sides of the pillbox contribute nothing to the flux, in the limit as the thickness \\epsilon goes to zero, so we are left with E_{above}^{\\perp} - E_{below} ^{\\perp} = \\frac{1}{\\epsilon_0} \\sigma \\label{2.31} \\tag{2.31} where E_{above}^{\\perp} denotes the component of \\vec{E} that is perpendicular to the surface immediately above, and E_{below} ^{\\perp} is the same, only just below the surface. For consistency, let \"upward\" be the positive direction for both. Conclusion: the normal component of \\vec{E} is discontinuous by an amount \\sigma / \\epsilon_0 at any boundary. In particular, where there is no surface charge, \\vec{E}^{\\perp} is continuous, as for instance at the surface of a uniformly charged solid sphere. The tangential component of \\vec{E} , by contrast, is always continuous. For if we apply Eq. 2.19, \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = 0 to the thin rectangular loop of Fig 2.37, the ends give nothing (as \\epsilon \\rightarrow 0 ), and the sides give (E_{above} ^{\\parallel} l - E_{below} ^{\\parallel} l) , so \\vec{E}_{above} ^{\\parallel} = \\vec{E}_{below} ^{\\parallel} \\label{2.32} \\tag{2.32} where \\vec{E}^{\\parallel} stands for the components of \\vec{E} parallel to the surface. The boundary conditions on \\vec{E} (Eqs. \\eqref{2.31} and \\eqref{2.32} ) can be combined into a single formula: \\vec{E}_{above} - \\vec{E}_{below} = \\frac{\\sigma}{\\epsilon_0} \\vu{n} \\label{2.33} where \\vu{n} is a unit vector perpendicular to the surface, pointing from \"below\" to \"above.\" The potential, meanwhile, is continuous across any boundary (Fig 2.38), since V_{above} - V_{below} = -\\int_{a}^{b} \\vec{E} \\cdot \\dd{\\vec{l}} as the path length shrinks to zero, so too does the integral V_{above} = V_{below} \\label{2.34} \\tag{2.34} However, the gradient of V inherits the discontinuity in \\vec{E} , since \\vec{E} - \\grad{V} , so \\grad{V}_{above} - \\grad{V}_{below} = - \\frac{\\sigma}{\\epsilon_0} \\vu{n} or more conveniently \\pdv{V_{above}}{n} - \\pdv{V_{below}}{n} = - \\frac{1}{\\epsilon_0} \\sigma \\label{2.36} \\tag{2.36} where \\pdv{V}{n} = \\grad{V} \\cdot \\vu{n} denotes the normal derivative of V (that is, the rate of change in the direction perpendicular to the surface). Please note that these boundary conditions relate the fields and potentials just above and just below the surface. For example, the derivatives in \\eqref{2.36} are the limiting values as we approach the surface from either side.","title":"2.3 - Electric Potential"},{"location":"ch2-3/#23-electric-potential","text":"","title":"2.3: Electric Potential"},{"location":"ch2-3/#231-introduction-to-potential","text":"The electric field E is not just any old vector function. It is a very special kind of vector function: one whose curl id zero. \\vec{E} = y \\vu{x} , for example, could not possibly be an electrostatic field; no set of charges, regardless of their sizes and positions, could ever produce such a field. We're going to exploit this special property of electric fields to reduce a vector problem (finding E ) to a much simpler scalar problem. The first theorem in Sect 1.6.2 asserts that any vector whose curl is zero is equal to the gradient of some scalar. What I'm going to do now amounts to a proof of that claim, in the context of electrostatics. Because \\nabla \\cross \\vec{E} = 0 , the line integral of E around any closed loop is zero (that follows from Stokes' theorem). Because \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = 0 , the line integral of E from point a to point b is the same for all paths (otherwise you could go out along path (i) and return along path (ii) - Fig 2.30 - and obtain \\oint \\vec{E} \\cdot \\dd{\\vec{l}} \\neq 0 ). Because the line integral is independent of path, we can define a function V(\\vec{r}) \\equiv - \\int _{O} ^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\label{2.21} \\tag{2.21} Here O is some standard reference point on which we have agreed beforehand; V then depends only on the point \\vec{r} . It is called the electric potential . The potential difference between two points a and b is \\begin{align} V(\\vec{b}) - V(\\vec{a}) & = & -\\int_{O}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} + \\int_{O}^{\\vec{a}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ & = & -\\int_{O}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} - \\int_{\\vec{a}}^{O} \\vec{E}\\cdot \\dd{\\vec{l}} \\\\ & = & - \\int_{\\vec{a}} ^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} \\end{align} \\label{2.22} \\tag{2.22} Now, the fundamental theorem for gradients states that V(\\vec{b}) - V(\\vec{a}) = \\int_{\\vec{a}} ^{\\vec{b}} (\\grad{V}) \\cdot \\dd{\\vec{l}} so \\int_{\\vec{a}}^{\\vec{b}} (\\grad{V})\\cdot \\dd{\\vec{l}} = - \\int_{\\vec{a}}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} Since, finally, this is true for any points a and b , the integrands must be equal: \\vec{E} = - \\grad{V} \\label{2.23} \\tag{2.23} Equation \\eqref{2.23} is the differential version of \\eqref{2.21} ; it says that the electric field is the gradient of a scalar potential, which is what we set out to prove. Notice the subtle but crucial role played by path independence (or, equivalently, the fact that \\nabla \\times \\vec{E} = 0 ) in this argument. If the line integral of E depended on the path taken, then the \"definition\" of V \\eqref{2.21} would be nonsense. It simply would not define a function, since changing the path would alter the value of V(\\vec{r}) . By the way, don't let the minus sign in \\eqref{2.23} distract you; it carries over from \\eqref{2.21} and is largely a matter of convention.","title":"2.3.1: Introduction to Potential"},{"location":"ch2-3/#232-comments-on-potential","text":"The name . The word \"potential\" is a hideous misnomer because it inevitably reminds you of potential energy . This is particularly insidious, because there is a connection between \"potential\" and \"potential energy,\" as you will see in Sect 2.4. I'm sorry that it is impossible to escape this word. The best I can do is to insist once and for all that \"potential\" and \"potential energy\" are completely different terms and should, by all rights, have different names. Incidentially, a surface over which the potential is constant is called an equipotential . Advantage of the potential formulation . If you know V, you can easily get E - just take the gradient: \\vec{E} =- \\grad{V} . This is quite extraordinary when you stop to think about it, for E is a vector quantity (three components), but V is a scalar (one component). How can one function possibly contain all the information that three independent functions carry? The answer is that the three components of E are not really as independent as they look; in fact, they are explicitly interrelated by the very condition we started with, \\nabla \\times \\vec{E} = 0 . In terms of components, \\pdv{E_x}{y} = \\pdv{E_y}{x}, \\qquad \\pdv{E_z}{y} = \\pdv{E_y}{z}, \\qquad \\pdv{E_x}{z} = \\pdv{E_z}{x} This brings us back to my observation at the beginning of Sect 2.3.1: E is a very special kind of vector.What the potential formulation does is to exploit this feature to maximum advantage, reducing a vector problem to a scalar one, in which there is no need to fuss with components. The reference point \\mathscr{O} . There is an essential ambiguity in the definition of potential, since the choice of reference point \\mathscr{O} was arbitrary. Changing reference points amounts to adding a constant K to the potential: V'(r) = -\\int_{\\mathscr{O}'}^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = - \\int_{\\mathscr{O}'} ^{\\mathscr{O}} \\vec{E} \\cdot \\dd{\\vec{l}} - \\int_{\\mathscr{O}}^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = K + V(\\vec{r}) where K is the line integral of E from the old reference point \\mathscr{O} to the new one \\mathscr{O}' . Of course, adding a constant to V will not affect the potential difference between two points, since the K's cancel out. Nor does the ambiguity affect the gradient of V: \\grad{V'} = \\grad{V} since the derivative of a constant is zero. That's why all such V's, differing only in their choice of reference point, correspond to the same field E Potential as such carries no real physical significance, for at any given point we can adjust its value at will by suitable relocation of \\mathscr{O} . In this sense, it is rather like altitude: if I ask you how high Denver is, you will probably tell me its height above sea level, because that is a convenient and traditional reference point. But we could as well agree to measure altitude above Washington, DC, or Greenwich, or wherever. That would add (or rather, subtract) a fixed amount from all our sea-level readings, but it wouldn't change anything about the real world. The only quantity of interest is the difference in altitude between two points, and that is the same whatever your reference level. Having said this, however, there is a \"natural\" spot to use for \\mathscr{O} in electrostatics - analogous to sea level for altitude - and that is a point infinitely far from the charge. Ordinarily, then, we s\"set the zero of potential at infinity.\" (Since V(\\mathscr{O}) = 0 , choosing a reference point is equivalent to selecting a place where V is to be zero.) But I must warn you that there is one special circumstance in which this convention fails: when the charge distribution itself extends to infinity. The symptom of trouble, in such cases, is that the potential blows up. For instance, the field of a uniformly charged plane is (\\sigma / 2 \\epsilon_0) \\vu{n} , as we found in Ex 2.5; if we naively put \\mathscr{O} = \\infty , then the potential at height z above the plane becomes V(z) = - \\int_{\\infty}^{z}\\frac{1}{2\\epsilon_0} \\sigma \\dd{z} = - \\frac{1}{2\\epsilon_0} \\sigma(z - \\infty) The remedy is simply to choose some other reference point (in this example you might use a point on the plane). Notice that the difficulty occurs only in textbook problems; in \"real life\" there is no such thing as a charge distribution that goes on forever, and we can always use infinity as our reference point. Potential obeys the superposition principle . The original superposition principle pertains to the force on a test charge Q. It says that the total force on Q is the vector sum of the forces attributable to the source charges individually: \\vec{F} = \\vec{F_1} + \\vec{F_2} + \\ldots Dividing through by Q, we see that the electric field, too, obeys the superposition principle: \\vec{E} = \\vec{E_1} + \\vec{E_2} + \\ldots \\label{2.38} Integrating from the common reference point to \\vec{r} , it follows that the potential also satisfies such a principle: V = V_1 + V_2 + \\ldots That is, the potential at any given point is the sum of the potentials due to all the source charges separately. Only this time it is an ordinary sum, not a vector sum, which makes it a lot easier to work with. Units of Potential . In our units, force is measured in newtons and charge in coulombs, so electric fields are in newtons per coulomb. Accordingly, potential is newton-meters per coulomb, or joules per coulomb. A joule per coulomb is a volt .","title":"2.3.2: Comments on Potential"},{"location":"ch2-3/#233-poissons-equation-and-laplaces-equation","text":"We found in Sect 2.3.1 that the electric field can be written as the gradient of a scalar potential \\vec{E} = - \\grad{V} The question arises, what do the divergence and curl of E , \\div{\\vec{E}} = \\frac{\\rho}{\\epsilon_0} \\qquad \\text{ and } \\qquad \\curl{\\vec{E}} = 0 look like, in terms of V? Well, \\div{\\vec{E}} = \\div(-\\grad{V}) = -\\laplacian{V} , so, apart from that persistent minus sign, the divergence of E is the Laplacian of V. Gauss's law, then, says \\laplacian{V} = -\\frac{\\rho}{\\epsilon_0} \\label{2.24} This is known as Poisson's equation . In regions where there is no charge, so \\rho = 0 , Poisson's equation reduces to Laplace's equation, \\laplacian{V} = 0 \\label{2.25} We'll explore this equation more fully in Chapter 3. So much for Gauss's law. What about the curl law? This says that \\curl{\\vec{E}} = \\curl(-\\grad{V}) = 0 But that's no condition on V - curl of gradient is always zero. Of course, we used the curl law to show that E could be expressed as the gradient of a scalar, so it's not really surprising that this works out: \\curl{\\vec{E}} = 0 permits our definition of V; in return, \\vec{E} = - \\grad{V} guarantees \\curl{\\vec{E}} = 0 . It only takes one differential equation (Poisson's) to determine V, because V is a scalar. For \\vec{E} we needed two, the divergence and the curl.","title":"2.3.3: Poisson's Equation and Laplace's Equation"},{"location":"ch2-3/#234-the-potential-of-a-localized-charge-distribution","text":"I defined V in terms of \\vec{E} \\eqref{2.21} . Ordinarily, though, it's E that we're looking for (if we already knew E , there wouldn't be much point in calculating V). The idea is that it might be easier to get V first, and then calculate E by taking the gradient. Typically, then, we know where the charge is (that is, we know \\rho ), and we want to find V. Now, Poisson's equation relates V and \\rho , but unfortunately it's \"the wrong way round\": it would give us \\rho if we knew V, whereas we want V, knowing \\rho . What we must do, then, is \"invert\" Poisson's equation. That's the program for this section, although I shall do it by roundabout means, beginning, as always, with a point charge at the origin. The electric field is \\vec{E} = (1 / 4 \\pi \\epsilon_0)(1 / r^2) \\vu{r} , and \\dd{\\vec{l}} = \\dd{r} \\vu{r} , and \\dd{\\vec{l}} = \\dd{r} \\vu{r} + r \\dd{\\theta} \\vu{\\theta} + r \\sin \\theta \\dd{\\theta} \\vu{\\phi} , so \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\dd{r} Setting the reference point at infinity, the potential of a point charge q at the origin is V(r) = - \\int_{\\mathscr{O}} ^r \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = \\frac{-1}{4 \\pi \\epsilon_0} \\int_{\\infty}^r \\frac{q}{r' ^2} \\dd{r'} \\\\ = \\left.\\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r'} \\right| ^r _{\\infty} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r} (You see here the advantage of using infinity for the reference point: it kills the lower limit on the integral.) Notice the sign of V; presumably the conventional minus sign in the definition was chosen in order to make the potential of a positive charge come out positive. It is useful to remember that regions of positive charge are potential \"hills,\" and electric field points \"downhill\" from plus toward minus. In general, the potential of a point charge q is V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{\\gr} where \\gr , as always, is the distance from q to \\vec{r} (Fig 2.32). Invoking the superposition principle, then, the potential of a collection of charges is V(r) = \\frac{1}{4\\pi \\epsilon_0} \\sum_{i=1} ^n \\frac{q_i}{\\gr _i} or, for a continuous distribution, V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r}')}{\\gr} \\dd{\\tau'} \\label{2.29} \\tag{2.29} This is the equation we were looking for, telling us how to compute V when we know \\rho ; it is, if you like, the \"solution\" to Poisson's equation, for a localized charge distribution. Compare \\eqref{2.29} with the corresponding formula for the electric field in terms of \\rho : \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{\\tau'} The main point is that the pesky unit vector \\vu{\\gr} is gone, so there is no need to fuss with components. The potentials of line and surface charges are V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\lambda(\\vec{r'})}{\\gr} \\dd{l'} \\qquad \\text{ and } \\qquad V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma(\\vec{r'})}{\\gr} \\dd{a'} I should warn you that everything in this section is predicated on the assumption that the reference point is at infinity. This is hardly apparent in \\eqref{2.29} , but remember that we got the equation from the potential of a point charge at the origin, (1/4 \\pi \\epsilon_0) (q / r) , which is valid only when \\mathscr{O} = \\infty . If you try to apply these formulas to one of those artificial problems in which the charge itself extends to infinity, the integral will diverge.","title":"2.3.4: The potential of a Localized Charge Distribution"},{"location":"ch2-3/#235-boundary-conditions","text":"In the typical electrostatic problem you are given a source charge distribution \\rho , and you want to find the electric field \\vec{E} it produces. Unless the symmetry of the problem allows a solution by Gauss's law, it is generally to your advantage to calculate the potential first, as an intermediate step. These are the three fundamental quantities of electrostatics: \\rho , \\vec{E} , and V . We have, in the course of our discussion, derived all six formulas interrelating them. These equations are neatly summarized in Fig. 2.35. We began with just two experimental observations: (1) the principle of superposition - a broad general rule applying to all electromagnetic forces, and (2) Coulomb's law - the fundamental law of electrostatics. From these, all else followed. You may have noticed, in studying the exercises in this chapter, that the electric field always undergoes a discontinuity when you cross a surface charge \\sigma . In fact, it is a simple matter to find the amount by which E changes at such a boundary. Suppose we draw a wafer-thin Gaussian pillbox, extending just barely over the edge in each direction (Fig. 2.36). Gauss's law says that \\oint _{S} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} = \\frac{1}{\\epsilon_0} \\sigma A where A is the area of the pillbox lid. If \\sigma varies from point to point or the surface is curved, we can simply pick A to be extremely small. Now, the sides of the pillbox contribute nothing to the flux, in the limit as the thickness \\epsilon goes to zero, so we are left with E_{above}^{\\perp} - E_{below} ^{\\perp} = \\frac{1}{\\epsilon_0} \\sigma \\label{2.31} \\tag{2.31} where E_{above}^{\\perp} denotes the component of \\vec{E} that is perpendicular to the surface immediately above, and E_{below} ^{\\perp} is the same, only just below the surface. For consistency, let \"upward\" be the positive direction for both. Conclusion: the normal component of \\vec{E} is discontinuous by an amount \\sigma / \\epsilon_0 at any boundary. In particular, where there is no surface charge, \\vec{E}^{\\perp} is continuous, as for instance at the surface of a uniformly charged solid sphere. The tangential component of \\vec{E} , by contrast, is always continuous. For if we apply Eq. 2.19, \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = 0 to the thin rectangular loop of Fig 2.37, the ends give nothing (as \\epsilon \\rightarrow 0 ), and the sides give (E_{above} ^{\\parallel} l - E_{below} ^{\\parallel} l) , so \\vec{E}_{above} ^{\\parallel} = \\vec{E}_{below} ^{\\parallel} \\label{2.32} \\tag{2.32} where \\vec{E}^{\\parallel} stands for the components of \\vec{E} parallel to the surface. The boundary conditions on \\vec{E} (Eqs. \\eqref{2.31} and \\eqref{2.32} ) can be combined into a single formula: \\vec{E}_{above} - \\vec{E}_{below} = \\frac{\\sigma}{\\epsilon_0} \\vu{n} \\label{2.33} where \\vu{n} is a unit vector perpendicular to the surface, pointing from \"below\" to \"above.\" The potential, meanwhile, is continuous across any boundary (Fig 2.38), since V_{above} - V_{below} = -\\int_{a}^{b} \\vec{E} \\cdot \\dd{\\vec{l}} as the path length shrinks to zero, so too does the integral V_{above} = V_{below} \\label{2.34} \\tag{2.34} However, the gradient of V inherits the discontinuity in \\vec{E} , since \\vec{E} - \\grad{V} , so \\grad{V}_{above} - \\grad{V}_{below} = - \\frac{\\sigma}{\\epsilon_0} \\vu{n} or more conveniently \\pdv{V_{above}}{n} - \\pdv{V_{below}}{n} = - \\frac{1}{\\epsilon_0} \\sigma \\label{2.36} \\tag{2.36} where \\pdv{V}{n} = \\grad{V} \\cdot \\vu{n} denotes the normal derivative of V (that is, the rate of change in the direction perpendicular to the surface). Please note that these boundary conditions relate the fields and potentials just above and just below the surface. For example, the derivatives in \\eqref{2.36} are the limiting values as we approach the surface from either side.","title":"2.3.5: Boundary Conditions"},{"location":"ch2-4/","text":"2.4: Work and Energy in Electrostatics 2.4.1: The Work it Takes to Move a Charge Suppose you have a stationary configuration of source charges, and you want to move a test charge Q from point a to point b (Fig. 2.39). Question : how much work will you have to do? At any point along the path, the electric force on Q is \\vec{F} = Q \\vec{E} ; the force you must exert, in opposition to the electric force, is -Q\\vec{E} . The work you do is therefore W = \\int_{a}^{b} \\vec{F} \\cdot \\dd{\\vec{l}} \\\\ = - Q \\int_{a}^{b} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = - Q[V(b) - V(a)] Notice that the answer is independent of the path you take from a to b; in mechanics, then, we would call the electrostatic force \"conservative.\" Dividing through by Q, we have V(b) - V(a) = \\frac{W}{Q} In words, the potential difference between points a and b is equal to the work per unit charge required to carry a particle from a to b. In particular, if you want to bring Q in from far away and stick it at point r, the work you must do is W = Q[V(\\vec{r}) - V(\\infty)], so if you have set the reference point at infinity, W = Q V(\\vec{r}) \\label{2.39} \\tag{2.39} In this sense, potential is potential energy (the work it takes to create a system) per unit charge (just as the field is force per unit charge). 2.4.2: The Energy of a Point Charge Distribution How much work would it take to assemble an entire collection of point charges? Imagine bringing in the charges, one by one, from far away (Fig 2.40). The first charge q_1 takes no work, since there is no field to fight against. Now bring in q_2 . According to \\eqref{2.39} this will cost you q_2 V_1(\\vec{r}_2) , where V_1 is the potential due to q_1 , and \\vec{r}_2 is the place we're putting q_2 : W_2 = \\frac{1}{4 \\pi \\epsilon_0} q_2 \\left( \\frac{q_1}{\\gr_{12}} \\right) ( \\gr_{12} is the distance between q_1 and q_2 , once they are in position). As you bring in each charge, nail it down in its final location, so it doesn't move when you bring in the next charge. Now bring in q_3 . This requires work q_3 V_{1,2}(\\vec{r}_3) , where V_{1,2} is the potential due to charges q_1 and q_2 , namely (1 / 4 \\pi \\epsilon_0) (q_1 / \\gr_{13} + q_2 / \\gr_{23} ) . Thus W_3 = \\frac{1}{4 \\pi \\epsilon_0} q_3 \\left( \\frac{q_1}{\\gr_{13}} + \\frac{q_2}{\\gr_{23}} \\right) Similarly, the extra work to bring in q_4 will be W_4 = \\frac{1}{4 \\pi \\epsilon_0} q_4 \\left( \\frac{q_1}{\\gr_{14}} + \\frac{q_2}{\\gr_{24}} + \\frac{q_3}{\\gr_{34}} \\right) The total work necessary to assemble the first four charges, then, is W = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q_1 q_2}{\\gr_{12}} + \\frac{q_1 q_3}{\\gr_{13}} + \\frac{q_1 q_4}{\\gr_{14}} + \\frac{q_2 q_3}{\\gr_{23}} + \\frac{q_2 q_4}{\\gr_{24}} + \\frac{q_3 q_4}{\\gr_{34}} \\right) You see the general rule: Take the product of each pair of charges, divide by their separation distance, and add it all up: W = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i = 1} ^{n} \\sum_{j > i} ^n \\frac{q_i q_j}{\\gr_{ij}} The stipulation j > i is to remind you not to count the same pair twice. A nicer way to accomplish this is intentionally to count each pair twice, and then divide by 2: W = \\frac{1}{8 \\pi \\epsilon_0} \\sum_{i = 1} ^n \\sum_{j \\neq i} \\frac{q_i q_j}{\\gr_{ij}} (we must still avoid i = j , of course). Notice that in this form the answer plainly does not depend on the order in which you assemble the charges, since every pair occurs in the sum. Finally, let's pull out the factor q_i : W = \\frac{1}{2} \\sum_{i = 1}^n q_i \\left( \\sum_{j \\neq i} ^n \\frac{1}{4 \\pi \\epsilon_0} \\frac{q_j}{\\gr_{ij}} \\right) The term in parentheses is the potential at point \\vec{r_i} (the position of q_i ) due to all the other charges - all of them, now, not just the ones that were present at some stage during the assembly. Thus, W = \\frac{1}{2} \\sum_{i = 1} ^n q_i V(\\vec{r_i}) \\label{2.42} \\tag{2.42} That's how much work it takes to assemble a configuration of point charges; it's also the amount of work you'd get back if you dismantled the system. In the meantime, it represents energy stored in the configuration (\"potential\" energy, if you insist, though for obvious reasons I prefer to avoid that word in this context). 2.4.3: The Energy of a Continuous Charge Distribution For a volume charge density \\rho , \\eqref{2.42} becomes W = \\frac{1}{2} \\int \\rho V \\dd{\\tau} \\label{2.43} \\tag{2.43} There is a lovely way to write this result, in which \\rho and V are eliminated in favor of \\vec{E} . First, use Gauss's law to express \\rho in terms of \\vec{E} \\rho = \\epsilon_0 \\div{\\vec{E}} \\qquad \\text{so,} \\qquad W = \\frac{\\epsilon_0}{2} \\int (\\div{\\vec{E}}) V \\dd{\\tau} Now, use integration by parts to transfer the derivative from \\vec{E} to V : W = \\frac{\\epsilon_0}{2} \\left[ - \\int \\vec{E} \\cdot (\\grad{V}) \\dd{\\tau} + \\oint V \\vec{E} \\cdot \\dd{\\vec{a}} \\right] But \\grad{V} = - \\vec{E} , so W = \\frac{\\epsilon_0}{2} \\left( \\int_{\\mathscr{V}} E^2 \\dd{\\tau} + \\oint V \\vec{E} \\cdot \\dd{\\vec{a}} \\right) \\label{2.44} \\tag{2.44} But what volume is this we're integrating over? Let's go back to the formula we started with, \\eqref{2.43} . From its derivation, it is clear that we should integrate over the region where the charge is located. But actually, any larger volume would do just as well: The \"extra\" territory we throw in will contribute nothing to the integral, since \\rho = 0 out there. With this in mind, we return to \\eqref{2.44} . What happens here, as we enlarge the volume beyond the minimum necessary to trap all the charge? Well, the integral of E^2 can only increase (the integrand being positive); evidently the surface integral must decrease accordingly to leave the sum intact. (In fact, at large distances from the charge, E goes like 1 / r^2 and V like 1/r , while the surface area grows like r^2 ; roughly speaking, then, the surface integral goes down like 1/r . Please understand: \\eqref{2.44} gives you the correct energy W, whatever volume you use (as long as it encloses all the charge), but the contribution of the volume integral goes up, and that of the surface integral goes down, as you take larger and larger volumes. In particular, why not integrate over all space? Then the surface integral goes to zero, and we are left with W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} \\quad \\text{(all space)} \\label{2.45} \\tag{2.45} Example 2.9 Find the energy of a uniformly charged spherical shell of total charge q and radius R Solution Use \\eqref{2.43} in the version appropriate to surface charges W = \\frac{1}{2} \\sigma V \\dd{a} Now, the potential at the surface of this sphere is (1/4 \\pi \\epsilon_0)q/R (a constant), so W = \\frac{1}{8\\pi \\epsilon_0} \\frac{q}{R} \\int \\sigma \\dd{a} = \\frac{1}{8 \\pi \\epsilon_0} \\frac{q^2}{R} Solution 2 Use \\eqref{2.45} . Inside the sphere, \\vec{E} = 0 ; outside: \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r} \\quad \\text{so} \\quad E^2 = \\frac{q^2}{(4 \\pi \\epsilon_0)^2 r^4} Therefore, W_{tot} = \\frac{\\epsilon_0}{2 (4 \\pi \\epsilon_0)^2}\\int \\left( \\frac{q^2}{r^4} \\right) (r^2 \\sin \\theta \\dd{r} \\dd{\\theta} \\dd{\\phi}) \\\\ = \\frac{1}{32 \\pi ^2 \\epsilon_0} q^2 4 \\pi \\int_{R} ^{\\infty} \\frac{1}{r^2} \\dd{r} = \\frac{1}{8 \\pi \\epsilon_0} \\frac{q^2}{R} 2.4.4: Comments on Electrostatic Energy A perplexing \"inconsistency\" Equation \\eqref{2.45} clearly implies that the energy of a stationary charge distribution is always positive. On the other hand, \\eqref{2.42} (from which \\eqref{2.45} was in fact derived), can be positive or negative. For instance, according to \\eqref{2.42} the energy of two equal but opposite charges a distance \\gr apart is -(1/4 \\pi \\epsilon_0) (q^2/\\gr) . What's gone wrong? Which equation is correct? The answer is that both are correct, but they speak to slightly different questions. Equation \\eqref{2.42} does not take into account the work necessary to make the point charges in the first place; we started with point charges and simply found the work required to bring them together. This is wise strategy, since \\eqref{2.45} indicates that the energy of a point charge is in fact infinite W = \\frac{\\epsilon_0}{2(4 \\pi \\epsilon_0)^2} \\int \\left( \\frac{q^2}{r^4} \\right) (r^2 \\sin \\theta \\dd{r} \\dd{\\theta} \\dd{\\phi}) = \\frac{q^2}{8 \\pi \\epsilon_0} \\int_{0} ^{\\infty} \\frac{1}{r^2} \\dd{r} = \\infty Equation \\eqref{2.45} is more complete , in the sense that it tells you the total energy stored in a charge configuration, but \\eqref{2.42} is more appropriate when you're dealing with point charges, because we prefer (for good reason!) to leave out that portion of the total energy that is attributable to the fabrication of the point charges themselves. In practice, after all, the point charges (electrons, say) are given to us ready-made; all we do is move them around. Since we did not put them together, and we cannot take them apart, it is immaterial how much work the process would involve. (Still, the infinite energy of a point charge is a recurring source of embarrassment for electromagnetic theory, afflicting the quantum version as well as the classical. We shall return to the problem in Chapter 11). Now, you may wonder where the inconsistency crept into an apparently water-tight derivation. The \"flaw\" lies between \\eqref{2.42} and \\eqref{2.43} : in the former, V(\\vec{r_i}) represents the potential due to all the other charges, but not q_i , whereas in the latter, V(\\vec{r}) is the full potential. For a continuous distribution, there is no distinction, since the amount of charge right at the point \\vec{r} is vanishingly small, and its contribution to the potential is zero. But in the presence of point charges you'd better stick with \\eqref{2.42} . Where is the energy stored? Equations \\eqref{2.43} and \\eqref{2.45} offer two different ways of calculating the same thing. The first is an integral over the charge distribution, the second is an integral over the field. These can involve completely different regions. For instance, in the case of a spherical shell, the charge is confined to the surface, whereas the electric field is everywhere outside this surface. Where is the energy, then? Is it stored in the field, as \\eqref{2.45} seems to suggest, or is it stored in the charge, as \\eqref{2.43} implies? At the present stage this is simply an unanswerable question: I can tell you what the total energy is, and I can provide you with several different ways to compute it, but it is impertinent to worry about where the energy is located. In the context of radiation theory (Chapter 11) it is useful (and in general relativity it is essential) to regard the energy as stored in the field, with a density \\frac{\\epsilon_0}{2} E^2 = \\text{ energy per unit volume} \\label{2.46} \\tag{2.46} But in electrostatics one could just as well say it is stored in the charge, with a density \\frac{1}{2} \\rho V . The difference is purely a matter of bookkeeping. The superposition principle . Because electrostatic energy is quadratic in the fields, it does not obey a superposition principle. The energy of a compound system is not the sum of the energies of its parts considered separately - there are also \"cross terms\": \\begin{align} W_{tot} & = & \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} = \\frac{\\epsilon_0}{2} \\int (\\vec{E_1} + \\vec{E_2})^2 \\dd{\\tau} \\\\ & = & \\frac{\\epsilon_0}{2} \\int (E_1 ^2 + E_2 ^2 + 2 \\vec{E_1} \\cdot \\vec{E_2}) \\dd{\\tau} \\\\ & = & W_1 + W_2 + \\epsilon_0 \\int \\vec{E_1} \\cdot \\vec{E_2} \\dd{\\tau} \\end{align} For example, if you double the charge everywhere, you quadruple the total energy.","title":"2.4 - Work and Energy in Electrostatics"},{"location":"ch2-4/#24-work-and-energy-in-electrostatics","text":"","title":"2.4: Work and Energy in Electrostatics"},{"location":"ch2-4/#241-the-work-it-takes-to-move-a-charge","text":"Suppose you have a stationary configuration of source charges, and you want to move a test charge Q from point a to point b (Fig. 2.39). Question : how much work will you have to do? At any point along the path, the electric force on Q is \\vec{F} = Q \\vec{E} ; the force you must exert, in opposition to the electric force, is -Q\\vec{E} . The work you do is therefore W = \\int_{a}^{b} \\vec{F} \\cdot \\dd{\\vec{l}} \\\\ = - Q \\int_{a}^{b} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = - Q[V(b) - V(a)] Notice that the answer is independent of the path you take from a to b; in mechanics, then, we would call the electrostatic force \"conservative.\" Dividing through by Q, we have V(b) - V(a) = \\frac{W}{Q} In words, the potential difference between points a and b is equal to the work per unit charge required to carry a particle from a to b. In particular, if you want to bring Q in from far away and stick it at point r, the work you must do is W = Q[V(\\vec{r}) - V(\\infty)], so if you have set the reference point at infinity, W = Q V(\\vec{r}) \\label{2.39} \\tag{2.39} In this sense, potential is potential energy (the work it takes to create a system) per unit charge (just as the field is force per unit charge).","title":"2.4.1: The Work it Takes to Move a Charge"},{"location":"ch2-4/#242-the-energy-of-a-point-charge-distribution","text":"How much work would it take to assemble an entire collection of point charges? Imagine bringing in the charges, one by one, from far away (Fig 2.40). The first charge q_1 takes no work, since there is no field to fight against. Now bring in q_2 . According to \\eqref{2.39} this will cost you q_2 V_1(\\vec{r}_2) , where V_1 is the potential due to q_1 , and \\vec{r}_2 is the place we're putting q_2 : W_2 = \\frac{1}{4 \\pi \\epsilon_0} q_2 \\left( \\frac{q_1}{\\gr_{12}} \\right) ( \\gr_{12} is the distance between q_1 and q_2 , once they are in position). As you bring in each charge, nail it down in its final location, so it doesn't move when you bring in the next charge. Now bring in q_3 . This requires work q_3 V_{1,2}(\\vec{r}_3) , where V_{1,2} is the potential due to charges q_1 and q_2 , namely (1 / 4 \\pi \\epsilon_0) (q_1 / \\gr_{13} + q_2 / \\gr_{23} ) . Thus W_3 = \\frac{1}{4 \\pi \\epsilon_0} q_3 \\left( \\frac{q_1}{\\gr_{13}} + \\frac{q_2}{\\gr_{23}} \\right) Similarly, the extra work to bring in q_4 will be W_4 = \\frac{1}{4 \\pi \\epsilon_0} q_4 \\left( \\frac{q_1}{\\gr_{14}} + \\frac{q_2}{\\gr_{24}} + \\frac{q_3}{\\gr_{34}} \\right) The total work necessary to assemble the first four charges, then, is W = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q_1 q_2}{\\gr_{12}} + \\frac{q_1 q_3}{\\gr_{13}} + \\frac{q_1 q_4}{\\gr_{14}} + \\frac{q_2 q_3}{\\gr_{23}} + \\frac{q_2 q_4}{\\gr_{24}} + \\frac{q_3 q_4}{\\gr_{34}} \\right) You see the general rule: Take the product of each pair of charges, divide by their separation distance, and add it all up: W = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i = 1} ^{n} \\sum_{j > i} ^n \\frac{q_i q_j}{\\gr_{ij}} The stipulation j > i is to remind you not to count the same pair twice. A nicer way to accomplish this is intentionally to count each pair twice, and then divide by 2: W = \\frac{1}{8 \\pi \\epsilon_0} \\sum_{i = 1} ^n \\sum_{j \\neq i} \\frac{q_i q_j}{\\gr_{ij}} (we must still avoid i = j , of course). Notice that in this form the answer plainly does not depend on the order in which you assemble the charges, since every pair occurs in the sum. Finally, let's pull out the factor q_i : W = \\frac{1}{2} \\sum_{i = 1}^n q_i \\left( \\sum_{j \\neq i} ^n \\frac{1}{4 \\pi \\epsilon_0} \\frac{q_j}{\\gr_{ij}} \\right) The term in parentheses is the potential at point \\vec{r_i} (the position of q_i ) due to all the other charges - all of them, now, not just the ones that were present at some stage during the assembly. Thus, W = \\frac{1}{2} \\sum_{i = 1} ^n q_i V(\\vec{r_i}) \\label{2.42} \\tag{2.42} That's how much work it takes to assemble a configuration of point charges; it's also the amount of work you'd get back if you dismantled the system. In the meantime, it represents energy stored in the configuration (\"potential\" energy, if you insist, though for obvious reasons I prefer to avoid that word in this context).","title":"2.4.2: The Energy of a Point Charge Distribution"},{"location":"ch2-4/#243-the-energy-of-a-continuous-charge-distribution","text":"For a volume charge density \\rho , \\eqref{2.42} becomes W = \\frac{1}{2} \\int \\rho V \\dd{\\tau} \\label{2.43} \\tag{2.43} There is a lovely way to write this result, in which \\rho and V are eliminated in favor of \\vec{E} . First, use Gauss's law to express \\rho in terms of \\vec{E} \\rho = \\epsilon_0 \\div{\\vec{E}} \\qquad \\text{so,} \\qquad W = \\frac{\\epsilon_0}{2} \\int (\\div{\\vec{E}}) V \\dd{\\tau} Now, use integration by parts to transfer the derivative from \\vec{E} to V : W = \\frac{\\epsilon_0}{2} \\left[ - \\int \\vec{E} \\cdot (\\grad{V}) \\dd{\\tau} + \\oint V \\vec{E} \\cdot \\dd{\\vec{a}} \\right] But \\grad{V} = - \\vec{E} , so W = \\frac{\\epsilon_0}{2} \\left( \\int_{\\mathscr{V}} E^2 \\dd{\\tau} + \\oint V \\vec{E} \\cdot \\dd{\\vec{a}} \\right) \\label{2.44} \\tag{2.44} But what volume is this we're integrating over? Let's go back to the formula we started with, \\eqref{2.43} . From its derivation, it is clear that we should integrate over the region where the charge is located. But actually, any larger volume would do just as well: The \"extra\" territory we throw in will contribute nothing to the integral, since \\rho = 0 out there. With this in mind, we return to \\eqref{2.44} . What happens here, as we enlarge the volume beyond the minimum necessary to trap all the charge? Well, the integral of E^2 can only increase (the integrand being positive); evidently the surface integral must decrease accordingly to leave the sum intact. (In fact, at large distances from the charge, E goes like 1 / r^2 and V like 1/r , while the surface area grows like r^2 ; roughly speaking, then, the surface integral goes down like 1/r . Please understand: \\eqref{2.44} gives you the correct energy W, whatever volume you use (as long as it encloses all the charge), but the contribution of the volume integral goes up, and that of the surface integral goes down, as you take larger and larger volumes. In particular, why not integrate over all space? Then the surface integral goes to zero, and we are left with W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} \\quad \\text{(all space)} \\label{2.45} \\tag{2.45}","title":"2.4.3: The Energy of a Continuous Charge Distribution"},{"location":"ch2-4/#example-29","text":"Find the energy of a uniformly charged spherical shell of total charge q and radius R Solution Use \\eqref{2.43} in the version appropriate to surface charges W = \\frac{1}{2} \\sigma V \\dd{a} Now, the potential at the surface of this sphere is (1/4 \\pi \\epsilon_0)q/R (a constant), so W = \\frac{1}{8\\pi \\epsilon_0} \\frac{q}{R} \\int \\sigma \\dd{a} = \\frac{1}{8 \\pi \\epsilon_0} \\frac{q^2}{R} Solution 2 Use \\eqref{2.45} . Inside the sphere, \\vec{E} = 0 ; outside: \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r} \\quad \\text{so} \\quad E^2 = \\frac{q^2}{(4 \\pi \\epsilon_0)^2 r^4} Therefore, W_{tot} = \\frac{\\epsilon_0}{2 (4 \\pi \\epsilon_0)^2}\\int \\left( \\frac{q^2}{r^4} \\right) (r^2 \\sin \\theta \\dd{r} \\dd{\\theta} \\dd{\\phi}) \\\\ = \\frac{1}{32 \\pi ^2 \\epsilon_0} q^2 4 \\pi \\int_{R} ^{\\infty} \\frac{1}{r^2} \\dd{r} = \\frac{1}{8 \\pi \\epsilon_0} \\frac{q^2}{R}","title":"Example 2.9"},{"location":"ch2-4/#244-comments-on-electrostatic-energy","text":"A perplexing \"inconsistency\" Equation \\eqref{2.45} clearly implies that the energy of a stationary charge distribution is always positive. On the other hand, \\eqref{2.42} (from which \\eqref{2.45} was in fact derived), can be positive or negative. For instance, according to \\eqref{2.42} the energy of two equal but opposite charges a distance \\gr apart is -(1/4 \\pi \\epsilon_0) (q^2/\\gr) . What's gone wrong? Which equation is correct? The answer is that both are correct, but they speak to slightly different questions. Equation \\eqref{2.42} does not take into account the work necessary to make the point charges in the first place; we started with point charges and simply found the work required to bring them together. This is wise strategy, since \\eqref{2.45} indicates that the energy of a point charge is in fact infinite W = \\frac{\\epsilon_0}{2(4 \\pi \\epsilon_0)^2} \\int \\left( \\frac{q^2}{r^4} \\right) (r^2 \\sin \\theta \\dd{r} \\dd{\\theta} \\dd{\\phi}) = \\frac{q^2}{8 \\pi \\epsilon_0} \\int_{0} ^{\\infty} \\frac{1}{r^2} \\dd{r} = \\infty Equation \\eqref{2.45} is more complete , in the sense that it tells you the total energy stored in a charge configuration, but \\eqref{2.42} is more appropriate when you're dealing with point charges, because we prefer (for good reason!) to leave out that portion of the total energy that is attributable to the fabrication of the point charges themselves. In practice, after all, the point charges (electrons, say) are given to us ready-made; all we do is move them around. Since we did not put them together, and we cannot take them apart, it is immaterial how much work the process would involve. (Still, the infinite energy of a point charge is a recurring source of embarrassment for electromagnetic theory, afflicting the quantum version as well as the classical. We shall return to the problem in Chapter 11). Now, you may wonder where the inconsistency crept into an apparently water-tight derivation. The \"flaw\" lies between \\eqref{2.42} and \\eqref{2.43} : in the former, V(\\vec{r_i}) represents the potential due to all the other charges, but not q_i , whereas in the latter, V(\\vec{r}) is the full potential. For a continuous distribution, there is no distinction, since the amount of charge right at the point \\vec{r} is vanishingly small, and its contribution to the potential is zero. But in the presence of point charges you'd better stick with \\eqref{2.42} . Where is the energy stored? Equations \\eqref{2.43} and \\eqref{2.45} offer two different ways of calculating the same thing. The first is an integral over the charge distribution, the second is an integral over the field. These can involve completely different regions. For instance, in the case of a spherical shell, the charge is confined to the surface, whereas the electric field is everywhere outside this surface. Where is the energy, then? Is it stored in the field, as \\eqref{2.45} seems to suggest, or is it stored in the charge, as \\eqref{2.43} implies? At the present stage this is simply an unanswerable question: I can tell you what the total energy is, and I can provide you with several different ways to compute it, but it is impertinent to worry about where the energy is located. In the context of radiation theory (Chapter 11) it is useful (and in general relativity it is essential) to regard the energy as stored in the field, with a density \\frac{\\epsilon_0}{2} E^2 = \\text{ energy per unit volume} \\label{2.46} \\tag{2.46} But in electrostatics one could just as well say it is stored in the charge, with a density \\frac{1}{2} \\rho V . The difference is purely a matter of bookkeeping. The superposition principle . Because electrostatic energy is quadratic in the fields, it does not obey a superposition principle. The energy of a compound system is not the sum of the energies of its parts considered separately - there are also \"cross terms\": \\begin{align} W_{tot} & = & \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} = \\frac{\\epsilon_0}{2} \\int (\\vec{E_1} + \\vec{E_2})^2 \\dd{\\tau} \\\\ & = & \\frac{\\epsilon_0}{2} \\int (E_1 ^2 + E_2 ^2 + 2 \\vec{E_1} \\cdot \\vec{E_2}) \\dd{\\tau} \\\\ & = & W_1 + W_2 + \\epsilon_0 \\int \\vec{E_1} \\cdot \\vec{E_2} \\dd{\\tau} \\end{align} For example, if you double the charge everywhere, you quadruple the total energy.","title":"2.4.4: Comments on Electrostatic Energy"},{"location":"ch2-5/","text":"2.5: Conductors 2.5.1: Basic Properties In an insulator , such as glass or rubber, each electron is on a short leash, attached to a particular atom. In a metallic conductor , by contrast, one or more electrons per atom are free to roam. (In liquid conductors such as salt water, it is ions that do the moving). A perfect conductor would contain an unlimited supply of free charges. In real life there are no perfect conductors, but metals come pretty close, for most purposes. From this definition, the basic electrostatic properties of ideal conductors immediately follow: (i) E = 0 inside a conductor . Why? Because if there were any field, those free charges would move, and it wouldn't be electrostatics any more. Hmm... that's hardly a satisfactory explanation; maybe all it proves is that you can't have electrostatics when conductors are present. We had better examine what happens when you put a conductor into an external electric field \\vec{E_0} (Fig. 2.42). Initially, the field will drive any free positive charges to the right, and negative ones to the left. (In practice, it's the negative charges - electrons - that do the moving, but when they depart, the right side is left with a net positive charge - the stationary nuclei - so it doesn't really matter which charges move; the effect is the same). When they come to the edge of the material, the charges pile up: plus on the right side, minus on the left. Now, these induced charges produce a field of their own, \\vec{E_1} , which, as you can see from the figure, is in the opposite direction to \\vec{E_0} . That's the crucial point, for it means that the field of the induced charges tends to cancel the original field. Charge will continue to flow until this cancellation is complete, and the resultant field inside the conductor is precisely zero. The whole process is practically instantaneous. (ii) \\rho = 0 inside a conductor. This follows from Gauss's law: if E is zero, so also is \\rho . There is still charge around, but exactly as much plus as minus, so the net charge density in the interior is zero. (iii) Any net charge resides on the surface . That's the only place left. (iv) A conductor is an equipotential . For if a and b are any two points within (or at the surface of) a given conductor, V(b) - V(a) = - \\int _{a} ^{b} \\vec{E} \\cdot \\dd{\\vec{l}} = 0 , and hence V(a) = V(b) . (v) E is perpendicular to the surface, just outside a conductor. Otherwise, as in (i), charge will immediately flow around the surface until it kills off the tangential component (Fig. 2.43). (Perpendicular to the surface, charge cannot flow, of course, since it is confined to the conducting object.) I think it is astonishing that the charge on a conductor flows to the surface. Because of their mutual repulsion, the charges naturally spread out as much as possible, but for all of them to go to the surface seems like a waste of the interior space. Surely we could do better, from the point of view of making each charge as possible from its neighbors, to sprinkle some of them throughout the volume. Well, it simply is not so. You do best to put all the charge on the surface, and this is true regardless of the size or shape of the conductor. The problem can also be phrased in terms of energy. Like any other free dynamical system, the charge on a conductor will seek the configuration that minimizes its potential energy. What property (iii) asserts is that the electrostatic energy of a solid object (with specified shape and total charge) is a minimum when that charge is spread over the surface. For instance, the energy of a sphere is (1 / 8 \\pi \\epsilon_0)(q^2 / R) if the charge is uniformly distributed over the surface, as we found in Ex 2.9, but it is greater (3/20 \\pi \\epsilon_0)(q^2 / R) if the charge is uniformly distributed throughout the volume (Prob. 2.34). 2.5.2: Induced Charges If you hold a charge +q near an uncharged conductor (Fig 2.44), the two will attract one another. The reason for this is that q will pull minus charges over to the near side and repel plus charges to the far side (Another way to think of it is that the charge moves around in such a way as to kill off the field of q for points inside the conductor, where the total field must be zero.) Since the negative induced charge is closer to q, there is a net force of attraction. (In chapter 3 we will calculate this force explicitly, for the case of a spherical conductor.) When I speak of the field, charge, or potential \"inside\" a conductor, I mean in the \"meat\" of the conductor. If there is some hollow cavity in the conductor, and within that cavity you put some charge, then the field in the cavity will not be zero. But in a remarkable way the cavity and its contents are electrically isolated from the outside world by the surrounding conductor (Fig. 2.45). No external fields penetrate the conductor; they are canceled at the outer surface by the induced charge there. Similarly, the field due to charges within the cavity is canceled, for all exterior points, by the induced charge on the inner surface. However, the compensating charge left over on the outer surface of the conductor effectively \"communicates\" the presence of q to the outside world. The total charge induced on the cavity wall is equal and opposite to the charge inside, for if we surround the cavity with a Gaussian surface, all points of which are in the conductor (Fig 2.45), \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = 0 , and hence (by Gauss's law) the net enclosed charge must be zero. But Q_{enc} = q + q_{induced} , so q_{induced} = - q . Then if the conductor as a whole is electrically neutral, there must be a charge +q on its outer surface. Example 2.10 An uncharged spherical conductor centered at the origin has a cavity of some weird shape carved out of it (Fig. 2.46). Somewhere within the cavity is a charge q. Question: What is the field outside the sphere? Solution At first glance, it would appear that the answer depends on the shape of the cavity and the location of the charge. But that's wrong: the answer is \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r} regardless. The conductor conceals from us all information concerning the nature of the cavity, revealing only the total charge it contains. How can this be? Well, the charge +q induces an opposite charge -q on the wall of the cavity, which distributes itself in such a way that its field cancels that of q, for all points exterior to the cavity. Since the conductor carries no net charge, this leaves +q to distribute itself uniformly over the surface of the sphere. (It's uniform because the asymmetrical influence of the point charge +q is negated by that of the induced charge -q on the inner surface.) For points outside the sphere, then, the only thing that survives is the field of the leftover +q, uniformly distributed over the outer surface. It may occur to you that in one respect this argument is open to challenge: There are actually three fields at work here: \\vec{E_q}, \\vec{E_{induced}} , and \\vec{E_{leftover}} . All we know for certain is that the sum of the three is zero inside the conductor, yet I claimed that the first two alone cancel, while the third is separately zero there. Moreover, even if the first two cancel within the conductor, who is to say they still cancel for points outside? They do not, after all, cancel for points inside the cavity. I cannot give you a completely satisfactory answer at the moment, but this much at least is true: there exists a way of distributing -q over the inner surface so as to cancel the field of q at all exterior points. For that same cavity could have been carved out of a huge spherical conductor with a radius of 27 miles or light years or whatever. In that case, the leftover +q on the outer surface is simply too far away to produce a significant field, and the other two fields would have to accomplish the cancellation by themselves. So we know they can do it... but are we sure they choose to? Perhaps for small spheres nature prefers some complicated three-way cancellation? Nope: As we'll see in the uniqueness theorems of Chapter 3, electrostatics is very stingy with its options; there is always precisely one way - no more - of distributing the charge on a conductor so as to make the field inside zero. Having found a possible way, we are guaranteed that no alternative exists, even in principle. If a cavity surrounded by conducting material is itself empty of charge, then the field within the cavity is zero. For any field line would have to begin and end on the cavity wall, going from a plus charge to a minus charge (Fig 2.47). Letting that field line be part of a closed loop, the rest of which is entirely inside the conductor (where E = 0), the integral \\oint \\vec{E} \\cdot \\dd{\\vec{l}} is distinctly positive , in violation of Eq. 2.19. It follows that E = 0 within an empty cavity, and there is in vact no charge on the surface of the cavity. (This is why you are relatively safe inside a metal car during a thunderstorm - you may get cooked, if lightning strikes, but you will not be electrocuted. The same principle applies to the placement of sensitive apparatus inside a grounded Faraday cage , to shield out stray electric fields. In practice, the enclosure doesn't even have to be solid conductor - chicken wire will often suffice.) 2.5.3: Surface Charge and the Force on a Conductor Because the field inside a conductor is zero, boundary condition Eq. 2.33 requires that the field immediately outside is \\vec{E} = \\frac{\\sigma}{\\epsilon_0} \\vu{n} \\label{2.48} \\tag{2.48} consistent with our earlier conclusion that the field is normal to the surface. In terms of potential, Eq. 2.36 yields \\sigma = - \\epsilon_0 \\pdv{V}{n} \\label{2.49} \\tag{2.49} These equations enable you to calculate the surface charge on a conductor, if you can determine \\vec{E} or V ; we shall use them frequently in the next chapter. In the presence of an electric field, a surface charge will experience a force; the force per unit area, \\vec{f} , is \\sigma \\vec{E} . But there's a problem here, for the electric field is discontinuous at a surface charge, so what are we supposed to use: \\vec{E}_{above}, \\vec{E}_{below} , or something in between? The answer is that we should use the average of the two \\vec{f} = \\sigma \\vec{E}_{average} = \\frac{1}{2} \\sigma (\\vec{E}_{above} + \\vec{E}_{below}) \\label{2.50} \\tag{2.50} Why the average? The reason is very simple, thought the telling makes it sound complicated: Let's focus our attention on a tiny patch of surface surrounding the point in question (Fig. 2.50). Make it small enough so it is essentially flat and the surface in question is essentially constant. The total field consists of two parts - that attributable to the patch itself, and that due to everything else (other regions of the surface, as well as any external sources that may be present) \\vec{E} = \\vec{E}_{patch} + \\vec{E}_{other} Now, the patch cannot exert a force on itself, any more than you can lift yourself by standing in a basket and pulling up on the handles. The force on the patch, then, is exclusively due to \\vec{E}_{other} , and this suffers no discontinuity (if we removed the patch, the field in the \"hole\" would be perfectly smooth). The discontinuity is due entirely to the charge on the patch, which puts out a field (\\sigma / 2 \\epsilon_0) on either side, pointing away from the surface. Thus, \\vec{E}_{above} = \\vec{E}_{other} + \\frac{\\sigma}{2 \\epsilon_0} \\vu{n} \\\\ \\vec{E}_{below} = \\vec{E}_{other} - \\frac{\\sigma}{2 \\epsilon_0} \\vu{n} \\\\ and hence \\vec{E}_{other} = \\frac{1}{2} (\\vec{E}_{above} + \\vec{E}_{below}) = \\vec{E}_{average} Averaging is really just a device for removing the contribution of the patch itself. That argument applies to any surface charge; in the particular case of a conductor, the field is zero inside and (\\sigma / \\epsilon_0)\\vu{n} outside ( \\eqref{2.48} , so the average is (\\sigma / 2 \\epsilon_0) \\vu{n} , and the force per unit area is f = \\frac{1}{2 \\epsilon_0} \\sigma ^2 \\vu{n} \\label{2.51} \\tag{2.51} This amounts to an outward electrostatic pressure on the surface, tending to draw the conductor into the field, regardless of the sign of \\sigma . Expressing the pressure in terms of the field just outside the surface P = \\frac{\\epsilon_0}{2} E^2 2.5.4: Capacitors Suppose we have two conductors, and we put charge +Q on one and -Q on the other (Fig 2.51). Since V is constant over a conductor, we can speak unambiguously of the potential difference between them: V = V_{+} - V_{-} = - \\int_{(-)}^{(+)} \\vec{E} \\cdot \\dd{\\vec{l}} We don't know how the charge distributes itself over the two conductors, and calculating the field would be a nightmare, if their shapes are complicated, but this much we do know: \\vec{E} is proportional to Q . For \\vec{E} is given by Coulomb's law: \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho}{\\gr^2} \\vu{\\gr} \\dd{\\tau} so if you double \\rho , you double \\vec{E} . Wait a minute! How do we know that doubling Q (and also -Q) simply doubles \\rho ? Maybe the charge moves around into a completely different configuration, quadrupling \\rho in some places and halving it in others, just so the total charge on the conductor is doubled. The fact is that this concern is unwarranted - doubling Q does double \\rho everywhere; it doesn't shift charge around. The proof will come in Chapter 3; for now you'll have to trust me. Since \\vec{E} is proportional to Q, so also is V. The constant of proportionality is called the capacitance of the arrangement C \\equiv \\frac{Q}{V} \\label{2.53} \\tag{2.53} Capacitance is a purely geometrical quantity, determined by the sizes, shapes, and separation of the two conductors. In SI units, C is measured in farads (F); a farad is a coulomb-per-volt. Actually this turns out to be inconveniently large; more practical units are the microfarad ( 10^{-6} F ) and the picofarad ( 10^{-12} F ) Notice that V is, by definition, the potential of the positive conductor less that of the negative one; likewise, Q is the charge of the positive conductor. Accordingly, capacitance is an intrinsically positive quantity. By the way, you will occasionally hear someone speak of the capacitance of a single conductor. In this case the \"second conductor\" is an imaginary spherical shell of infinite radius surrounding the one conductor. It contributes nothing to the field, so the capacitance is given by \\eqref{2.53} , where V is the potential with infinity as the reference point. Example 2.11 Find the capacitance of a parallel-plate capacitor consisting of two metal surfaces of area A held a distance d apart (Fig. 2.52) Solution If we put +Q on the top and -Q on the bottom, they will spread out uniformly over the two surfaces, provided the area is reasonably large and the separation small. The surface charge density, then, is \\sigma = Q / A on the top plate, and so the field, according to Ex. 2.6, is (1 / \\epsilon_0) Q / A . The potential difference between the plates is therefore V = \\frac{Q}{A \\epsilon_0} d and hence C = \\frac{A \\epsilon_0}{d} \\label{2.54} \\tag{2.54} If, for instance, the plates are square with sides 1 cm long, and they are held 1 mm apart, then the capacitance is 9 \\times 10^{-13} F Example 2.12 Find the capacitance of two concentric spherical metal shells, with radii a and b. Solution Place charge +Q on the inner sphere, and -Q on the outer one. The field between the spheres is \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q}{r^2} \\vec{r} so the potential difference between them is V = - \\int_{b}^{a} \\vec{E} \\cdot \\dd{\\vec{l}} = - \\frac{Q}{4 \\pi \\epsilon_0} \\int_{b}^a \\frac{1}{r^2} \\dd{r} = \\frac{Q}{4 \\pi \\epsilon_0} \\left( \\frac{1}{a} - \\frac{1}{b} \\right) As promised, V is proportional to Q; the capacitance is C = \\frac{Q}{V} = 4 \\pi \\epsilon_0 \\frac{ab}{(b - a)} To \"charge up\" a capacitor, you have to remove electrons from the positive plate and carry them to the negative plate. In doing so, you fight against the electric field, which is pulling them back toward the positive conductor and pushing them away from the negative one. How much work does it take, then, to charge the capacitor up to a final amount Q ? Suppose that at some intermediate stage in the process the charge on the positive plate is q , so that the potential difference is q / C . According to Eq. 2.38, the work you must do to transport the next piece of charge dq is \\dd{W} = \\left( \\frac{q}{C} \\right) \\dd{q} The total work necessary, then, to go from q = 0 to q = Q , is W = \\int_{0} ^Q \\left( \\frac{q}{C} \\dd{q} \\right) = \\frac{1}{2} \\frac{Q^2}{C} or, since Q = CV , W = \\frac{1}{2} C V^2 \\label{2.55} \\tag{2.55} where V is the final potential of the capacitor.","title":"2.5 - Conductors"},{"location":"ch2-5/#25-conductors","text":"","title":"2.5: Conductors"},{"location":"ch2-5/#251-basic-properties","text":"In an insulator , such as glass or rubber, each electron is on a short leash, attached to a particular atom. In a metallic conductor , by contrast, one or more electrons per atom are free to roam. (In liquid conductors such as salt water, it is ions that do the moving). A perfect conductor would contain an unlimited supply of free charges. In real life there are no perfect conductors, but metals come pretty close, for most purposes. From this definition, the basic electrostatic properties of ideal conductors immediately follow: (i) E = 0 inside a conductor . Why? Because if there were any field, those free charges would move, and it wouldn't be electrostatics any more. Hmm... that's hardly a satisfactory explanation; maybe all it proves is that you can't have electrostatics when conductors are present. We had better examine what happens when you put a conductor into an external electric field \\vec{E_0} (Fig. 2.42). Initially, the field will drive any free positive charges to the right, and negative ones to the left. (In practice, it's the negative charges - electrons - that do the moving, but when they depart, the right side is left with a net positive charge - the stationary nuclei - so it doesn't really matter which charges move; the effect is the same). When they come to the edge of the material, the charges pile up: plus on the right side, minus on the left. Now, these induced charges produce a field of their own, \\vec{E_1} , which, as you can see from the figure, is in the opposite direction to \\vec{E_0} . That's the crucial point, for it means that the field of the induced charges tends to cancel the original field. Charge will continue to flow until this cancellation is complete, and the resultant field inside the conductor is precisely zero. The whole process is practically instantaneous. (ii) \\rho = 0 inside a conductor. This follows from Gauss's law: if E is zero, so also is \\rho . There is still charge around, but exactly as much plus as minus, so the net charge density in the interior is zero. (iii) Any net charge resides on the surface . That's the only place left. (iv) A conductor is an equipotential . For if a and b are any two points within (or at the surface of) a given conductor, V(b) - V(a) = - \\int _{a} ^{b} \\vec{E} \\cdot \\dd{\\vec{l}} = 0 , and hence V(a) = V(b) . (v) E is perpendicular to the surface, just outside a conductor. Otherwise, as in (i), charge will immediately flow around the surface until it kills off the tangential component (Fig. 2.43). (Perpendicular to the surface, charge cannot flow, of course, since it is confined to the conducting object.) I think it is astonishing that the charge on a conductor flows to the surface. Because of their mutual repulsion, the charges naturally spread out as much as possible, but for all of them to go to the surface seems like a waste of the interior space. Surely we could do better, from the point of view of making each charge as possible from its neighbors, to sprinkle some of them throughout the volume. Well, it simply is not so. You do best to put all the charge on the surface, and this is true regardless of the size or shape of the conductor. The problem can also be phrased in terms of energy. Like any other free dynamical system, the charge on a conductor will seek the configuration that minimizes its potential energy. What property (iii) asserts is that the electrostatic energy of a solid object (with specified shape and total charge) is a minimum when that charge is spread over the surface. For instance, the energy of a sphere is (1 / 8 \\pi \\epsilon_0)(q^2 / R) if the charge is uniformly distributed over the surface, as we found in Ex 2.9, but it is greater (3/20 \\pi \\epsilon_0)(q^2 / R) if the charge is uniformly distributed throughout the volume (Prob. 2.34).","title":"2.5.1: Basic Properties"},{"location":"ch2-5/#252-induced-charges","text":"If you hold a charge +q near an uncharged conductor (Fig 2.44), the two will attract one another. The reason for this is that q will pull minus charges over to the near side and repel plus charges to the far side (Another way to think of it is that the charge moves around in such a way as to kill off the field of q for points inside the conductor, where the total field must be zero.) Since the negative induced charge is closer to q, there is a net force of attraction. (In chapter 3 we will calculate this force explicitly, for the case of a spherical conductor.) When I speak of the field, charge, or potential \"inside\" a conductor, I mean in the \"meat\" of the conductor. If there is some hollow cavity in the conductor, and within that cavity you put some charge, then the field in the cavity will not be zero. But in a remarkable way the cavity and its contents are electrically isolated from the outside world by the surrounding conductor (Fig. 2.45). No external fields penetrate the conductor; they are canceled at the outer surface by the induced charge there. Similarly, the field due to charges within the cavity is canceled, for all exterior points, by the induced charge on the inner surface. However, the compensating charge left over on the outer surface of the conductor effectively \"communicates\" the presence of q to the outside world. The total charge induced on the cavity wall is equal and opposite to the charge inside, for if we surround the cavity with a Gaussian surface, all points of which are in the conductor (Fig 2.45), \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = 0 , and hence (by Gauss's law) the net enclosed charge must be zero. But Q_{enc} = q + q_{induced} , so q_{induced} = - q . Then if the conductor as a whole is electrically neutral, there must be a charge +q on its outer surface.","title":"2.5.2: Induced Charges"},{"location":"ch2-5/#example-210","text":"An uncharged spherical conductor centered at the origin has a cavity of some weird shape carved out of it (Fig. 2.46). Somewhere within the cavity is a charge q. Question: What is the field outside the sphere? Solution At first glance, it would appear that the answer depends on the shape of the cavity and the location of the charge. But that's wrong: the answer is \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r} regardless. The conductor conceals from us all information concerning the nature of the cavity, revealing only the total charge it contains. How can this be? Well, the charge +q induces an opposite charge -q on the wall of the cavity, which distributes itself in such a way that its field cancels that of q, for all points exterior to the cavity. Since the conductor carries no net charge, this leaves +q to distribute itself uniformly over the surface of the sphere. (It's uniform because the asymmetrical influence of the point charge +q is negated by that of the induced charge -q on the inner surface.) For points outside the sphere, then, the only thing that survives is the field of the leftover +q, uniformly distributed over the outer surface. It may occur to you that in one respect this argument is open to challenge: There are actually three fields at work here: \\vec{E_q}, \\vec{E_{induced}} , and \\vec{E_{leftover}} . All we know for certain is that the sum of the three is zero inside the conductor, yet I claimed that the first two alone cancel, while the third is separately zero there. Moreover, even if the first two cancel within the conductor, who is to say they still cancel for points outside? They do not, after all, cancel for points inside the cavity. I cannot give you a completely satisfactory answer at the moment, but this much at least is true: there exists a way of distributing -q over the inner surface so as to cancel the field of q at all exterior points. For that same cavity could have been carved out of a huge spherical conductor with a radius of 27 miles or light years or whatever. In that case, the leftover +q on the outer surface is simply too far away to produce a significant field, and the other two fields would have to accomplish the cancellation by themselves. So we know they can do it... but are we sure they choose to? Perhaps for small spheres nature prefers some complicated three-way cancellation? Nope: As we'll see in the uniqueness theorems of Chapter 3, electrostatics is very stingy with its options; there is always precisely one way - no more - of distributing the charge on a conductor so as to make the field inside zero. Having found a possible way, we are guaranteed that no alternative exists, even in principle. If a cavity surrounded by conducting material is itself empty of charge, then the field within the cavity is zero. For any field line would have to begin and end on the cavity wall, going from a plus charge to a minus charge (Fig 2.47). Letting that field line be part of a closed loop, the rest of which is entirely inside the conductor (where E = 0), the integral \\oint \\vec{E} \\cdot \\dd{\\vec{l}} is distinctly positive , in violation of Eq. 2.19. It follows that E = 0 within an empty cavity, and there is in vact no charge on the surface of the cavity. (This is why you are relatively safe inside a metal car during a thunderstorm - you may get cooked, if lightning strikes, but you will not be electrocuted. The same principle applies to the placement of sensitive apparatus inside a grounded Faraday cage , to shield out stray electric fields. In practice, the enclosure doesn't even have to be solid conductor - chicken wire will often suffice.)","title":"Example 2.10"},{"location":"ch2-5/#253-surface-charge-and-the-force-on-a-conductor","text":"Because the field inside a conductor is zero, boundary condition Eq. 2.33 requires that the field immediately outside is \\vec{E} = \\frac{\\sigma}{\\epsilon_0} \\vu{n} \\label{2.48} \\tag{2.48} consistent with our earlier conclusion that the field is normal to the surface. In terms of potential, Eq. 2.36 yields \\sigma = - \\epsilon_0 \\pdv{V}{n} \\label{2.49} \\tag{2.49} These equations enable you to calculate the surface charge on a conductor, if you can determine \\vec{E} or V ; we shall use them frequently in the next chapter. In the presence of an electric field, a surface charge will experience a force; the force per unit area, \\vec{f} , is \\sigma \\vec{E} . But there's a problem here, for the electric field is discontinuous at a surface charge, so what are we supposed to use: \\vec{E}_{above}, \\vec{E}_{below} , or something in between? The answer is that we should use the average of the two \\vec{f} = \\sigma \\vec{E}_{average} = \\frac{1}{2} \\sigma (\\vec{E}_{above} + \\vec{E}_{below}) \\label{2.50} \\tag{2.50} Why the average? The reason is very simple, thought the telling makes it sound complicated: Let's focus our attention on a tiny patch of surface surrounding the point in question (Fig. 2.50). Make it small enough so it is essentially flat and the surface in question is essentially constant. The total field consists of two parts - that attributable to the patch itself, and that due to everything else (other regions of the surface, as well as any external sources that may be present) \\vec{E} = \\vec{E}_{patch} + \\vec{E}_{other} Now, the patch cannot exert a force on itself, any more than you can lift yourself by standing in a basket and pulling up on the handles. The force on the patch, then, is exclusively due to \\vec{E}_{other} , and this suffers no discontinuity (if we removed the patch, the field in the \"hole\" would be perfectly smooth). The discontinuity is due entirely to the charge on the patch, which puts out a field (\\sigma / 2 \\epsilon_0) on either side, pointing away from the surface. Thus, \\vec{E}_{above} = \\vec{E}_{other} + \\frac{\\sigma}{2 \\epsilon_0} \\vu{n} \\\\ \\vec{E}_{below} = \\vec{E}_{other} - \\frac{\\sigma}{2 \\epsilon_0} \\vu{n} \\\\ and hence \\vec{E}_{other} = \\frac{1}{2} (\\vec{E}_{above} + \\vec{E}_{below}) = \\vec{E}_{average} Averaging is really just a device for removing the contribution of the patch itself. That argument applies to any surface charge; in the particular case of a conductor, the field is zero inside and (\\sigma / \\epsilon_0)\\vu{n} outside ( \\eqref{2.48} , so the average is (\\sigma / 2 \\epsilon_0) \\vu{n} , and the force per unit area is f = \\frac{1}{2 \\epsilon_0} \\sigma ^2 \\vu{n} \\label{2.51} \\tag{2.51} This amounts to an outward electrostatic pressure on the surface, tending to draw the conductor into the field, regardless of the sign of \\sigma . Expressing the pressure in terms of the field just outside the surface P = \\frac{\\epsilon_0}{2} E^2","title":"2.5.3: Surface Charge and the Force on a Conductor"},{"location":"ch2-5/#254-capacitors","text":"Suppose we have two conductors, and we put charge +Q on one and -Q on the other (Fig 2.51). Since V is constant over a conductor, we can speak unambiguously of the potential difference between them: V = V_{+} - V_{-} = - \\int_{(-)}^{(+)} \\vec{E} \\cdot \\dd{\\vec{l}} We don't know how the charge distributes itself over the two conductors, and calculating the field would be a nightmare, if their shapes are complicated, but this much we do know: \\vec{E} is proportional to Q . For \\vec{E} is given by Coulomb's law: \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho}{\\gr^2} \\vu{\\gr} \\dd{\\tau} so if you double \\rho , you double \\vec{E} . Wait a minute! How do we know that doubling Q (and also -Q) simply doubles \\rho ? Maybe the charge moves around into a completely different configuration, quadrupling \\rho in some places and halving it in others, just so the total charge on the conductor is doubled. The fact is that this concern is unwarranted - doubling Q does double \\rho everywhere; it doesn't shift charge around. The proof will come in Chapter 3; for now you'll have to trust me. Since \\vec{E} is proportional to Q, so also is V. The constant of proportionality is called the capacitance of the arrangement C \\equiv \\frac{Q}{V} \\label{2.53} \\tag{2.53} Capacitance is a purely geometrical quantity, determined by the sizes, shapes, and separation of the two conductors. In SI units, C is measured in farads (F); a farad is a coulomb-per-volt. Actually this turns out to be inconveniently large; more practical units are the microfarad ( 10^{-6} F ) and the picofarad ( 10^{-12} F ) Notice that V is, by definition, the potential of the positive conductor less that of the negative one; likewise, Q is the charge of the positive conductor. Accordingly, capacitance is an intrinsically positive quantity. By the way, you will occasionally hear someone speak of the capacitance of a single conductor. In this case the \"second conductor\" is an imaginary spherical shell of infinite radius surrounding the one conductor. It contributes nothing to the field, so the capacitance is given by \\eqref{2.53} , where V is the potential with infinity as the reference point.","title":"2.5.4: Capacitors"},{"location":"ch2-5/#example-211","text":"Find the capacitance of a parallel-plate capacitor consisting of two metal surfaces of area A held a distance d apart (Fig. 2.52) Solution If we put +Q on the top and -Q on the bottom, they will spread out uniformly over the two surfaces, provided the area is reasonably large and the separation small. The surface charge density, then, is \\sigma = Q / A on the top plate, and so the field, according to Ex. 2.6, is (1 / \\epsilon_0) Q / A . The potential difference between the plates is therefore V = \\frac{Q}{A \\epsilon_0} d and hence C = \\frac{A \\epsilon_0}{d} \\label{2.54} \\tag{2.54} If, for instance, the plates are square with sides 1 cm long, and they are held 1 mm apart, then the capacitance is 9 \\times 10^{-13} F","title":"Example 2.11"},{"location":"ch2-5/#example-212","text":"Find the capacitance of two concentric spherical metal shells, with radii a and b. Solution Place charge +Q on the inner sphere, and -Q on the outer one. The field between the spheres is \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q}{r^2} \\vec{r} so the potential difference between them is V = - \\int_{b}^{a} \\vec{E} \\cdot \\dd{\\vec{l}} = - \\frac{Q}{4 \\pi \\epsilon_0} \\int_{b}^a \\frac{1}{r^2} \\dd{r} = \\frac{Q}{4 \\pi \\epsilon_0} \\left( \\frac{1}{a} - \\frac{1}{b} \\right) As promised, V is proportional to Q; the capacitance is C = \\frac{Q}{V} = 4 \\pi \\epsilon_0 \\frac{ab}{(b - a)} To \"charge up\" a capacitor, you have to remove electrons from the positive plate and carry them to the negative plate. In doing so, you fight against the electric field, which is pulling them back toward the positive conductor and pushing them away from the negative one. How much work does it take, then, to charge the capacitor up to a final amount Q ? Suppose that at some intermediate stage in the process the charge on the positive plate is q , so that the potential difference is q / C . According to Eq. 2.38, the work you must do to transport the next piece of charge dq is \\dd{W} = \\left( \\frac{q}{C} \\right) \\dd{q} The total work necessary, then, to go from q = 0 to q = Q , is W = \\int_{0} ^Q \\left( \\frac{q}{C} \\dd{q} \\right) = \\frac{1}{2} \\frac{Q^2}{C} or, since Q = CV , W = \\frac{1}{2} C V^2 \\label{2.55} \\tag{2.55} where V is the final potential of the capacitor.","title":"Example 2.12"},{"location":"ch3-1/","text":"3.1: Laplace's Equation 3.1.1: Introduction The primary task of electrostatics is to find the electric field of a given stationary charge distribution. In principle, this purpose is accomplished by Coulomb's law, in the form of \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{\\tau'} \\label{3.1} Unfortunately, integrals of this type can be difficult to calculate for any but the simplest charge configurations. Occasionally we can get around this by exploiting symmetry and using Gauss's law, but ordinarily the best strategy is first to calculate the potential , V, which is given by the somewhat more tractable V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r}')}{\\gr} \\dd{\\tau'} \\tagl{3.2} Still, even this integral is often too tough to handle analytically. Moreover, in problems involving conductors \\rho itself may not be known in advance; since charge is free to move around, the only thing we control directly is the total charge (or perhaps the potential) of each conductor. In such cases, it is fruitful to recast the problem in differential form, using Poisson's equation \\laplacian{V} = - \\frac{1}{\\epsilon_0} \\rho \\label{3.3} which, together with appropriate boundary conditions, is equivalent to \\eqref{3.2} . Very often, in fact, we are interested in finding the potential in a region where \\rho = 0 . (If \\rho = 0 everywhere, of course, then V = 0 , and there is nothing further to say - that's not what I mean. There may be plenty of charge elsewhere, but we're confining our attention to places where there is no charge.) In this case, Poisson's equation reduces to Laplace's equation \\laplacian{V} = 0 \\label{3.4} or, written out in Cartesian coordinates, \\frac{\\partial^2{V}}{\\partial{x^2}} + \\frac{\\partial^2 V}{\\partial{y^2}} + \\frac{\\partial^2{V}}{\\partial{z^2}} = 0 \\label{3.5} This formula is so fundamental to the subject that one might almost say electrostatics is the study of Laplace's equation. At the same time, it is a ubiquitous equation, appearing in such diverse branches of physics as gravitation and magnetism, the theory of heat, and the study of soap bubbles. In mathematics, it plays a major role in analytic function theory. To get a feel for Laplace's equation and its solutions (which are called harmonic functions ), we shall begin with the one- and two-dimensional versions, which are easier to picture, and illustrate all the essential properties of the three-dimensional case. 3.1.2: Laplace's Equation in One Dimension Suppose V depends on only one variable, x. Then Laplace's equation becomes \\frac{d^2 V}{dx^2} = 0 The general solution is V(x) = mx + b \\label{3.6} \\tag{3.6} the equation for a straight line. It contains two undetermined constants (m and b), as is appropriate for a second-order (ordinary) differential equation. They are fixed, in any particular case, by the boundary conditions of that problem. For instance, it might be specified that V = 4 at x = 1 and V = 0 at x = 5 . In that case, m = -1 and b = 5 , so V = - x + 5 (See Fig. 3.1) I want to call your attention to two features of this result; they may seem silly and obvious in one dimension, where I can write down the general solution explicitly, but the analogs in two and three dimensions are powerful and by no means obvious: V(x) is the average of V(x + a) and V(x - a) for any a: V(x) = \\frac{1}{2} [V(x + a) + V(x-a)] Laplace's equation is a kind of averaging instruction; it tells you to assign to the point x the average of the values to the left and to the right of x. Solutions to Laplace's equation are, in this sense, as boring as they could possibly be, and yet fit the end points properly. Laplace's equation tolerates no local maxima or minima ; extreme values of V must occur at the end points. Actually, this is a consequence of (1), for if there were a local maximum, V would be greater at that point than on either side, and therefore could not be the average. (Ordinarily, you expect the second derivative to be negative at a maximum and positive at a minimum. Since Laplace's equation requires, on the contrary, that the second derivative is zero, it seems reasonable that solutions should exhibit no extrema. However, this is not a proof, since there exist functions that have maxima and minima at points where the second derivative vanishes: x^4 for example, has such a minimum point at x=0 ). 3.1.3: Laplace's Equation in Two Dimensions If V depends on two variables, Laplace's equation becomes \\frac{\\partial^2{V}}{\\partial{x^2}} + \\frac{\\partial^2{V}}{\\partial{y^2}} = 0 This is no longer an ordinary differential equation (that is, one involving ordinary derivatives only); it is a partial differential equation. As a consequence, some of the simple rules you may be familiar with do not apply. For instance, the general solution to this equation doesn't contain just two arbitrary constants - or, for that matter, any finite number - despite the fact that it's a second order equation. Indeed, one cannot write down a \"general solution\" (at least, not in a closed form like \\eqref{3.6} ). Nevertheless, it is possible to deduce certain properties common to all solutions. It may help to have a physical example in mind. Picture a thin rubber sheet (or a soap film) stretched over some support. For definiteness, suppose you take a cardboard box, cut a wavy line all the way around, and remove the top part (Fig. 3.2). Now glue a tightly stretched rubber membrane over the box, so that it fits like a drum head (it won't be a flat drumhead, of course, unless you choose to cut the edges off straight). Now, if you lay out the coordinates (x, y) on the bottom of the box, the height V(x, y) of the sheet above the point (x, y) will satisfy Laplace's equation. (The one-dimensional analog would be a rubber band stretched between two points. Of course, it would form a straight line.) Actually, the equation satisfied by a rubber sheet is \\frac{\\partial}{\\partial{x}} \\left( g \\pdv{V}{y} \\right) + \\frac{\\partial}{\\partial{y}} \\left( g \\pdv{V}{y} \\right) = 0, \\\\ \\qquad \\text{ where } g = \\left[ 1 + \\left( \\pdv{V}{x} \\right)^2 + \\left( \\pdv{V}{y} \\right)^2 \\right]^{-1/2} Harmonic functions in two dimensions have the same properties we noted in one dimension: The value of V at a point (x, y) is the average of those around the point. More precisely, if you draw a circle of any radius R about the point (x, y), the average value of V on the circle is equal to the value at the center: V(x, y) = \\frac{1}{2 \\pi R} \\oint_{\\text{circle}} = V \\dd{l} (This, incidentally, suggests the method of relaxation, on which computer solutions to Laplace's equation are based: Starting with specified values for V at the boundary, and reasonable guesses for V on a grid of interior points, the first pass reassigns to each point the average of its nearest neighbors. The second pass repeats this process, using the corrected values, and so on. After a few iterations, the numbers begin to settle down, so that subsequent passes produce negligible changes, and a numerical solution to Laplace's equation, with the given boundary values, has been achieved.) V has no local maxima or minima; all extrema occur at the boundaries. (As before, this follows from (1)). Again, Laplace's equation picks the most featureless function possible, consistent with the boundary conditions: no hills, no valleys, just the smoothest conceivable surface. For instance, if you put a ping-pong ball on the stretched rubber sheet of Fig 3.2, it will roll over to one side and fall off - it will not find a \"pocket\" somewhere to settle into, for Laplace's equation allows no such dents in the surface. From a geometrical point of view, just as a straight line is the shortest distance between two points, so a harmonic function in two dimensions minimizes the surface area spanning the given boundary line. 3.1.4: Laplace's Equation in Three Dimensions In three dimensions I can neither provide you with an explicit solution (as in one dimension) nor offer a suggestive physical example to guide your intuition (as I did in two dimensions). Nevertheless, the same two properties remain true, and this time I will sketch a proof. For a proof that does not rely on Coulomb's law (only on Laplace's equation), see Prob. 3.37 The value of V at point r is the average value of V over a spherical surface of radius R centered at r : V(\\vec{r}) = \\frac{1}{4 \\pi R^2} \\oint_{\\text{sphere}} V \\dd{a} As a consequence, V can have no local maxima or minima; the extreme values of V must occur at the boundaries (For if V had a local maximum at r , then by the very nature of the maximum I could draw a sphere around r over which all the values of V - and a fortiori the average - would be less than at r .) Proof: V is a solution to the three-dimensional Laplace's equation. Then the value of V at point r is the average value of V over a spherical surface of radius R centered at r Let's begin by calculating the average potential over a spherical surface of radius R due to a single point charge q located outside the sphere. We may as well center the sphere at the origin and choose coordinates so that q lies on the z-axis (Fig 3.3). The potential at a point on the surface is V = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{\\gr} where \\gr ^2 = z^2 + R^2 - 2z R \\cos \\theta so V_{\\text{ave}} = \\frac{1}{4\\pi R^2} \\frac{1}{4 \\pi \\epsilon_0} \\int [z^2 + R^2 - 2 z R \\cos \\theta]^{-1/2} R^2 \\sin \\theta \\dd{\\theta} \\dd{\\phi} \\\\ = \\left. \\frac{q}{4 \\pi \\epsilon_0} \\frac{1}{2 z R} \\sqrt{z^2 + R^2 - 2 z R \\cos\\theta} \\right|_{0} ^{\\pi} \\\\ = \\frac{q}{4 \\pi \\epsilon_0} \\frac{1}{2zR} [(z + R) - (z-R)] = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{z} But this is precisely the potential due to q at the center of the sphere! By the superposition principle, the same goes for any collection of charges outside the sphere: their average potential over the sphere is equal to the net potential they produce at the center 3.1.5: Boundary Conditions and Uniqueness Theorems Laplace's equation does not by itself determine V; in addition, suitable boundary conditions must be supplied. This raises a delicate question: What are appropriate boundary conditions, sufficient to determine the answer and yet not so strong as to generate inconsistencies? The one-dimensional case is easy, for here the general solution V = mx + b contains two arbitrary constants, and we therefore require two boundary conditions. We might, for instance, specify the value of the function at each end, or we might give the value of the function and its derivative at one end, or the value at one end and the derivative at the other, and so on. But we cannot get away with just the value or just the derivative at one end - this is insufficient information. Nor would it do to specify the derivatives at both ends - this would be either redundant (if the two are equal) or inconsistent (if they are not). In two or three dimensions we are confronted by a partial differential equation, and it is not so obvious what would constitute acceptable boundary conditions. Is the shape of a taut rubber membrane, for instance, uniquely determined by the frame over which it is stretched, or, like a canning jar lid, can it snap from one stable configuration to another? The answer, as I think your intuition would suggest, that V is uniquely determined by its value at the boundary (canning jars evidently do not obey Laplace's equation). However, other boundary conditions can also be used (see Prob. 3.5). The proof that a proposed set of boundary conditions will suffice is usually presented in the form of a uniqueness theorem. There are many such theorems for electrostatics, all sharing the same basic format - I'll show you the two most useful ones: First Uniqueness Theorem: The solution to Laplace's equation in some volume \\mathscr{V} is uniquely determined if V is specified on the boundary surface \\mathscr{S} . In Fig. 3.5 I have drawn such a region and its boundary. (There could also be \"islands\" inside, so long as V is given on all their surfaces; also, the outer boundary could be at infinity, where V is ordinarily taken to be zero.) Proof : Suppose there were two solutions to Laplace's equation: \\laplacian{V_1} = 0 \\quad \\text{and} \\quad \\laplacian{V_2} = 0 both of which assume the specified value on the surface. I want to prove that they must be equal. The trick is to look at their difference : V_3 \\equiv V_1 - V_2 This obeys Laplace's equation (obviously) \\laplacian{V_3} = \\laplacian{V_1} - \\laplacian{V_2} = 0 and it takes the value zero on all boundaries (since V_1 and V_2 are equal there). But Laplace's equation allows no local maxima or minima - all extrema occur on the boundaries. So the maximum and minimum of V_3 are both zero. Therefore V_3 must be zero everywhere, and hence V_1 = V_2 Example 3.1 Show that the potential is constant inside an enclosure completely surrounded by conducting material, provided there is no charge within the enclosure. Solution The potential on the cavity wall is some constant V_0 (that's item (iv) in Sect. 2.5.1), so the potential inside is a function that satisfies Laplace's equation and has the constant value V_0 at the boundary. It doesn't take a genius to think of one solution to this problem: V = V_0 everywhere. The uniqueness theorem guarantees that this is the only solution. (It follows that the field inside an empty cavity is zero - the same result we found in Sect. 2.5.2 on rather different grounds.) The uniqueness theorem is a license to your imagination. It doesn't matter how you come by your solution; if (a) it satisfies Laplace's equation and (b) it has the correct value on the boundaries, then it's right . You'll see the power of this argument when we come to the method of images. Incidentally, it is easy to improve on the first uniqueness theorem: I assumed there was no charge inside the region in question, so the potential obeyed Laplace's equation, but we may as well throw in some charge (in which case V obeys Poisson's equation). Corollary: The potential in a volume \\mathscr{V} is uniquely determined if (a) the charge density throughout the region, and (b) the value of V on all boundaries, are specified The argument is the same, only this time \\laplacian{V_1} = -\\frac{1}{\\epsilon_0} \\rho, \\qquad \\laplacian{V_2} = - \\frac{1}{\\epsilon_0} \\rho so \\laplacian{V_3} = \\laplacian{V_1} - \\laplacian{V_2} = - \\frac{1}{\\epsilon_0} \\rho + \\frac{1}{\\epsilon_0} \\rho = 0 Once again the difference (V_3 \\equiv V_1 - V_2) satisfies Laplace's equation and has the value zero on all boundaries, so V_3 = 0 and hence V_1 = V_2 3.1.6: Conductors and the Second Uniqueness Theorem The simplest way to set the boundary conditions for an electrostatic problem is to specify the value of V on all surfaces surrounding the region of interest. And this situation often occurs in practice: In the laboratory, we have conductors connected to batteries, which maintain a given potential, or to ground , which is the experimentalist's word for V = 0 . However, there are other circumstances in which we do not know the potential at the boundary, but rather the charges on various conducting surfaces. Suppose I put Q_a on the first conductor, Q_b on the second conductor, and so on - I'm not telling you how the charge distributes itself over each conducting surface, because as soon as I put it on, it moves around in a way I do not control. And for good measure, let's say there is some specified charge density \\rho in the region between the conductors. Is the electric field now uniquely determined? Or are there perhaps a number of different ways the charges could arrange themselves on their respective conductors, each leading to a different field? Second uniqueness theorem : In a volume \\mathscr{V} surrounded by conductors and containing a specified charge density \\rho , the electric field is uniquely determined if the total charge on each conductor is given (Fig. 3.6). (The region as a whole can be bounded by another conductor, or else unbounded.) Proof : Suppose there are two fields satisfying the conditions of the problem. Both obey Gauss's law in differential form in the space betwen the conductors: \\div{\\vec{E_1}} = \\frac{1}{\\epsilon_0} \\rho, \\qquad \\div{\\vec{E_2}} = \\frac{1}{\\epsilon_0} \\rho And both obey Gauss's law in integral form for a Gaussian surface enclosing each conductor \\oint_{i_{th} \\text{ conducting surface}} \\vec{E_1} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_i \\quad \\text{ and } \\\\ \\oint_{i_{th} \\text{ conducting surface}} \\vec{E_2} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_i \\quad \\text{ and } \\\\ Likewise, for the outer boundary (whether this is just inside an enclosing conductor at infinity), \\oint_{\\text{outer boundary}} \\vec{E_1} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{tot} \\\\ \\oint_{\\text{outer boundary}} \\vec{E_2} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{tot} As before, we examine the difference \\vec{E_3} \\equiv \\vec{E_1} - \\vec{E_2} which obeys \\div{\\vec{E_3}} = 0 \\label{3.7} \\tag{3.7} in the region between the conductors, and \\oint \\vec{E_3} \\cdot \\dd{\\vec{a}} = 0 \\label{3.8} \\tag{3.8} over each boundary surface. Now there is one final piece of information we must exploit: Although we do not know how the charge Q_i distributes itself over the _i_th conductor, we do know that each conductor is an equipotential, and hence V_3 is a constant (not necessarily the same constant) over each conducting surface. (It need not be zero, for the potentials V_1 and V_2 may not be equal - all we know for sure is that both are constant over any given conductor.) Next comes a trick. Invoking the product rule \\div{(V_3 \\vec{E_3})} = V_3 (\\div{\\vec{E_3}}) + \\vec{E_3} \\cdot (\\grad{V_3}) = - (E_3)^2 Here I have used \\eqref{3.7} and \\vec{E_3} = - \\grad{V_3} . Integrating this over \\mathscr{V} and applying the divergence theorem to the left side: \\int_{\\mathscr{V}} \\div{(V_3 \\vec{E_3})} \\dd{\\tau} = \\oint_{S} V_3 \\vec{E_3} \\cdot \\dd{\\vec{a}} = - \\int_{\\mathscr{V}} (E_3)^2 \\dd{\\tau} The surface integral covers all boundaries of the region in question - the conductors and outer boundary. Now V_3 is a constant over each surface (if the outer boundary is invinity, V_3 = 0 there), so it comes outside each integral, and what remains is zero, according to \\eqref{3.8} . Therefore \\int_{\\mathscr{V}}(E_3)^2 \\dd{\\tau} = 0 The integrand is never negative, so the only way the integral can vanish is if E_3 = 0 everywhere. Consequently, \\vec{E_1} = \\vec{E_2} and the theorem is proved. This proof was not easy, and there is a real danger that the theorem itself will seem more plausible to you than the proof. In case you think the second uniqueness theorem is \"obvious,\" consider this example of Purcell's: Figure 3.7 shows a simple electrostatic configuration, consisting of four conductors with charges \\pm Q , situated so that the plusses are near the minuses. It all looks very comfortable. Now, what happens if we join them in pairs, by tiny wires, as indicated in Fig. 3.8? Since the positive charges are very near negative charges (which is where they like to be) you might well guess that nothing will happen - the configuration looks stable. Well, that sounds reasonable, but it's wrong. The configuration in Fig. 3.8 is impossible. For there are now effectively two conductors, and the total charge on each is zero. One possible way to distribute zero charge over these conductors is to have no accumulation of charge anywhere, and hence zero field everywhere (Fig. 3.9). By the second uniqueness theorem, this must be the solution: The charge will flow down the tiny wires, canceling itself off.","title":"3.1 - Laplace's Equation"},{"location":"ch3-1/#31-laplaces-equation","text":"","title":"3.1: Laplace's Equation"},{"location":"ch3-1/#311-introduction","text":"The primary task of electrostatics is to find the electric field of a given stationary charge distribution. In principle, this purpose is accomplished by Coulomb's law, in the form of \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{\\tau'} \\label{3.1} Unfortunately, integrals of this type can be difficult to calculate for any but the simplest charge configurations. Occasionally we can get around this by exploiting symmetry and using Gauss's law, but ordinarily the best strategy is first to calculate the potential , V, which is given by the somewhat more tractable V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r}')}{\\gr} \\dd{\\tau'} \\tagl{3.2} Still, even this integral is often too tough to handle analytically. Moreover, in problems involving conductors \\rho itself may not be known in advance; since charge is free to move around, the only thing we control directly is the total charge (or perhaps the potential) of each conductor. In such cases, it is fruitful to recast the problem in differential form, using Poisson's equation \\laplacian{V} = - \\frac{1}{\\epsilon_0} \\rho \\label{3.3} which, together with appropriate boundary conditions, is equivalent to \\eqref{3.2} . Very often, in fact, we are interested in finding the potential in a region where \\rho = 0 . (If \\rho = 0 everywhere, of course, then V = 0 , and there is nothing further to say - that's not what I mean. There may be plenty of charge elsewhere, but we're confining our attention to places where there is no charge.) In this case, Poisson's equation reduces to Laplace's equation \\laplacian{V} = 0 \\label{3.4} or, written out in Cartesian coordinates, \\frac{\\partial^2{V}}{\\partial{x^2}} + \\frac{\\partial^2 V}{\\partial{y^2}} + \\frac{\\partial^2{V}}{\\partial{z^2}} = 0 \\label{3.5} This formula is so fundamental to the subject that one might almost say electrostatics is the study of Laplace's equation. At the same time, it is a ubiquitous equation, appearing in such diverse branches of physics as gravitation and magnetism, the theory of heat, and the study of soap bubbles. In mathematics, it plays a major role in analytic function theory. To get a feel for Laplace's equation and its solutions (which are called harmonic functions ), we shall begin with the one- and two-dimensional versions, which are easier to picture, and illustrate all the essential properties of the three-dimensional case.","title":"3.1.1: Introduction"},{"location":"ch3-1/#312-laplaces-equation-in-one-dimension","text":"Suppose V depends on only one variable, x. Then Laplace's equation becomes \\frac{d^2 V}{dx^2} = 0 The general solution is V(x) = mx + b \\label{3.6} \\tag{3.6} the equation for a straight line. It contains two undetermined constants (m and b), as is appropriate for a second-order (ordinary) differential equation. They are fixed, in any particular case, by the boundary conditions of that problem. For instance, it might be specified that V = 4 at x = 1 and V = 0 at x = 5 . In that case, m = -1 and b = 5 , so V = - x + 5 (See Fig. 3.1) I want to call your attention to two features of this result; they may seem silly and obvious in one dimension, where I can write down the general solution explicitly, but the analogs in two and three dimensions are powerful and by no means obvious: V(x) is the average of V(x + a) and V(x - a) for any a: V(x) = \\frac{1}{2} [V(x + a) + V(x-a)] Laplace's equation is a kind of averaging instruction; it tells you to assign to the point x the average of the values to the left and to the right of x. Solutions to Laplace's equation are, in this sense, as boring as they could possibly be, and yet fit the end points properly. Laplace's equation tolerates no local maxima or minima ; extreme values of V must occur at the end points. Actually, this is a consequence of (1), for if there were a local maximum, V would be greater at that point than on either side, and therefore could not be the average. (Ordinarily, you expect the second derivative to be negative at a maximum and positive at a minimum. Since Laplace's equation requires, on the contrary, that the second derivative is zero, it seems reasonable that solutions should exhibit no extrema. However, this is not a proof, since there exist functions that have maxima and minima at points where the second derivative vanishes: x^4 for example, has such a minimum point at x=0 ).","title":"3.1.2: Laplace's Equation in One Dimension"},{"location":"ch3-1/#313-laplaces-equation-in-two-dimensions","text":"If V depends on two variables, Laplace's equation becomes \\frac{\\partial^2{V}}{\\partial{x^2}} + \\frac{\\partial^2{V}}{\\partial{y^2}} = 0 This is no longer an ordinary differential equation (that is, one involving ordinary derivatives only); it is a partial differential equation. As a consequence, some of the simple rules you may be familiar with do not apply. For instance, the general solution to this equation doesn't contain just two arbitrary constants - or, for that matter, any finite number - despite the fact that it's a second order equation. Indeed, one cannot write down a \"general solution\" (at least, not in a closed form like \\eqref{3.6} ). Nevertheless, it is possible to deduce certain properties common to all solutions. It may help to have a physical example in mind. Picture a thin rubber sheet (or a soap film) stretched over some support. For definiteness, suppose you take a cardboard box, cut a wavy line all the way around, and remove the top part (Fig. 3.2). Now glue a tightly stretched rubber membrane over the box, so that it fits like a drum head (it won't be a flat drumhead, of course, unless you choose to cut the edges off straight). Now, if you lay out the coordinates (x, y) on the bottom of the box, the height V(x, y) of the sheet above the point (x, y) will satisfy Laplace's equation. (The one-dimensional analog would be a rubber band stretched between two points. Of course, it would form a straight line.) Actually, the equation satisfied by a rubber sheet is \\frac{\\partial}{\\partial{x}} \\left( g \\pdv{V}{y} \\right) + \\frac{\\partial}{\\partial{y}} \\left( g \\pdv{V}{y} \\right) = 0, \\\\ \\qquad \\text{ where } g = \\left[ 1 + \\left( \\pdv{V}{x} \\right)^2 + \\left( \\pdv{V}{y} \\right)^2 \\right]^{-1/2} Harmonic functions in two dimensions have the same properties we noted in one dimension: The value of V at a point (x, y) is the average of those around the point. More precisely, if you draw a circle of any radius R about the point (x, y), the average value of V on the circle is equal to the value at the center: V(x, y) = \\frac{1}{2 \\pi R} \\oint_{\\text{circle}} = V \\dd{l} (This, incidentally, suggests the method of relaxation, on which computer solutions to Laplace's equation are based: Starting with specified values for V at the boundary, and reasonable guesses for V on a grid of interior points, the first pass reassigns to each point the average of its nearest neighbors. The second pass repeats this process, using the corrected values, and so on. After a few iterations, the numbers begin to settle down, so that subsequent passes produce negligible changes, and a numerical solution to Laplace's equation, with the given boundary values, has been achieved.) V has no local maxima or minima; all extrema occur at the boundaries. (As before, this follows from (1)). Again, Laplace's equation picks the most featureless function possible, consistent with the boundary conditions: no hills, no valleys, just the smoothest conceivable surface. For instance, if you put a ping-pong ball on the stretched rubber sheet of Fig 3.2, it will roll over to one side and fall off - it will not find a \"pocket\" somewhere to settle into, for Laplace's equation allows no such dents in the surface. From a geometrical point of view, just as a straight line is the shortest distance between two points, so a harmonic function in two dimensions minimizes the surface area spanning the given boundary line.","title":"3.1.3: Laplace's Equation in Two Dimensions"},{"location":"ch3-1/#314-laplaces-equation-in-three-dimensions","text":"In three dimensions I can neither provide you with an explicit solution (as in one dimension) nor offer a suggestive physical example to guide your intuition (as I did in two dimensions). Nevertheless, the same two properties remain true, and this time I will sketch a proof. For a proof that does not rely on Coulomb's law (only on Laplace's equation), see Prob. 3.37 The value of V at point r is the average value of V over a spherical surface of radius R centered at r : V(\\vec{r}) = \\frac{1}{4 \\pi R^2} \\oint_{\\text{sphere}} V \\dd{a} As a consequence, V can have no local maxima or minima; the extreme values of V must occur at the boundaries (For if V had a local maximum at r , then by the very nature of the maximum I could draw a sphere around r over which all the values of V - and a fortiori the average - would be less than at r .) Proof: V is a solution to the three-dimensional Laplace's equation. Then the value of V at point r is the average value of V over a spherical surface of radius R centered at r Let's begin by calculating the average potential over a spherical surface of radius R due to a single point charge q located outside the sphere. We may as well center the sphere at the origin and choose coordinates so that q lies on the z-axis (Fig 3.3). The potential at a point on the surface is V = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{\\gr} where \\gr ^2 = z^2 + R^2 - 2z R \\cos \\theta so V_{\\text{ave}} = \\frac{1}{4\\pi R^2} \\frac{1}{4 \\pi \\epsilon_0} \\int [z^2 + R^2 - 2 z R \\cos \\theta]^{-1/2} R^2 \\sin \\theta \\dd{\\theta} \\dd{\\phi} \\\\ = \\left. \\frac{q}{4 \\pi \\epsilon_0} \\frac{1}{2 z R} \\sqrt{z^2 + R^2 - 2 z R \\cos\\theta} \\right|_{0} ^{\\pi} \\\\ = \\frac{q}{4 \\pi \\epsilon_0} \\frac{1}{2zR} [(z + R) - (z-R)] = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{z} But this is precisely the potential due to q at the center of the sphere! By the superposition principle, the same goes for any collection of charges outside the sphere: their average potential over the sphere is equal to the net potential they produce at the center","title":"3.1.4: Laplace's Equation in Three Dimensions"},{"location":"ch3-1/#315-boundary-conditions-and-uniqueness-theorems","text":"Laplace's equation does not by itself determine V; in addition, suitable boundary conditions must be supplied. This raises a delicate question: What are appropriate boundary conditions, sufficient to determine the answer and yet not so strong as to generate inconsistencies? The one-dimensional case is easy, for here the general solution V = mx + b contains two arbitrary constants, and we therefore require two boundary conditions. We might, for instance, specify the value of the function at each end, or we might give the value of the function and its derivative at one end, or the value at one end and the derivative at the other, and so on. But we cannot get away with just the value or just the derivative at one end - this is insufficient information. Nor would it do to specify the derivatives at both ends - this would be either redundant (if the two are equal) or inconsistent (if they are not). In two or three dimensions we are confronted by a partial differential equation, and it is not so obvious what would constitute acceptable boundary conditions. Is the shape of a taut rubber membrane, for instance, uniquely determined by the frame over which it is stretched, or, like a canning jar lid, can it snap from one stable configuration to another? The answer, as I think your intuition would suggest, that V is uniquely determined by its value at the boundary (canning jars evidently do not obey Laplace's equation). However, other boundary conditions can also be used (see Prob. 3.5). The proof that a proposed set of boundary conditions will suffice is usually presented in the form of a uniqueness theorem. There are many such theorems for electrostatics, all sharing the same basic format - I'll show you the two most useful ones: First Uniqueness Theorem: The solution to Laplace's equation in some volume \\mathscr{V} is uniquely determined if V is specified on the boundary surface \\mathscr{S} . In Fig. 3.5 I have drawn such a region and its boundary. (There could also be \"islands\" inside, so long as V is given on all their surfaces; also, the outer boundary could be at infinity, where V is ordinarily taken to be zero.) Proof : Suppose there were two solutions to Laplace's equation: \\laplacian{V_1} = 0 \\quad \\text{and} \\quad \\laplacian{V_2} = 0 both of which assume the specified value on the surface. I want to prove that they must be equal. The trick is to look at their difference : V_3 \\equiv V_1 - V_2 This obeys Laplace's equation (obviously) \\laplacian{V_3} = \\laplacian{V_1} - \\laplacian{V_2} = 0 and it takes the value zero on all boundaries (since V_1 and V_2 are equal there). But Laplace's equation allows no local maxima or minima - all extrema occur on the boundaries. So the maximum and minimum of V_3 are both zero. Therefore V_3 must be zero everywhere, and hence V_1 = V_2","title":"3.1.5: Boundary Conditions and Uniqueness Theorems"},{"location":"ch3-1/#example-31","text":"Show that the potential is constant inside an enclosure completely surrounded by conducting material, provided there is no charge within the enclosure. Solution The potential on the cavity wall is some constant V_0 (that's item (iv) in Sect. 2.5.1), so the potential inside is a function that satisfies Laplace's equation and has the constant value V_0 at the boundary. It doesn't take a genius to think of one solution to this problem: V = V_0 everywhere. The uniqueness theorem guarantees that this is the only solution. (It follows that the field inside an empty cavity is zero - the same result we found in Sect. 2.5.2 on rather different grounds.) The uniqueness theorem is a license to your imagination. It doesn't matter how you come by your solution; if (a) it satisfies Laplace's equation and (b) it has the correct value on the boundaries, then it's right . You'll see the power of this argument when we come to the method of images. Incidentally, it is easy to improve on the first uniqueness theorem: I assumed there was no charge inside the region in question, so the potential obeyed Laplace's equation, but we may as well throw in some charge (in which case V obeys Poisson's equation). Corollary: The potential in a volume \\mathscr{V} is uniquely determined if (a) the charge density throughout the region, and (b) the value of V on all boundaries, are specified The argument is the same, only this time \\laplacian{V_1} = -\\frac{1}{\\epsilon_0} \\rho, \\qquad \\laplacian{V_2} = - \\frac{1}{\\epsilon_0} \\rho so \\laplacian{V_3} = \\laplacian{V_1} - \\laplacian{V_2} = - \\frac{1}{\\epsilon_0} \\rho + \\frac{1}{\\epsilon_0} \\rho = 0 Once again the difference (V_3 \\equiv V_1 - V_2) satisfies Laplace's equation and has the value zero on all boundaries, so V_3 = 0 and hence V_1 = V_2","title":"Example 3.1"},{"location":"ch3-1/#316-conductors-and-the-second-uniqueness-theorem","text":"The simplest way to set the boundary conditions for an electrostatic problem is to specify the value of V on all surfaces surrounding the region of interest. And this situation often occurs in practice: In the laboratory, we have conductors connected to batteries, which maintain a given potential, or to ground , which is the experimentalist's word for V = 0 . However, there are other circumstances in which we do not know the potential at the boundary, but rather the charges on various conducting surfaces. Suppose I put Q_a on the first conductor, Q_b on the second conductor, and so on - I'm not telling you how the charge distributes itself over each conducting surface, because as soon as I put it on, it moves around in a way I do not control. And for good measure, let's say there is some specified charge density \\rho in the region between the conductors. Is the electric field now uniquely determined? Or are there perhaps a number of different ways the charges could arrange themselves on their respective conductors, each leading to a different field? Second uniqueness theorem : In a volume \\mathscr{V} surrounded by conductors and containing a specified charge density \\rho , the electric field is uniquely determined if the total charge on each conductor is given (Fig. 3.6). (The region as a whole can be bounded by another conductor, or else unbounded.) Proof : Suppose there are two fields satisfying the conditions of the problem. Both obey Gauss's law in differential form in the space betwen the conductors: \\div{\\vec{E_1}} = \\frac{1}{\\epsilon_0} \\rho, \\qquad \\div{\\vec{E_2}} = \\frac{1}{\\epsilon_0} \\rho And both obey Gauss's law in integral form for a Gaussian surface enclosing each conductor \\oint_{i_{th} \\text{ conducting surface}} \\vec{E_1} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_i \\quad \\text{ and } \\\\ \\oint_{i_{th} \\text{ conducting surface}} \\vec{E_2} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_i \\quad \\text{ and } \\\\ Likewise, for the outer boundary (whether this is just inside an enclosing conductor at infinity), \\oint_{\\text{outer boundary}} \\vec{E_1} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{tot} \\\\ \\oint_{\\text{outer boundary}} \\vec{E_2} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{tot} As before, we examine the difference \\vec{E_3} \\equiv \\vec{E_1} - \\vec{E_2} which obeys \\div{\\vec{E_3}} = 0 \\label{3.7} \\tag{3.7} in the region between the conductors, and \\oint \\vec{E_3} \\cdot \\dd{\\vec{a}} = 0 \\label{3.8} \\tag{3.8} over each boundary surface. Now there is one final piece of information we must exploit: Although we do not know how the charge Q_i distributes itself over the _i_th conductor, we do know that each conductor is an equipotential, and hence V_3 is a constant (not necessarily the same constant) over each conducting surface. (It need not be zero, for the potentials V_1 and V_2 may not be equal - all we know for sure is that both are constant over any given conductor.) Next comes a trick. Invoking the product rule \\div{(V_3 \\vec{E_3})} = V_3 (\\div{\\vec{E_3}}) + \\vec{E_3} \\cdot (\\grad{V_3}) = - (E_3)^2 Here I have used \\eqref{3.7} and \\vec{E_3} = - \\grad{V_3} . Integrating this over \\mathscr{V} and applying the divergence theorem to the left side: \\int_{\\mathscr{V}} \\div{(V_3 \\vec{E_3})} \\dd{\\tau} = \\oint_{S} V_3 \\vec{E_3} \\cdot \\dd{\\vec{a}} = - \\int_{\\mathscr{V}} (E_3)^2 \\dd{\\tau} The surface integral covers all boundaries of the region in question - the conductors and outer boundary. Now V_3 is a constant over each surface (if the outer boundary is invinity, V_3 = 0 there), so it comes outside each integral, and what remains is zero, according to \\eqref{3.8} . Therefore \\int_{\\mathscr{V}}(E_3)^2 \\dd{\\tau} = 0 The integrand is never negative, so the only way the integral can vanish is if E_3 = 0 everywhere. Consequently, \\vec{E_1} = \\vec{E_2} and the theorem is proved. This proof was not easy, and there is a real danger that the theorem itself will seem more plausible to you than the proof. In case you think the second uniqueness theorem is \"obvious,\" consider this example of Purcell's: Figure 3.7 shows a simple electrostatic configuration, consisting of four conductors with charges \\pm Q , situated so that the plusses are near the minuses. It all looks very comfortable. Now, what happens if we join them in pairs, by tiny wires, as indicated in Fig. 3.8? Since the positive charges are very near negative charges (which is where they like to be) you might well guess that nothing will happen - the configuration looks stable. Well, that sounds reasonable, but it's wrong. The configuration in Fig. 3.8 is impossible. For there are now effectively two conductors, and the total charge on each is zero. One possible way to distribute zero charge over these conductors is to have no accumulation of charge anywhere, and hence zero field everywhere (Fig. 3.9). By the second uniqueness theorem, this must be the solution: The charge will flow down the tiny wires, canceling itself off.","title":"3.1.6: Conductors and the Second Uniqueness Theorem"},{"location":"ch3-2/","text":"3.2: The Method of Images 3.1.1: The Classic Image Problem Suppose a point charge q is held a distance d above an infinite grounded conducting plane (Fig. 3.10). Question : what is the potential in the region above the plane? It's not just (1/4 \\pi \\epsilon_0) q / \\gr , for q will induce a certain amount of negative charge on the nearby surface of the conductor; the total potential is due in part to q directly, and in part to this induced charge. But how can we possibly calculate the potential, when we don't know how much charge is induced or how it is distributed? From a mathematical point of view, our problem is to solve Poisson's equation in the region z > 0 , with a single point charge q at (0, 0, d) , subject to the boundary conditions V = 0 when z = 0 (since the conducting plane is grounded) V \\rightarrow 0 far from the charge (that is, for x^2 + y^2 + z^2 \\gg d^2 The first uniqueness theorem (actually, its corollary) guarantees that there is only one function that meets these requirements. If by trick or clever guess we can discover such a function, it's got to be the answer. Trick: Forget about the actual problem; we're going to study a completely different situation. This new configuration consists of two point charges, +q at (0, 0, d) and -q at (0, 0, -d) , and no conducting plane (Fig. 3.11). For this configuration I can easily write down the potential: V(x, y, z) = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{q}{\\sqrt{x^2 + y^2 + (z - d)^2 }} - \\frac{q}{\\sqrt{x^2 + y^2 + (z + d)^2}} \\right] \\label{3.9} \\tag{3.9} It follows that V = 0 when z = 0 V \\rightarrow 0 for x^2 + y^2 + z^2 \\gg d^2 and the only charge in the region z > 0 is the point charge +q at (0, 0, d) . But these are precisely the conditions of the original problem! Evidently the second configuration happens to produce exactly the same potential as the first configuration, in the \"upper\" region z \\geq 0 . (The \"lower\" region, z < 0 , is completely different, but who cares? The upper part is all we need.) Conclusion : The potential of a point charge above an infinite grounded conductor is given by \\eqref{3.9} , for z > 0 . Notice the crucial role played by the uniqueness theorem in this argument: without it, no one would believe this solution, since it was obtained for a completely different charge distribution. But the uniqueness theorem certifies it: If it satisfies Poisson's equation in the region of interest, and assumes the correct value at the boundaries, then it must be right. 3.2.2: Induced Surface Charge Now that we know the potential, it is a straightforward matter to compute the surface charge \\sigma induced on the conductor. According to Eq. 2.49, \\sigma = - \\epsilon_0 \\pdv{V}{n} where \\partial V / \\partial n is the normal derivative of V at the surface. In this case the normal direction is the z direction, so \\sigma = \\left. - \\epsilon_0 \\pdv{V}{z} \\right|_{z = 0} From Eq. 3.9 \\pdv{V}{z} = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{-q (z - d)}{[x^2 + y^2 + (z - d)^2]^{3/2}} + \\frac{q(z + d)}{[x^2 + y^2 + (z + d)^2 ]^{3/2}} \\right] so \\sigma(x, y) = \\frac{-qd}{2 \\pi (x^2 + y^2 + d^2)^{3/2}} \\label{3.10} \\tag{3.10} As expected, the induced charge is negative (assuming q is positive) and greatest at x = y = 0 . While we're at it, let's compute the total induced charge Q = \\int \\sigma \\dd{a} This integral, over the xy plane, could be done in Cartesian coordinates, with \\dd{a} = \\dd{x} \\dd{y} , but it's easier to use polar coordinates (r, \\phi) , with r^2 = x^2 + y^2 and \\dd{a} = r \\dd{r} \\dd{\\phi} . Then \\sigma(r) = \\frac{-qd}{2 \\pi (r^2 + d^2)^{3/2}} and Q = \\int_{0} ^{2\\pi} \\int_{0} ^{\\infty} \\frac{-qd}{2 \\pi (r^2 + d^2)^{3/2}} r \\dd{r} \\dd{\\phi} = \\left. \\frac{qd}{\\sqrt{r^2 + d^2}} \\right|_{0} ^{\\infty} = -q \\label{3.11} \\tag{3.11} The total charge induced on the plane is -q , as (with benefit of hindsight) you can perhaps convince yourself it had to be. 3.2.3: Force and Energy The charge q is attracted toward the plane, because of the negative induced charge. Let's calculate the force of attraction. Since the potential in the vicinity of q is the same as in the analog problem (the one with +q and -q but no conductor), so also is the field and, therefore, the force \\vec{F} = -\\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{(2d)^2} \\vu{z} \\label{3.12} \\tag{3.12} Beware : It is easy to get carried away, and assume that everything is the same in the two problems. Energy, however, is not the same. With the two point charges and no conductor, Eq. 2.42 gives W = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{2d} But for a single charge and conducting plane, the energy is half this W = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{4d} \\label{3.14} \\tag{3.14} Why half? Think of the energy stored in the fields (Eq. 2.45): W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} In the first case, both the upper region (z > 0) and the lower region (z < 0) contribute, and by symmetry they contribute equally. But in the second case, only the upper region contains a nonzero field, and hence the energy is half as great. Of course, one could also determine the energy by calculating the work required to bring q in from infinity. The force required (to oppose the electrical force in \\eqref{3.12} is (1 / 4 \\pi \\epsilon_0)(q^2/4z^2) \\vu{z} , so \\begin{align} W & = & \\int _{\\infty} ^{d} \\vec{F} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{\\infty} ^d \\frac{q^2}{4z^2} \\dd{z} \\\\ & = & \\frac{1}{4 \\pi \\epsilon_0} \\left. \\left( - \\frac{q^2}{4z} \\right) \\right|_{\\infty} ^d = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{4d} \\end{align} As I move q toward the conductor, I do work only on q. It is true that induced charge is moving in over the conductor, but this costs me nothing, since the whole conductor is at potential zero. By contrast, if I simultaneously bring in two point charges (with no conductor), I do work on both of them, and the total is (again) twice as great. 3.2.4: Other Image Problems The method just described is not limited to a single point charge; any stationary charge distribution near a grounded conducting plane can be treated in the same way, by introducing its mirror image - hence the name method of images. (Remember that the image charges have the opposite sign; this is what guarantees that the xy plane will be at potential zero.) There are also some exotic problems that can be handled in similar fashion; the nicest of these is the following. Example 3.2 A point charge q is situated a distance a from the center of a grounded conducting sphere of radius R (Fig. 3.12). Find the potential outside the sphere Solution Examine the completely different configuration, consisting of the point charge q together with another point charge q' = - \\frac{R}{a} q \\label{3.15} \\tag{3.15} placed a distance b = \\frac{R^2}{a} \\label{3.16} \\tag{3.16} to the right of the center of the sphere (Fig 3.13). No conductor, now - just the two point charges. The potential of this configuration is V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{\\gr} + \\frac{q'}{\\gr '} \\right) \\label{3.17} \\tag{3.17} where \\gr and \\gr' are the distances from q and q' , respectively. Now, it happens (see Prob. 3.8) that this potential vanishes at all points on the sphere, and therefore fits the boundary conditions for our original problem, in the exterior region. Conclusion : \\eqref{3.17} is the potential of a point charge near a grounded conducting sphere. (Notice that b is less than R , so the \"image\" charge q' is safely inside the sphere - you cannot put image charges in the region where you are calculating V ; that would change \\rho , and you'd be solving Poisson's equation with the wrong source.) In particular, the force of attraction between the charge and the sphere is F = \\frac{1}{4\\pi \\epsilon_0} \\frac{q q'}{(a - b)^2} = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2 R a}{(a^2 - R^2)^2} \\label{3.18} \\tag{3.18} The method of images is delightfully simple... when it works. But it is as much an art as a science, for you must somehow think up just the right \"auxiliary\" configuration, and for most shapes this is forbiddingly complicated, if not impossible.","title":"3.2 - The Method of Images"},{"location":"ch3-2/#32-the-method-of-images","text":"","title":"3.2: The Method of Images"},{"location":"ch3-2/#311-the-classic-image-problem","text":"Suppose a point charge q is held a distance d above an infinite grounded conducting plane (Fig. 3.10). Question : what is the potential in the region above the plane? It's not just (1/4 \\pi \\epsilon_0) q / \\gr , for q will induce a certain amount of negative charge on the nearby surface of the conductor; the total potential is due in part to q directly, and in part to this induced charge. But how can we possibly calculate the potential, when we don't know how much charge is induced or how it is distributed? From a mathematical point of view, our problem is to solve Poisson's equation in the region z > 0 , with a single point charge q at (0, 0, d) , subject to the boundary conditions V = 0 when z = 0 (since the conducting plane is grounded) V \\rightarrow 0 far from the charge (that is, for x^2 + y^2 + z^2 \\gg d^2 The first uniqueness theorem (actually, its corollary) guarantees that there is only one function that meets these requirements. If by trick or clever guess we can discover such a function, it's got to be the answer. Trick: Forget about the actual problem; we're going to study a completely different situation. This new configuration consists of two point charges, +q at (0, 0, d) and -q at (0, 0, -d) , and no conducting plane (Fig. 3.11). For this configuration I can easily write down the potential: V(x, y, z) = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{q}{\\sqrt{x^2 + y^2 + (z - d)^2 }} - \\frac{q}{\\sqrt{x^2 + y^2 + (z + d)^2}} \\right] \\label{3.9} \\tag{3.9} It follows that V = 0 when z = 0 V \\rightarrow 0 for x^2 + y^2 + z^2 \\gg d^2 and the only charge in the region z > 0 is the point charge +q at (0, 0, d) . But these are precisely the conditions of the original problem! Evidently the second configuration happens to produce exactly the same potential as the first configuration, in the \"upper\" region z \\geq 0 . (The \"lower\" region, z < 0 , is completely different, but who cares? The upper part is all we need.) Conclusion : The potential of a point charge above an infinite grounded conductor is given by \\eqref{3.9} , for z > 0 . Notice the crucial role played by the uniqueness theorem in this argument: without it, no one would believe this solution, since it was obtained for a completely different charge distribution. But the uniqueness theorem certifies it: If it satisfies Poisson's equation in the region of interest, and assumes the correct value at the boundaries, then it must be right.","title":"3.1.1: The Classic Image Problem"},{"location":"ch3-2/#322-induced-surface-charge","text":"Now that we know the potential, it is a straightforward matter to compute the surface charge \\sigma induced on the conductor. According to Eq. 2.49, \\sigma = - \\epsilon_0 \\pdv{V}{n} where \\partial V / \\partial n is the normal derivative of V at the surface. In this case the normal direction is the z direction, so \\sigma = \\left. - \\epsilon_0 \\pdv{V}{z} \\right|_{z = 0} From Eq. 3.9 \\pdv{V}{z} = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{-q (z - d)}{[x^2 + y^2 + (z - d)^2]^{3/2}} + \\frac{q(z + d)}{[x^2 + y^2 + (z + d)^2 ]^{3/2}} \\right] so \\sigma(x, y) = \\frac{-qd}{2 \\pi (x^2 + y^2 + d^2)^{3/2}} \\label{3.10} \\tag{3.10} As expected, the induced charge is negative (assuming q is positive) and greatest at x = y = 0 . While we're at it, let's compute the total induced charge Q = \\int \\sigma \\dd{a} This integral, over the xy plane, could be done in Cartesian coordinates, with \\dd{a} = \\dd{x} \\dd{y} , but it's easier to use polar coordinates (r, \\phi) , with r^2 = x^2 + y^2 and \\dd{a} = r \\dd{r} \\dd{\\phi} . Then \\sigma(r) = \\frac{-qd}{2 \\pi (r^2 + d^2)^{3/2}} and Q = \\int_{0} ^{2\\pi} \\int_{0} ^{\\infty} \\frac{-qd}{2 \\pi (r^2 + d^2)^{3/2}} r \\dd{r} \\dd{\\phi} = \\left. \\frac{qd}{\\sqrt{r^2 + d^2}} \\right|_{0} ^{\\infty} = -q \\label{3.11} \\tag{3.11} The total charge induced on the plane is -q , as (with benefit of hindsight) you can perhaps convince yourself it had to be.","title":"3.2.2: Induced Surface Charge"},{"location":"ch3-2/#323-force-and-energy","text":"The charge q is attracted toward the plane, because of the negative induced charge. Let's calculate the force of attraction. Since the potential in the vicinity of q is the same as in the analog problem (the one with +q and -q but no conductor), so also is the field and, therefore, the force \\vec{F} = -\\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{(2d)^2} \\vu{z} \\label{3.12} \\tag{3.12} Beware : It is easy to get carried away, and assume that everything is the same in the two problems. Energy, however, is not the same. With the two point charges and no conductor, Eq. 2.42 gives W = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{2d} But for a single charge and conducting plane, the energy is half this W = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{4d} \\label{3.14} \\tag{3.14} Why half? Think of the energy stored in the fields (Eq. 2.45): W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} In the first case, both the upper region (z > 0) and the lower region (z < 0) contribute, and by symmetry they contribute equally. But in the second case, only the upper region contains a nonzero field, and hence the energy is half as great. Of course, one could also determine the energy by calculating the work required to bring q in from infinity. The force required (to oppose the electrical force in \\eqref{3.12} is (1 / 4 \\pi \\epsilon_0)(q^2/4z^2) \\vu{z} , so \\begin{align} W & = & \\int _{\\infty} ^{d} \\vec{F} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{\\infty} ^d \\frac{q^2}{4z^2} \\dd{z} \\\\ & = & \\frac{1}{4 \\pi \\epsilon_0} \\left. \\left( - \\frac{q^2}{4z} \\right) \\right|_{\\infty} ^d = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{4d} \\end{align} As I move q toward the conductor, I do work only on q. It is true that induced charge is moving in over the conductor, but this costs me nothing, since the whole conductor is at potential zero. By contrast, if I simultaneously bring in two point charges (with no conductor), I do work on both of them, and the total is (again) twice as great.","title":"3.2.3: Force and Energy"},{"location":"ch3-2/#324-other-image-problems","text":"The method just described is not limited to a single point charge; any stationary charge distribution near a grounded conducting plane can be treated in the same way, by introducing its mirror image - hence the name method of images. (Remember that the image charges have the opposite sign; this is what guarantees that the xy plane will be at potential zero.) There are also some exotic problems that can be handled in similar fashion; the nicest of these is the following.","title":"3.2.4: Other Image Problems"},{"location":"ch3-2/#example-32","text":"A point charge q is situated a distance a from the center of a grounded conducting sphere of radius R (Fig. 3.12). Find the potential outside the sphere Solution Examine the completely different configuration, consisting of the point charge q together with another point charge q' = - \\frac{R}{a} q \\label{3.15} \\tag{3.15} placed a distance b = \\frac{R^2}{a} \\label{3.16} \\tag{3.16} to the right of the center of the sphere (Fig 3.13). No conductor, now - just the two point charges. The potential of this configuration is V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{\\gr} + \\frac{q'}{\\gr '} \\right) \\label{3.17} \\tag{3.17} where \\gr and \\gr' are the distances from q and q' , respectively. Now, it happens (see Prob. 3.8) that this potential vanishes at all points on the sphere, and therefore fits the boundary conditions for our original problem, in the exterior region. Conclusion : \\eqref{3.17} is the potential of a point charge near a grounded conducting sphere. (Notice that b is less than R , so the \"image\" charge q' is safely inside the sphere - you cannot put image charges in the region where you are calculating V ; that would change \\rho , and you'd be solving Poisson's equation with the wrong source.) In particular, the force of attraction between the charge and the sphere is F = \\frac{1}{4\\pi \\epsilon_0} \\frac{q q'}{(a - b)^2} = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2 R a}{(a^2 - R^2)^2} \\label{3.18} \\tag{3.18} The method of images is delightfully simple... when it works. But it is as much an art as a science, for you must somehow think up just the right \"auxiliary\" configuration, and for most shapes this is forbiddingly complicated, if not impossible.","title":"Example 3.2"},{"location":"ch3-3/","text":"3.3: Separation of Variables In this section we shall attack Laplace's equation directly, using the method of separation of variables , which is the physicist's favorite tool for solving partial differential equations. The method is applicable in circumstances where the potential (V) or the charge density (\\sigma) is specified on the boundaries of some region, and we are asked to find the potential in the interior. The basic strategy is very simple: We look for solutions that are products of functions, each of which depends on only one of the coordinates. The algebraic details, however, can be formidable, so I'm going to develop the method through a sequence of examples. We'll start with Cartesian coordinates and then do spherical coordinates (I'll leave the cylindrical case for you to tackle on your own, in Prob 3.24). 3.3.1: Cartesian Coordinates Example 3.3 Two infinite grounded metal plates lie parallel to the xz plane, one at y = 0 , the other at y = a (Fig. 3.17). The left end, at x = 0 , is closed off with an infinite strip insulated from the two plates, and maintained at a specific potential V_0(y) . Find the potential inside this 'slot.' Solution The configuration is independent of z, so this is really a two-dimensional problem. In mathematical terms, we must solve Laplace's equation, \\frac{\\partial ^2 V}{\\partial{x^2}} + \\frac{\\partial ^2 V}{\\partial{y^2}} = 0 \\label{3.20} \\tag{3.20} subject to the boundary conditions (i) V = 0 when y = 0 (ii) V = 0 when y = a (iii) V = V_0(y) when x = 0 (iv) V \\rightarrow 0 as x \\rightarrow \\infty (The latter, although not explicitly stated in the problem, is necessary on physical grounds: as you get farther and farther away from the \"hot\" strip at x = 0 , the potential should drop to zero.) Since the potential is specified on all boundaries, the answer is uniquely determined. The first step is to look for solutions in the form of products: V(x, y) = X(x)Y(y) \\tagl{3.22} On the face of it, this is an absurd restriction - the overwhelming majority of solutions to Laplace's equation do not have such a form. For example, V(x, y) = (5x + 6y) satisfies the equation, but you can't express it as the product of a function of x times a function of y. Obviously, we're only going to get a tiny subset of all possible solutions by this means, and it would be a miracle if one of them happened to fit the boundary conditions of our problem... But hang on, because the solutions we do get are very special, and it turns out that by pasting them together we can construct the general solution. Anyway, putting \\eqref{3.22} into \\eqref{3.20} we obtain Y \\frac{d^2X}{dx^2} + X \\frac{d^2 Y}{dy^2} = 0 The next step is to \"separate the variables\" (that is, collect all the x-dependence into one term and all the y-dependence into another). Typically, this is accomplished by dividing through by V: \\frac{1}{X} \\frac{d^2 X}{dx^2} + \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = 0 \\tagl{3.23} Here the first term depends only on x and the second term only on y; in other words, we have an equation of the form f(x) + g(y) = 0 \\label{3.24} \\tag{3.24} Now, there's only one way this could possibly be true: f and g must both be constant . For what if f(x) changed, as you vary x - then if we held y fixed and fiddled with x, the sum f(x) + g(y) would change, in violation of \\eqref{3.24} , which says it's always zero. (That's a simple but somehow rather elusive argument; don't accept it without due thought, because the whole method rides on it.) It follows from \\eqref{3.23} , then, that \\frac{1}{X} \\frac{d^2 X}{dx^2} = C_1 \\quad \\text{ and } \\quad \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = C_2, \\quad \\text{ with } C_1 + C_2 = 0 \\tagl{3.25} One of these constants is positive, the other negative (or perhaps both are zero). In general, one must investigate all possibilities; however in our particular problem we need C_1 positive and C_2 negative, for reasons that will appear in a moment. Thus \\frac{d^2X}{dx^2} = k^2 X, \\qquad \\frac{d^2 Y}{dy^2} = - k^2 Y \\tagl{3.26} Notice what has happened: A partial differential equation has been converted into two ordinary differential equations. The advantage of this is obvious - ordinary differential equations are a lot easier to solve. Indeed: X(x) = A e^{kt} + B e^{-kt}, \\qquad Y(y) = C \\sin ky + D \\cos ky so V(x, y) = (A e^{kt} + B e^{-kt})(C \\sin ky + D \\cos ky) \\tagl{3.27} This is the appropriate separable solution to Laplace's equation; it remains to impose the boundary conditions, and see what they tell us about the constants. To begin at the end, condition (iv) requires tha A equal zero. Absorbing B into C and D, we are left with V(x, y) = e^{-kx} (C\\sin ky + D \\cos ky) Condition (i) now demands that D equal zero V(x, y) = C ^{-kx} \\sin ky \\tagl{3.28} Meanwhile (ii) yields \\sin ka = 0 , from which it follows that k = \\frac{n \\pi}{a} \\quad (n = 1, 2, 3, \\ldots) \\tagl{3.29} (At this point you can see why I chose C_1 positive and C_2 negative: If X were sinusoidal, we could never manage for it to go to zero at infinity, and if Y were exponential we could not make it vanish at both 0 and a . Incidentally, n = 0 is no good, for in that case the potential vanishes everywhere. And we have already excluded negative n's) That's as far as we can go, using separable solutions, and unless V_0(y) just happens to have the form \\sin(n \\pi / a) for some integer n , we simply can't fit the final boundary condition at x = 0 . But now comes the crucial step that redeems the method: Separation of variables has given us an infinite family of solutions (one for each n), and whereas none of them by itself satisfies the final boundary condition, it is possible to combine them in a way that does . Laplace's equation is linear, in the sense that if V_1, V_2, V_3, \\ldots satisfy it, so does any linear combination, for \\laplacian{V} = \\alpha_1 \\laplacian{V_1} + \\alpha_2 \\laplacian{V_2} + \\ldots = 0 \\alpha_1 + 0 \\alpha_2 + \\ldots = 0 Exploiting this fact, we can patch together the separable solutions \\eqref{3.28} to construct a much more general solution: V(x, y) = \\sum_{n=1} ^{\\infty} C_n e^{-n \\pi x / a} \\sin (n \\pi y / a) \\tagl{3.30} This still satisfies three of the boundary conditions; the question is, can we (by astute choice of the coefficients C_n ) fit the final boundary condition (iii)? V(0, y) = \\sum_{n=1} ^{\\infty} C_n \\sin (n \\pi y / a) = V_0(y) \\tagl{3.31} Well, you may recognize this sum - it's a Fourier sine series. And Dirichlet's theorem guarantees that virtually any function V_0(y) - it can even have a finite number of discontinuities - can be expanded in such a series. But how do we actually determine the coefficients C_n , buried as they are in that infinite sum? The device for accomplishing this is so lovely it deserves a name - I call it Fourier's trick , though it seems Euler had used essentially the same idea somewhat earlier. Here's how it goes: Multiply \\eqref{3.31} by \\sin(n' \\pi y /a) (where n' is a positive integer), and integrate from 0 to a: \\sum_{n=1} ^{\\infty} C_n \\int_{0} ^{a} \\sin(n \\pi y / a) \\sin(n' \\pi y/a) \\dd{y} = \\int_{0} ^a V_0(y) \\sin (n' \\pi /a) \\dd{y} \\tagl{3.32} You can work out the integral on the left yourself; the answer is \\int_{0} ^a \\sin (n \\pi y /a) \\sin (n' \\pi y / a) \\dd{y} = \\begin{cases} 0 & \\quad \\text{if } n' \\neq n \\\\ \\frac{a}{2} & \\quad \\text{if } n' = n \\end{cases} \\tagl{3.33} Thus all the terms in the series drop out, save only the one where n = n' , and the left side of \\eqref{3.32} reduces to (a/2)C_{n'} . Conclusion : C_n = \\frac{2}{a} \\int_{0}^a V_0(y) \\sin (n \\pi y /a) \\dd{y} \\tagl{3.34} That does it: \\eqref{3.30} is the solution, with coefficients given by eqref{3.34} . As a concrete example, suppose the strip at x = 0 is a metal plate with constant potential V_0 (remember, it's insulated from the grounded plates at y = 0 and y = a . Then C_n = \\frac{2V_0}{a} \\int_0 ^a \\sin (n \\pi y / a) \\dd y \\\\ = \\frac{2 V_0}{n \\pi} (1 - \\cos n \\pi) = \\begin{cases} 0 & \\quad \\text{if n is even } \\\\ \\frac{4 V_0}{n \\pi} & \\quad \\text{if n is odd} \\end{cases} \\tagl{3.35} Thus V(x, y) = \\frac{4 V_0}{\\pi} \\sum_{n = 1, 3, 5, \\ldots} \\frac{1}{n} e^{- n \\pi x / a} \\sin (n \\pi y / a) \\tagl{3.36} Figure 3.18 is a plot of this potential; Fig. 3.10 shows how the first few terms in the Fourier series combine to make a better and better approximation to the constant V_0 : (a) is the n=1 term only, (b) includes n up to 5, (c) is the sum of the first 10 terms, and (d) is the sum of the first 100 terms. Incidentally, the infinite series in Eq. 3.36 can be summed explicitly (try your hand at it if you like); the result is V(x, y) = \\frac{2V_0}{\\pi} \\tan^{-1} \\left( \\frac{\\sin(\\pi y / a)}{\\sinh(\\pi x /a )} \\right) \\tagl{3.37} In this form, it is easy to check that Laplace's equation is obeyed and the four boundary conditions are satisfied The success of this method hinged on two extraordinary properties of the separable solutions \\eqref{3.28} and \\eqref{3.29} : completeness and orthogonality . A set of functions f_n(y) is said to be complete if any other function f(y) can be expressed as a linear combination of them: f(y) = \\sum_{n=1} ^{\\infty} C_n f_n(y) \\tagl{3.38} The functions \\sin (n \\pi y/a) are complete on the interval 0 \\leq y \\leq a . It was this fact, guaranteed by Dirichlet's theorem, that assured us \\eqref{3.31} could be satisfied, given the proper choice of the coefficients C_n . (The proof of completeness, for a particular set of functions, is an extremely difficult business, and I'm afraid physicists tend to assume it's true and leave the checking to others.) A set of functions is orthogonal if the integral of the product of any two different members of the set is zero: \\int_0 ^a f_n(y) f_{n'} (y) \\dd{y} = 0 \\quad \\text{for } n' \\neq n The sine functions are orthogonal \\eqref{3.33} ; that is the property on which Fourier's trick is based, allowing us to kill off all terms but one in the infinite series and thereby solve for the coefficients C_n (Proof of orthogonality is generally quite simple, either by direct integration or by analysis of the differential equation from which the functions came.) Example 3.4 Two infinitely-long grounded metal plates, again at y=0 and y=a are connected at x= \\pm b by metal strips maintained at a constant potential V_0 , as shown in Fig. 3.20 (a thin layer of insulation at each corner prevents them from shorting out). Find the potential inside the resulting rectangular pipe. Solution Once again, the configuration is independent of z. Our problem is to solve Laplace's equation \\frac{\\partial ^2 V}{\\partial{x^2}} + \\frac{\\partial ^2 V}{\\partial{y^2}} = 0 subject to the boundary conditions (i) V = 0 when y = 0 (ii) V = 0 when y = a (iii) V = V_0 when x = b (iv) V = V_0 when x = -b The argument runs as before, up to \\eqref{3.27} : V(x, y) = (A e^{kt} + B e^{-kt})(C \\sin ky + D \\cos ky) This time, however, we cannot set A = 0 ; the region in question does not extend to x = \\infty , so e^{kx} is perfectly acceptable. On the other hand, the situation is symmetric with respect to x, so V(-x, y) = V(x, y) , and it follows that A = B . Using e^{kx} + e^{-kx} = 2 \\cosh kx and absorbing 2A into C and D , we have V(x, y) = \\cosh kx (C \\sin ky + D\\cos ky) Boundary conditions (i) and (ii) require, as before, that D = 0 and k = n\\pi /a , so V(x, y) = C \\cosh (n \\pi x /a )\\sin(n \\pi y/a) \\tagl{3.41} Because V(x, y) is even in x, it will automatically meet conditions (iv) if it fits (iii). It remains, therefore, to construct the general linear combination V(x, y) = \\sum _{n=1}^{\\infty} C_n \\cosh (n \\pi x / a) \\sin(n \\pi y /a) and pick the coefficients C_n in such a way as to satisfy condition (iii): V(b, y) = \\sum_{n=1}^{\\infty} C_n \\cosh (n \\pi b /a) \\sin(n \\pi y/a) = V_0 This is the same problem in Fourier analysis that we faced before; I quote the result from \\eqref{3.35} ; C_n \\cosh (n \\pi b/a) = \\begin{cases} 0 & \\quad \\text {if n is even} \\\\ \\frac{4 V_0}{n \\pi} & \\quad \\text{if n is odd} \\end{cases} Conclusion : The potential in this case is given by V(x, y) = \\frac{4 V_0}{\\pi} \\sum_{n=1, 3, 5\\ldots} \\frac{1}{n} \\frac{\\cosh(n \\pi x/a)}{\\cosh(n \\pi b/a)} \\sin(n \\pi y/a) \\tagl{3.42} This function is shown in Fig. 3.21 Example 3.5 An infinitely long rectangular metal pipe (sides a and b) is grounded, but one end, at x = 0 , a 'hot' plate is maintained at a specified potential V_0(y, z) , as indicated in Fig. 3.22. Find the potential inside the pipe. Solution This is genuinely a three-dimensional problem, \\frac{\\partial ^2 V}{\\partial{x^2}} + \\frac{\\partial ^2 V}{\\partial{y^2}} + \\frac{\\partial ^2 V}{\\partial{z^2}} = 0 \\tagl{3.43} subject to the boundary conditions - (i) V = 0 when y = 0 - (ii) V = 0 when y = a - (iii) V = 0 when z = 0 - (iv) V = 0 when z = b - (v) V \\rightarrow 0 as x \\rightarrow \\infty - (vi) V = V_0(y, z) whem x = 0 As always, we look for solutions that are products: V(x, y, z) = X(x)Y(y)Z(z) \\tagl{3.45} Putting this into \\eqref{3.43} and dividing by V, we find \\frac{1}{X} \\frac{d^2 X}{dx^2} + \\frac{1}{Y} \\frac{d^2 Y}{dy^2} + \\frac{1}{Z} \\frac{d^2 Z}{dz^2} = 0 It follows that \\frac{1}{X} \\frac{d^2 X}{dx^2} = C_1 , \\quad \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = C_2 , \\quad \\frac{1}{Z} \\frac{d^2 Z}{dz^2} = C_3 , \\text{ with } C_1 + C_2 + C_3 = 0 Our previous experience in Ex. 3.3 suggests that C_1 must be positive, C_2 and C_3 negative. Setting C_2 = -k^2 and C_3 = -l^2 , we have C_1 = k^2 + l^2 , and hence \\frac{d^2 X}{dx^2} = (k^2 + l^2)X, \\quad \\frac{d^2 Y}{dy^2} = -k^2 Y, \\quad \\frac{d^2 Z}{dz^2} = -l^2 Z \\tagl{3.46} Once again, separation of variables has turned a partial differential equation into ordinary differential equations. The solutions are \\begin{align*} X(x) & = A e^{\\sqrt{k^2 + l^2} x} + B e^{- \\sqrt{k^2 + l^2} x} \\\\ Y(y) & = C \\sin ky + D \\cos ky \\\\ Z(z) & = E \\sin lz + F \\cos lz \\end{align*} Boundary condition (v) implies A =0 , (i) gives D = 0 , and (iii) yields F = 0 whereas (ii) and (iv) require that k = n\\pi /a and l =m \\pi /b , where n and m are positive integers. Combining the remaining constants, we are left with V(x, y, z) = C e^{-\\pi \\sqrt{(n/a)^2 + (m/b)^2}x} \\sin (n \\pi y / a) \\sin(m \\pi z /b) \\tagl{3.47} This solution meets all the boundary conditions except (vi). It contains two unspecified integers (n and m), and the most general linear combination is a double sum V(x, y, z) = \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C e^{-\\pi \\sqrt{(n/a)^2 + (m/b)^2}x} \\sin (n \\pi y / a) \\sin(m \\pi z /b) = V_0(y, z) \\tagl{3.48} We hope to fit the remaining boundary condition, V(0, y, z) = \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C \\sin (n \\pi y / a) \\sin(m \\pi z /b) = V_0(y, z) \\tagl{3.49} by appropriate choice of the coefficients C_{n, m} . To determine these constants, we multiply by \\sin(n' n \\pi y/a) \\sin(m' \\pi z / b) , where n' and m' are arbitrary positive integers, and integrate \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C_{n, m} \\int_0 ^a \\sin (n \\pi y/a) \\sin(n' \\pi y/a) \\dd{y} \\int_0 ^b \\sin(m \\pi z/b) \\sin (m' \\pi z/b) \\dd{z} \\\\ = \\int_0 ^a \\int_0 ^b V_0(y, z) \\sin(n' \\pi y/a) \\sin(m' \\pi z/b) \\dd{y} \\dd{z} Quoting \\eqref{3.33} , the left side is (ab/4) C_{n', m'} , so C_{n, m} = \\frac{4}{ab} \\int_0 ^a \\int_0 ^b V_0(y, z) \\sin (n \\pi y/a) \\sin(m\\pi z/b) \\dd{y} \\dd{z} \\tagl{3.50} Equation \\eqref{3.48} , with the coefficients given by \\eqref{3.50} , is the solution to our problem. For instance, if the end of the tube is a conductor at constant potential V_0 , C_{n, m} = \\frac{4}{ab} \\int_0 ^a \\sin(n \\pi y/a) \\dd{y} \\int_0 ^b \\sin(m \\pi z/b) \\dd{z} \\\\ = \\begin{cases} 0 & \\qquad \\text{if n or m is even} \\\\ \\frac{16 V_0}{\\pi^2 nm} & \\qquad \\text{if n and m are odd} \\end{cases} \\tagl{3.51} In this case, V(x, y, z) = \\frac{16V_0}{\\pi^2} \\sum_{n,m=1,3,5,\\ldots} ^{\\infty} \\frac{1}{nm} e^{-\\pi \\sqrt{(n/a)^2 + (m/b)^2}x} \\sin(n \\pi y/a) \\sin(m \\pi z/b) \\tagl{3.52} Notice that successive terms decrease rapidly; a reasonable approximation would be obtained by keeping only the first few. 3.3.2: Spherical Coordinates In the examples considered so far, Cartesian coordinates were clearly appropriate, since the boundaries were planes. For round objects, spherical coordinates are more natural. In the spherical system, Laplace's equation reads: \\frac{1}{r^2} \\pdv{}{r} \\left( r^2 \\pdv{V}{r} \\right) + \\frac{1}{r^2 \\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{V}{\\theta} \\right) + \\frac{1}{r^2\\sin ^2 \\theta} \\frac{\\partial ^2 V}{\\partial \\phi ^2} = 0 \\tagl{3.53} I shall assume the problem has azimuthal symmetry , so that V is independent of \\phi ; In that case, \\eqref{3.53} reduces to \\pdv{}{r} \\left( r^2 \\pdv{V}{r} \\right) + \\frac{1}{\\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{V}{\\theta} \\right) = 0 \\tagl{3.54} As before, we look for solutions that are products: V(r, \\theta) = R(r) \\Theta (\\theta) \\tagl{3.55} Putting this into \\eqref{3.54} , and dividing by V , \\frac{1}{R} \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) + \\frac{1}{\\Theta \\sin \\theta} \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = 0 \\tagl{3.56} Since the first term depends only on r , and the second only on \\theta , it follows that each must be a constant: \\frac{1}{R} \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) = l(l+1), \\quad \\frac{1}{\\Theta \\sin \\theta} \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = -l(l+1) \\tagl{3.57} Here l(l+1) is just a fancy way of writing the separation constant, whose convenience will appear shortly. As always, separation of variables has converted a partial differential equation into ordinary differential equations. The radial equation, \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) = l(l+1)R \\tagl{3.58} has the general solution R(r) = A r^l + \\frac{B}{r^{l+1}} \\tagl{3.59} as you can easily check; A and B are the two arbitrary constants to be expected in the solution of a second-order differential equation. But the angular equation, \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = -l(l+1) \\sin \\theta \\Theta \\tagl{3.60} is not so simple. The solutions are Legendre polynomials in the variable \\cos \\theta : \\Theta (\\theta ) = P_l (\\cos \\theta ) \\tagl{3.61} P_l (x) is most conveniently defined by the Rodrigues formula : P_l(x) \\equiv \\frac{1}{2^l l!}\\left( \\dv{}{x} \\right)^l (x^2 - 1)^l \\tagl{3.62} The first few Legendre polynomials are listed: Legendre Polynomials P_0 - P_5 \\begin{align*} P_0(x) & = 1 \\\\ P_1(x) & = x \\\\ P_2(x) & = (3x^2 - 1)/2 \\\\ P_3(x) & = (5x^3 - 3x)/2 \\\\ P_4(x) & = (35x^4 - 30x^2 + 3)/8 \\\\ P_5(x) & = (63x^5 - 70x^3 + 15x)/8 \\end{align*} Notice that P_l(x) is (as the name suggests) an _l_th-order polynomial in x; it contains only even powers if l is even, and only odd powers if l is odd. The factor in front (1/2^l l! was chosen in order that P_l(1) = 1 \\tagl{3.63} The Rodrigues formula obviously only works for nonnegative integer values of l. Moreover, it provides us with only one solution. But \\eqref{3.60} is second-order, and it should possess two independent solutions for every value of l . It turns out that these \"other solutions\" blow up at \\theta = 0 and/or \\theta = \\pi , and are therefore unacceptable on physical grounds. For instance, the second solution for l=0 is \\Theta(\\theta) = \\ln \\left( \\tan \\frac{\\theta}{2} \\right) \\tagl{3.64} You might want to check for yourself that this satisfies \\eqref{3.60} . In the case of azimuthal symmetry, then, the most general separable solution to Laplace's equation, consistent with minimal physical requirements, is V(r, \\theta) = \\left( A r^l + \\frac{B}{r^{l+1}} \\right) P_l(\\cos \\theta) (There was no need to include an overall constant in \\eqref{3.61} because it can be absorbed into A and B at this stage.) As before, separation of variables yields an infinite set of solutions, one for each l . The general solution is the linear combination of separable solutions: V(r, \\theta) = \\sum_{l=0} ^{\\infty} \\left( A r^l + \\frac{B}{r^{l+1}} \\right) P_l(\\cos \\theta) \\tagl{3.65} The following examples illustrate the power of this important result. Example 3.6 The potential V_0(\\theta) is specified on the surface of a hollow sphere, of radius R . Find the potential inside the sphere. Solution In this case, B_l = 0 for all l , otherwise the potential would blow up at the origin. Thus, V(r, \\theta) = \\sum_{l = 0} ^{\\infty} A_l r^l P_l(\\cos \\theta) \\tagl{3.66} At r = R this must match the specified function V_0(\\theta) : V(R, \\theta) = \\sum_{l=0}^\\infty A_l R^l P_l(\\cos \\theta) = V_0(\\theta) \\tagl{3.67} Can this equation be satisfied, for an appropriate choice of coefficients A_l ? Yes: The Legendre polynomials (like the sines) constitute a complete set of functions, on the interval -1 \\leq x \\leq 1 (0 \\leq \\theta \\leq \\pi) . How do we determine the constants? Again, by Fourier's trick, for the Legendre polynomials (like the sines) are orthogonal functions: \\begin{align*} \\int_{-1}^1 P_l(x) P_{l'}(x) \\dd{x} & = \\int_0 ^\\pi P_l(\\cos \\theta) P_{l'}(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\\\ & = \\begin{cases} 0, & \\quad \\text{if } l' \\neq l \\\\ \\frac{2}{2l +1} , & \\quad \\text{if } l' = l \\end{cases} \\end{align*} \\tagl{3.68} Thus, multiplying \\eqref{3.67} by P_{l'}(\\cos \\theta) \\sin \\theta and integrating, we have A_{l'} R^{l'} \\frac{2}{2l' + 1} = \\int_{0} ^\\pi V_0(\\theta) P_{l'}(\\cos \\theta) \\sin \\theta \\dd{\\theta} or A_l = \\frac{2l+1}{2R^l} \\int_0 ^\\pi V_0(\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.69} \\eqref{3.66} is the solution to our problem, with the coefficients given by \\eqref{3.69} . It can be difficult to evaluate integrals of the form \\eqref{3.69} analytically, and in practice it is often easier to solve \\eqref{3.67} \"by eyeball.\" For instance, suppose we are told that the potential on the sphere is V_0(\\theta) = k \\sin^2 (\\theta/2) \\tagl{3.70} where k is constant. Using the half-angle formula, we rewrite this as V_0(\\theta) = \\frac{k}{2}(1 - \\cos \\theta) = \\frac{k}{2} [P_0(\\cos \\theta) - P_1 (\\cos \\theta)] Putting this into \\eqref{3.67} , we read off immediately that A_0 = k/2 , A_1 = -k/(2R) , and all other A_l 's vanish. Therefore V(r, \\theta) = \\frac{k}{2} \\left[ r^0 P_{0}(\\cos \\theta) - \\frac{r^1}{R} P_1 (\\cos \\theta) \\right] = \\frac{k}{2} \\left( 1 - \\frac{r}{R} \\cos \\theta \\right) \\tagl{3.71} Example 3.7 The potential V_0(\\theta) is again specified on the surface of a sphere of radius R , but this time we are asked to find the potential outside , assuming there is no charge there. Solution In this case it's the A_l 's that must be zero (or else V would not go to zero at \\infty ), so V(r, \\theta) = \\sum_{l=0} ^\\infty \\frac{B_l}{r^{l+1}} P_l(\\cos \\theta) = V_0(\\theta) \\tagl{3.72} Multiplying by P_{l'}(\\cos \\theta) \\sin \\theta and integrating - exploiting, again, the orthogonality relation 3.68 - we have \\frac{B_{l'}}{R^{l'+1}} \\frac{2}{2l' + 1} = \\int_0 ^\\pi V_0(\\theta) P_{l'}(\\cos \\theta) \\sin \\theta \\dd{\\theta} or B_l = \\frac{2l + 1}{2} R^{l+1} \\int_0 ^\\pi V_0(\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.73} \\eqref{3.72} , with the coefficients given by \\eqref{3.73} , is the solution to our problem. Example 3.8 An uncharged metal sphere of radius R is placed in an otherwise uniform electric field \\vec{E} = E_0 \\vu{z} . The field will push positive charge to the 'northern' surface of the sphere, and - symmetrically - negative charge to the 'southern' surface (Fig. 3.24). This induced charge, in turn, distorts the field in the neighborhood of the sphere. Find the potential in the region outside the sphere. Solution The sphere is an equipotential - we may as well set it to zero. Then by symmetry the entire xy plane is at potential zero. This time, however, V does not go to zero at large z . In fact, far from the sphere the field is E_0 \\vu{z} and hence V \\rightarrow - E_0 z + C Since V = 0 in the equatorial plane, the constant C must be zero. Accordingly, the boundary conditions for this problem are - (i) V = 0 when r = R - (ii) V \\rightarrow - E_0 r \\cos \\theta for r \\gg R We must fit these boundary conditions with a function of the form \\eqref{3.65} . The first condition yields A_l R^l + \\frac{B_l}{R^{l+1}} = 0 or B_l = -A_l R^{2l+1} \\tagl{3.75} so V(r, \\theta) = \\sum_{l = 0} ^{\\infty} A_l \\left( r^l - \\frac{R^{2l+1}}{r^{l+1}} \\right) P_l(\\cos \\theta) For r \\gg R , the second term in parentheses is negligible, and therefore the condition (ii) requires that \\sum_{l=0}^\\infty A_l R^{l} P_l (\\cos \\theta) = - E_0 r \\cos \\theta Evidently only one term is present: l = 1 . In fact, since P_1(\\cos \\theta) = \\cos \\theta we can read off immediately A_1 = - E_0, \\qquad \\text{ all other }A_l's \\text{ zero} Conclusion : V(r, \\theta) = - E_0 \\left( r - \\frac{R^3}{r^2} \\right) \\cos \\theta \\tagl{3.76} The first term (-E_0 r \\cos \\theta) is due to the external field; the contribution attributable to the induced charge is E_0 \\frac{R^3}{r^2} \\cos \\theta If you want to know the induced charge density, it can be calculated in the usual way: \\sigma(\\theta) = - \\epsilon_0 \\left. \\pdv{V}{r} \\right|_{r = R} = \\epsilon_0 E_0 \\left. \\left( 1 + 2 \\frac{R^3}{r^3} \\right) \\cos \\theta \\right|_{r = R} = 3 \\epsilon_0 E_0 \\cos \\theta \\tagl{3.77} As expected, it is positive in the 'northern' hemisphere 0 \\leq \\theta \\leq \\pi /2 and negative in the 'southern' \\pi/2 \\leq \\theta \\leq \\pi . Example 3.9 A specified charge density \\sigma_0(\\theta) is glued over the surface of a spherical shell of radius R . Find the resulting potential inside and outside the sphere. Solution You could, of course, do this by direct integration: V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma_0}{\\gr} \\dd{a} but separation of variables is often easier. For the interior region, we have V(r, \\theta) = \\sum_{l = 0}^\\infty A_l r^l P_l (\\cos \\theta) \\quad (r \\leq R) \\tagl{3.78} (no B_l terms - they blow up at the origin); in the exterior region V(r, \\theta) = \\sum_{l=0}^\\infty \\frac{B_l}{r^{l+1}} P_l(\\cos \\theta) \\quad (r \\geq R) \\tagl{3.79} (no A_l terms - they don't go to zero at infinity). These two functions must be joined together by the appropriate boundary conditions at the surface itself. First, the potential is continuous at r = R (Eq. 2.34): \\sum_{l=0}^\\infty A_l R^l P_l(\\cos \\theta) = \\sum_{l=0} ^\\infty \\frac{B_l}{R^{l+1}} P_l(\\cos \\theta) \\tagl{3.80} It follows that the coefficients of like Legendre polynomial are equal: B_l = A_l R^{2l+1} \\tagl{3.81} (To prove that formally, multiply both sides of \\eqref{3.80} by P_{l'} (\\cos \\theta)\\sin \\theta and integrate from 0 to \\pi , using the orthogonality relation \\eqref{3.68} .) Second, the radial derivative of V suffers a discontinuity at the surface (Eq. 2.36): \\left. \\left( \\pdv{V_{out}}{r} - \\pdv{V_{in}}{r} \\right) \\right|_{r = R} = - \\frac{1}{\\epsilon_0} \\sigma_0(\\theta) \\tagl{3.82} Thus, - \\sum_{l=0}^\\infty (l+1) \\frac{B_l}{R^{l+2}} P_l(\\cos \\theta) - \\sum_{l=0}^\\infty l A_l R^{l-1} P_l(\\cos \\theta) = - \\frac{1}{\\epsilon_0} \\sigma_0 (\\theta) or, using \\eqref{3.81} , \\sum_{l=0}^\\infty (2l+1) A_l R^{l-1} P_l(\\cos \\theta) = \\frac{1}{\\epsilon_0} \\sigma_0(\\theta) \\tagl{3.83} From here, the coefficients can be determined using Fourier's trick A_l = \\frac{1}{2 \\epsilon_0 R^{l-1}} \\int_0 ^\\pi \\sigma_0 (\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.84} Equations 3.78 and 3.79 constitute the solution to our problem, with the coefficients given by \\eqref{3.81} and \\eqref{3.84} . For instance, if \\sigma_0(\\theta) = k \\cos \\theta = k P_1 (\\cos \\theta) \\tagl{3.85} for some constant k , then all the A_l 's are zero except for l = 1 , and A_1 = \\frac{k}{2 \\epsilon_0} \\int_0 ^\\pi [P_1(\\cos \\theta)]^2 \\sin \\theta \\dd{\\theta} = \\frac{k}{3\\epsilon_0} The potential inside the sphere is therefore V(r, \\theta) = \\frac{k}{3 \\epsilon_0} r \\cos \\theta \\quad (r \\leq R) \\tagl{3.86} whereas outside the sphere V(r, \\theta) = \\frac{kR^3}{3 \\epsilon_0} \\frac{1}{r^2} \\cos \\theta \\quad (r \\geq R) \\tagl{3.87} In particular, if \\sigma_0(\\theta) is the induced charge on a metal sphere in an external field E_0(\\vu{z}) , so that k = 3 \\epsilon_0 E_0 \\eqref{3.77} , then the potential inside is E_0 r \\cos \\theta = E_0 z , and the field is -E_0 \\vu{z} - exactly right to cancel off the external field, as of course it should be. Outside the sphere the potential due to this surface charge is E_0 \\frac{R^3}{r^2} \\cos \\theta consistent with our conclusion in Example 3.8.","title":"3.3 - Separation of Variables"},{"location":"ch3-3/#33-separation-of-variables","text":"In this section we shall attack Laplace's equation directly, using the method of separation of variables , which is the physicist's favorite tool for solving partial differential equations. The method is applicable in circumstances where the potential (V) or the charge density (\\sigma) is specified on the boundaries of some region, and we are asked to find the potential in the interior. The basic strategy is very simple: We look for solutions that are products of functions, each of which depends on only one of the coordinates. The algebraic details, however, can be formidable, so I'm going to develop the method through a sequence of examples. We'll start with Cartesian coordinates and then do spherical coordinates (I'll leave the cylindrical case for you to tackle on your own, in Prob 3.24).","title":"3.3: Separation of Variables"},{"location":"ch3-3/#331-cartesian-coordinates","text":"","title":"3.3.1: Cartesian Coordinates"},{"location":"ch3-3/#example-33","text":"Two infinite grounded metal plates lie parallel to the xz plane, one at y = 0 , the other at y = a (Fig. 3.17). The left end, at x = 0 , is closed off with an infinite strip insulated from the two plates, and maintained at a specific potential V_0(y) . Find the potential inside this 'slot.' Solution The configuration is independent of z, so this is really a two-dimensional problem. In mathematical terms, we must solve Laplace's equation, \\frac{\\partial ^2 V}{\\partial{x^2}} + \\frac{\\partial ^2 V}{\\partial{y^2}} = 0 \\label{3.20} \\tag{3.20} subject to the boundary conditions (i) V = 0 when y = 0 (ii) V = 0 when y = a (iii) V = V_0(y) when x = 0 (iv) V \\rightarrow 0 as x \\rightarrow \\infty (The latter, although not explicitly stated in the problem, is necessary on physical grounds: as you get farther and farther away from the \"hot\" strip at x = 0 , the potential should drop to zero.) Since the potential is specified on all boundaries, the answer is uniquely determined. The first step is to look for solutions in the form of products: V(x, y) = X(x)Y(y) \\tagl{3.22} On the face of it, this is an absurd restriction - the overwhelming majority of solutions to Laplace's equation do not have such a form. For example, V(x, y) = (5x + 6y) satisfies the equation, but you can't express it as the product of a function of x times a function of y. Obviously, we're only going to get a tiny subset of all possible solutions by this means, and it would be a miracle if one of them happened to fit the boundary conditions of our problem... But hang on, because the solutions we do get are very special, and it turns out that by pasting them together we can construct the general solution. Anyway, putting \\eqref{3.22} into \\eqref{3.20} we obtain Y \\frac{d^2X}{dx^2} + X \\frac{d^2 Y}{dy^2} = 0 The next step is to \"separate the variables\" (that is, collect all the x-dependence into one term and all the y-dependence into another). Typically, this is accomplished by dividing through by V: \\frac{1}{X} \\frac{d^2 X}{dx^2} + \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = 0 \\tagl{3.23} Here the first term depends only on x and the second term only on y; in other words, we have an equation of the form f(x) + g(y) = 0 \\label{3.24} \\tag{3.24} Now, there's only one way this could possibly be true: f and g must both be constant . For what if f(x) changed, as you vary x - then if we held y fixed and fiddled with x, the sum f(x) + g(y) would change, in violation of \\eqref{3.24} , which says it's always zero. (That's a simple but somehow rather elusive argument; don't accept it without due thought, because the whole method rides on it.) It follows from \\eqref{3.23} , then, that \\frac{1}{X} \\frac{d^2 X}{dx^2} = C_1 \\quad \\text{ and } \\quad \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = C_2, \\quad \\text{ with } C_1 + C_2 = 0 \\tagl{3.25} One of these constants is positive, the other negative (or perhaps both are zero). In general, one must investigate all possibilities; however in our particular problem we need C_1 positive and C_2 negative, for reasons that will appear in a moment. Thus \\frac{d^2X}{dx^2} = k^2 X, \\qquad \\frac{d^2 Y}{dy^2} = - k^2 Y \\tagl{3.26} Notice what has happened: A partial differential equation has been converted into two ordinary differential equations. The advantage of this is obvious - ordinary differential equations are a lot easier to solve. Indeed: X(x) = A e^{kt} + B e^{-kt}, \\qquad Y(y) = C \\sin ky + D \\cos ky so V(x, y) = (A e^{kt} + B e^{-kt})(C \\sin ky + D \\cos ky) \\tagl{3.27} This is the appropriate separable solution to Laplace's equation; it remains to impose the boundary conditions, and see what they tell us about the constants. To begin at the end, condition (iv) requires tha A equal zero. Absorbing B into C and D, we are left with V(x, y) = e^{-kx} (C\\sin ky + D \\cos ky) Condition (i) now demands that D equal zero V(x, y) = C ^{-kx} \\sin ky \\tagl{3.28} Meanwhile (ii) yields \\sin ka = 0 , from which it follows that k = \\frac{n \\pi}{a} \\quad (n = 1, 2, 3, \\ldots) \\tagl{3.29} (At this point you can see why I chose C_1 positive and C_2 negative: If X were sinusoidal, we could never manage for it to go to zero at infinity, and if Y were exponential we could not make it vanish at both 0 and a . Incidentally, n = 0 is no good, for in that case the potential vanishes everywhere. And we have already excluded negative n's) That's as far as we can go, using separable solutions, and unless V_0(y) just happens to have the form \\sin(n \\pi / a) for some integer n , we simply can't fit the final boundary condition at x = 0 . But now comes the crucial step that redeems the method: Separation of variables has given us an infinite family of solutions (one for each n), and whereas none of them by itself satisfies the final boundary condition, it is possible to combine them in a way that does . Laplace's equation is linear, in the sense that if V_1, V_2, V_3, \\ldots satisfy it, so does any linear combination, for \\laplacian{V} = \\alpha_1 \\laplacian{V_1} + \\alpha_2 \\laplacian{V_2} + \\ldots = 0 \\alpha_1 + 0 \\alpha_2 + \\ldots = 0 Exploiting this fact, we can patch together the separable solutions \\eqref{3.28} to construct a much more general solution: V(x, y) = \\sum_{n=1} ^{\\infty} C_n e^{-n \\pi x / a} \\sin (n \\pi y / a) \\tagl{3.30} This still satisfies three of the boundary conditions; the question is, can we (by astute choice of the coefficients C_n ) fit the final boundary condition (iii)? V(0, y) = \\sum_{n=1} ^{\\infty} C_n \\sin (n \\pi y / a) = V_0(y) \\tagl{3.31} Well, you may recognize this sum - it's a Fourier sine series. And Dirichlet's theorem guarantees that virtually any function V_0(y) - it can even have a finite number of discontinuities - can be expanded in such a series. But how do we actually determine the coefficients C_n , buried as they are in that infinite sum? The device for accomplishing this is so lovely it deserves a name - I call it Fourier's trick , though it seems Euler had used essentially the same idea somewhat earlier. Here's how it goes: Multiply \\eqref{3.31} by \\sin(n' \\pi y /a) (where n' is a positive integer), and integrate from 0 to a: \\sum_{n=1} ^{\\infty} C_n \\int_{0} ^{a} \\sin(n \\pi y / a) \\sin(n' \\pi y/a) \\dd{y} = \\int_{0} ^a V_0(y) \\sin (n' \\pi /a) \\dd{y} \\tagl{3.32} You can work out the integral on the left yourself; the answer is \\int_{0} ^a \\sin (n \\pi y /a) \\sin (n' \\pi y / a) \\dd{y} = \\begin{cases} 0 & \\quad \\text{if } n' \\neq n \\\\ \\frac{a}{2} & \\quad \\text{if } n' = n \\end{cases} \\tagl{3.33} Thus all the terms in the series drop out, save only the one where n = n' , and the left side of \\eqref{3.32} reduces to (a/2)C_{n'} . Conclusion : C_n = \\frac{2}{a} \\int_{0}^a V_0(y) \\sin (n \\pi y /a) \\dd{y} \\tagl{3.34} That does it: \\eqref{3.30} is the solution, with coefficients given by eqref{3.34} . As a concrete example, suppose the strip at x = 0 is a metal plate with constant potential V_0 (remember, it's insulated from the grounded plates at y = 0 and y = a . Then C_n = \\frac{2V_0}{a} \\int_0 ^a \\sin (n \\pi y / a) \\dd y \\\\ = \\frac{2 V_0}{n \\pi} (1 - \\cos n \\pi) = \\begin{cases} 0 & \\quad \\text{if n is even } \\\\ \\frac{4 V_0}{n \\pi} & \\quad \\text{if n is odd} \\end{cases} \\tagl{3.35} Thus V(x, y) = \\frac{4 V_0}{\\pi} \\sum_{n = 1, 3, 5, \\ldots} \\frac{1}{n} e^{- n \\pi x / a} \\sin (n \\pi y / a) \\tagl{3.36} Figure 3.18 is a plot of this potential; Fig. 3.10 shows how the first few terms in the Fourier series combine to make a better and better approximation to the constant V_0 : (a) is the n=1 term only, (b) includes n up to 5, (c) is the sum of the first 10 terms, and (d) is the sum of the first 100 terms. Incidentally, the infinite series in Eq. 3.36 can be summed explicitly (try your hand at it if you like); the result is V(x, y) = \\frac{2V_0}{\\pi} \\tan^{-1} \\left( \\frac{\\sin(\\pi y / a)}{\\sinh(\\pi x /a )} \\right) \\tagl{3.37} In this form, it is easy to check that Laplace's equation is obeyed and the four boundary conditions are satisfied The success of this method hinged on two extraordinary properties of the separable solutions \\eqref{3.28} and \\eqref{3.29} : completeness and orthogonality . A set of functions f_n(y) is said to be complete if any other function f(y) can be expressed as a linear combination of them: f(y) = \\sum_{n=1} ^{\\infty} C_n f_n(y) \\tagl{3.38} The functions \\sin (n \\pi y/a) are complete on the interval 0 \\leq y \\leq a . It was this fact, guaranteed by Dirichlet's theorem, that assured us \\eqref{3.31} could be satisfied, given the proper choice of the coefficients C_n . (The proof of completeness, for a particular set of functions, is an extremely difficult business, and I'm afraid physicists tend to assume it's true and leave the checking to others.) A set of functions is orthogonal if the integral of the product of any two different members of the set is zero: \\int_0 ^a f_n(y) f_{n'} (y) \\dd{y} = 0 \\quad \\text{for } n' \\neq n The sine functions are orthogonal \\eqref{3.33} ; that is the property on which Fourier's trick is based, allowing us to kill off all terms but one in the infinite series and thereby solve for the coefficients C_n (Proof of orthogonality is generally quite simple, either by direct integration or by analysis of the differential equation from which the functions came.)","title":"Example 3.3"},{"location":"ch3-3/#example-34","text":"Two infinitely-long grounded metal plates, again at y=0 and y=a are connected at x= \\pm b by metal strips maintained at a constant potential V_0 , as shown in Fig. 3.20 (a thin layer of insulation at each corner prevents them from shorting out). Find the potential inside the resulting rectangular pipe. Solution Once again, the configuration is independent of z. Our problem is to solve Laplace's equation \\frac{\\partial ^2 V}{\\partial{x^2}} + \\frac{\\partial ^2 V}{\\partial{y^2}} = 0 subject to the boundary conditions (i) V = 0 when y = 0 (ii) V = 0 when y = a (iii) V = V_0 when x = b (iv) V = V_0 when x = -b The argument runs as before, up to \\eqref{3.27} : V(x, y) = (A e^{kt} + B e^{-kt})(C \\sin ky + D \\cos ky) This time, however, we cannot set A = 0 ; the region in question does not extend to x = \\infty , so e^{kx} is perfectly acceptable. On the other hand, the situation is symmetric with respect to x, so V(-x, y) = V(x, y) , and it follows that A = B . Using e^{kx} + e^{-kx} = 2 \\cosh kx and absorbing 2A into C and D , we have V(x, y) = \\cosh kx (C \\sin ky + D\\cos ky) Boundary conditions (i) and (ii) require, as before, that D = 0 and k = n\\pi /a , so V(x, y) = C \\cosh (n \\pi x /a )\\sin(n \\pi y/a) \\tagl{3.41} Because V(x, y) is even in x, it will automatically meet conditions (iv) if it fits (iii). It remains, therefore, to construct the general linear combination V(x, y) = \\sum _{n=1}^{\\infty} C_n \\cosh (n \\pi x / a) \\sin(n \\pi y /a) and pick the coefficients C_n in such a way as to satisfy condition (iii): V(b, y) = \\sum_{n=1}^{\\infty} C_n \\cosh (n \\pi b /a) \\sin(n \\pi y/a) = V_0 This is the same problem in Fourier analysis that we faced before; I quote the result from \\eqref{3.35} ; C_n \\cosh (n \\pi b/a) = \\begin{cases} 0 & \\quad \\text {if n is even} \\\\ \\frac{4 V_0}{n \\pi} & \\quad \\text{if n is odd} \\end{cases} Conclusion : The potential in this case is given by V(x, y) = \\frac{4 V_0}{\\pi} \\sum_{n=1, 3, 5\\ldots} \\frac{1}{n} \\frac{\\cosh(n \\pi x/a)}{\\cosh(n \\pi b/a)} \\sin(n \\pi y/a) \\tagl{3.42} This function is shown in Fig. 3.21","title":"Example 3.4"},{"location":"ch3-3/#example-35","text":"An infinitely long rectangular metal pipe (sides a and b) is grounded, but one end, at x = 0 , a 'hot' plate is maintained at a specified potential V_0(y, z) , as indicated in Fig. 3.22. Find the potential inside the pipe. Solution This is genuinely a three-dimensional problem, \\frac{\\partial ^2 V}{\\partial{x^2}} + \\frac{\\partial ^2 V}{\\partial{y^2}} + \\frac{\\partial ^2 V}{\\partial{z^2}} = 0 \\tagl{3.43} subject to the boundary conditions - (i) V = 0 when y = 0 - (ii) V = 0 when y = a - (iii) V = 0 when z = 0 - (iv) V = 0 when z = b - (v) V \\rightarrow 0 as x \\rightarrow \\infty - (vi) V = V_0(y, z) whem x = 0 As always, we look for solutions that are products: V(x, y, z) = X(x)Y(y)Z(z) \\tagl{3.45} Putting this into \\eqref{3.43} and dividing by V, we find \\frac{1}{X} \\frac{d^2 X}{dx^2} + \\frac{1}{Y} \\frac{d^2 Y}{dy^2} + \\frac{1}{Z} \\frac{d^2 Z}{dz^2} = 0 It follows that \\frac{1}{X} \\frac{d^2 X}{dx^2} = C_1 , \\quad \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = C_2 , \\quad \\frac{1}{Z} \\frac{d^2 Z}{dz^2} = C_3 , \\text{ with } C_1 + C_2 + C_3 = 0 Our previous experience in Ex. 3.3 suggests that C_1 must be positive, C_2 and C_3 negative. Setting C_2 = -k^2 and C_3 = -l^2 , we have C_1 = k^2 + l^2 , and hence \\frac{d^2 X}{dx^2} = (k^2 + l^2)X, \\quad \\frac{d^2 Y}{dy^2} = -k^2 Y, \\quad \\frac{d^2 Z}{dz^2} = -l^2 Z \\tagl{3.46} Once again, separation of variables has turned a partial differential equation into ordinary differential equations. The solutions are \\begin{align*} X(x) & = A e^{\\sqrt{k^2 + l^2} x} + B e^{- \\sqrt{k^2 + l^2} x} \\\\ Y(y) & = C \\sin ky + D \\cos ky \\\\ Z(z) & = E \\sin lz + F \\cos lz \\end{align*} Boundary condition (v) implies A =0 , (i) gives D = 0 , and (iii) yields F = 0 whereas (ii) and (iv) require that k = n\\pi /a and l =m \\pi /b , where n and m are positive integers. Combining the remaining constants, we are left with V(x, y, z) = C e^{-\\pi \\sqrt{(n/a)^2 + (m/b)^2}x} \\sin (n \\pi y / a) \\sin(m \\pi z /b) \\tagl{3.47} This solution meets all the boundary conditions except (vi). It contains two unspecified integers (n and m), and the most general linear combination is a double sum V(x, y, z) = \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C e^{-\\pi \\sqrt{(n/a)^2 + (m/b)^2}x} \\sin (n \\pi y / a) \\sin(m \\pi z /b) = V_0(y, z) \\tagl{3.48} We hope to fit the remaining boundary condition, V(0, y, z) = \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C \\sin (n \\pi y / a) \\sin(m \\pi z /b) = V_0(y, z) \\tagl{3.49} by appropriate choice of the coefficients C_{n, m} . To determine these constants, we multiply by \\sin(n' n \\pi y/a) \\sin(m' \\pi z / b) , where n' and m' are arbitrary positive integers, and integrate \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C_{n, m} \\int_0 ^a \\sin (n \\pi y/a) \\sin(n' \\pi y/a) \\dd{y} \\int_0 ^b \\sin(m \\pi z/b) \\sin (m' \\pi z/b) \\dd{z} \\\\ = \\int_0 ^a \\int_0 ^b V_0(y, z) \\sin(n' \\pi y/a) \\sin(m' \\pi z/b) \\dd{y} \\dd{z} Quoting \\eqref{3.33} , the left side is (ab/4) C_{n', m'} , so C_{n, m} = \\frac{4}{ab} \\int_0 ^a \\int_0 ^b V_0(y, z) \\sin (n \\pi y/a) \\sin(m\\pi z/b) \\dd{y} \\dd{z} \\tagl{3.50} Equation \\eqref{3.48} , with the coefficients given by \\eqref{3.50} , is the solution to our problem. For instance, if the end of the tube is a conductor at constant potential V_0 , C_{n, m} = \\frac{4}{ab} \\int_0 ^a \\sin(n \\pi y/a) \\dd{y} \\int_0 ^b \\sin(m \\pi z/b) \\dd{z} \\\\ = \\begin{cases} 0 & \\qquad \\text{if n or m is even} \\\\ \\frac{16 V_0}{\\pi^2 nm} & \\qquad \\text{if n and m are odd} \\end{cases} \\tagl{3.51} In this case, V(x, y, z) = \\frac{16V_0}{\\pi^2} \\sum_{n,m=1,3,5,\\ldots} ^{\\infty} \\frac{1}{nm} e^{-\\pi \\sqrt{(n/a)^2 + (m/b)^2}x} \\sin(n \\pi y/a) \\sin(m \\pi z/b) \\tagl{3.52} Notice that successive terms decrease rapidly; a reasonable approximation would be obtained by keeping only the first few.","title":"Example 3.5"},{"location":"ch3-3/#332-spherical-coordinates","text":"In the examples considered so far, Cartesian coordinates were clearly appropriate, since the boundaries were planes. For round objects, spherical coordinates are more natural. In the spherical system, Laplace's equation reads: \\frac{1}{r^2} \\pdv{}{r} \\left( r^2 \\pdv{V}{r} \\right) + \\frac{1}{r^2 \\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{V}{\\theta} \\right) + \\frac{1}{r^2\\sin ^2 \\theta} \\frac{\\partial ^2 V}{\\partial \\phi ^2} = 0 \\tagl{3.53} I shall assume the problem has azimuthal symmetry , so that V is independent of \\phi ; In that case, \\eqref{3.53} reduces to \\pdv{}{r} \\left( r^2 \\pdv{V}{r} \\right) + \\frac{1}{\\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{V}{\\theta} \\right) = 0 \\tagl{3.54} As before, we look for solutions that are products: V(r, \\theta) = R(r) \\Theta (\\theta) \\tagl{3.55} Putting this into \\eqref{3.54} , and dividing by V , \\frac{1}{R} \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) + \\frac{1}{\\Theta \\sin \\theta} \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = 0 \\tagl{3.56} Since the first term depends only on r , and the second only on \\theta , it follows that each must be a constant: \\frac{1}{R} \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) = l(l+1), \\quad \\frac{1}{\\Theta \\sin \\theta} \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = -l(l+1) \\tagl{3.57} Here l(l+1) is just a fancy way of writing the separation constant, whose convenience will appear shortly. As always, separation of variables has converted a partial differential equation into ordinary differential equations. The radial equation, \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) = l(l+1)R \\tagl{3.58} has the general solution R(r) = A r^l + \\frac{B}{r^{l+1}} \\tagl{3.59} as you can easily check; A and B are the two arbitrary constants to be expected in the solution of a second-order differential equation. But the angular equation, \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = -l(l+1) \\sin \\theta \\Theta \\tagl{3.60} is not so simple. The solutions are Legendre polynomials in the variable \\cos \\theta : \\Theta (\\theta ) = P_l (\\cos \\theta ) \\tagl{3.61} P_l (x) is most conveniently defined by the Rodrigues formula : P_l(x) \\equiv \\frac{1}{2^l l!}\\left( \\dv{}{x} \\right)^l (x^2 - 1)^l \\tagl{3.62} The first few Legendre polynomials are listed: Legendre Polynomials P_0 - P_5 \\begin{align*} P_0(x) & = 1 \\\\ P_1(x) & = x \\\\ P_2(x) & = (3x^2 - 1)/2 \\\\ P_3(x) & = (5x^3 - 3x)/2 \\\\ P_4(x) & = (35x^4 - 30x^2 + 3)/8 \\\\ P_5(x) & = (63x^5 - 70x^3 + 15x)/8 \\end{align*} Notice that P_l(x) is (as the name suggests) an _l_th-order polynomial in x; it contains only even powers if l is even, and only odd powers if l is odd. The factor in front (1/2^l l! was chosen in order that P_l(1) = 1 \\tagl{3.63} The Rodrigues formula obviously only works for nonnegative integer values of l. Moreover, it provides us with only one solution. But \\eqref{3.60} is second-order, and it should possess two independent solutions for every value of l . It turns out that these \"other solutions\" blow up at \\theta = 0 and/or \\theta = \\pi , and are therefore unacceptable on physical grounds. For instance, the second solution for l=0 is \\Theta(\\theta) = \\ln \\left( \\tan \\frac{\\theta}{2} \\right) \\tagl{3.64} You might want to check for yourself that this satisfies \\eqref{3.60} . In the case of azimuthal symmetry, then, the most general separable solution to Laplace's equation, consistent with minimal physical requirements, is V(r, \\theta) = \\left( A r^l + \\frac{B}{r^{l+1}} \\right) P_l(\\cos \\theta) (There was no need to include an overall constant in \\eqref{3.61} because it can be absorbed into A and B at this stage.) As before, separation of variables yields an infinite set of solutions, one for each l . The general solution is the linear combination of separable solutions: V(r, \\theta) = \\sum_{l=0} ^{\\infty} \\left( A r^l + \\frac{B}{r^{l+1}} \\right) P_l(\\cos \\theta) \\tagl{3.65} The following examples illustrate the power of this important result.","title":"3.3.2: Spherical Coordinates"},{"location":"ch3-3/#example-36","text":"The potential V_0(\\theta) is specified on the surface of a hollow sphere, of radius R . Find the potential inside the sphere. Solution In this case, B_l = 0 for all l , otherwise the potential would blow up at the origin. Thus, V(r, \\theta) = \\sum_{l = 0} ^{\\infty} A_l r^l P_l(\\cos \\theta) \\tagl{3.66} At r = R this must match the specified function V_0(\\theta) : V(R, \\theta) = \\sum_{l=0}^\\infty A_l R^l P_l(\\cos \\theta) = V_0(\\theta) \\tagl{3.67} Can this equation be satisfied, for an appropriate choice of coefficients A_l ? Yes: The Legendre polynomials (like the sines) constitute a complete set of functions, on the interval -1 \\leq x \\leq 1 (0 \\leq \\theta \\leq \\pi) . How do we determine the constants? Again, by Fourier's trick, for the Legendre polynomials (like the sines) are orthogonal functions: \\begin{align*} \\int_{-1}^1 P_l(x) P_{l'}(x) \\dd{x} & = \\int_0 ^\\pi P_l(\\cos \\theta) P_{l'}(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\\\ & = \\begin{cases} 0, & \\quad \\text{if } l' \\neq l \\\\ \\frac{2}{2l +1} , & \\quad \\text{if } l' = l \\end{cases} \\end{align*} \\tagl{3.68} Thus, multiplying \\eqref{3.67} by P_{l'}(\\cos \\theta) \\sin \\theta and integrating, we have A_{l'} R^{l'} \\frac{2}{2l' + 1} = \\int_{0} ^\\pi V_0(\\theta) P_{l'}(\\cos \\theta) \\sin \\theta \\dd{\\theta} or A_l = \\frac{2l+1}{2R^l} \\int_0 ^\\pi V_0(\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.69} \\eqref{3.66} is the solution to our problem, with the coefficients given by \\eqref{3.69} . It can be difficult to evaluate integrals of the form \\eqref{3.69} analytically, and in practice it is often easier to solve \\eqref{3.67} \"by eyeball.\" For instance, suppose we are told that the potential on the sphere is V_0(\\theta) = k \\sin^2 (\\theta/2) \\tagl{3.70} where k is constant. Using the half-angle formula, we rewrite this as V_0(\\theta) = \\frac{k}{2}(1 - \\cos \\theta) = \\frac{k}{2} [P_0(\\cos \\theta) - P_1 (\\cos \\theta)] Putting this into \\eqref{3.67} , we read off immediately that A_0 = k/2 , A_1 = -k/(2R) , and all other A_l 's vanish. Therefore V(r, \\theta) = \\frac{k}{2} \\left[ r^0 P_{0}(\\cos \\theta) - \\frac{r^1}{R} P_1 (\\cos \\theta) \\right] = \\frac{k}{2} \\left( 1 - \\frac{r}{R} \\cos \\theta \\right) \\tagl{3.71}","title":"Example 3.6"},{"location":"ch3-3/#example-37","text":"The potential V_0(\\theta) is again specified on the surface of a sphere of radius R , but this time we are asked to find the potential outside , assuming there is no charge there. Solution In this case it's the A_l 's that must be zero (or else V would not go to zero at \\infty ), so V(r, \\theta) = \\sum_{l=0} ^\\infty \\frac{B_l}{r^{l+1}} P_l(\\cos \\theta) = V_0(\\theta) \\tagl{3.72} Multiplying by P_{l'}(\\cos \\theta) \\sin \\theta and integrating - exploiting, again, the orthogonality relation 3.68 - we have \\frac{B_{l'}}{R^{l'+1}} \\frac{2}{2l' + 1} = \\int_0 ^\\pi V_0(\\theta) P_{l'}(\\cos \\theta) \\sin \\theta \\dd{\\theta} or B_l = \\frac{2l + 1}{2} R^{l+1} \\int_0 ^\\pi V_0(\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.73} \\eqref{3.72} , with the coefficients given by \\eqref{3.73} , is the solution to our problem.","title":"Example 3.7"},{"location":"ch3-3/#example-38","text":"An uncharged metal sphere of radius R is placed in an otherwise uniform electric field \\vec{E} = E_0 \\vu{z} . The field will push positive charge to the 'northern' surface of the sphere, and - symmetrically - negative charge to the 'southern' surface (Fig. 3.24). This induced charge, in turn, distorts the field in the neighborhood of the sphere. Find the potential in the region outside the sphere. Solution The sphere is an equipotential - we may as well set it to zero. Then by symmetry the entire xy plane is at potential zero. This time, however, V does not go to zero at large z . In fact, far from the sphere the field is E_0 \\vu{z} and hence V \\rightarrow - E_0 z + C Since V = 0 in the equatorial plane, the constant C must be zero. Accordingly, the boundary conditions for this problem are - (i) V = 0 when r = R - (ii) V \\rightarrow - E_0 r \\cos \\theta for r \\gg R We must fit these boundary conditions with a function of the form \\eqref{3.65} . The first condition yields A_l R^l + \\frac{B_l}{R^{l+1}} = 0 or B_l = -A_l R^{2l+1} \\tagl{3.75} so V(r, \\theta) = \\sum_{l = 0} ^{\\infty} A_l \\left( r^l - \\frac{R^{2l+1}}{r^{l+1}} \\right) P_l(\\cos \\theta) For r \\gg R , the second term in parentheses is negligible, and therefore the condition (ii) requires that \\sum_{l=0}^\\infty A_l R^{l} P_l (\\cos \\theta) = - E_0 r \\cos \\theta Evidently only one term is present: l = 1 . In fact, since P_1(\\cos \\theta) = \\cos \\theta we can read off immediately A_1 = - E_0, \\qquad \\text{ all other }A_l's \\text{ zero} Conclusion : V(r, \\theta) = - E_0 \\left( r - \\frac{R^3}{r^2} \\right) \\cos \\theta \\tagl{3.76} The first term (-E_0 r \\cos \\theta) is due to the external field; the contribution attributable to the induced charge is E_0 \\frac{R^3}{r^2} \\cos \\theta If you want to know the induced charge density, it can be calculated in the usual way: \\sigma(\\theta) = - \\epsilon_0 \\left. \\pdv{V}{r} \\right|_{r = R} = \\epsilon_0 E_0 \\left. \\left( 1 + 2 \\frac{R^3}{r^3} \\right) \\cos \\theta \\right|_{r = R} = 3 \\epsilon_0 E_0 \\cos \\theta \\tagl{3.77} As expected, it is positive in the 'northern' hemisphere 0 \\leq \\theta \\leq \\pi /2 and negative in the 'southern' \\pi/2 \\leq \\theta \\leq \\pi .","title":"Example 3.8"},{"location":"ch3-3/#example-39","text":"A specified charge density \\sigma_0(\\theta) is glued over the surface of a spherical shell of radius R . Find the resulting potential inside and outside the sphere. Solution You could, of course, do this by direct integration: V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma_0}{\\gr} \\dd{a} but separation of variables is often easier. For the interior region, we have V(r, \\theta) = \\sum_{l = 0}^\\infty A_l r^l P_l (\\cos \\theta) \\quad (r \\leq R) \\tagl{3.78} (no B_l terms - they blow up at the origin); in the exterior region V(r, \\theta) = \\sum_{l=0}^\\infty \\frac{B_l}{r^{l+1}} P_l(\\cos \\theta) \\quad (r \\geq R) \\tagl{3.79} (no A_l terms - they don't go to zero at infinity). These two functions must be joined together by the appropriate boundary conditions at the surface itself. First, the potential is continuous at r = R (Eq. 2.34): \\sum_{l=0}^\\infty A_l R^l P_l(\\cos \\theta) = \\sum_{l=0} ^\\infty \\frac{B_l}{R^{l+1}} P_l(\\cos \\theta) \\tagl{3.80} It follows that the coefficients of like Legendre polynomial are equal: B_l = A_l R^{2l+1} \\tagl{3.81} (To prove that formally, multiply both sides of \\eqref{3.80} by P_{l'} (\\cos \\theta)\\sin \\theta and integrate from 0 to \\pi , using the orthogonality relation \\eqref{3.68} .) Second, the radial derivative of V suffers a discontinuity at the surface (Eq. 2.36): \\left. \\left( \\pdv{V_{out}}{r} - \\pdv{V_{in}}{r} \\right) \\right|_{r = R} = - \\frac{1}{\\epsilon_0} \\sigma_0(\\theta) \\tagl{3.82} Thus, - \\sum_{l=0}^\\infty (l+1) \\frac{B_l}{R^{l+2}} P_l(\\cos \\theta) - \\sum_{l=0}^\\infty l A_l R^{l-1} P_l(\\cos \\theta) = - \\frac{1}{\\epsilon_0} \\sigma_0 (\\theta) or, using \\eqref{3.81} , \\sum_{l=0}^\\infty (2l+1) A_l R^{l-1} P_l(\\cos \\theta) = \\frac{1}{\\epsilon_0} \\sigma_0(\\theta) \\tagl{3.83} From here, the coefficients can be determined using Fourier's trick A_l = \\frac{1}{2 \\epsilon_0 R^{l-1}} \\int_0 ^\\pi \\sigma_0 (\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.84} Equations 3.78 and 3.79 constitute the solution to our problem, with the coefficients given by \\eqref{3.81} and \\eqref{3.84} . For instance, if \\sigma_0(\\theta) = k \\cos \\theta = k P_1 (\\cos \\theta) \\tagl{3.85} for some constant k , then all the A_l 's are zero except for l = 1 , and A_1 = \\frac{k}{2 \\epsilon_0} \\int_0 ^\\pi [P_1(\\cos \\theta)]^2 \\sin \\theta \\dd{\\theta} = \\frac{k}{3\\epsilon_0} The potential inside the sphere is therefore V(r, \\theta) = \\frac{k}{3 \\epsilon_0} r \\cos \\theta \\quad (r \\leq R) \\tagl{3.86} whereas outside the sphere V(r, \\theta) = \\frac{kR^3}{3 \\epsilon_0} \\frac{1}{r^2} \\cos \\theta \\quad (r \\geq R) \\tagl{3.87} In particular, if \\sigma_0(\\theta) is the induced charge on a metal sphere in an external field E_0(\\vu{z}) , so that k = 3 \\epsilon_0 E_0 \\eqref{3.77} , then the potential inside is E_0 r \\cos \\theta = E_0 z , and the field is -E_0 \\vu{z} - exactly right to cancel off the external field, as of course it should be. Outside the sphere the potential due to this surface charge is E_0 \\frac{R^3}{r^2} \\cos \\theta consistent with our conclusion in Example 3.8.","title":"Example 3.9"},{"location":"ch3-4/","text":"3.4: Multipole Expansion 3.4.1: Approximate Potentials at Large Distances If you are very far away from a localized charge distribution, it \"looks\" like a point charge, and the potential is - to good approximation - (1/4 \\pi \\epsilon_0) Q/r , where Q is the total charge. We have often used this as a check on formulas for V . But what if Q is zero? You might reply that the potential is then approximately zero, and of course, you're right in a sense (indeed, the potential at large r is pretty small even if Q is not zero). But we're looking for something a bit more informative than that. Example 3.10 A (physical) electric dipole consists of two equal and opposite charges (\\pm q) separated by a distance d . Find the approximate potential at points far from the dipole Solution Let \\gr_- be the distance from -q and \\gr_{+} be the distance from +q (Fig. 3.26). Then V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{\\gr_{+}} - \\frac{q}{\\gr_{-}} \\right) and (from the law of cosines), \\gr_{\\pm} ^2 = r^2 + (d/2)^2 \\mp r d \\cos \\theta = r^2 \\left( 1 \\mp \\frac{d}{r} \\cos \\theta + \\frac{d^2}{4r^2} \\right) We're interested in the regime r \\gg d , so the third term is negligible, and the binomial expansion yields \\frac{1}{\\gr_{\\pm}} \\approx \\frac{1}{r} \\left( 1 \\mp \\frac{d}{r} \\cos \\theta \\right) ^{-1/2} \\approx \\frac{1}{r} \\left( 1 \\pm \\frac{d}{2r} \\cos \\theta \\right) Thus \\frac{1}{\\gr_+} - \\frac{1}{\\gr_{-}} \\approx \\frac{d}{r^2} \\cos \\theta and hence V(r) \\approx \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd \\cos \\theta}{r^2} \\tagl{3.90} The potential of a dipole goes like 1/r^2 at large r ; as we might have anticipated, it falls off more rapidly than the potential of a point charge. If we put together a pair of equal and opposite dipoles to make a quadrupole , the potential goes like 1/r^3 ; for back-to-back quadrupoles (an octopole ), it goes like 1/r^4 , and so on. Figure 3.27 summarizes the hierarchy; for completeness I have included the electric monopole (point charge), whose potential, of course, goes like 1/r Example 3.10 pertains to a very special charge configuration. I propose now to develop a systematic expansion for the potential of any localized charge distribution, in powers of 1/r . Figure 3.28 defines the relevant variables; the potential at r is given by V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{1}{\\gr} \\rho(\\vec{r'}) \\dd{\\tau'} \\tagl{3.91} Using the law of cosines, \\gr ^2 = r^2 + (r')^2 - 2r r' \\cos \\alpha = r^2 \\left[ 1 + \\left( \\frac{r'}{r} \\right)^2 - 2 \\left( \\frac{r'}{r} \\right)\\cos \\alpha \\right] where \\alpha is the angle between \\vec{r} and \\vec{r'} . Thus, \\gr = r \\sqrt{1 + \\epsilon} \\tagl{3.92} with \\epsilon \\equiv \\left( \\frac{r'}{r} \\right)\\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right) For points well outside the charge distribution, \\epsilon is much less than 1, and this invites a binomial expansion: \\frac{1}{\\gr} = \\frac{1}{r} (1 + \\epsilon)^{-1/2} = \\frac{1}{r} \\left( 1 - \\frac{1}{2} \\epsilon + \\frac{3}{8} \\epsilon^2 - \\frac{5}{16} \\epsilon^3 + \\ldots \\right) \\tagl{3.93} or, in terms of r, r' , and \\alpha , \\begin{align*} \\frac{1}{\\gr} & = \\frac{1}{r} \\left[ 1 - \\frac{1}{2} \\left( \\frac{r'}{r} \\right) \\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right) + \\frac{3}{8} \\left( \\frac{r'}{r} \\right)^2 \\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right)^2 \\right. \\\\ & \\qquad \\left. - \\frac{5}{16} \\left( \\frac{r'}{r} \\right)^3 \\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right)^3 + \\ldots \\right] \\\\ & = \\frac{1}{r} \\left[ 1 + \\left( \\frac{r'}{r} \\right)(\\cos \\alpha) + \\left( \\frac{r'}{r} \\right) \\left( \\frac{3 \\cos ^2 \\alpha - 1}{2} \\right) \\right. \\\\ & \\qquad \\left. \\left( \\frac{r'}{r} \\right)^3 \\left( \\frac{5\\cos ^3 \\alpha - 3 \\cos \\alpha}{2} \\right) + \\ldots \\right] \\end{align*} In the last step, I have collected together like powers of (r'/r) ; surprisingly, their coefficients (the terms in parentheses) are Legendre polynomials! The remarkable result is that \\frac{1}{\\gr} = \\frac{1}{r} \\sum_{n=0}^\\infty \\left( \\frac{r'}{r} \\right)^n P_n (\\cos \\alpha) \\tagl{3.94} Substituting this back into \\eqref{3.91} , and noting that r is constant, as far as the integration is concerned, I conclude that V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{n=0} ^\\infty \\frac{1}{r^{(n+1)}} \\int (r') P_n(\\cos \\alpha) \\rho(\\vec{r'}) \\dd{\\tau'} \\tagl{3.95} or, more explicitly, V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{1}{r} \\int \\rho(\\vec{r'}) \\dd{\\tau'} + \\frac{1}{r^2} \\int r' \\cos \\alpha \\rho(\\vec{r'})\\dd{\\tau'} \\right. \\\\ + \\left. \\frac{1}{r^3} \\int (r')^2 \\left( \\frac{3}{2} \\cos^2 \\alpha - \\frac{1}{2} \\right) \\rho(\\vec{r'}) \\dd{\\tau'} + \\ldots \\right] \\tagl{3.96} This is the desired result - the multipole expansion of V in powers of 1/r . The first term (n=0) is the monopole contribution (it goes like 1/r ); the second (n=1) is the dipole (it goes like 1/r^2 ); the third is quadrupole; the fourth octopole, and so on. Remember that \\alpha is the angle between \\vec{r} and \\vec{r'} , so the integrals depend on the direction to the field point. If you are interested in the potential along the z' axis (or - putting it the other way round - if you orient your \\vec{r'} coordinates so the z' axis lies along \\vec{r} ), then \\alpha is the usual polar angle \\theta' . As it stands, \\eqref{3.95} is exact , but it is useful primarily as an approximation scheme: the lowest nonzero term in the expansion provides the approximate potential at large r , and the successive terms tell us how to improve the approximation if greater precision is required. 3.4.2: The Monopole and Dipole Terms Ordinarily, the multipole expansion is dominated (at large r) by the monopole term: V_{mon} (\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q}{r} where Q =\\int \\rho \\dd{\\tau} is the total charge of the configuration. This is just what we expect for the approximate potential at large distances from the charge. For a point charge at the origin, V_{mon} is the exact potential, not merely a first approximation at large r; in this case, all the higher multipoles vanish. If the total charge is zero, the dominant term in the potential will be the dipole (unless, of course, it also vanishes): V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\int r' \\cos \\alpha \\rho(\\vec{r'}) \\dd{\\tau'} Since \\alpha is the angle between r' and r (Fig 2.38), r' \\cos \\alpha = \\vu{r} \\cdot \\vec{r'} and the dipole potential can be written more succinctly: V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\vu{r} \\cdot \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} This integral (which does not depend on \\vec{r} ) is called the dipole moment of the distribution: \\vec{p} \\equiv \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} \\tagl{3.98} and the dipole contribution to the potential simplifies to V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} \\tagl{3.99} The dipole moment is determined by the geometry (size, shape, and density) of the charge distribution. \\eqref{3.98} translates in the usual way (Sect 2.1.4) for point, line, and surface charges. Thus, the dipole moment for a collection of point charges is \\vec{p} = \\sum_{i=1} ^n q_i \\vec{r'}_i \\tagl{3.100} For a physical dipole (equal and opposite charges \\pm q ), \\vec{p} = q\\vec{r'_+} - q \\vec{r_- ' } = q(\\vec{r' _+} - \\vec{r'_-}) = q \\vec{d} \\tagl{3.101} where \\vec{d} is the vector from the negative charge to the positive one (Fig. 3.29). Is this consistent with what we got in Example 3.10? Yes: If you put \\eqref{3.101} into \\eqref{3.99} , you recover \\eqref{3.90} . Notice, however, that this is only the approximate potential of the physical dipole - evidently there are higher multipole contributions. Of course, as you go farther and farther away, V_{dip} becomes a better and better approximation, since the higher terms die off more rapidly with increasing r . By the same token, at a fixed r the dipole approximation improves as you shrink the separation distance d . To construct a perfect dipole whose potential is given exactly by \\eqref{3.99} , you'd have to let d approach zero. Unfortunately you then lose the dipole term too, unless you simultaneously arrange for q to go to infinity! A physical dipole becomes a pure dipole, then, in the rather artificial limit d \\rightarrow 0. q \\rightarrow \\infty , with the product qd = p held fixed. When someone uses the word \"dipole,\" you can't always tell whether they mean a physical dipole (with finite separation between the charges) or an ideal dipole. If in doubt, assume that d is small enough that you can safely apply \\eqref{3.99} . Dipole moments are vectors , and they add accordingly: if you have two dipoles \\vec{p_1} and \\vec{p_2} , the total dipole moment is \\vec{p_1} + \\vec{p_2} . For instance, with four charges at the corners of a square, as shown in Fig. 3.30, the net dipole moment is zero. You can see this by combining the charges in pairs or by adding up the four contributions individually using \\eqref{3.100} . This is a quadrupole , as I indicated earlier, and its potential is dominated by the quadrupole term in the multipole expansion. 3.4.3: Origin of Coordinates in Multipole Expansions I mentioned earlier that a point charge at the origin constitutes a \"pure\" monopole. If it is not at the origin, it's no longer a pure monopole. For instance, the charge in Fig. 3.32 has a dipole moment \\vec{p} = q d \\vu{y} , and a corresponding dipole term in its potential. The monopole potential (1/4 \\pi \\epsilon_0) q/r is not quite correct for this configuration; rather, the exact potential is (1/4 \\pi \\epsilon_0) q/\\gr . The multipole expansion is, remember, a series in inverse powers of r (the distance to the origin), and when we expand 1/\\gr , we get all powers, not just the first. So moving the origin (or, what amounts to the same thing, moving the charge) can radically alter a multipole expansion. The monopole moment Q does not change, since the total charge is obviously independent of the coordinate system. (In Fig. 3.32, the monopole term was unaffected when we moved q away from the origin - it's just that it was no longer the whole story: a dipole term - and for that matter all higher poles - appeared as well.) Ordinarily, the dipole moment does change when you shift the origin, but there is an important exception: If the total charge is zero, then the dipole moment is independent of the choice of origin. For suppose we displace the origin by an amount \\vec{a} (Fig. 3.33). The new dipole moment is then \\begin{align*} \\vec{p_2} & = \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} = \\int (\\vec{r'} - \\vec{a} ) \\rho (\\vec{r'}) \\dd{\\tau'} \\\\ & = \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} - \\vec{a} \\int \\rho(\\vec{r'}) \\dd{\\tau'} = \\vec{p} - Q \\vec{a} \\end{align*} In particular, if Q = 0 , the \\vec{p_2} = \\vec{p} . So if someone asks for the dipole moment in Fig 3.34(a), you can answer with confidence \" q \\vec{d} ,\" but if you're asked for the dipole moment in Fig 3.34(b), the appropriate response would be \"With respect to what origin?\" 3.4.4: The Electric Field of a Dipole So far we have only worked with potentials . Now I would like to calculate the electric field of a (perfect) dipole. If we choose coordinates so that \\vec{p} is at the origin and points in the z direction (Fig. 3.36), then the potential at r, \\theta is \\eqref{3.99} : V_{dip} (r, \\theta) = \\frac{\\vu{r} \\cdot \\vec{p}}{4 \\pi \\epsilon_0 r^2} = \\frac{p \\cos \\theta}{4 \\pi \\epsilon_0 r^2} \\tagl{3.102} To get the field, we take the negative gradient of V : \\begin{align*} E_r & = - \\pdv{V}{r} = \\frac{2 p \\cos \\theta}{4 \\pi \\epsilon_0 r^3} \\\\ E_\\theta & = - \\frac{1}{r} \\pdv{V}{\\theta} = \\frac{p \\sin \\theta}{4 \\pi \\epsilon_0 r^3} \\\\ E_\\phi & = - \\frac{1}{r \\sin \\theta} \\pdv{V}{\\phi} = 0 \\end{align*} Thus, \\vec{E_{dip}} (r, \\theta) = \\frac{p}{4 \\pi \\epsilon_0 r^3}(2 \\cos \\theta \\vu{r} + \\sin \\theta \\vu{\\theta}) \\tagl{3.103} This formula makes explicit reference to a particular coordinate system (spherical) and assumes a particular orientation for \\vec{p} (along z). It can be recast in a coordinate-free form, analogous to the potential in \\eqref{3.99} - See problem 3.36. Notice that the dipole falls off as the inverse cube of r; the monopole field (Q / 4 \\pi \\epsilon_0 r^2) \\vu{r} goes as the inverse square, of course. Quadrupole fields go like 1/r^4 , octopole like 1/r^5 , and so on. (This merely reflects how the respective potentials fall off - the gradient introduces another factor of 1/r ). Figure 3.37(a) shows the field lines of a \"pure\" dipole \\eqref{3.103} . For comparison, I have also sketched the field lines for a \"physical\" dipole, in Fig 3.37(b). Notice how similar the two pictures become if you blot out the central region; up close, however, they are entirely different. Only for points r \\gg d does \\eqref{3.103} represent a valid approximation to the field of a physical dipole. As I mentioned earlier, this regime can be reached either by going to large r or by squeezing the charges very close together.","title":"3.4 - Multipole Expansion"},{"location":"ch3-4/#34-multipole-expansion","text":"","title":"3.4: Multipole Expansion"},{"location":"ch3-4/#341-approximate-potentials-at-large-distances","text":"If you are very far away from a localized charge distribution, it \"looks\" like a point charge, and the potential is - to good approximation - (1/4 \\pi \\epsilon_0) Q/r , where Q is the total charge. We have often used this as a check on formulas for V . But what if Q is zero? You might reply that the potential is then approximately zero, and of course, you're right in a sense (indeed, the potential at large r is pretty small even if Q is not zero). But we're looking for something a bit more informative than that.","title":"3.4.1: Approximate Potentials at Large Distances"},{"location":"ch3-4/#example-310","text":"A (physical) electric dipole consists of two equal and opposite charges (\\pm q) separated by a distance d . Find the approximate potential at points far from the dipole Solution Let \\gr_- be the distance from -q and \\gr_{+} be the distance from +q (Fig. 3.26). Then V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{\\gr_{+}} - \\frac{q}{\\gr_{-}} \\right) and (from the law of cosines), \\gr_{\\pm} ^2 = r^2 + (d/2)^2 \\mp r d \\cos \\theta = r^2 \\left( 1 \\mp \\frac{d}{r} \\cos \\theta + \\frac{d^2}{4r^2} \\right) We're interested in the regime r \\gg d , so the third term is negligible, and the binomial expansion yields \\frac{1}{\\gr_{\\pm}} \\approx \\frac{1}{r} \\left( 1 \\mp \\frac{d}{r} \\cos \\theta \\right) ^{-1/2} \\approx \\frac{1}{r} \\left( 1 \\pm \\frac{d}{2r} \\cos \\theta \\right) Thus \\frac{1}{\\gr_+} - \\frac{1}{\\gr_{-}} \\approx \\frac{d}{r^2} \\cos \\theta and hence V(r) \\approx \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd \\cos \\theta}{r^2} \\tagl{3.90} The potential of a dipole goes like 1/r^2 at large r ; as we might have anticipated, it falls off more rapidly than the potential of a point charge. If we put together a pair of equal and opposite dipoles to make a quadrupole , the potential goes like 1/r^3 ; for back-to-back quadrupoles (an octopole ), it goes like 1/r^4 , and so on. Figure 3.27 summarizes the hierarchy; for completeness I have included the electric monopole (point charge), whose potential, of course, goes like 1/r Example 3.10 pertains to a very special charge configuration. I propose now to develop a systematic expansion for the potential of any localized charge distribution, in powers of 1/r . Figure 3.28 defines the relevant variables; the potential at r is given by V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{1}{\\gr} \\rho(\\vec{r'}) \\dd{\\tau'} \\tagl{3.91} Using the law of cosines, \\gr ^2 = r^2 + (r')^2 - 2r r' \\cos \\alpha = r^2 \\left[ 1 + \\left( \\frac{r'}{r} \\right)^2 - 2 \\left( \\frac{r'}{r} \\right)\\cos \\alpha \\right] where \\alpha is the angle between \\vec{r} and \\vec{r'} . Thus, \\gr = r \\sqrt{1 + \\epsilon} \\tagl{3.92} with \\epsilon \\equiv \\left( \\frac{r'}{r} \\right)\\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right) For points well outside the charge distribution, \\epsilon is much less than 1, and this invites a binomial expansion: \\frac{1}{\\gr} = \\frac{1}{r} (1 + \\epsilon)^{-1/2} = \\frac{1}{r} \\left( 1 - \\frac{1}{2} \\epsilon + \\frac{3}{8} \\epsilon^2 - \\frac{5}{16} \\epsilon^3 + \\ldots \\right) \\tagl{3.93} or, in terms of r, r' , and \\alpha , \\begin{align*} \\frac{1}{\\gr} & = \\frac{1}{r} \\left[ 1 - \\frac{1}{2} \\left( \\frac{r'}{r} \\right) \\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right) + \\frac{3}{8} \\left( \\frac{r'}{r} \\right)^2 \\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right)^2 \\right. \\\\ & \\qquad \\left. - \\frac{5}{16} \\left( \\frac{r'}{r} \\right)^3 \\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right)^3 + \\ldots \\right] \\\\ & = \\frac{1}{r} \\left[ 1 + \\left( \\frac{r'}{r} \\right)(\\cos \\alpha) + \\left( \\frac{r'}{r} \\right) \\left( \\frac{3 \\cos ^2 \\alpha - 1}{2} \\right) \\right. \\\\ & \\qquad \\left. \\left( \\frac{r'}{r} \\right)^3 \\left( \\frac{5\\cos ^3 \\alpha - 3 \\cos \\alpha}{2} \\right) + \\ldots \\right] \\end{align*} In the last step, I have collected together like powers of (r'/r) ; surprisingly, their coefficients (the terms in parentheses) are Legendre polynomials! The remarkable result is that \\frac{1}{\\gr} = \\frac{1}{r} \\sum_{n=0}^\\infty \\left( \\frac{r'}{r} \\right)^n P_n (\\cos \\alpha) \\tagl{3.94} Substituting this back into \\eqref{3.91} , and noting that r is constant, as far as the integration is concerned, I conclude that V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{n=0} ^\\infty \\frac{1}{r^{(n+1)}} \\int (r') P_n(\\cos \\alpha) \\rho(\\vec{r'}) \\dd{\\tau'} \\tagl{3.95} or, more explicitly, V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{1}{r} \\int \\rho(\\vec{r'}) \\dd{\\tau'} + \\frac{1}{r^2} \\int r' \\cos \\alpha \\rho(\\vec{r'})\\dd{\\tau'} \\right. \\\\ + \\left. \\frac{1}{r^3} \\int (r')^2 \\left( \\frac{3}{2} \\cos^2 \\alpha - \\frac{1}{2} \\right) \\rho(\\vec{r'}) \\dd{\\tau'} + \\ldots \\right] \\tagl{3.96} This is the desired result - the multipole expansion of V in powers of 1/r . The first term (n=0) is the monopole contribution (it goes like 1/r ); the second (n=1) is the dipole (it goes like 1/r^2 ); the third is quadrupole; the fourth octopole, and so on. Remember that \\alpha is the angle between \\vec{r} and \\vec{r'} , so the integrals depend on the direction to the field point. If you are interested in the potential along the z' axis (or - putting it the other way round - if you orient your \\vec{r'} coordinates so the z' axis lies along \\vec{r} ), then \\alpha is the usual polar angle \\theta' . As it stands, \\eqref{3.95} is exact , but it is useful primarily as an approximation scheme: the lowest nonzero term in the expansion provides the approximate potential at large r , and the successive terms tell us how to improve the approximation if greater precision is required.","title":"Example 3.10"},{"location":"ch3-4/#342-the-monopole-and-dipole-terms","text":"Ordinarily, the multipole expansion is dominated (at large r) by the monopole term: V_{mon} (\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q}{r} where Q =\\int \\rho \\dd{\\tau} is the total charge of the configuration. This is just what we expect for the approximate potential at large distances from the charge. For a point charge at the origin, V_{mon} is the exact potential, not merely a first approximation at large r; in this case, all the higher multipoles vanish. If the total charge is zero, the dominant term in the potential will be the dipole (unless, of course, it also vanishes): V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\int r' \\cos \\alpha \\rho(\\vec{r'}) \\dd{\\tau'} Since \\alpha is the angle between r' and r (Fig 2.38), r' \\cos \\alpha = \\vu{r} \\cdot \\vec{r'} and the dipole potential can be written more succinctly: V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\vu{r} \\cdot \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} This integral (which does not depend on \\vec{r} ) is called the dipole moment of the distribution: \\vec{p} \\equiv \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} \\tagl{3.98} and the dipole contribution to the potential simplifies to V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} \\tagl{3.99} The dipole moment is determined by the geometry (size, shape, and density) of the charge distribution. \\eqref{3.98} translates in the usual way (Sect 2.1.4) for point, line, and surface charges. Thus, the dipole moment for a collection of point charges is \\vec{p} = \\sum_{i=1} ^n q_i \\vec{r'}_i \\tagl{3.100} For a physical dipole (equal and opposite charges \\pm q ), \\vec{p} = q\\vec{r'_+} - q \\vec{r_- ' } = q(\\vec{r' _+} - \\vec{r'_-}) = q \\vec{d} \\tagl{3.101} where \\vec{d} is the vector from the negative charge to the positive one (Fig. 3.29). Is this consistent with what we got in Example 3.10? Yes: If you put \\eqref{3.101} into \\eqref{3.99} , you recover \\eqref{3.90} . Notice, however, that this is only the approximate potential of the physical dipole - evidently there are higher multipole contributions. Of course, as you go farther and farther away, V_{dip} becomes a better and better approximation, since the higher terms die off more rapidly with increasing r . By the same token, at a fixed r the dipole approximation improves as you shrink the separation distance d . To construct a perfect dipole whose potential is given exactly by \\eqref{3.99} , you'd have to let d approach zero. Unfortunately you then lose the dipole term too, unless you simultaneously arrange for q to go to infinity! A physical dipole becomes a pure dipole, then, in the rather artificial limit d \\rightarrow 0. q \\rightarrow \\infty , with the product qd = p held fixed. When someone uses the word \"dipole,\" you can't always tell whether they mean a physical dipole (with finite separation between the charges) or an ideal dipole. If in doubt, assume that d is small enough that you can safely apply \\eqref{3.99} . Dipole moments are vectors , and they add accordingly: if you have two dipoles \\vec{p_1} and \\vec{p_2} , the total dipole moment is \\vec{p_1} + \\vec{p_2} . For instance, with four charges at the corners of a square, as shown in Fig. 3.30, the net dipole moment is zero. You can see this by combining the charges in pairs or by adding up the four contributions individually using \\eqref{3.100} . This is a quadrupole , as I indicated earlier, and its potential is dominated by the quadrupole term in the multipole expansion.","title":"3.4.2: The Monopole and Dipole Terms"},{"location":"ch3-4/#343-origin-of-coordinates-in-multipole-expansions","text":"I mentioned earlier that a point charge at the origin constitutes a \"pure\" monopole. If it is not at the origin, it's no longer a pure monopole. For instance, the charge in Fig. 3.32 has a dipole moment \\vec{p} = q d \\vu{y} , and a corresponding dipole term in its potential. The monopole potential (1/4 \\pi \\epsilon_0) q/r is not quite correct for this configuration; rather, the exact potential is (1/4 \\pi \\epsilon_0) q/\\gr . The multipole expansion is, remember, a series in inverse powers of r (the distance to the origin), and when we expand 1/\\gr , we get all powers, not just the first. So moving the origin (or, what amounts to the same thing, moving the charge) can radically alter a multipole expansion. The monopole moment Q does not change, since the total charge is obviously independent of the coordinate system. (In Fig. 3.32, the monopole term was unaffected when we moved q away from the origin - it's just that it was no longer the whole story: a dipole term - and for that matter all higher poles - appeared as well.) Ordinarily, the dipole moment does change when you shift the origin, but there is an important exception: If the total charge is zero, then the dipole moment is independent of the choice of origin. For suppose we displace the origin by an amount \\vec{a} (Fig. 3.33). The new dipole moment is then \\begin{align*} \\vec{p_2} & = \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} = \\int (\\vec{r'} - \\vec{a} ) \\rho (\\vec{r'}) \\dd{\\tau'} \\\\ & = \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} - \\vec{a} \\int \\rho(\\vec{r'}) \\dd{\\tau'} = \\vec{p} - Q \\vec{a} \\end{align*} In particular, if Q = 0 , the \\vec{p_2} = \\vec{p} . So if someone asks for the dipole moment in Fig 3.34(a), you can answer with confidence \" q \\vec{d} ,\" but if you're asked for the dipole moment in Fig 3.34(b), the appropriate response would be \"With respect to what origin?\"","title":"3.4.3: Origin of Coordinates in Multipole Expansions"},{"location":"ch3-4/#344-the-electric-field-of-a-dipole","text":"So far we have only worked with potentials . Now I would like to calculate the electric field of a (perfect) dipole. If we choose coordinates so that \\vec{p} is at the origin and points in the z direction (Fig. 3.36), then the potential at r, \\theta is \\eqref{3.99} : V_{dip} (r, \\theta) = \\frac{\\vu{r} \\cdot \\vec{p}}{4 \\pi \\epsilon_0 r^2} = \\frac{p \\cos \\theta}{4 \\pi \\epsilon_0 r^2} \\tagl{3.102} To get the field, we take the negative gradient of V : \\begin{align*} E_r & = - \\pdv{V}{r} = \\frac{2 p \\cos \\theta}{4 \\pi \\epsilon_0 r^3} \\\\ E_\\theta & = - \\frac{1}{r} \\pdv{V}{\\theta} = \\frac{p \\sin \\theta}{4 \\pi \\epsilon_0 r^3} \\\\ E_\\phi & = - \\frac{1}{r \\sin \\theta} \\pdv{V}{\\phi} = 0 \\end{align*} Thus, \\vec{E_{dip}} (r, \\theta) = \\frac{p}{4 \\pi \\epsilon_0 r^3}(2 \\cos \\theta \\vu{r} + \\sin \\theta \\vu{\\theta}) \\tagl{3.103} This formula makes explicit reference to a particular coordinate system (spherical) and assumes a particular orientation for \\vec{p} (along z). It can be recast in a coordinate-free form, analogous to the potential in \\eqref{3.99} - See problem 3.36. Notice that the dipole falls off as the inverse cube of r; the monopole field (Q / 4 \\pi \\epsilon_0 r^2) \\vu{r} goes as the inverse square, of course. Quadrupole fields go like 1/r^4 , octopole like 1/r^5 , and so on. (This merely reflects how the respective potentials fall off - the gradient introduces another factor of 1/r ). Figure 3.37(a) shows the field lines of a \"pure\" dipole \\eqref{3.103} . For comparison, I have also sketched the field lines for a \"physical\" dipole, in Fig 3.37(b). Notice how similar the two pictures become if you blot out the central region; up close, however, they are entirely different. Only for points r \\gg d does \\eqref{3.103} represent a valid approximation to the field of a physical dipole. As I mentioned earlier, this regime can be reached either by going to large r or by squeezing the charges very close together.","title":"3.4.4: The Electric Field of a Dipole"},{"location":"ch4-1/","text":"4.1: Polarization 4.1.1: Dielectrics This chapter is all about what happens to an electric field when you take matter into account. Matter, of course, comes in many varieties - phase, composition, state, etc. - and depending upon which type of matter we're dealing with, the electrostatic field response can be very different. Nevertheless, most everyday objects belong (at least, to good approximation) to one of two large classes: conductors and insulators (or dielectrics ). We have already gone over what happens to an electrostatic field in a conductor; the \"unlimited\" free charges within a conductor distribute themselves through the material so as to form an equipotential. In practice, this usually means that many electrons (one or two per atom, in a typical metal) are not associated with a particular nucleus, but roam around at will. In dielectrics, by contrast, all charges are attached to specific atoms or molecules - they cannot escape their leash, and can only move a bit within the atom or molecule. Such microscopic displacements are not as dramatic as the wholesale rearrangement of charge in a conductor, but their cumulative effects account for the characteristic behavior of dielectric materials. There are actually two principal mechanisms by which electric fields can distort the charge distribution of a dielectric atom or molecule: stretching and rotating. In the next two sections I'll discuss these processes. 4.1.2: Induced Dipoles Say we have a totally neutral atom and place it in an electric field E ? What happens? At first guess, you might think \"Nothing at all! The atom is not charged, so the field has no effect on it.\" That's incorrect. Although the atom as a whole is electrically neutral (just like the dipoles we looked at in the last chapter), there is a positively charged core (the nucleus) and negatively charged electron(s) surrounding it. These two regions of charge within the atom are influenced by the field: the nucleus is pushed in the direction of the field, and the electrons the opposite way. In principle, if the field is large enough, it can pull the atom apart completely, \"ionizing\" it (the substance then becomes a conductor). With less extreme fields, however, an equilibrium is soon established, for if the center of the electron cloud does not coincide with the nucleus, these positive and negative charges attract one another, and that holds the atom together. The two opposing forces - E pulling the electrons and nucleus apart, and their mutual attraction drawing them back together - reach a balance, leaving the atom polarized , with plus charge shifted slightly one way, and minus the other. The atom now has a tiny dipole moment p , which points in the same direction as E . Typically, this induced dipole moment is approximately proportional to the field (as long as the latter is not too strong): \\vec{p} = \\alpha \\vec{E} \\tagl{4.1} The constant of proportionality \\alpha is called atomic polarizability . Its value depends on the detailed structure of the atom in question. Table 4.1 lists some experimentally determined atomic polarizabilities. Example 4.1 A primitive model for an atom consists of a point nucleus (+q) surrounded by a uniformly charged spherical cloud (-q) of radius a (Fig 4.1). Calculate the atomic polarizability of such an atom. Solution In the presence of an external field E , the nucleus will be shifted slightly to the right and the electron cloud to the left, as shown in Fig 4.2. (Because the actual displacements involved are extremely small, as you'll see in Prob 4.1, it is reasonable to assume that the electron cloud retains its spherical shape.) Say that equilibrium occurs when the nucleus is displaced a distance d from the center of the sphere. At that point, the external field pushing the nucleus to the right exactly balances the internal field pulling it to the left: E = E_e , where E_e is the field produced by the electron cloud. Now the field at a distance d from the center of a uniformly charged sphere is E_e = \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd}{a^3} At equilibrium, then E = \\frac{1}{4 \\pi \\epsilon_0 } \\frac{qd}{a^3}, \\quad \\text{ or } p = qd = (4 \\pi \\epsilon_0 a^3 ) E The atomic polarizability is therefore \\alpha = 4 \\pi \\epsilon_0 a^3 = 3 \\epsilon_0 v \\tagl{4.2} where v is the volume of the atom. Although this atomic model is extremely crude, the result \\eqref{4.2} is not too bad - it's accurate to within a factor of four or so for many simple atoms. For molecules the situation is not quite so simple, because frequently they polarize more readily in some directions than in others. Carbon dioxide (Fig 4.3), for instance, has a polarizability of 4.5 \\times 10^{-40} when you apply the field along the axis of the molecule, but only 2 \\times 10^{-40} for fields perpendicular to this direction. When the field is at some angle to the axis, you must first resolve it into parallel and perpendicular components, and multiply each component by the pertinent polarizability \\vec{p} = a_{\\perp} E_{\\perp} + \\alpha_{\\parallel} E_{\\parallel} In this case, the induced dipole moment may not even be in the same direction as E . And CO_2 is relatively simple, as molecules go, since at least the atoms arrange themselves in a straight line; for a completely asymmetrical molecule, \\eqref{4.1} is replaced by the most general linear relation between E and p : \\begin{align*} p_x = \\alpha_{xx} E_x + \\alpha_{xy} E_y + \\alpha_{xz} E_z\\\\ p_y = \\alpha_{yx} E_x + \\alpha_{yy} E_y + \\alpha_{yz} E_z\\\\ p_z = \\alpha_{zx} E_x + \\alpha_{zy} E_y + \\alpha_{zz} E_z \\end{align*} \\tagl{4.3} The set of nine constants \\alpha_{ij} constitute the polarizability tensor for the molecule. Their values depend on the orientation of the axes you use, though it is always possible to choose \"principal\" axes such that all off-diagonal terms vanish, leaving just three nonzero polarizabilities. 4.1.3: Alignment of Polar Molecules The neutral atom discussed in section 4.1.2 had no dipole moment to start with - p was entirely induced by the applied field. Some molecules have built-in, permanent dipole moments. In the water molecule, for example, the electrons tend to cluster around the oxygen atom (Fig 4.4), and since the molecule is bent at 105^{\\circ} , this leaves a negative charge at the vertex and a positive charge on the opposite side. (The dipole moment of water is unusually large: 6.1 \\times 10^{-30} C \\cdot m ; in fact, this is what accounts for its effectiveness as a solvent.) What happens when such molecules (called polar molecules ) are placed in an electric field? If the field is uniform, the force on the positive end, \\vec{F_+} = q \\vec{E} , exactly cancels the force on the negative end, \\vec{F_-} = - q \\vec{E} (Fig 4.5). However, there will be a torque: \\vec{N } = (\\vec{r_{+}} \\cross \\vec{F_+}) + (\\vec{r_{-}} \\cross \\vec{F_{-}}) \\\\ = \\left[ ( \\vec{d}/2) \\cross (q \\vec{E}) \\right] + \\left[ ( -\\vec{d}/2) \\cross (- q \\vec{E}) \\right] = q \\vec{d} \\cross \\vec{E} Thus a dipole \\vec{p} = q \\vec{d} in a uniform field \\vec{E} experiences a torque \\vec{N} = \\vec{p} \\cross \\vec{E} \\tagl{4.4} Notice that N is in such a direction as to line p up parallel to E ; a polar molecule that is free to rotate will swing around until it points in the direction of the applied field. If the field is nonuniform, so that \\vec{F_{+}} does not exactly balance \\vec{F_-} , there will be a net force on the dipole, in addition to the torque. Of course, E must change rather abruptly for there to be significant variation in the space of one molecule, so this is not ordinarily a major consideration in discussing the behavior of dielectrics. Nevertheless, the formula for the force on a dipole in a nonuniform field is of some interest: \\vec{F} = \\vec{F_{+}} + \\vec{F_-} = q(\\vec{E_+} - \\vec{E_-}) = q(\\Delta \\vec{E}) where \\Delta \\vec{E} represents the difference between the field at the plus end and the field at the minus end. Assuming the dipole is very short, we may use Eq 1.35 to approximate the small change in E \\Delta \\vec{E} = (\\vec{d} \\cdot \\grad ) \\vec{E} and therefore \\vec{F} = ( \\vec{p} \\cdot \\grad) \\vec{E} \\tagl{4.5} For a \"perfect\" dipole of infinitesimal length, \\eqref{4.4} gives the torque about the center of the dipole even in a nonuniform field; about any other point, \\vec{N} = ( \\vec{p} \\cross \\vec{E}) + (\\vec{r} \\cross \\vec{F}) . 4.1.4: Polarization In the previous two sections, we have considered the effect of an external electric field on an individual atom or molecule. We are now in a position to answer (quantitatively) the original question: What happens to a piece of dielectric material when it is placed in an electric field? If the substance consists of neutral atoms (or nonpolar molecules), the field will induce in each a tiny dipole moment, pointing in the same direction as the field. If the material is made up of polar molecules, each permanent dipole will experience a torque, tending to line it up along the field direction. (Random thermal motions compete with this process, so the alignment is never complete, especially at higher temperatures, and disappears almost at once when the field is removed.) Notice that these two mechanisms produce the same basic result: a lot of little dipoles pointing along the direction of the field - the material becomes polarized . A convenient measure of this effect is \\vec{P} = \\text{ dipole moment per unit volume } which is called the polarization . From now on we shall not worry much about how the polarization got there. Actually, the two mechanisms I described are not as clear-cut as I tried to pretend. Even in polar molecules there will be some polarization by displacement (though generally it is a lot easier to rotate a molecule than to stretch it, so the second mechanism dominates). It's even possible in some materials to \"freeze in\" polarization, so that it persists after the field is removed. But let's forget for a moment about the cause of the polarization, and let's study the field that a chunk of polarized material itself produces. Then in section 4.3 we'll put it all together: the original field, which was responsible for P , plus the new field, which is due to P .","title":"4.1 - Polarization"},{"location":"ch4-1/#41-polarization","text":"","title":"4.1: Polarization"},{"location":"ch4-1/#411-dielectrics","text":"This chapter is all about what happens to an electric field when you take matter into account. Matter, of course, comes in many varieties - phase, composition, state, etc. - and depending upon which type of matter we're dealing with, the electrostatic field response can be very different. Nevertheless, most everyday objects belong (at least, to good approximation) to one of two large classes: conductors and insulators (or dielectrics ). We have already gone over what happens to an electrostatic field in a conductor; the \"unlimited\" free charges within a conductor distribute themselves through the material so as to form an equipotential. In practice, this usually means that many electrons (one or two per atom, in a typical metal) are not associated with a particular nucleus, but roam around at will. In dielectrics, by contrast, all charges are attached to specific atoms or molecules - they cannot escape their leash, and can only move a bit within the atom or molecule. Such microscopic displacements are not as dramatic as the wholesale rearrangement of charge in a conductor, but their cumulative effects account for the characteristic behavior of dielectric materials. There are actually two principal mechanisms by which electric fields can distort the charge distribution of a dielectric atom or molecule: stretching and rotating. In the next two sections I'll discuss these processes.","title":"4.1.1: Dielectrics"},{"location":"ch4-1/#412-induced-dipoles","text":"Say we have a totally neutral atom and place it in an electric field E ? What happens? At first guess, you might think \"Nothing at all! The atom is not charged, so the field has no effect on it.\" That's incorrect. Although the atom as a whole is electrically neutral (just like the dipoles we looked at in the last chapter), there is a positively charged core (the nucleus) and negatively charged electron(s) surrounding it. These two regions of charge within the atom are influenced by the field: the nucleus is pushed in the direction of the field, and the electrons the opposite way. In principle, if the field is large enough, it can pull the atom apart completely, \"ionizing\" it (the substance then becomes a conductor). With less extreme fields, however, an equilibrium is soon established, for if the center of the electron cloud does not coincide with the nucleus, these positive and negative charges attract one another, and that holds the atom together. The two opposing forces - E pulling the electrons and nucleus apart, and their mutual attraction drawing them back together - reach a balance, leaving the atom polarized , with plus charge shifted slightly one way, and minus the other. The atom now has a tiny dipole moment p , which points in the same direction as E . Typically, this induced dipole moment is approximately proportional to the field (as long as the latter is not too strong): \\vec{p} = \\alpha \\vec{E} \\tagl{4.1} The constant of proportionality \\alpha is called atomic polarizability . Its value depends on the detailed structure of the atom in question. Table 4.1 lists some experimentally determined atomic polarizabilities.","title":"4.1.2: Induced Dipoles"},{"location":"ch4-1/#example-41","text":"A primitive model for an atom consists of a point nucleus (+q) surrounded by a uniformly charged spherical cloud (-q) of radius a (Fig 4.1). Calculate the atomic polarizability of such an atom. Solution In the presence of an external field E , the nucleus will be shifted slightly to the right and the electron cloud to the left, as shown in Fig 4.2. (Because the actual displacements involved are extremely small, as you'll see in Prob 4.1, it is reasonable to assume that the electron cloud retains its spherical shape.) Say that equilibrium occurs when the nucleus is displaced a distance d from the center of the sphere. At that point, the external field pushing the nucleus to the right exactly balances the internal field pulling it to the left: E = E_e , where E_e is the field produced by the electron cloud. Now the field at a distance d from the center of a uniformly charged sphere is E_e = \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd}{a^3} At equilibrium, then E = \\frac{1}{4 \\pi \\epsilon_0 } \\frac{qd}{a^3}, \\quad \\text{ or } p = qd = (4 \\pi \\epsilon_0 a^3 ) E The atomic polarizability is therefore \\alpha = 4 \\pi \\epsilon_0 a^3 = 3 \\epsilon_0 v \\tagl{4.2} where v is the volume of the atom. Although this atomic model is extremely crude, the result \\eqref{4.2} is not too bad - it's accurate to within a factor of four or so for many simple atoms. For molecules the situation is not quite so simple, because frequently they polarize more readily in some directions than in others. Carbon dioxide (Fig 4.3), for instance, has a polarizability of 4.5 \\times 10^{-40} when you apply the field along the axis of the molecule, but only 2 \\times 10^{-40} for fields perpendicular to this direction. When the field is at some angle to the axis, you must first resolve it into parallel and perpendicular components, and multiply each component by the pertinent polarizability \\vec{p} = a_{\\perp} E_{\\perp} + \\alpha_{\\parallel} E_{\\parallel} In this case, the induced dipole moment may not even be in the same direction as E . And CO_2 is relatively simple, as molecules go, since at least the atoms arrange themselves in a straight line; for a completely asymmetrical molecule, \\eqref{4.1} is replaced by the most general linear relation between E and p : \\begin{align*} p_x = \\alpha_{xx} E_x + \\alpha_{xy} E_y + \\alpha_{xz} E_z\\\\ p_y = \\alpha_{yx} E_x + \\alpha_{yy} E_y + \\alpha_{yz} E_z\\\\ p_z = \\alpha_{zx} E_x + \\alpha_{zy} E_y + \\alpha_{zz} E_z \\end{align*} \\tagl{4.3} The set of nine constants \\alpha_{ij} constitute the polarizability tensor for the molecule. Their values depend on the orientation of the axes you use, though it is always possible to choose \"principal\" axes such that all off-diagonal terms vanish, leaving just three nonzero polarizabilities.","title":"Example 4.1"},{"location":"ch4-1/#413-alignment-of-polar-molecules","text":"The neutral atom discussed in section 4.1.2 had no dipole moment to start with - p was entirely induced by the applied field. Some molecules have built-in, permanent dipole moments. In the water molecule, for example, the electrons tend to cluster around the oxygen atom (Fig 4.4), and since the molecule is bent at 105^{\\circ} , this leaves a negative charge at the vertex and a positive charge on the opposite side. (The dipole moment of water is unusually large: 6.1 \\times 10^{-30} C \\cdot m ; in fact, this is what accounts for its effectiveness as a solvent.) What happens when such molecules (called polar molecules ) are placed in an electric field? If the field is uniform, the force on the positive end, \\vec{F_+} = q \\vec{E} , exactly cancels the force on the negative end, \\vec{F_-} = - q \\vec{E} (Fig 4.5). However, there will be a torque: \\vec{N } = (\\vec{r_{+}} \\cross \\vec{F_+}) + (\\vec{r_{-}} \\cross \\vec{F_{-}}) \\\\ = \\left[ ( \\vec{d}/2) \\cross (q \\vec{E}) \\right] + \\left[ ( -\\vec{d}/2) \\cross (- q \\vec{E}) \\right] = q \\vec{d} \\cross \\vec{E} Thus a dipole \\vec{p} = q \\vec{d} in a uniform field \\vec{E} experiences a torque \\vec{N} = \\vec{p} \\cross \\vec{E} \\tagl{4.4} Notice that N is in such a direction as to line p up parallel to E ; a polar molecule that is free to rotate will swing around until it points in the direction of the applied field. If the field is nonuniform, so that \\vec{F_{+}} does not exactly balance \\vec{F_-} , there will be a net force on the dipole, in addition to the torque. Of course, E must change rather abruptly for there to be significant variation in the space of one molecule, so this is not ordinarily a major consideration in discussing the behavior of dielectrics. Nevertheless, the formula for the force on a dipole in a nonuniform field is of some interest: \\vec{F} = \\vec{F_{+}} + \\vec{F_-} = q(\\vec{E_+} - \\vec{E_-}) = q(\\Delta \\vec{E}) where \\Delta \\vec{E} represents the difference between the field at the plus end and the field at the minus end. Assuming the dipole is very short, we may use Eq 1.35 to approximate the small change in E \\Delta \\vec{E} = (\\vec{d} \\cdot \\grad ) \\vec{E} and therefore \\vec{F} = ( \\vec{p} \\cdot \\grad) \\vec{E} \\tagl{4.5} For a \"perfect\" dipole of infinitesimal length, \\eqref{4.4} gives the torque about the center of the dipole even in a nonuniform field; about any other point, \\vec{N} = ( \\vec{p} \\cross \\vec{E}) + (\\vec{r} \\cross \\vec{F}) .","title":"4.1.3: Alignment of Polar Molecules"},{"location":"ch4-1/#414-polarization","text":"In the previous two sections, we have considered the effect of an external electric field on an individual atom or molecule. We are now in a position to answer (quantitatively) the original question: What happens to a piece of dielectric material when it is placed in an electric field? If the substance consists of neutral atoms (or nonpolar molecules), the field will induce in each a tiny dipole moment, pointing in the same direction as the field. If the material is made up of polar molecules, each permanent dipole will experience a torque, tending to line it up along the field direction. (Random thermal motions compete with this process, so the alignment is never complete, especially at higher temperatures, and disappears almost at once when the field is removed.) Notice that these two mechanisms produce the same basic result: a lot of little dipoles pointing along the direction of the field - the material becomes polarized . A convenient measure of this effect is \\vec{P} = \\text{ dipole moment per unit volume } which is called the polarization . From now on we shall not worry much about how the polarization got there. Actually, the two mechanisms I described are not as clear-cut as I tried to pretend. Even in polar molecules there will be some polarization by displacement (though generally it is a lot easier to rotate a molecule than to stretch it, so the second mechanism dominates). It's even possible in some materials to \"freeze in\" polarization, so that it persists after the field is removed. But let's forget for a moment about the cause of the polarization, and let's study the field that a chunk of polarized material itself produces. Then in section 4.3 we'll put it all together: the original field, which was responsible for P , plus the new field, which is due to P .","title":"4.1.4: Polarization"},{"location":"ch4-2/","text":"4.2: The Field of a Polarized Object 4.2.1: Bound Charges Suppose we have a piece of polarized material - that is, an object containing a lot of microscopic dipoles lined up. The dipole moment per unit volume P is given. Question : What is the field produced by this object (not the field that may have caused the polarization, but the field the polarization itself causes)? Well, we know what the field of an individual dipole looks like, so why not chop the material up into infinitesimal dipoles and integrate to get the total? As usual, it's easier to work with the potential. For a single dipole p (Eq. 3.99) V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{\\gr}}{\\gr ^2} \\tagl{4.8} where \\vec{\\gr} is the vector from the dipole to the point at which we are evaluating the potential (Fig 4.8). In the present context, we have a dipole moment \\vec{p} = \\vec{P} \\dd \\tau' in each volume element \\dd \\tau' , so the total potential is V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int_{V} \\frac{\\vec{P}(\\vec{r'}) \\cdot \\vu{\\gr}}{\\gr ^2} \\dd \\tau' That does it, in principle. But a little sleight of hand casts this integral into a much more illuminating form. Observing that \\grad ' \\left( \\frac{1}{\\gr} \\right) = \\frac{\\vu{\\gr}}{\\gr ^2} where the differentiation is with respect to the source coordinates (r'), we have V = \\frac{1}{4 \\pi \\epsilon_0} \\int _V \\vec{P} \\cdot \\grad ' \\left( \\frac{1}{\\gr} \\right) \\dd \\tau' Peeling the \\grad leftwards with integration by parts, we have V = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\int _V \\grad' \\cdot \\left( \\frac{\\vec{P}}{\\gr} \\right) \\dd \\tau' - \\int _V \\frac{1}{\\gr} ( \\grad' \\cdot \\vec{P} ) \\dd \\tau' \\right] The left-hand integral is a volume integral of a divergence, so with Gauss's law V = \\frac{1}{4 \\pi \\epsilon_0 } \\oint _S \\frac{1}{\\gr } \\vec{P} \\cdot \\dd \\vec{a'} - \\frac{1}{4 \\pi \\epsilon_0} \\int _V \\frac{1}{\\gr} ( \\grad' \\cdot \\vec{P} ) \\dd \\tau' \\tagl{4.10} The first term looks like the potential of a surface charge \\sigma_b \\equiv \\vec{P} \\cdot \\vu{n} \\tagl{4.11} while the second term looks like the potential of a volume charge \\rho_b \\equiv - \\div \\vec{P} \\tagl{4.12} With these definitions, \\eqref{4.10} becomes V(\\vec{r} = \\frac{1}{4 \\pi \\epsilon_0} \\oint_S \\frac{\\sigma_b}{\\gr} \\dd a' + \\frac{1}{4 \\pi \\epsilon_0} \\int_V \\frac{\\rho_b}{\\gr} \\dd \\tau' \\tagl{4.13} What this means is that the potential (and hence also the field) of a polarized object is the same as that produced by a volume charge density \\rho_b = - \\div \\vec{P} plus a surface charge density \\sigma_b = \\vec{P} \\cdot \\vu{n} . Instead of integrating the contributions of all the infinitesimal dipoles, we could just find those bound charges , and then calculate the fields they produce, in the same way we calculate the field of any other volume and surface charges. Example 4.2 Find the electric field produced by a uniformly polarized sphere of radius R Solution We may as well choose the z axis to coincide with the direction of polarization (Fig 4.9). The volume bound charge density \\rho_b is zero, since \\vec{P} is uniform. The surface bound charge density is then \\sigma_b = \\vec{P} \\cdot \\vu{n} = P \\cos \\theta where \\theta is the usual spherical coordinate. What we want, then is the field produced by a charge density P \\cos \\theta plastered over the surface of a sphere. We happen to have already computed that potential in Exercise 3.9: V(r, \\theta) = \\begin{cases} \\frac{P}{3 \\epsilon_0} r \\cos \\theta, & \\qquad \\text{ for } r \\leq R \\\\ \\frac{P}{3 \\epsilon_0} \\frac{R^3}{r^2} \\cos \\theta, & \\qquad \\text{ for } r \\geq R \\end{cases} Since r \\cos \\theta = z , the field inside the sphere is uniform : \\vec{E} = - \\grad V = - \\frac{P}{3 \\epsilon_0} \\vu{z} = - \\frac{1}{3 \\epsilon_0} \\vec{P} , \\quad \\text{ for } r < R \\tagl{4.14} This is a pretty remarkable result, and will be very useful in what follows. Outside the sphere the potential is the same as that of a perfect dipole at the origin V = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} , \\quad \\text{ for } r \\geq R \\tagl{4.15} whose dipole moment is, not surprisingly, equal to the total dipole moment of the sphere: \\vec{p} = \\frac{4}{3} \\pi R^3 \\vec{P} \\tagl{4.16} The field of the uniformly polarized sphere is shown in Fig 4.10. 4.2.2: Physical Interpretation of Bound Charges In the last section we found that the field of a polarized object is identical to the field that would be produced by a certain distribution of \"bound charges,\" \\sigma_b and \\rho_b . But this conclusion emerged in the course of abstract manipulations on the integral in Eq. 4.9, and left us with no clue as to the physical meaning of these bound charges. Indeed, some authors give you the impression that bound charges are in some sense \"fictitious\" - mere bookkeeping devices used to facilitate the calculation of fields. Nothing could be further from the truth: \\rho_b and \\sigma_b represent perfectly genuine accumulations of charge. In this section I'll explain how polarization leads to these charge distributions. The basic idea is very simple: Suppose we have a long string of dipoles, as shown in Fig. 4.11. Along the line, the head of one effectively cancels the tail of its neighbor, but at the ends there are two charges left over: plus at the right end and minus at the left. It is as if we had peeled off an electron at one end and carried it all the way down to the other end, though in fact no single electron made the whole trip - a lot of tiny displacements add up to one large one. We call the net charge at the ends a bound charge to remind ourselves that it cannot be removed; in a dielectric every electron is attached to a specific atom or molecule. But apart from that, bound charge is no different from any other kind. To calculate the actual amount of bound charge resulting from a given polarization, examine a \"tube\" of dielectric parallel to P. The dipole moment of the tiny chunk shown in Fig. 4.12 is P(Ad) , where A is the cross-sectional area of the tube and dis the length of the chunk. In terms of the charge (q) at the end, this same dipole moment can be written qd . The bound charge that piles up at the right end of the tube is therefore q = PA If the ends have been sliced off perpendicularly, the surface charge density is \\sigma_b = \\frac{q}{A} = P For an oblique cut (Fig 4.13), the charge is still the same, but A = A_{end} \\cos \\theta so \\sigma_b = \\frac{1}{A_{end}} = P \\cos \\theta = \\vec{P} \\cdot \\vu{n} The effect of the polarization, then, is to paint a bound charge \\sigma_b = \\vec{P} \\cdot \\vu{n} over the surface of the material. This is exactly what we found by more rigorous means in Sect. 4.2.1. But now we know where the bound charge comes from. If the polarization is nonuniform, we get accumulations of bound charge within the material, as well as on the surface. A glance at Fig. 4.14 suggests that a diverging P results in a pileup of negative charge. Indeed, the net bound charge \\int \\rho_b \\dd \\tau in a given volume is equal and opposite to the amount that has been pushed out through the surface. The latter (by the same reasoning we used before) is \\vec{P} \\cdot \\vu{n} per unit area, so \\int_v \\rho_b \\dd \\tau = - \\oint _S \\vec{P} \\cdot \\dd \\vec{a} = - \\int_V (\\div \\vec{P}) \\dd \\tau Since this is true for any volume, we have \\rho_b = - \\div \\vec{P} confirming, again, the more rigorous conclusion of Sect. 4.2.1. Example 4.3 There is another way of analyzing the uniformly polarized sphere, which nicely illustrates the idea of bound charge. What we have, really, is two spheres of charge: a positive sphere and a negative sphere. Without polarization the two are superimposed and cancel completely. But when the material is uniformly polarized, all the plus charges move slightly upward (the z direction) and all the minus charges move slightly downward (Fig 4.15). The two spheres no longer overlap perfectly: at the top there's a 'cap' of leftover positive charge and at the bottom a cap of negative charge. This 'leftover' charge is precisely the bound surface charge \\sigma_b In Prob 2.18, you calculated the field in the region of overlap between two uniformly charged spheres; the answer was \\vec{E} = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{1 \\vec{d}}{R^3} where q is the total charge of the positive sphere, d is the vector from the negative center to the positive center, and R is the radius of the sphere. We can express this in terms of the polarization of the sphere, \\vec{p} = q \\vec{d} = (\\frac{4}{3} \\pi R^3 )\\vec{P} , as \\vec{E} = - \\frac{1}{3 \\epsilon_0} \\vec{P} Meanwhile, for points outside, it is as though all charge on each sphere were concentrated at the respective center. We have, then, a dipole, with potential V = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} (Remember that d is some small fraction of an atomic radius; Fig 4.15 is grossly exaggerated). These answers agree, of course, wtih the results from Ex 4.2. 4.2.3: The Field Inside a Dielectric I have been sloppy about the distinction between \"pure\" dipoles and \"physical\" dipoles. In developing the theory of bound charges, I assumed we were working with the pure kind - indeed, I stated in Eq 4.8 the formula for the potential of a perfect dipole. And yet an actual polarized dielectric consists of physical dipoles, albeit extremely tiny ones. What is more, I presumed to represent discrete molecular dipoles by a continuous density function P . How should I justify this method? Outside the dielectric there is no real problem: here we are far away from the molecules ( \\gr is many times greater than the separation distance between plus and minus charges), so the dipole potential dominates overwhelmingly and the detailed \"graininess\" of the source is blurred by distance. Inside the dielectric, however, we can hardly pretend to be far from all the dipoles, and the procedure I used in Sect. 4.2.1 is open to serious challenge. In fact, when you stop to think about it, the electric field inside matter must be fantastically complicated, on the microscopic level. If you happen to be very near an electron, the field is gigantic, whereas a short distance away it may be small or may point in a totally different direction. Moreover, an instant later, as the atoms move about, the field will have altered entirely. This true microscopic field would be utterly impossible to calculate, nor would it be of much interest if you could. Just as, for macroscopic purposes, we regard water as a continuous fluid, ignoring its molecular structure, so also we can ignore the microscopic bumps and wrinkles in the electric field inside matter, and concentrate on the macroscopic field. This is defined as the average field over regions large enough to contain many thousands of atoms (so that the uninteresting microscopic fluctuations are smoothed over), and yet small enough to ensure that we do not wash out any significant large-scale variations in the field. (In practice, this means we must average over regions much smaller than the dimensions of the object itself.) Ordinarily, the macroscopic field is what people mean when they speak of \"the\" field inside matter. It remains to show that the macroscopic field is what we actually obtain when we use the methods of Sect. 4.2.1. The argument is subtle, so hang on. Suppose I want to calculate the macroscopic field at some point r within a dielectric. I know I must average the true (microscopic) field over an appropriate volume, so let me draw a small sphere about r , of radius, say, a thousand times the size of a molecule. The macroscopic field at r, then, consists of two parts: the average field over the sphere due to all charges outside, plus the average due to all charges inside: \\vec{E} = \\vec{E}_{out} + \\vec{E}_{in} You proved in problem 3.47 that the average field over a sphere produced by charges outside is equal to the field they produce at the center, so \\vec{E_{out}} is the field at r due to the dipoles exterior to the sphere. These are far enough away that we can safely use Eq 4.9 V_{out} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{outside} \\frac{\\vec{P}(\\vec{r'}) \\cdot \\vu{\\gr}}{\\gr ^2} \\dd \\tau' \\tagl{4.17} The dipoles inside the sphere are too close to treat in this fashion. But fortunately all we need is their average field, which we already know (Eq 3.105) \\vec{E}_{in} = -\\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p}}{R^3} regardless of the details of the charge distribution within the sphere. The only relevant quantity is the total dipole moment, \\vec{p} = (\\frac{4}{3} \\pi R^3 )\\vec{P} : \\vec{E_{in}} = - \\frac{1}{3 \\epsilon_0} \\vec{P} \\tagl{4.18} Now, by assumption, the sphere is small enough that P does not vary significantly over its volume, so the term left out of the integral in \\eqref{4.17} corresponds to the field at the center of a uniformly polarized sphere, to wit: -(1/3\\epsilon_0 )\\vec{P} \\eqref{4.14} . But this is precisely what \\vec{E_{in}} puts back in! The macroscopic field, then, is given by the potential V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\vec{P}(\\vec{r'}) \\cdot \\vu{\\gr}}{\\gr ^2} \\dd \\tau' \\tagl{4.19} where the integral runs over the entire volume of the dielectric. This is, of course, what we used under the assumption of perfect dipoles in Sect 4.2.1; without realizing it, we were correctly calculating the averaged, macroscopic field, for points inside the dielectric. Notice that this argument all revolves around the curious fact that the average field over any sphere (due to the charge inside) is the same as the field at the center of a uniformly polarized sphere with the same total dipole moment. This means that no matter how crazy the actual microscopic charge configuration, we can replace it with a nice smooth distribution of perfect dipoles, if all we care about is the macroscopic (average) field. Incidentally, while the argument ostensibly relies on the spherical shape I chose to average over, the macroscopic field is certainly independent of the geometry of the averaging region, and this is reflected in the final answer \\eqref{4.19} . Presumably one could reproduce the same argument for a cube or ellipsoid or whatever by performing some more grueling calculations.","title":"4.2 - The Field of a Polarized Object"},{"location":"ch4-2/#42-the-field-of-a-polarized-object","text":"","title":"4.2: The Field of a Polarized Object"},{"location":"ch4-2/#421-bound-charges","text":"Suppose we have a piece of polarized material - that is, an object containing a lot of microscopic dipoles lined up. The dipole moment per unit volume P is given. Question : What is the field produced by this object (not the field that may have caused the polarization, but the field the polarization itself causes)? Well, we know what the field of an individual dipole looks like, so why not chop the material up into infinitesimal dipoles and integrate to get the total? As usual, it's easier to work with the potential. For a single dipole p (Eq. 3.99) V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{\\gr}}{\\gr ^2} \\tagl{4.8} where \\vec{\\gr} is the vector from the dipole to the point at which we are evaluating the potential (Fig 4.8). In the present context, we have a dipole moment \\vec{p} = \\vec{P} \\dd \\tau' in each volume element \\dd \\tau' , so the total potential is V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int_{V} \\frac{\\vec{P}(\\vec{r'}) \\cdot \\vu{\\gr}}{\\gr ^2} \\dd \\tau' That does it, in principle. But a little sleight of hand casts this integral into a much more illuminating form. Observing that \\grad ' \\left( \\frac{1}{\\gr} \\right) = \\frac{\\vu{\\gr}}{\\gr ^2} where the differentiation is with respect to the source coordinates (r'), we have V = \\frac{1}{4 \\pi \\epsilon_0} \\int _V \\vec{P} \\cdot \\grad ' \\left( \\frac{1}{\\gr} \\right) \\dd \\tau' Peeling the \\grad leftwards with integration by parts, we have V = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\int _V \\grad' \\cdot \\left( \\frac{\\vec{P}}{\\gr} \\right) \\dd \\tau' - \\int _V \\frac{1}{\\gr} ( \\grad' \\cdot \\vec{P} ) \\dd \\tau' \\right] The left-hand integral is a volume integral of a divergence, so with Gauss's law V = \\frac{1}{4 \\pi \\epsilon_0 } \\oint _S \\frac{1}{\\gr } \\vec{P} \\cdot \\dd \\vec{a'} - \\frac{1}{4 \\pi \\epsilon_0} \\int _V \\frac{1}{\\gr} ( \\grad' \\cdot \\vec{P} ) \\dd \\tau' \\tagl{4.10} The first term looks like the potential of a surface charge \\sigma_b \\equiv \\vec{P} \\cdot \\vu{n} \\tagl{4.11} while the second term looks like the potential of a volume charge \\rho_b \\equiv - \\div \\vec{P} \\tagl{4.12} With these definitions, \\eqref{4.10} becomes V(\\vec{r} = \\frac{1}{4 \\pi \\epsilon_0} \\oint_S \\frac{\\sigma_b}{\\gr} \\dd a' + \\frac{1}{4 \\pi \\epsilon_0} \\int_V \\frac{\\rho_b}{\\gr} \\dd \\tau' \\tagl{4.13} What this means is that the potential (and hence also the field) of a polarized object is the same as that produced by a volume charge density \\rho_b = - \\div \\vec{P} plus a surface charge density \\sigma_b = \\vec{P} \\cdot \\vu{n} . Instead of integrating the contributions of all the infinitesimal dipoles, we could just find those bound charges , and then calculate the fields they produce, in the same way we calculate the field of any other volume and surface charges.","title":"4.2.1: Bound Charges"},{"location":"ch4-2/#example-42","text":"Find the electric field produced by a uniformly polarized sphere of radius R Solution We may as well choose the z axis to coincide with the direction of polarization (Fig 4.9). The volume bound charge density \\rho_b is zero, since \\vec{P} is uniform. The surface bound charge density is then \\sigma_b = \\vec{P} \\cdot \\vu{n} = P \\cos \\theta where \\theta is the usual spherical coordinate. What we want, then is the field produced by a charge density P \\cos \\theta plastered over the surface of a sphere. We happen to have already computed that potential in Exercise 3.9: V(r, \\theta) = \\begin{cases} \\frac{P}{3 \\epsilon_0} r \\cos \\theta, & \\qquad \\text{ for } r \\leq R \\\\ \\frac{P}{3 \\epsilon_0} \\frac{R^3}{r^2} \\cos \\theta, & \\qquad \\text{ for } r \\geq R \\end{cases} Since r \\cos \\theta = z , the field inside the sphere is uniform : \\vec{E} = - \\grad V = - \\frac{P}{3 \\epsilon_0} \\vu{z} = - \\frac{1}{3 \\epsilon_0} \\vec{P} , \\quad \\text{ for } r < R \\tagl{4.14} This is a pretty remarkable result, and will be very useful in what follows. Outside the sphere the potential is the same as that of a perfect dipole at the origin V = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} , \\quad \\text{ for } r \\geq R \\tagl{4.15} whose dipole moment is, not surprisingly, equal to the total dipole moment of the sphere: \\vec{p} = \\frac{4}{3} \\pi R^3 \\vec{P} \\tagl{4.16} The field of the uniformly polarized sphere is shown in Fig 4.10.","title":"Example 4.2"},{"location":"ch4-2/#422-physical-interpretation-of-bound-charges","text":"In the last section we found that the field of a polarized object is identical to the field that would be produced by a certain distribution of \"bound charges,\" \\sigma_b and \\rho_b . But this conclusion emerged in the course of abstract manipulations on the integral in Eq. 4.9, and left us with no clue as to the physical meaning of these bound charges. Indeed, some authors give you the impression that bound charges are in some sense \"fictitious\" - mere bookkeeping devices used to facilitate the calculation of fields. Nothing could be further from the truth: \\rho_b and \\sigma_b represent perfectly genuine accumulations of charge. In this section I'll explain how polarization leads to these charge distributions. The basic idea is very simple: Suppose we have a long string of dipoles, as shown in Fig. 4.11. Along the line, the head of one effectively cancels the tail of its neighbor, but at the ends there are two charges left over: plus at the right end and minus at the left. It is as if we had peeled off an electron at one end and carried it all the way down to the other end, though in fact no single electron made the whole trip - a lot of tiny displacements add up to one large one. We call the net charge at the ends a bound charge to remind ourselves that it cannot be removed; in a dielectric every electron is attached to a specific atom or molecule. But apart from that, bound charge is no different from any other kind. To calculate the actual amount of bound charge resulting from a given polarization, examine a \"tube\" of dielectric parallel to P. The dipole moment of the tiny chunk shown in Fig. 4.12 is P(Ad) , where A is the cross-sectional area of the tube and dis the length of the chunk. In terms of the charge (q) at the end, this same dipole moment can be written qd . The bound charge that piles up at the right end of the tube is therefore q = PA If the ends have been sliced off perpendicularly, the surface charge density is \\sigma_b = \\frac{q}{A} = P For an oblique cut (Fig 4.13), the charge is still the same, but A = A_{end} \\cos \\theta so \\sigma_b = \\frac{1}{A_{end}} = P \\cos \\theta = \\vec{P} \\cdot \\vu{n} The effect of the polarization, then, is to paint a bound charge \\sigma_b = \\vec{P} \\cdot \\vu{n} over the surface of the material. This is exactly what we found by more rigorous means in Sect. 4.2.1. But now we know where the bound charge comes from. If the polarization is nonuniform, we get accumulations of bound charge within the material, as well as on the surface. A glance at Fig. 4.14 suggests that a diverging P results in a pileup of negative charge. Indeed, the net bound charge \\int \\rho_b \\dd \\tau in a given volume is equal and opposite to the amount that has been pushed out through the surface. The latter (by the same reasoning we used before) is \\vec{P} \\cdot \\vu{n} per unit area, so \\int_v \\rho_b \\dd \\tau = - \\oint _S \\vec{P} \\cdot \\dd \\vec{a} = - \\int_V (\\div \\vec{P}) \\dd \\tau Since this is true for any volume, we have \\rho_b = - \\div \\vec{P} confirming, again, the more rigorous conclusion of Sect. 4.2.1.","title":"4.2.2: Physical Interpretation of Bound Charges"},{"location":"ch4-2/#example-43","text":"There is another way of analyzing the uniformly polarized sphere, which nicely illustrates the idea of bound charge. What we have, really, is two spheres of charge: a positive sphere and a negative sphere. Without polarization the two are superimposed and cancel completely. But when the material is uniformly polarized, all the plus charges move slightly upward (the z direction) and all the minus charges move slightly downward (Fig 4.15). The two spheres no longer overlap perfectly: at the top there's a 'cap' of leftover positive charge and at the bottom a cap of negative charge. This 'leftover' charge is precisely the bound surface charge \\sigma_b In Prob 2.18, you calculated the field in the region of overlap between two uniformly charged spheres; the answer was \\vec{E} = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{1 \\vec{d}}{R^3} where q is the total charge of the positive sphere, d is the vector from the negative center to the positive center, and R is the radius of the sphere. We can express this in terms of the polarization of the sphere, \\vec{p} = q \\vec{d} = (\\frac{4}{3} \\pi R^3 )\\vec{P} , as \\vec{E} = - \\frac{1}{3 \\epsilon_0} \\vec{P} Meanwhile, for points outside, it is as though all charge on each sphere were concentrated at the respective center. We have, then, a dipole, with potential V = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} (Remember that d is some small fraction of an atomic radius; Fig 4.15 is grossly exaggerated). These answers agree, of course, wtih the results from Ex 4.2.","title":"Example 4.3"},{"location":"ch4-2/#423-the-field-inside-a-dielectric","text":"I have been sloppy about the distinction between \"pure\" dipoles and \"physical\" dipoles. In developing the theory of bound charges, I assumed we were working with the pure kind - indeed, I stated in Eq 4.8 the formula for the potential of a perfect dipole. And yet an actual polarized dielectric consists of physical dipoles, albeit extremely tiny ones. What is more, I presumed to represent discrete molecular dipoles by a continuous density function P . How should I justify this method? Outside the dielectric there is no real problem: here we are far away from the molecules ( \\gr is many times greater than the separation distance between plus and minus charges), so the dipole potential dominates overwhelmingly and the detailed \"graininess\" of the source is blurred by distance. Inside the dielectric, however, we can hardly pretend to be far from all the dipoles, and the procedure I used in Sect. 4.2.1 is open to serious challenge. In fact, when you stop to think about it, the electric field inside matter must be fantastically complicated, on the microscopic level. If you happen to be very near an electron, the field is gigantic, whereas a short distance away it may be small or may point in a totally different direction. Moreover, an instant later, as the atoms move about, the field will have altered entirely. This true microscopic field would be utterly impossible to calculate, nor would it be of much interest if you could. Just as, for macroscopic purposes, we regard water as a continuous fluid, ignoring its molecular structure, so also we can ignore the microscopic bumps and wrinkles in the electric field inside matter, and concentrate on the macroscopic field. This is defined as the average field over regions large enough to contain many thousands of atoms (so that the uninteresting microscopic fluctuations are smoothed over), and yet small enough to ensure that we do not wash out any significant large-scale variations in the field. (In practice, this means we must average over regions much smaller than the dimensions of the object itself.) Ordinarily, the macroscopic field is what people mean when they speak of \"the\" field inside matter. It remains to show that the macroscopic field is what we actually obtain when we use the methods of Sect. 4.2.1. The argument is subtle, so hang on. Suppose I want to calculate the macroscopic field at some point r within a dielectric. I know I must average the true (microscopic) field over an appropriate volume, so let me draw a small sphere about r , of radius, say, a thousand times the size of a molecule. The macroscopic field at r, then, consists of two parts: the average field over the sphere due to all charges outside, plus the average due to all charges inside: \\vec{E} = \\vec{E}_{out} + \\vec{E}_{in} You proved in problem 3.47 that the average field over a sphere produced by charges outside is equal to the field they produce at the center, so \\vec{E_{out}} is the field at r due to the dipoles exterior to the sphere. These are far enough away that we can safely use Eq 4.9 V_{out} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{outside} \\frac{\\vec{P}(\\vec{r'}) \\cdot \\vu{\\gr}}{\\gr ^2} \\dd \\tau' \\tagl{4.17} The dipoles inside the sphere are too close to treat in this fashion. But fortunately all we need is their average field, which we already know (Eq 3.105) \\vec{E}_{in} = -\\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p}}{R^3} regardless of the details of the charge distribution within the sphere. The only relevant quantity is the total dipole moment, \\vec{p} = (\\frac{4}{3} \\pi R^3 )\\vec{P} : \\vec{E_{in}} = - \\frac{1}{3 \\epsilon_0} \\vec{P} \\tagl{4.18} Now, by assumption, the sphere is small enough that P does not vary significantly over its volume, so the term left out of the integral in \\eqref{4.17} corresponds to the field at the center of a uniformly polarized sphere, to wit: -(1/3\\epsilon_0 )\\vec{P} \\eqref{4.14} . But this is precisely what \\vec{E_{in}} puts back in! The macroscopic field, then, is given by the potential V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\vec{P}(\\vec{r'}) \\cdot \\vu{\\gr}}{\\gr ^2} \\dd \\tau' \\tagl{4.19} where the integral runs over the entire volume of the dielectric. This is, of course, what we used under the assumption of perfect dipoles in Sect 4.2.1; without realizing it, we were correctly calculating the averaged, macroscopic field, for points inside the dielectric. Notice that this argument all revolves around the curious fact that the average field over any sphere (due to the charge inside) is the same as the field at the center of a uniformly polarized sphere with the same total dipole moment. This means that no matter how crazy the actual microscopic charge configuration, we can replace it with a nice smooth distribution of perfect dipoles, if all we care about is the macroscopic (average) field. Incidentally, while the argument ostensibly relies on the spherical shape I chose to average over, the macroscopic field is certainly independent of the geometry of the averaging region, and this is reflected in the final answer \\eqref{4.19} . Presumably one could reproduce the same argument for a cube or ellipsoid or whatever by performing some more grueling calculations.","title":"4.2.3: The Field Inside a Dielectric"},{"location":"ch4-3/","text":"4.3: The Electric Displacement 4.3.1: Gauss's Law in the Presence of Dielectrics In Section 4.2 we found that the effect of polarization is to produce accumulations of (bound) charge, \\rho_b = - \\div \\vec{P} within the dielectric and \\sigma_b = \\vec{P} \\cdot \\vu{n} on the surface. The field due to polarization of the medium is just the field of this bound charge. We are now ready to put it all together: the field attributable to bound charge plus the field due to everything else (which, for want of a better term, we call free charge , \\rho_f ). The free charge might consist of electrons on a conductor or ions embedded in the dielectric material or whatever; any charge, in other words, that is not a result of polarization. Within the dielectric, the total charge density can be written \\rho = \\rho_b + \\rho_f \\tagl{4.20} and Gauss's law reads \\epsilon_0 \\div \\vec{E} = \\rho = \\rho_b + \\rho_f = - \\div \\vec{P} + \\rho_f where E is now the total field, not just that portion generated by polarization. It is convenient to combine the two divergence terms: \\div (\\epsilon_0 \\vec{E} + \\vec{P}) = \\rho_f The expression in parentheses is known as the electric displacement and is designated by the letter D : \\vec{D} \\equiv \\epsilon_0 \\vec{E} + \\vec{P} \\tagl{4.21} In terms of D , Gauss's law then reads \\div \\vec{D} = \\rho_f \\tagl{4.22} or in integral form \\oint \\vec{D} \\cdot \\dd \\vec{a} = Q_{f_{enc}} \\tagl{4.23} where Q_{f_{enc}} denotes the total free charge enclosed in the volume. This is a particularly useful way to express Gauss's law, in the context of dielectrics, because it makes reference only to the free charges , and free charge is the stuff we control. Bound charge comes along for the ride: when we put the free charge in place, a certain polarization automatically arises, by the mechanisms of Sect 4.1, and this polarization produces the bound charge. In a typical problem, therefore, we know \\rho_f , but we do not (initially) know \\rho_b ; \\eqref{4.23} lets us go right to work with the information at hand. In particular, whenever the requisite symmetry is present, we can immediately calculate D by the standard Gauss's law methods. Example 4.4 A long straight wire, carrying uniform line charge \\lambda , is surrounded by rubber insulation out to a radius a (Fig 4.17). Find the electric displacement. Solution Drawing a cylindrical Gaussian surface, of radius s and length L , and applying \\eqref{4.23} we find D( 2\\pi s L) = \\lambda L Therefore \\vec{D} = \\frac{\\lambda}{2 \\pi s} \\vu{s} \\tagl{4.24} Notice that this formula holds both within the insulation and outside it. In the latter region, \\vec{P} = 0 so \\vec{E} =\\frac{1}{\\epsilon_0} \\vec{D} = \\frac{\\lambda}{2 \\pi \\epsilon_0 s} \\vu{s}, \\quad \\text{ for } s > a Inside the rubber, the electric field cannot be determined, since we do not know P . Hold on a tick! We got all the way to a field we can calculate by Gauss's law, but we have left out the surface bound charge \\sigma_b . What happened to it? To be more precise, \\eqref{4.22} works within a dielectric, but we cannot apply Gauss's law precisely at the boundary of the dielectric, because the local \\rho_b blows up there, taking \\div \\vec{E} with it. The polarization drops abruptly to zero outside the material, so its derivative is a delta function. The surface bound charge is precisely this term, so in this sense it is actually included in \\rho_b , but we ordinarily prefer to handle it separately as \\sigma_b . We could even picture the edge of the dielectric as having some finite thickness, within which the polarization drops off to zero (which is probably a more realistic model anyway), in which case there is no \\sigma_b , \\rho_b varies rapidly but smoothly, and Gauss's law can safely be applied everywhere. In any case, we can use \\eqref{4.23} safely without fear of this \"defect.\" 4.3.2: A Deceptive Parallel Our expression for the divergence of the displacement looks just like Gauss's law, only the total charge density \\rho is replaced by the free charge density \\rho_f , and \\vec{D} is substituted for \\epsilon_0 \\vec{E} . For this reason, you may be tempted to conclude that D is \"just like\" E (apart from the factor \\epsilon_0 ), except that its source is \\rho_f instead of \\rho . That is, it's tempting to say \"To solve problems involving dielectrics, you just forget all about the bound charge - calculate the field as you ordinarily would, only call the answer D instead of E .\" This reasoning is seductive, but the conclusion is false; in particular there is no \"Coulomb's law\" for D : \\vec{D}(\\vec{r}) \\neq \\frac{1}{4 \\pi} \\int \\frac{\\vu{\\gr}}{\\gr ^2} \\rho_f(\\vec{r'}) \\dd \\tau' This is because the divergence alone is insufficient to determine a vector field; you need to know its curl as well. One tends to forget this in the case of electrostatics because we usually don't care about the curl of E anyway. But the curl of D is not always zero, even in electrostatics, since there is no reason, in general, to suppose that the curl of P vanishes: \\curl \\vec{D} = \\epsilon_0 (\\curl \\vec{E}) + (\\curl \\vec{P}) = \\curl \\vec{P} \\tagl{4.25} Sometimes it does, but more often it does not. The bar electret of Prob 4.11 is one example of this: here there is no free charge anywhere, so if you really believe that the only source of D is \\rho_f you will be forced to conclude that \\vec{D} = 0 everywhere, and hence that \\vec{E} = (-1 / \\epsilon_0) \\vec{P} inside and \\vec{E} = 0 outside the electret, which is obviously wrong. And because \\curl \\vec{D} \\neq 0 in general, D cannot be expressed as the gradient of a scalar - there is no \"potential\" for D . Advice : When you are asked to compute the electric displacement, first look for symmetry. If the problem exhibits spherical, cylindrical, or plane symmetry, then you can get D directly from Eq. 4.23 by the usual Gauss's law methods. (Evidently in such cases \\curl \\vec{P} is automatically zero, but since symmetry alone dictates the answer, you're not really obliged to worry about the curl.) If the requisite symmetry is absent, you'll have to think of another approach, and, in particular, you must not assume that D is determined exclusively by the free charge. 4.3.3: Boundary Conditions The electrostatic boundary conditions we had in Sect 2.3 can be re-cast in terms of D . \\eqref{4.23} tells us the discontinuity in the component perpendicular to an interface: D_{above} ^{\\perp} - D_{below} ^{\\perp} = \\sigma_f \\tagl{4.26} while \\eqref{4.25} gives the discontinuity in parallel components: \\vec{D}_{above} ^{\\parallel} - \\vec{D}_{below} ^{\\parallel} = \\vec{P}_{above} ^{\\parallel} - \\vec{P}_{below} ^{\\parallel} \\tagl{4.27} In the presence of dielectrics, these are sometimes more useful than the corresponding boundary conditions on E (Eqs 2.31 and 2.32): E_{above} ^{\\perp} - E_{below} ^{\\perp} = \\frac{1}{\\epsilon_0} \\sigma \\tagl{4.28} and \\vec{E}_{above} ^{\\parallel} - \\vec{E}_{below} ^{\\parallel} = 0 \\tagl{4.29}","title":"4.3 - The Electric Displacement"},{"location":"ch4-3/#43-the-electric-displacement","text":"","title":"4.3: The Electric Displacement"},{"location":"ch4-3/#431-gausss-law-in-the-presence-of-dielectrics","text":"In Section 4.2 we found that the effect of polarization is to produce accumulations of (bound) charge, \\rho_b = - \\div \\vec{P} within the dielectric and \\sigma_b = \\vec{P} \\cdot \\vu{n} on the surface. The field due to polarization of the medium is just the field of this bound charge. We are now ready to put it all together: the field attributable to bound charge plus the field due to everything else (which, for want of a better term, we call free charge , \\rho_f ). The free charge might consist of electrons on a conductor or ions embedded in the dielectric material or whatever; any charge, in other words, that is not a result of polarization. Within the dielectric, the total charge density can be written \\rho = \\rho_b + \\rho_f \\tagl{4.20} and Gauss's law reads \\epsilon_0 \\div \\vec{E} = \\rho = \\rho_b + \\rho_f = - \\div \\vec{P} + \\rho_f where E is now the total field, not just that portion generated by polarization. It is convenient to combine the two divergence terms: \\div (\\epsilon_0 \\vec{E} + \\vec{P}) = \\rho_f The expression in parentheses is known as the electric displacement and is designated by the letter D : \\vec{D} \\equiv \\epsilon_0 \\vec{E} + \\vec{P} \\tagl{4.21} In terms of D , Gauss's law then reads \\div \\vec{D} = \\rho_f \\tagl{4.22} or in integral form \\oint \\vec{D} \\cdot \\dd \\vec{a} = Q_{f_{enc}} \\tagl{4.23} where Q_{f_{enc}} denotes the total free charge enclosed in the volume. This is a particularly useful way to express Gauss's law, in the context of dielectrics, because it makes reference only to the free charges , and free charge is the stuff we control. Bound charge comes along for the ride: when we put the free charge in place, a certain polarization automatically arises, by the mechanisms of Sect 4.1, and this polarization produces the bound charge. In a typical problem, therefore, we know \\rho_f , but we do not (initially) know \\rho_b ; \\eqref{4.23} lets us go right to work with the information at hand. In particular, whenever the requisite symmetry is present, we can immediately calculate D by the standard Gauss's law methods.","title":"4.3.1: Gauss's Law in the Presence of Dielectrics"},{"location":"ch4-3/#example-44","text":"A long straight wire, carrying uniform line charge \\lambda , is surrounded by rubber insulation out to a radius a (Fig 4.17). Find the electric displacement. Solution Drawing a cylindrical Gaussian surface, of radius s and length L , and applying \\eqref{4.23} we find D( 2\\pi s L) = \\lambda L Therefore \\vec{D} = \\frac{\\lambda}{2 \\pi s} \\vu{s} \\tagl{4.24} Notice that this formula holds both within the insulation and outside it. In the latter region, \\vec{P} = 0 so \\vec{E} =\\frac{1}{\\epsilon_0} \\vec{D} = \\frac{\\lambda}{2 \\pi \\epsilon_0 s} \\vu{s}, \\quad \\text{ for } s > a Inside the rubber, the electric field cannot be determined, since we do not know P . Hold on a tick! We got all the way to a field we can calculate by Gauss's law, but we have left out the surface bound charge \\sigma_b . What happened to it? To be more precise, \\eqref{4.22} works within a dielectric, but we cannot apply Gauss's law precisely at the boundary of the dielectric, because the local \\rho_b blows up there, taking \\div \\vec{E} with it. The polarization drops abruptly to zero outside the material, so its derivative is a delta function. The surface bound charge is precisely this term, so in this sense it is actually included in \\rho_b , but we ordinarily prefer to handle it separately as \\sigma_b . We could even picture the edge of the dielectric as having some finite thickness, within which the polarization drops off to zero (which is probably a more realistic model anyway), in which case there is no \\sigma_b , \\rho_b varies rapidly but smoothly, and Gauss's law can safely be applied everywhere. In any case, we can use \\eqref{4.23} safely without fear of this \"defect.\"","title":"Example 4.4"},{"location":"ch4-3/#432-a-deceptive-parallel","text":"Our expression for the divergence of the displacement looks just like Gauss's law, only the total charge density \\rho is replaced by the free charge density \\rho_f , and \\vec{D} is substituted for \\epsilon_0 \\vec{E} . For this reason, you may be tempted to conclude that D is \"just like\" E (apart from the factor \\epsilon_0 ), except that its source is \\rho_f instead of \\rho . That is, it's tempting to say \"To solve problems involving dielectrics, you just forget all about the bound charge - calculate the field as you ordinarily would, only call the answer D instead of E .\" This reasoning is seductive, but the conclusion is false; in particular there is no \"Coulomb's law\" for D : \\vec{D}(\\vec{r}) \\neq \\frac{1}{4 \\pi} \\int \\frac{\\vu{\\gr}}{\\gr ^2} \\rho_f(\\vec{r'}) \\dd \\tau' This is because the divergence alone is insufficient to determine a vector field; you need to know its curl as well. One tends to forget this in the case of electrostatics because we usually don't care about the curl of E anyway. But the curl of D is not always zero, even in electrostatics, since there is no reason, in general, to suppose that the curl of P vanishes: \\curl \\vec{D} = \\epsilon_0 (\\curl \\vec{E}) + (\\curl \\vec{P}) = \\curl \\vec{P} \\tagl{4.25} Sometimes it does, but more often it does not. The bar electret of Prob 4.11 is one example of this: here there is no free charge anywhere, so if you really believe that the only source of D is \\rho_f you will be forced to conclude that \\vec{D} = 0 everywhere, and hence that \\vec{E} = (-1 / \\epsilon_0) \\vec{P} inside and \\vec{E} = 0 outside the electret, which is obviously wrong. And because \\curl \\vec{D} \\neq 0 in general, D cannot be expressed as the gradient of a scalar - there is no \"potential\" for D . Advice : When you are asked to compute the electric displacement, first look for symmetry. If the problem exhibits spherical, cylindrical, or plane symmetry, then you can get D directly from Eq. 4.23 by the usual Gauss's law methods. (Evidently in such cases \\curl \\vec{P} is automatically zero, but since symmetry alone dictates the answer, you're not really obliged to worry about the curl.) If the requisite symmetry is absent, you'll have to think of another approach, and, in particular, you must not assume that D is determined exclusively by the free charge.","title":"4.3.2: A Deceptive Parallel"},{"location":"ch4-3/#433-boundary-conditions","text":"The electrostatic boundary conditions we had in Sect 2.3 can be re-cast in terms of D . \\eqref{4.23} tells us the discontinuity in the component perpendicular to an interface: D_{above} ^{\\perp} - D_{below} ^{\\perp} = \\sigma_f \\tagl{4.26} while \\eqref{4.25} gives the discontinuity in parallel components: \\vec{D}_{above} ^{\\parallel} - \\vec{D}_{below} ^{\\parallel} = \\vec{P}_{above} ^{\\parallel} - \\vec{P}_{below} ^{\\parallel} \\tagl{4.27} In the presence of dielectrics, these are sometimes more useful than the corresponding boundary conditions on E (Eqs 2.31 and 2.32): E_{above} ^{\\perp} - E_{below} ^{\\perp} = \\frac{1}{\\epsilon_0} \\sigma \\tagl{4.28} and \\vec{E}_{above} ^{\\parallel} - \\vec{E}_{below} ^{\\parallel} = 0 \\tagl{4.29}","title":"4.3.3: Boundary Conditions"},{"location":"ch4-4/","text":"4.4: Linear Dielectrics 4.4.1: Susceptibility, Permittivity, Dielectric Constant In the first few sections of this chapter we did not commit ourselves as to the cause of P ; we dealt only with the effects of polarization. From the qualitative essence of 4.1, though, we know that the polarization of a dielectric ordinarily results from an electric field, which lines up the atomic or molecular dipoles. For many substances, in fact, the polarization is proportional to the field, provided E is not too strong: \\vec{P} = \\epsilon_0 \\chi_e \\vec{E} \\tagl{4.30} The constant of proportionality, \\chi_e , is called the electric susceptibility of the medium (a factor of \\epsilon_0 has been extracted to make \\chi_e dimensionless). The value of \\chi_e depends on the microscopic structure of the substance in question (and also on external conditions such as temperature). I shall call materials that obey \\eqref{4.30} linear dielectrics . In modern optical applications, especially, nonlinear materials have become increasingly important. For these there is a second term relating P to E - typically a cubic term. In general, Eq 4.30 can be regarded as the first (nonzero) term in the Taylor expansion of P in powers of E . Note that E in \\eqref{4.30} is the total field; it may be due in part to free charges and in part to the polarization itself. If, for instance, we put a piece of dielectric into an external field \\vec{E_0} , we cannot compute P directly from the linear susceptibility relation; the external field will polarize the material, and this polarization will produce its own field, which then contributes to the total field, and this in turn modifies the polarization, which... Breaking out of this infinite regress is not always easy. You'll see some examples in a moment. The simplest approach is to begin with the displacement , at least in those cases where D can be deduced directly from the free charge distribution. In linear media we have \\vec{D} = \\epsilon_0 \\vec{E} + \\vec{P} = \\epsilon_0 \\vec{E} + \\epsilon_0 \\chi_e \\vec{E} = \\epsilon_0 (1 + \\chi_e) \\vec{E} \\tagl{4.31} so D is also proportional to E \\vec{E} = \\epsilon \\vec{E} \\tagl{4.32} where \\epsilon \\equiv \\epsilon_0 (1 + \\chi_e) \\tagl{4.33} This new constant \\epsilon is called the permittivity of the material. (In vacuum, where there is no matter to polarize, the susceptibility is zero, and the permittivity is \\epsilon_0 . That's why \\epsilon_0 is called the permittivity of free space. I dislike the term, for it suggest that the vacuum is just a special kind of linear dielectric, in which the permittivity happens to have the value 8.85 \\times 10^{-12} C^2 / N \\cdot m^2 .) If you remove a factor of \\epsilon_0 , the remaining dimensionless quantity \\epsilon_r = 1 + \\chi _e = \\frac{\\epsilon}{\\epsilon_0} \\tagl{4.34} is called the relative permittivity , or dielectric constant , of the material. Dielectric constants for some common substances are listed in Table 4.2. (Notice that \\epsilon_r is greater than 1, for all ordinary materials.) Of course, the permittivity and the dielectric constant do not convey any information that was not already available in the susceptibility, nor is there anything essentially new in Eq 4.32: the physics of linear dielectrics is all contained in \\eqref{4.30} Example 4.5 A metal sphere of radius a carries a charge Q (Fig 4.20). It is surrounded, out to radius b , by linear dielectric material of permittivity \\epsilon . Find the potential at the center (relative to infinity). Solution To compute V, we need to know E; to find E, we might first try to locate the bound charge; we could get the bound charge from P , but we can't calculate P unless we already know E . What we do know is the free charge, and our arrangement is spherically symmetric, so we can go straight for D using Eq 4.23: \\vec{D} = \\frac{Q}{4 \\pi r^2} \\vu{r}, \\quad \\text{ for all points } r > a Inside the conducting sphere, all our electrostatic fields are zero. We then obtain E via Eq 4.32: \\vec{E} = \\begin{cases} \\frac{Q}{4 \\pi \\epsilon r^2} \\vu{r} & \\quad \\text{ for } a < r < b \\\\ \\frac{Q}{4 \\pi \\epsilon_0 r^2} \\vu{r} & \\quad \\text{ for } r > b \\end{cases} We get the potential at the center by integrating E V = - \\int _{\\infty} ^0 \\vec{E} \\cdot \\dd \\vec{l} = \\\\ - \\int _{\\infty} ^b \\left( \\frac{Q}{4 \\pi \\epsilon_0 r^2} \\right) \\dd r - \\int_b ^a \\left( \\frac{Q}{4 \\pi \\epsilon r^2} \\right) \\dd r\\\\ = \\frac{Q}{4 \\pi } \\left( \\frac{1}{\\epsilon_0 b} + \\frac{1}{\\epsilon a} - \\frac{1}{\\epsilon b} \\right) In this case, we didn't need to compute the polarization or the bound charge explicitly, but we can easily do so now that we have E : \\vec{P} = \\epsilon_0 \\chi_e \\vec{E} = \\frac{\\epsilon_0 \\chi_e Q}{4 \\pi \\epsilon r^2} \\vu{r} within the dielectric, so that \\rho_b = - \\div \\vec{P} = 0 and \\sigma_b = \\vec{P} \\cdot \\vu{n} = \\begin{cases} \\frac{\\epsilon_0 \\chi_e Q}{4 \\pi \\epsilon b^2} & \\qquad \\text{ at the outer surface } \\\\ \\frac{- \\epsilon_0 \\chi_e Q}{4 \\pi \\epsilon a^2} & \\qquad \\text{ at the inner surface } \\end{cases} Notice that the surface bound charge at a is negative ( \\vu{n} points outward with respect to the dielectric, which is + \\vu{n} at b, but -\\vu{r} at a). This is natural, since the charge on the metal sphere attracts its opposite in all the dielectric molecules. It is this layer of negative charge that reduces the field, within the dielectric, from 1 / 4 \\pi \\epsilon_0 (Q / r^2) \\vu{r} to 1 / 4 \\pi \\epsilon (Q / r^2) \\vu{r} . In this respect, a dielectric is rather like an imperfect conductor: on a conducting shell the induced surface charge would be such as to cancel out the field of Q entirely in the region a < r < b ; the dielectric does the best it can, but the cancellation is only partial. Since linear dielectrics give us cases where P and D are proportional to E , you might suppose that linear dielectrics escape the defect in the parallel between E and D . Does it not follow that their curls, like E 's, must vanish? Unfortunately, it does not, for the line integral of P around a closed path that straddles the boundary between one type of material and another need not be zero, even though the integral of E around the same loop must be. The reason is that the proportionality factor \\epsilon_0 \\chi_e is different on the two sides. For instance, at the interface between a polarized dielectric and the vacuum (Fig 4.21), P is zero on one side but not on the other. Around this loop, \\oint \\vec{P} \\cdot \\dd \\vec{l} \\neq 0 , and hence, by Stokes' theorem, the curl of P cannot vanish everywhere within the loop (in fact, it is infinite at the boundary). Of course, if space is entirely filled with a homogeneous linear dielectric, then this objection is void; in this rather special circumstance \\div \\vec{D} = \\rho_f \\quad \\text{and} \\quad \\curl \\vec{D} = 0 so D can be found from the free charge just as though the dielectric were not there: \\vec{D} = \\epsilon_0 \\vec{E_{vac}} where \\vec{E_{vac}} is the field the same charge distribution would produce in the absence of any dielectric. According to \\eqref{4.32} and \\eqref{4.34} , therefore, \\vec{E} = \\frac{1}{\\epsilon} \\vec{D} = \\frac{1}{\\epsilon_r} \\vec{E_{vac}} \\tagl{4.35} Conclusion: when all space is filled with a homogeneous linear dielectric, the field everywhere is simply reduced by a factor of one over the dielectric constant. (Actually it's not necessary for the dielectric to fill all space; in regions where the field is zero anyway, it can hardly matter whether the dielectric is present or not, since there's no polarization in any event.) For example, if a free charge q is embedded in a large dielectric, the field it produces is \\vec{E} = \\frac{1}{4 \\pi \\epsilon} \\frac{q}{r^2} \\vu{r} \\tagl{4.36} (that's \\epsilon , not \\epsilon_0 ), and the force it exerts on nearby charges is reduced accordingly. But it's not that there is anything wrong with Coulomb's law; rather, the polarization of the medium partially \"shields\" the charge, by surrounding it with bound charge of the opposite sign (Fig 4.22) Example 4.6 A parallel-plate capacitor (Fig 4.23) is filled with insulating material of dielectric constant \\epsilon_r . What effect does this have on its capacitance? Solution Since the field is confined to the space between the plates, the dielectric will reduce E , and hence also the potential difference V, by a factor 1 / \\epsilon_r . Accordingly, the capacitance C = Q / V is increased by a factor of the dielectric constant C = \\epsilon_r C_{vac} \\tagl{4.37} This is, in fact, a common way to beef up a capacitor A crystal is generally easier to polarize in some directions than others, and in this case Eq 4.30 is replaced by the general linear relation \\begin{align*} P_x & = \\epsilon_0 (\\chi_{e,xx} E_x + \\chi_{e, xy} E_y + \\chi_{e, xz} E_z) \\\\ P_y & = \\epsilon_0 (\\chi_{e,yx} E_x + \\chi_{e, yy} E_y + \\chi_{e, yz} E_z) \\\\ P_z & = \\epsilon_0 (\\chi_{e,zx} E_x + \\chi_{e, zy} E_y + \\chi_{e, zz} E_z) \\\\ \\end{align*} \\tagl{4.38} just as Eq. 4.1 was superseded by Eq. 4.3 for asymmetrical molecules. The nine coefficients constitute the susceptibility tensor 4.4.2: Boundary Value Problems with Linear Dielectrics In a (homogeneous isotropic) linear dielectric, the bound charge density is proportional to the free charge density \\rho_b = - \\div \\vec{P} = - \\div \\left( \\epsilon_0 \\frac{\\chi_e}{\\epsilon} \\vec{D} \\right) = - \\left( \\frac{\\chi_e}{1 + \\chi_e} \\right) \\rho_f \\tagl{4.39} In particular, unless free charge is actually embedded in the material, \\rho = 0 and any net charge must reside at the surface. Within such a dielectric, then, the potential obeys Laplace's equation, and all the machinery of Chapter 3 carries over. It is convenient, however, to rewrite the boundary conditions in a way that makes reference only to the free charge. Equation 4.26 says \\epsilon_{above} E_{above} ^{\\perp} - \\epsilon_{below} E_{below} ^{\\perp} = \\sigma_f \\tagl{4.40} or, in terms of the potential, \\epsilon_{above} \\pdv{V_{above}}{n} - \\epsilon_{below} \\pdv{V_{below}}{n} = - \\sigma_f \\tagl{4.41} whereas the potential itself is, of course, continuous (Eq 2.34): V_{above} = V_{below} \\tagl{4.42} Example 4.7 A sphere of homogeneous linear dielectric material is placed in an otherwise uniform electric field \\vec{E_0} (Fig 4.27). Find the electric field inside the sphere Solution This is very similar to Ex 3.8, in which an uncharged conducting sphere was introduced into a uniform field. In that case, the field of the induced charge canceled \\vec{E_0} within the sphere. In a dielectric, the cancellation from the bound charge is incomplete. Our problem is to solve Laplace's equation, for V_{in}(r, \\theta) when r \\leq R and V_{out}(r, \\theta) when r \\geq R , subject to the boundary conditions \\tag{i} V_{in} = V_{out} \\qquad \\text{ at } r = R \\tag{ii} \\epsilon \\pdv{V_{in}}{r} = \\epsilon_0 \\pdv{V_{out}}{r} \\qquad \\text{ at } r = R \\tag{iii} V_{out} \\rightarrow - E_0 r \\cos \\theta \\qquad \\text{ for } r \\gg R (The second of these follows from Eq 4.41, since there is no free charge at the surface.) Inside the sphere, Eq 3.65 says V_{in}(r, \\theta) = \\sum_{l=0} ^{\\infty} A_l r^l P_l(\\cos \\theta) \\tagl{4.44} outside the sphere, in view of (iii), we have V_{out}(r, \\theta) = - E_0 r \\cos \\theta + \\sum_{l=0} ^\\infty \\frac{B_l}{r^{l+1}} P_l(\\cos \\theta) \\tagl{4.45} Boundary condition (i) requires that \\sum_{l=0} ^{\\infty} A_l R^l P_l(\\cos \\theta) = - E_0 R \\cos \\theta + \\sum_{l=0} ^\\infty \\frac{B_l}{R^{l+1}} P_l(\\cos \\theta) so A_l R^l = \\frac{B_l}{R_{l+1}}, \\qquad \\text{ for } l \\neq 1 \\\\ A_1 R = - E_0 R + \\frac{B_1}{R^2} \\tagl{4.46} Meanwhile, condition (ii) yields \\epsilon_r \\sum_{l=0} ^\\infty l A_l R^{l-1} P_l (\\cos \\theta) = - E_0 \\cos \\theta - \\sum_{l=0} ^\\infty \\frac{(l+1) B_l}{R^{l+2}} P_l(\\cos theta) so \\begin{align*} \\epsilon_r l A_l R^{l-1} & = - \\frac{(l+1) B_l }{B^{l+2}} , \\text{ for } l \\neq 1 \\\\ \\epsilon_r A_1 & = - E_0 - \\frac{2 B_1}{R^3} \\end{align*} \\tagl{4.47} It follows that A_l = B_l = 0 \\qquad \\text{ for } l \\neq 1\\\\ A_1 = - \\frac{3}{\\epsilon_r + 2} E_0 \\quad B_1 = \\frac{\\epsilon_r - 1}{\\epsilon_r + 2} R^3 E_0 \\tagl 4.48 Evidently V_{in} (r, \\theta) = - \\frac{3 E_0}{\\epsilon_r + 2} r \\cos \\theta = - \\frac{3E_0}{\\epsilon_r + 2} z We should be used to finding that the field within a polarized sphere is uniform, but it's still a surprising result: \\vec{E} = \\frac{3}{\\epsilon_r + 2} \\vec{E_0} \\tagl{4.49} Example 4.8 Suppose the entire region below the plane z = 0 in Fig 4.28 is filled with uniform linear dielectric material of susceptibility \\chi_e . Calculate the force on a point charge q situated a distance d above the origin. Solution The surface bound charge on the xy plane is of opposite sign to q , so the force will be attractive. (In view of Eq 4.39, there is no volume bound charge.) Let us first calculate \\sigma_b , using \\eqref{4.11} and \\eqref{4.30} : \\sigma_b = \\vec{P} \\cdot \\vu{n} = P_z = \\epsilon_0 \\chi_e E_z where E_z is the z-component of the total field inside the dielectric, at z = 0 . This field is due in part to q and in part to the bound charge itself. From Coulomb's law, the former contribution is - \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{(r^2 + d^2)} \\cos \\theta = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd}{(r^2 + d^2)^{3/2}} where r = \\sqrt{x^2 + y^2} is the distance from the origin. We can immediately read off the z-component of the field of the bound charge using Gauss's law as - \\sigma_b / 2 \\epsilon_0 . Thus, \\sigma_b = \\epsilon_0 \\chi_e \\left[ - \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd}{(r^2 + d^2)^{3/2}} - \\frac{\\sigma_b}{2 \\epsilon_0} \\right] which we can solve for \\sigma_b \\sigma_b = - \\frac{1}{2 \\pi} \\left( \\frac{\\chi_e}{\\chi_e + 2} \\right) \\frac{qd}{(r^2 + d^2)^{3/2}} \\tagl{4.50} Apart from the factor \\chi_e / (\\chi_e + 2) this is exactly the same as the induced charge on an infinite conducting plane under similar circumstances (Eq 3.10). Evidently the total bound charge is q_b = - \\left( \\frac{\\chi_e}{\\chi_e + 2} \\right)q \\tagl{4.51} We could, of course, get the field of \\sigma_b by direct integration \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\int \\left( \\frac{\\vu{\\gr}}{\\gr ^2} \\sigma_b \\dd a \\right) But, as in the case of the conducting plane, there is a nicer solution by the method of images. If we replace the dielectric by a single point charge q_b at the image position (0, 0, -d), we have V = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{1}{\\sqrt{x^2 + y^2 +(z-d)^2}} + \\frac{q_b}{\\sqrt{x^2 + y^2 + (z + d)^2}} \\right] \\tagl{4.52} in the region z > 0 . Meanwhile, a charge (q + q_b) at (0, 0, d) yields the potential V = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{q + q_b}{\\sqrt{x^2 + y^2 + (z - d)^2}} \\right] \\tagl{4.53} for the region z < 0 . Taken together, \\eqref{4.52} and \\eqref{4.53} constitute a function that satisfies Poisson's equation with a point charge q at (0, 0, d), which goes to zero at infinity, which is continuous at the boundary z = 0 , and whose normal derivative exhibits the discontinuity appropriate to a surface charge \\sigma_b at z = 0 : - \\epsilon_0 \\left( \\left. \\pdv{V}{z} \\right| _{z = 0+} - \\left. \\pdv{V}{z} \\right|_{z = 0-} \\right) = - \\frac{1}{2 \\pi} \\left( \\frac{\\chi_e}{\\chi_e + 2} \\right) Accordingly, this is the correct potential for our problem. In particular, the force on q is: \\vec{F} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q q_b}{(2d)^2} \\vu{z} = - \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{\\chi_e}{\\chi_e + 2} \\right) \\frac{q^2}{4 d^2} \\vu{z} \\tagl{4.54} I do not claim to have provided a compelling motivation for \\eqref{4.52} and \\eqref{4.53} - like all image solutions, this one owes its justification to the fact that it works : it solves Poisson's equation, and it meets the boundary conditions. Still, discovering an image solution is not entirely a matter of guesswork. There are at least two \"rules of the game\": (1) You must never put an image charge into the region where you're computing the potential. (2) The image charges must add up to the correct total in each region. 4.4.3: Energy in Dielectric Systems It takes work to charge up a capacitor (Eq 2.55): W = \\frac{1}{2} C V^2 If the capacitor is filled with linear dielectric, its capacitance exceeds the vacuum value by a factor of the dielectric constant C = \\epsilon_r C_{vac} as we found in Ex. 4.6. Evidently, the work necessary to charge a dielectric-filled capacitor is increased by the same factor. The reason is pretty clear: you have to pump more (free) charge, to achieve a given potential, because part of the field is canceled off by the bound charges. In chapter 2 we got a general formula for the energy stored in any electrodynamic system W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd \\tau \\tagl{4.55} The case of the dielectric-filled capacitor suggests that this should be changed to W = \\frac{\\epsilon_0}{2} \\int \\epsilon_r E^2 \\dd \\tau = \\frac{1}{2} \\int \\vec{D} \\cdot \\vec{E} \\dd \\tau in the presence of linear dielectrics. To prove it, suppose the dielectric material is fixed in position, and we bring in the free charge, a bit at a time. As \\rho_f is increased by an amount \\Delta \\rho_f , the polarization will change and with it the bound charge distribution; but we're interested only in the work done on the incremental free charge: \\Delta W = \\int (\\Delta \\rho_f) V \\dd \\tau \\tagl{4.56} Since \\div \\vec{D} = \\rho_f, \\Delta \\rho_f = \\div (\\Delta \\vec{D}) , where \\Delta \\vec{D} is the resulting change in D , so \\Delta W = \\int [ \\div ( \\Delta D) ] V \\dd \\tau Now \\div [ (\\Delta D) V ] = [ \\div (\\Delta \\vec{D})] V + \\Delta \\vec{D} \\cdot (\\grad V) and hence, integrating by parts, \\Delta W = \\in \\div [ (\\Delta \\vec{D}) V] \\dd \\tau + \\int ( \\Delta \\vec{D}) \\cdot \\vec{E} \\dd \\tau The divergence theorem turns the first term into a surface integral, which vanishes if we integrate over all space. Therefore, the work done is equal to \\Delta W = \\int (\\Delta \\vec{D}) \\cdot \\vec{E} \\dd \\tau \\tagl{4.57} So far, this applies to any material. In the specific case of a linear dielectric, \\frac{1}{2} \\Delta ( \\vec{D} \\cdot \\vec{E} ) = \\frac{1}{2} \\Delta (\\epsilon E^2) = \\epsilon (\\Delta \\vec{E}) \\cdot \\vec{E} = ( \\Delta \\vec{D}) \\cdot \\vec{E} (for infinitesimal increments). Thus \\Delta W = \\Delta \\left( \\frac{1}{2} \\int \\vec{D} \\cdot \\vec{E} \\dd \\tau \\right) The total work done, then, as we build the free charge up from zero to the final configuration is W = \\frac{1}{2} \\int \\vec{D} \\cdot \\vec{E} \\dd \\tau \\tagl{4.58} as anticipated. It may puzzle you that Eq. 4.55, which we derived quite generally in Chapter 2, does not seem to apply in the presence of dielectrics, where it is replaced by Eq. 4.58. The point is not that one or the other of these equations is wrong, but rather that they address somewhat different questions. The distinction is subtle, so let's go right back to the beginning: What do we mean by \"the energy of a system\"? Answer: It is the work required to assemble the system. Very well - but when dielectrics are involved, there are two quite different ways one might construe this process: We bring in all the charges (free and bound), one by one, with tweezers, and glue each one down in its proper final location. If this is what you mean by \"assemble the system,\" then Eq. 4.55 is your formula for the energy stored. Notice, however, that this will not include the work involved in stretching and twisting the dielectric molecules (if we picture the positive and negative charges as held together by tiny springs, it does not include the spring energy, \\frac{1}{2} k x^2 , associated with polarizing each molecule). With the unpolarized dielectric in place, we bring in the free charges, one by one, allowing the dielectric to respond as it sees fit. If this is what you mean by \"assemble the system\" (and ordinarily it is, since free charge is what we actually push around), then Eq. 4.58 is the formula you want. In this case the \"spring\" energy is included, albeit indirectly, because the force you must apply to the free charge depends on the disposition of the bound charge; as you move the free charge, you are automatically stretching those \"springs.\" Example 4.9 A sphere of radius R is filled with material of dielectric constant \\epsilon_r and uniform embedded free charge \\rho_f . What is the energy of this configuration? Solution From Gauss's law, the displacement is \\vec{D}(r) = \\begin{cases} \\frac{\\rho_f}{3} \\vec{r} & \\qquad ( r < R) \\\\ \\frac{\\rho_f}{3} \\frac{R^3}{r^2} \\vu{r} & \\qquad ( r > R ) \\end{cases} So the electric field is \\vec{E}(r) = \\begin{cases} \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\vec{r} & \\qquad ( r < R) \\\\ \\frac{\\rho_f}{3 \\epsilon_0} \\frac{R^3}{r^2} \\vu{r} & \\qquad ( r > R ) \\end{cases} The purely electrostatic energy is \\begin{align*} W & = \\frac{\\epsilon_0}{2} \\left[ \\left( \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r } \\right) ^2 \\int _0 ^R r^2 4 \\pi r^2 \\dd r + \\left( \\frac{\\rho_f}{3 \\epsilon_0} \\right)^2 R^6 \\int_R ^\\infty \\frac{1}{r^4} 4 \\pi r^2 \\dd r \\right] \\\\ & = \\frac{2 \\pi}{9 \\epsilon_0} \\rho_f ^2 R^5 \\left( \\frac{1}{5 \\epsilon_r ^2} + 1 \\right) \\end{align*} But the total energy (Eq 4.58) is \\begin{align*} W_2 & = \\frac{1}{2} \\left[ \\left( \\frac{\\rho_f}{3} \\right)\\left( \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r } \\right) \\int_0 ^R r^2 4 \\pi r ^2 \\dd r + \\left( \\frac{\\rho_f R^3}{3} \\right) \\left( \\frac{\\rho_f R^3}{3 \\epsilon_0} \\right) \\int _R ^\\infty \\frac{1}{r^4} 4 \\pi r^2 \\dd r \\right] \\\\ & = \\frac{2 \\pi}{9 \\epsilon_0}\\rho_f ^2 R^5 \\left( \\frac{1}{5 \\epsilon_r} + 1 \\right) \\end{align*} Notice that W_1 < W_2 - that's because W_1 does not include the energy involved in stretching the molecules. Let's check that W_2 is the work done on the free charge in assembling the system. We start with the (uncharged, unpolarized) dielectric sphere, and bring in the free charge in infinitesimal installments (dq), filling out the sphere layer by layer. When we have reached radius r' , the electric field is \\vec{E}(r) = \\begin{cases} \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\vec{r} & \\quad (r < r') \\\\ \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\frac{r' ^3}{r^2} \\vu{r} & \\quad (r' < r < R ) \\\\ \\frac{\\rho_f}{3 \\epsilon_0} \\frac{r' ^3}{r^2} \\vu{r} & \\quad ( r > R) \\end{cases} The work required to bring the next dq in from infinity to r' is \\begin{align*} \\dd W & = - \\dd q \\left[ \\int_{\\infty} ^R \\vec{E} \\cdot \\dd \\vec{l} + \\int _R ^{r'} \\vec{E} \\cdot \\dd \\vec{l} \\right] \\\\ & = - \\dd q \\left[ \\frac{\\rho_f r' ^3}{3 \\epsilon_0} \\int_{\\infty} ^R \\frac{1}{r^2} \\dd r + \\frac{\\rho_f r' ^3}{3 \\epsilon_0 \\epsilon_r} \\int _{R} ^{r'} \\frac{1}{r^2} \\dd r \\right] \\\\ & = \\frac{\\rho_f r' ^3}{3 \\epsilon_0} \\left[ \\frac{1}{R} + \\frac{1}{\\epsilon_r} \\left( \\frac{1}{r'} - \\frac{1}{R} \\right) \\right] \\dd q \\end{align*} This increases the radius (r') \\dd q = \\rho_f 4 \\pi r' ^2 \\dd r' so the total work done, in going from r'=0 to r' = R is \\begin{align*} W & = \\frac{4 \\pi \\rho_f ^2}{3 \\epsilon_0} \\left[ \\frac{1}{R} \\left( 1 - \\frac{1}{ \\epsilon_r} \\right) \\int_0 ^R r' ^5 + \\frac{1}{\\epsilon_r} \\int_0 ^R r' ^4 \\dd r' \\right] \\\\ & = \\frac{2 \\pi}{9 \\epsilon_0}\\rho_f ^2 R^5 \\left( \\frac{1}{5 \\epsilon_r} + 1 \\right) = W_2 \\end{align*} Evidently the energy \"stored in the springs\" is W_{sprint} = W_2 - W_1 = \\frac{2 \\pi}{45 \\epsilon_0 \\epsilon_r ^2} \\rho _f ^2 R^5 (\\epsilon_r - 1) I would like to confirm this in an explicit model. Picture the dielectric as a collection of tiny proto-dipoles, each consisting of +q and -q attached to a spring of constant k and equilibrium length 0, so in the absence of any field the positive and negative ends coincide. One end of each dipole is nailed in position (like the nuclei in a solid), but the other end is free to move in response to any imposed field. Let \\dd \\tau be the volume assigned to each proto-dipole (the dipole itself may occupy only a small portion of this space). With the field turned on, the electric force on the free end is balanced by the spring force; the charges separate by a distance d: qE = kd . In our case \\vec{E} = \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\vec{r} The resulting dipole moment is p = qd and the polarization is P = p / \\dd \\tau so k = \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r d^2} P r \\dd \\tau The energy of this particular spring is \\dd W_{spring} = \\frac{1}{2} k d^2 = \\frac{\\rho_f}{6 \\epsilon_0 \\epsilon_r} P r \\dd \\tau and hence the total is W_{spring} = \\frac{\\rho_f}{6 \\epsilon_0 \\epsilon_r} \\int P r \\dd \\tau Now \\vec{P} = \\epsilon_0 \\chi_e \\vec{E} = \\epsilon_0 \\chi_e \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r } \\vec{r} = \\frac{(\\epsilon_r - 1) \\rho_f}{3 \\epsilon_r} \\vec{r} so W_{spring} = \\frac{\\rho_f}{6 \\epsilon_0 \\epsilon_r} \\frac{(\\epsilon_r - 1) \\rho_f}{3 \\epsilon_r} 4 \\pi \\int_0 ^R r^4 \\dd r = \\frac{2 \\pi}{45 \\epsilon_0 \\epsilon_r ^2} \\rho_f ^2 R^5 (\\epsilon_r - 1) and it works out perfectly. It is sometimes alleged that Eq. 4.58 represents the energy even for nonlinear dielectrics, but this is false: To proceed beyond Eq. 4.57, one must assume linearity. In fact, for dissipative systems the whole notion of \"stored energy\" loses its meaning, because the work done depends not only on the final configuration but on how it got there. If the molecular \"springs\" are allowed to have some friction, for instance, then W_{spring} can be made as large as you like, by assembling the charges in such a way that the spring is obliged to expand and contract many times before reaching its final state. In particular, you get nonsensical results if you try to apply Eq. 4.58 to electrets, with frozen-in polarization (see Prob. 4.27). 4.4.4: Forces on Dielectrics Just as a conductor is attracted into an electric field (Eq. 2.51), so too is a dielectric - and for essentially the same reason: the bound charge tends to accumulate near the free charge of the opposite sign. But the calculation of forces on dielectrics can be surprisingly tricky. Consider, for example, the case of a slab of linear dielectric material, partially inserted between the plates of a parallel-plate capacitor (Fig. 4.30). We have always pretended that the field is uniform inside a parallel-plate capacitor, and zero outside. If this were literally true, there would be no net force on the dielectric at all, since the field everywhere would be perpendicular to the plates. However, there is in reality a fringing field around the edges, which for most purposes can be ignored but in this case is responsible for the whole effect. (Indeed, the field could not terminate abruptly at the edge of the capacitor, for if it did, the line integral of E around the closed loop shown in Fig. 4.31 would not be zero.) It is this nonuniform fringing field that pulls the dielectric into the capacitor. Fringing fields are notoriously difficult to calculate; luckily, we can avoid this altogether, by the following ingenious method. Let W be the energy of the system - it depends, of course, on the amount of overlap. If I pull the dielectric out an infinitesimal distance dx, the energy is changed by an amount equal to the work done: \\dd W = F_{me} \\dd x \\tagl{4.59} where F_{me} is the force I mus exert, to counteract the electrical force F on the dielectric. Thus, the electrical force on the slab is F = - \\dv{W}{x} \\tagl{4.60} Now, the energy stored in the capacitor is W = \\frac{1}{2} C V^2 \\tagl{4.61} and the capacitance in this case is C = \\frac{\\epsilon_0 w}{d} (\\epsilon_r l - \\chi_e x) \\tagl{4.62} where l is the length of the plates (Fig 4.30). Let's assume that the total charge on the plates is held constant (Q = CV) , as the dielectric moves. In terms of Q , W = \\frac{1}{2} \\frac{Q^2}{C} \\tagl{4.63} so F = - \\dv{W}{x} = \\frac{1}{2} \\frac{Q^2}{C^2} \\dv{C}{x} = \\frac{1}{2} V^2 \\dv{C}{x} \\tagl{4.64} But \\dv{C}{x} = - \\frac{\\epsilon_0 \\chi_e w}{d} and hence F = - \\frac{\\epsilon_0 \\chi_e w}{2d} V^2 \\tagl{4.65} (The minus sign indicates that the force is in the negative x direction; the dielectric is pulled into the capacitor.) It is a common error to use Eq. 4.61 (with V constant), rather than Eq. 4.63 (with Q constant), in computing the force. One then obtains F = - \\frac{1}{2} V^2 \\dv{C}{x} which is off by a sign. It is, of course, possible to maintain the capacitor at a fixed potential, by connecting it up to a battery. But in that case the battery also does work as the dielectric moves; instead of Eq. 4.59, we now have \\dd W = F_{me} \\dd x + V \\dd Q \\tagl{4.66} where V \\dd Q is the work done by the battery. It follows that F = - \\dv{W}{x} + V \\dv{Q}{x} = - \\frac{1}{2} V^2 \\dv{C}{x} + V^2 \\dv{C}{x} = \\frac{1}{2} V^2 \\dv{C}{x} \\tagl{4.67} the same as before, with the correct sign. Please understand: The force on the dielectric cannot possibly depend on whether you plan to hold Q constant or V constant - it is determined entirely by the distribution of charge, free and bound. It's simpler to calculate the force assuming constant Q , because then you don't have to worry about work done by the battery; but if you insist, it can be done correctly either way. Notice that we were able to determine the force without knowing anything about the fringing fields that are ultimately responsible for it! Of course, it's built into the whole structure of electrostatics that \\curl \\vec{E} = 0 , and hence that the fringing fields must be present; we're not really getting something for nothing here - just cleverly exploiting the internal consistency of the theory. The energy stored in the fringing fields themselves (which was not accounted for in this derivation) stays constant, as the slab moves; what does change is the energy well inside the capacitor, where the field is nice and uniform.","title":"4.4 - Linear Dielectrics"},{"location":"ch4-4/#44-linear-dielectrics","text":"","title":"4.4: Linear Dielectrics"},{"location":"ch4-4/#441-susceptibility-permittivity-dielectric-constant","text":"In the first few sections of this chapter we did not commit ourselves as to the cause of P ; we dealt only with the effects of polarization. From the qualitative essence of 4.1, though, we know that the polarization of a dielectric ordinarily results from an electric field, which lines up the atomic or molecular dipoles. For many substances, in fact, the polarization is proportional to the field, provided E is not too strong: \\vec{P} = \\epsilon_0 \\chi_e \\vec{E} \\tagl{4.30} The constant of proportionality, \\chi_e , is called the electric susceptibility of the medium (a factor of \\epsilon_0 has been extracted to make \\chi_e dimensionless). The value of \\chi_e depends on the microscopic structure of the substance in question (and also on external conditions such as temperature). I shall call materials that obey \\eqref{4.30} linear dielectrics . In modern optical applications, especially, nonlinear materials have become increasingly important. For these there is a second term relating P to E - typically a cubic term. In general, Eq 4.30 can be regarded as the first (nonzero) term in the Taylor expansion of P in powers of E . Note that E in \\eqref{4.30} is the total field; it may be due in part to free charges and in part to the polarization itself. If, for instance, we put a piece of dielectric into an external field \\vec{E_0} , we cannot compute P directly from the linear susceptibility relation; the external field will polarize the material, and this polarization will produce its own field, which then contributes to the total field, and this in turn modifies the polarization, which... Breaking out of this infinite regress is not always easy. You'll see some examples in a moment. The simplest approach is to begin with the displacement , at least in those cases where D can be deduced directly from the free charge distribution. In linear media we have \\vec{D} = \\epsilon_0 \\vec{E} + \\vec{P} = \\epsilon_0 \\vec{E} + \\epsilon_0 \\chi_e \\vec{E} = \\epsilon_0 (1 + \\chi_e) \\vec{E} \\tagl{4.31} so D is also proportional to E \\vec{E} = \\epsilon \\vec{E} \\tagl{4.32} where \\epsilon \\equiv \\epsilon_0 (1 + \\chi_e) \\tagl{4.33} This new constant \\epsilon is called the permittivity of the material. (In vacuum, where there is no matter to polarize, the susceptibility is zero, and the permittivity is \\epsilon_0 . That's why \\epsilon_0 is called the permittivity of free space. I dislike the term, for it suggest that the vacuum is just a special kind of linear dielectric, in which the permittivity happens to have the value 8.85 \\times 10^{-12} C^2 / N \\cdot m^2 .) If you remove a factor of \\epsilon_0 , the remaining dimensionless quantity \\epsilon_r = 1 + \\chi _e = \\frac{\\epsilon}{\\epsilon_0} \\tagl{4.34} is called the relative permittivity , or dielectric constant , of the material. Dielectric constants for some common substances are listed in Table 4.2. (Notice that \\epsilon_r is greater than 1, for all ordinary materials.) Of course, the permittivity and the dielectric constant do not convey any information that was not already available in the susceptibility, nor is there anything essentially new in Eq 4.32: the physics of linear dielectrics is all contained in \\eqref{4.30}","title":"4.4.1: Susceptibility, Permittivity, Dielectric Constant"},{"location":"ch4-4/#example-45","text":"A metal sphere of radius a carries a charge Q (Fig 4.20). It is surrounded, out to radius b , by linear dielectric material of permittivity \\epsilon . Find the potential at the center (relative to infinity). Solution To compute V, we need to know E; to find E, we might first try to locate the bound charge; we could get the bound charge from P , but we can't calculate P unless we already know E . What we do know is the free charge, and our arrangement is spherically symmetric, so we can go straight for D using Eq 4.23: \\vec{D} = \\frac{Q}{4 \\pi r^2} \\vu{r}, \\quad \\text{ for all points } r > a Inside the conducting sphere, all our electrostatic fields are zero. We then obtain E via Eq 4.32: \\vec{E} = \\begin{cases} \\frac{Q}{4 \\pi \\epsilon r^2} \\vu{r} & \\quad \\text{ for } a < r < b \\\\ \\frac{Q}{4 \\pi \\epsilon_0 r^2} \\vu{r} & \\quad \\text{ for } r > b \\end{cases} We get the potential at the center by integrating E V = - \\int _{\\infty} ^0 \\vec{E} \\cdot \\dd \\vec{l} = \\\\ - \\int _{\\infty} ^b \\left( \\frac{Q}{4 \\pi \\epsilon_0 r^2} \\right) \\dd r - \\int_b ^a \\left( \\frac{Q}{4 \\pi \\epsilon r^2} \\right) \\dd r\\\\ = \\frac{Q}{4 \\pi } \\left( \\frac{1}{\\epsilon_0 b} + \\frac{1}{\\epsilon a} - \\frac{1}{\\epsilon b} \\right) In this case, we didn't need to compute the polarization or the bound charge explicitly, but we can easily do so now that we have E : \\vec{P} = \\epsilon_0 \\chi_e \\vec{E} = \\frac{\\epsilon_0 \\chi_e Q}{4 \\pi \\epsilon r^2} \\vu{r} within the dielectric, so that \\rho_b = - \\div \\vec{P} = 0 and \\sigma_b = \\vec{P} \\cdot \\vu{n} = \\begin{cases} \\frac{\\epsilon_0 \\chi_e Q}{4 \\pi \\epsilon b^2} & \\qquad \\text{ at the outer surface } \\\\ \\frac{- \\epsilon_0 \\chi_e Q}{4 \\pi \\epsilon a^2} & \\qquad \\text{ at the inner surface } \\end{cases} Notice that the surface bound charge at a is negative ( \\vu{n} points outward with respect to the dielectric, which is + \\vu{n} at b, but -\\vu{r} at a). This is natural, since the charge on the metal sphere attracts its opposite in all the dielectric molecules. It is this layer of negative charge that reduces the field, within the dielectric, from 1 / 4 \\pi \\epsilon_0 (Q / r^2) \\vu{r} to 1 / 4 \\pi \\epsilon (Q / r^2) \\vu{r} . In this respect, a dielectric is rather like an imperfect conductor: on a conducting shell the induced surface charge would be such as to cancel out the field of Q entirely in the region a < r < b ; the dielectric does the best it can, but the cancellation is only partial. Since linear dielectrics give us cases where P and D are proportional to E , you might suppose that linear dielectrics escape the defect in the parallel between E and D . Does it not follow that their curls, like E 's, must vanish? Unfortunately, it does not, for the line integral of P around a closed path that straddles the boundary between one type of material and another need not be zero, even though the integral of E around the same loop must be. The reason is that the proportionality factor \\epsilon_0 \\chi_e is different on the two sides. For instance, at the interface between a polarized dielectric and the vacuum (Fig 4.21), P is zero on one side but not on the other. Around this loop, \\oint \\vec{P} \\cdot \\dd \\vec{l} \\neq 0 , and hence, by Stokes' theorem, the curl of P cannot vanish everywhere within the loop (in fact, it is infinite at the boundary). Of course, if space is entirely filled with a homogeneous linear dielectric, then this objection is void; in this rather special circumstance \\div \\vec{D} = \\rho_f \\quad \\text{and} \\quad \\curl \\vec{D} = 0 so D can be found from the free charge just as though the dielectric were not there: \\vec{D} = \\epsilon_0 \\vec{E_{vac}} where \\vec{E_{vac}} is the field the same charge distribution would produce in the absence of any dielectric. According to \\eqref{4.32} and \\eqref{4.34} , therefore, \\vec{E} = \\frac{1}{\\epsilon} \\vec{D} = \\frac{1}{\\epsilon_r} \\vec{E_{vac}} \\tagl{4.35} Conclusion: when all space is filled with a homogeneous linear dielectric, the field everywhere is simply reduced by a factor of one over the dielectric constant. (Actually it's not necessary for the dielectric to fill all space; in regions where the field is zero anyway, it can hardly matter whether the dielectric is present or not, since there's no polarization in any event.) For example, if a free charge q is embedded in a large dielectric, the field it produces is \\vec{E} = \\frac{1}{4 \\pi \\epsilon} \\frac{q}{r^2} \\vu{r} \\tagl{4.36} (that's \\epsilon , not \\epsilon_0 ), and the force it exerts on nearby charges is reduced accordingly. But it's not that there is anything wrong with Coulomb's law; rather, the polarization of the medium partially \"shields\" the charge, by surrounding it with bound charge of the opposite sign (Fig 4.22)","title":"Example 4.5"},{"location":"ch4-4/#example-46","text":"A parallel-plate capacitor (Fig 4.23) is filled with insulating material of dielectric constant \\epsilon_r . What effect does this have on its capacitance? Solution Since the field is confined to the space between the plates, the dielectric will reduce E , and hence also the potential difference V, by a factor 1 / \\epsilon_r . Accordingly, the capacitance C = Q / V is increased by a factor of the dielectric constant C = \\epsilon_r C_{vac} \\tagl{4.37} This is, in fact, a common way to beef up a capacitor A crystal is generally easier to polarize in some directions than others, and in this case Eq 4.30 is replaced by the general linear relation \\begin{align*} P_x & = \\epsilon_0 (\\chi_{e,xx} E_x + \\chi_{e, xy} E_y + \\chi_{e, xz} E_z) \\\\ P_y & = \\epsilon_0 (\\chi_{e,yx} E_x + \\chi_{e, yy} E_y + \\chi_{e, yz} E_z) \\\\ P_z & = \\epsilon_0 (\\chi_{e,zx} E_x + \\chi_{e, zy} E_y + \\chi_{e, zz} E_z) \\\\ \\end{align*} \\tagl{4.38} just as Eq. 4.1 was superseded by Eq. 4.3 for asymmetrical molecules. The nine coefficients constitute the susceptibility tensor","title":"Example 4.6"},{"location":"ch4-4/#442-boundary-value-problems-with-linear-dielectrics","text":"In a (homogeneous isotropic) linear dielectric, the bound charge density is proportional to the free charge density \\rho_b = - \\div \\vec{P} = - \\div \\left( \\epsilon_0 \\frac{\\chi_e}{\\epsilon} \\vec{D} \\right) = - \\left( \\frac{\\chi_e}{1 + \\chi_e} \\right) \\rho_f \\tagl{4.39} In particular, unless free charge is actually embedded in the material, \\rho = 0 and any net charge must reside at the surface. Within such a dielectric, then, the potential obeys Laplace's equation, and all the machinery of Chapter 3 carries over. It is convenient, however, to rewrite the boundary conditions in a way that makes reference only to the free charge. Equation 4.26 says \\epsilon_{above} E_{above} ^{\\perp} - \\epsilon_{below} E_{below} ^{\\perp} = \\sigma_f \\tagl{4.40} or, in terms of the potential, \\epsilon_{above} \\pdv{V_{above}}{n} - \\epsilon_{below} \\pdv{V_{below}}{n} = - \\sigma_f \\tagl{4.41} whereas the potential itself is, of course, continuous (Eq 2.34): V_{above} = V_{below} \\tagl{4.42}","title":"4.4.2: Boundary Value Problems with Linear Dielectrics"},{"location":"ch4-4/#example-47","text":"A sphere of homogeneous linear dielectric material is placed in an otherwise uniform electric field \\vec{E_0} (Fig 4.27). Find the electric field inside the sphere Solution This is very similar to Ex 3.8, in which an uncharged conducting sphere was introduced into a uniform field. In that case, the field of the induced charge canceled \\vec{E_0} within the sphere. In a dielectric, the cancellation from the bound charge is incomplete. Our problem is to solve Laplace's equation, for V_{in}(r, \\theta) when r \\leq R and V_{out}(r, \\theta) when r \\geq R , subject to the boundary conditions \\tag{i} V_{in} = V_{out} \\qquad \\text{ at } r = R \\tag{ii} \\epsilon \\pdv{V_{in}}{r} = \\epsilon_0 \\pdv{V_{out}}{r} \\qquad \\text{ at } r = R \\tag{iii} V_{out} \\rightarrow - E_0 r \\cos \\theta \\qquad \\text{ for } r \\gg R (The second of these follows from Eq 4.41, since there is no free charge at the surface.) Inside the sphere, Eq 3.65 says V_{in}(r, \\theta) = \\sum_{l=0} ^{\\infty} A_l r^l P_l(\\cos \\theta) \\tagl{4.44} outside the sphere, in view of (iii), we have V_{out}(r, \\theta) = - E_0 r \\cos \\theta + \\sum_{l=0} ^\\infty \\frac{B_l}{r^{l+1}} P_l(\\cos \\theta) \\tagl{4.45} Boundary condition (i) requires that \\sum_{l=0} ^{\\infty} A_l R^l P_l(\\cos \\theta) = - E_0 R \\cos \\theta + \\sum_{l=0} ^\\infty \\frac{B_l}{R^{l+1}} P_l(\\cos \\theta) so A_l R^l = \\frac{B_l}{R_{l+1}}, \\qquad \\text{ for } l \\neq 1 \\\\ A_1 R = - E_0 R + \\frac{B_1}{R^2} \\tagl{4.46} Meanwhile, condition (ii) yields \\epsilon_r \\sum_{l=0} ^\\infty l A_l R^{l-1} P_l (\\cos \\theta) = - E_0 \\cos \\theta - \\sum_{l=0} ^\\infty \\frac{(l+1) B_l}{R^{l+2}} P_l(\\cos theta) so \\begin{align*} \\epsilon_r l A_l R^{l-1} & = - \\frac{(l+1) B_l }{B^{l+2}} , \\text{ for } l \\neq 1 \\\\ \\epsilon_r A_1 & = - E_0 - \\frac{2 B_1}{R^3} \\end{align*} \\tagl{4.47} It follows that A_l = B_l = 0 \\qquad \\text{ for } l \\neq 1\\\\ A_1 = - \\frac{3}{\\epsilon_r + 2} E_0 \\quad B_1 = \\frac{\\epsilon_r - 1}{\\epsilon_r + 2} R^3 E_0 \\tagl 4.48 Evidently V_{in} (r, \\theta) = - \\frac{3 E_0}{\\epsilon_r + 2} r \\cos \\theta = - \\frac{3E_0}{\\epsilon_r + 2} z We should be used to finding that the field within a polarized sphere is uniform, but it's still a surprising result: \\vec{E} = \\frac{3}{\\epsilon_r + 2} \\vec{E_0} \\tagl{4.49}","title":"Example 4.7"},{"location":"ch4-4/#example-48","text":"Suppose the entire region below the plane z = 0 in Fig 4.28 is filled with uniform linear dielectric material of susceptibility \\chi_e . Calculate the force on a point charge q situated a distance d above the origin. Solution The surface bound charge on the xy plane is of opposite sign to q , so the force will be attractive. (In view of Eq 4.39, there is no volume bound charge.) Let us first calculate \\sigma_b , using \\eqref{4.11} and \\eqref{4.30} : \\sigma_b = \\vec{P} \\cdot \\vu{n} = P_z = \\epsilon_0 \\chi_e E_z where E_z is the z-component of the total field inside the dielectric, at z = 0 . This field is due in part to q and in part to the bound charge itself. From Coulomb's law, the former contribution is - \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{(r^2 + d^2)} \\cos \\theta = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd}{(r^2 + d^2)^{3/2}} where r = \\sqrt{x^2 + y^2} is the distance from the origin. We can immediately read off the z-component of the field of the bound charge using Gauss's law as - \\sigma_b / 2 \\epsilon_0 . Thus, \\sigma_b = \\epsilon_0 \\chi_e \\left[ - \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd}{(r^2 + d^2)^{3/2}} - \\frac{\\sigma_b}{2 \\epsilon_0} \\right] which we can solve for \\sigma_b \\sigma_b = - \\frac{1}{2 \\pi} \\left( \\frac{\\chi_e}{\\chi_e + 2} \\right) \\frac{qd}{(r^2 + d^2)^{3/2}} \\tagl{4.50} Apart from the factor \\chi_e / (\\chi_e + 2) this is exactly the same as the induced charge on an infinite conducting plane under similar circumstances (Eq 3.10). Evidently the total bound charge is q_b = - \\left( \\frac{\\chi_e}{\\chi_e + 2} \\right)q \\tagl{4.51} We could, of course, get the field of \\sigma_b by direct integration \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\int \\left( \\frac{\\vu{\\gr}}{\\gr ^2} \\sigma_b \\dd a \\right) But, as in the case of the conducting plane, there is a nicer solution by the method of images. If we replace the dielectric by a single point charge q_b at the image position (0, 0, -d), we have V = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{1}{\\sqrt{x^2 + y^2 +(z-d)^2}} + \\frac{q_b}{\\sqrt{x^2 + y^2 + (z + d)^2}} \\right] \\tagl{4.52} in the region z > 0 . Meanwhile, a charge (q + q_b) at (0, 0, d) yields the potential V = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{q + q_b}{\\sqrt{x^2 + y^2 + (z - d)^2}} \\right] \\tagl{4.53} for the region z < 0 . Taken together, \\eqref{4.52} and \\eqref{4.53} constitute a function that satisfies Poisson's equation with a point charge q at (0, 0, d), which goes to zero at infinity, which is continuous at the boundary z = 0 , and whose normal derivative exhibits the discontinuity appropriate to a surface charge \\sigma_b at z = 0 : - \\epsilon_0 \\left( \\left. \\pdv{V}{z} \\right| _{z = 0+} - \\left. \\pdv{V}{z} \\right|_{z = 0-} \\right) = - \\frac{1}{2 \\pi} \\left( \\frac{\\chi_e}{\\chi_e + 2} \\right) Accordingly, this is the correct potential for our problem. In particular, the force on q is: \\vec{F} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q q_b}{(2d)^2} \\vu{z} = - \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{\\chi_e}{\\chi_e + 2} \\right) \\frac{q^2}{4 d^2} \\vu{z} \\tagl{4.54} I do not claim to have provided a compelling motivation for \\eqref{4.52} and \\eqref{4.53} - like all image solutions, this one owes its justification to the fact that it works : it solves Poisson's equation, and it meets the boundary conditions. Still, discovering an image solution is not entirely a matter of guesswork. There are at least two \"rules of the game\": (1) You must never put an image charge into the region where you're computing the potential. (2) The image charges must add up to the correct total in each region.","title":"Example 4.8"},{"location":"ch4-4/#443-energy-in-dielectric-systems","text":"It takes work to charge up a capacitor (Eq 2.55): W = \\frac{1}{2} C V^2 If the capacitor is filled with linear dielectric, its capacitance exceeds the vacuum value by a factor of the dielectric constant C = \\epsilon_r C_{vac} as we found in Ex. 4.6. Evidently, the work necessary to charge a dielectric-filled capacitor is increased by the same factor. The reason is pretty clear: you have to pump more (free) charge, to achieve a given potential, because part of the field is canceled off by the bound charges. In chapter 2 we got a general formula for the energy stored in any electrodynamic system W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd \\tau \\tagl{4.55} The case of the dielectric-filled capacitor suggests that this should be changed to W = \\frac{\\epsilon_0}{2} \\int \\epsilon_r E^2 \\dd \\tau = \\frac{1}{2} \\int \\vec{D} \\cdot \\vec{E} \\dd \\tau in the presence of linear dielectrics. To prove it, suppose the dielectric material is fixed in position, and we bring in the free charge, a bit at a time. As \\rho_f is increased by an amount \\Delta \\rho_f , the polarization will change and with it the bound charge distribution; but we're interested only in the work done on the incremental free charge: \\Delta W = \\int (\\Delta \\rho_f) V \\dd \\tau \\tagl{4.56} Since \\div \\vec{D} = \\rho_f, \\Delta \\rho_f = \\div (\\Delta \\vec{D}) , where \\Delta \\vec{D} is the resulting change in D , so \\Delta W = \\int [ \\div ( \\Delta D) ] V \\dd \\tau Now \\div [ (\\Delta D) V ] = [ \\div (\\Delta \\vec{D})] V + \\Delta \\vec{D} \\cdot (\\grad V) and hence, integrating by parts, \\Delta W = \\in \\div [ (\\Delta \\vec{D}) V] \\dd \\tau + \\int ( \\Delta \\vec{D}) \\cdot \\vec{E} \\dd \\tau The divergence theorem turns the first term into a surface integral, which vanishes if we integrate over all space. Therefore, the work done is equal to \\Delta W = \\int (\\Delta \\vec{D}) \\cdot \\vec{E} \\dd \\tau \\tagl{4.57} So far, this applies to any material. In the specific case of a linear dielectric, \\frac{1}{2} \\Delta ( \\vec{D} \\cdot \\vec{E} ) = \\frac{1}{2} \\Delta (\\epsilon E^2) = \\epsilon (\\Delta \\vec{E}) \\cdot \\vec{E} = ( \\Delta \\vec{D}) \\cdot \\vec{E} (for infinitesimal increments). Thus \\Delta W = \\Delta \\left( \\frac{1}{2} \\int \\vec{D} \\cdot \\vec{E} \\dd \\tau \\right) The total work done, then, as we build the free charge up from zero to the final configuration is W = \\frac{1}{2} \\int \\vec{D} \\cdot \\vec{E} \\dd \\tau \\tagl{4.58} as anticipated. It may puzzle you that Eq. 4.55, which we derived quite generally in Chapter 2, does not seem to apply in the presence of dielectrics, where it is replaced by Eq. 4.58. The point is not that one or the other of these equations is wrong, but rather that they address somewhat different questions. The distinction is subtle, so let's go right back to the beginning: What do we mean by \"the energy of a system\"? Answer: It is the work required to assemble the system. Very well - but when dielectrics are involved, there are two quite different ways one might construe this process: We bring in all the charges (free and bound), one by one, with tweezers, and glue each one down in its proper final location. If this is what you mean by \"assemble the system,\" then Eq. 4.55 is your formula for the energy stored. Notice, however, that this will not include the work involved in stretching and twisting the dielectric molecules (if we picture the positive and negative charges as held together by tiny springs, it does not include the spring energy, \\frac{1}{2} k x^2 , associated with polarizing each molecule). With the unpolarized dielectric in place, we bring in the free charges, one by one, allowing the dielectric to respond as it sees fit. If this is what you mean by \"assemble the system\" (and ordinarily it is, since free charge is what we actually push around), then Eq. 4.58 is the formula you want. In this case the \"spring\" energy is included, albeit indirectly, because the force you must apply to the free charge depends on the disposition of the bound charge; as you move the free charge, you are automatically stretching those \"springs.\"","title":"4.4.3: Energy in Dielectric Systems"},{"location":"ch4-4/#example-49","text":"A sphere of radius R is filled with material of dielectric constant \\epsilon_r and uniform embedded free charge \\rho_f . What is the energy of this configuration? Solution From Gauss's law, the displacement is \\vec{D}(r) = \\begin{cases} \\frac{\\rho_f}{3} \\vec{r} & \\qquad ( r < R) \\\\ \\frac{\\rho_f}{3} \\frac{R^3}{r^2} \\vu{r} & \\qquad ( r > R ) \\end{cases} So the electric field is \\vec{E}(r) = \\begin{cases} \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\vec{r} & \\qquad ( r < R) \\\\ \\frac{\\rho_f}{3 \\epsilon_0} \\frac{R^3}{r^2} \\vu{r} & \\qquad ( r > R ) \\end{cases} The purely electrostatic energy is \\begin{align*} W & = \\frac{\\epsilon_0}{2} \\left[ \\left( \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r } \\right) ^2 \\int _0 ^R r^2 4 \\pi r^2 \\dd r + \\left( \\frac{\\rho_f}{3 \\epsilon_0} \\right)^2 R^6 \\int_R ^\\infty \\frac{1}{r^4} 4 \\pi r^2 \\dd r \\right] \\\\ & = \\frac{2 \\pi}{9 \\epsilon_0} \\rho_f ^2 R^5 \\left( \\frac{1}{5 \\epsilon_r ^2} + 1 \\right) \\end{align*} But the total energy (Eq 4.58) is \\begin{align*} W_2 & = \\frac{1}{2} \\left[ \\left( \\frac{\\rho_f}{3} \\right)\\left( \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r } \\right) \\int_0 ^R r^2 4 \\pi r ^2 \\dd r + \\left( \\frac{\\rho_f R^3}{3} \\right) \\left( \\frac{\\rho_f R^3}{3 \\epsilon_0} \\right) \\int _R ^\\infty \\frac{1}{r^4} 4 \\pi r^2 \\dd r \\right] \\\\ & = \\frac{2 \\pi}{9 \\epsilon_0}\\rho_f ^2 R^5 \\left( \\frac{1}{5 \\epsilon_r} + 1 \\right) \\end{align*} Notice that W_1 < W_2 - that's because W_1 does not include the energy involved in stretching the molecules. Let's check that W_2 is the work done on the free charge in assembling the system. We start with the (uncharged, unpolarized) dielectric sphere, and bring in the free charge in infinitesimal installments (dq), filling out the sphere layer by layer. When we have reached radius r' , the electric field is \\vec{E}(r) = \\begin{cases} \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\vec{r} & \\quad (r < r') \\\\ \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\frac{r' ^3}{r^2} \\vu{r} & \\quad (r' < r < R ) \\\\ \\frac{\\rho_f}{3 \\epsilon_0} \\frac{r' ^3}{r^2} \\vu{r} & \\quad ( r > R) \\end{cases} The work required to bring the next dq in from infinity to r' is \\begin{align*} \\dd W & = - \\dd q \\left[ \\int_{\\infty} ^R \\vec{E} \\cdot \\dd \\vec{l} + \\int _R ^{r'} \\vec{E} \\cdot \\dd \\vec{l} \\right] \\\\ & = - \\dd q \\left[ \\frac{\\rho_f r' ^3}{3 \\epsilon_0} \\int_{\\infty} ^R \\frac{1}{r^2} \\dd r + \\frac{\\rho_f r' ^3}{3 \\epsilon_0 \\epsilon_r} \\int _{R} ^{r'} \\frac{1}{r^2} \\dd r \\right] \\\\ & = \\frac{\\rho_f r' ^3}{3 \\epsilon_0} \\left[ \\frac{1}{R} + \\frac{1}{\\epsilon_r} \\left( \\frac{1}{r'} - \\frac{1}{R} \\right) \\right] \\dd q \\end{align*} This increases the radius (r') \\dd q = \\rho_f 4 \\pi r' ^2 \\dd r' so the total work done, in going from r'=0 to r' = R is \\begin{align*} W & = \\frac{4 \\pi \\rho_f ^2}{3 \\epsilon_0} \\left[ \\frac{1}{R} \\left( 1 - \\frac{1}{ \\epsilon_r} \\right) \\int_0 ^R r' ^5 + \\frac{1}{\\epsilon_r} \\int_0 ^R r' ^4 \\dd r' \\right] \\\\ & = \\frac{2 \\pi}{9 \\epsilon_0}\\rho_f ^2 R^5 \\left( \\frac{1}{5 \\epsilon_r} + 1 \\right) = W_2 \\end{align*} Evidently the energy \"stored in the springs\" is W_{sprint} = W_2 - W_1 = \\frac{2 \\pi}{45 \\epsilon_0 \\epsilon_r ^2} \\rho _f ^2 R^5 (\\epsilon_r - 1) I would like to confirm this in an explicit model. Picture the dielectric as a collection of tiny proto-dipoles, each consisting of +q and -q attached to a spring of constant k and equilibrium length 0, so in the absence of any field the positive and negative ends coincide. One end of each dipole is nailed in position (like the nuclei in a solid), but the other end is free to move in response to any imposed field. Let \\dd \\tau be the volume assigned to each proto-dipole (the dipole itself may occupy only a small portion of this space). With the field turned on, the electric force on the free end is balanced by the spring force; the charges separate by a distance d: qE = kd . In our case \\vec{E} = \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\vec{r} The resulting dipole moment is p = qd and the polarization is P = p / \\dd \\tau so k = \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r d^2} P r \\dd \\tau The energy of this particular spring is \\dd W_{spring} = \\frac{1}{2} k d^2 = \\frac{\\rho_f}{6 \\epsilon_0 \\epsilon_r} P r \\dd \\tau and hence the total is W_{spring} = \\frac{\\rho_f}{6 \\epsilon_0 \\epsilon_r} \\int P r \\dd \\tau Now \\vec{P} = \\epsilon_0 \\chi_e \\vec{E} = \\epsilon_0 \\chi_e \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r } \\vec{r} = \\frac{(\\epsilon_r - 1) \\rho_f}{3 \\epsilon_r} \\vec{r} so W_{spring} = \\frac{\\rho_f}{6 \\epsilon_0 \\epsilon_r} \\frac{(\\epsilon_r - 1) \\rho_f}{3 \\epsilon_r} 4 \\pi \\int_0 ^R r^4 \\dd r = \\frac{2 \\pi}{45 \\epsilon_0 \\epsilon_r ^2} \\rho_f ^2 R^5 (\\epsilon_r - 1) and it works out perfectly. It is sometimes alleged that Eq. 4.58 represents the energy even for nonlinear dielectrics, but this is false: To proceed beyond Eq. 4.57, one must assume linearity. In fact, for dissipative systems the whole notion of \"stored energy\" loses its meaning, because the work done depends not only on the final configuration but on how it got there. If the molecular \"springs\" are allowed to have some friction, for instance, then W_{spring} can be made as large as you like, by assembling the charges in such a way that the spring is obliged to expand and contract many times before reaching its final state. In particular, you get nonsensical results if you try to apply Eq. 4.58 to electrets, with frozen-in polarization (see Prob. 4.27).","title":"Example 4.9"},{"location":"ch4-4/#444-forces-on-dielectrics","text":"Just as a conductor is attracted into an electric field (Eq. 2.51), so too is a dielectric - and for essentially the same reason: the bound charge tends to accumulate near the free charge of the opposite sign. But the calculation of forces on dielectrics can be surprisingly tricky. Consider, for example, the case of a slab of linear dielectric material, partially inserted between the plates of a parallel-plate capacitor (Fig. 4.30). We have always pretended that the field is uniform inside a parallel-plate capacitor, and zero outside. If this were literally true, there would be no net force on the dielectric at all, since the field everywhere would be perpendicular to the plates. However, there is in reality a fringing field around the edges, which for most purposes can be ignored but in this case is responsible for the whole effect. (Indeed, the field could not terminate abruptly at the edge of the capacitor, for if it did, the line integral of E around the closed loop shown in Fig. 4.31 would not be zero.) It is this nonuniform fringing field that pulls the dielectric into the capacitor. Fringing fields are notoriously difficult to calculate; luckily, we can avoid this altogether, by the following ingenious method. Let W be the energy of the system - it depends, of course, on the amount of overlap. If I pull the dielectric out an infinitesimal distance dx, the energy is changed by an amount equal to the work done: \\dd W = F_{me} \\dd x \\tagl{4.59} where F_{me} is the force I mus exert, to counteract the electrical force F on the dielectric. Thus, the electrical force on the slab is F = - \\dv{W}{x} \\tagl{4.60} Now, the energy stored in the capacitor is W = \\frac{1}{2} C V^2 \\tagl{4.61} and the capacitance in this case is C = \\frac{\\epsilon_0 w}{d} (\\epsilon_r l - \\chi_e x) \\tagl{4.62} where l is the length of the plates (Fig 4.30). Let's assume that the total charge on the plates is held constant (Q = CV) , as the dielectric moves. In terms of Q , W = \\frac{1}{2} \\frac{Q^2}{C} \\tagl{4.63} so F = - \\dv{W}{x} = \\frac{1}{2} \\frac{Q^2}{C^2} \\dv{C}{x} = \\frac{1}{2} V^2 \\dv{C}{x} \\tagl{4.64} But \\dv{C}{x} = - \\frac{\\epsilon_0 \\chi_e w}{d} and hence F = - \\frac{\\epsilon_0 \\chi_e w}{2d} V^2 \\tagl{4.65} (The minus sign indicates that the force is in the negative x direction; the dielectric is pulled into the capacitor.) It is a common error to use Eq. 4.61 (with V constant), rather than Eq. 4.63 (with Q constant), in computing the force. One then obtains F = - \\frac{1}{2} V^2 \\dv{C}{x} which is off by a sign. It is, of course, possible to maintain the capacitor at a fixed potential, by connecting it up to a battery. But in that case the battery also does work as the dielectric moves; instead of Eq. 4.59, we now have \\dd W = F_{me} \\dd x + V \\dd Q \\tagl{4.66} where V \\dd Q is the work done by the battery. It follows that F = - \\dv{W}{x} + V \\dv{Q}{x} = - \\frac{1}{2} V^2 \\dv{C}{x} + V^2 \\dv{C}{x} = \\frac{1}{2} V^2 \\dv{C}{x} \\tagl{4.67} the same as before, with the correct sign. Please understand: The force on the dielectric cannot possibly depend on whether you plan to hold Q constant or V constant - it is determined entirely by the distribution of charge, free and bound. It's simpler to calculate the force assuming constant Q , because then you don't have to worry about work done by the battery; but if you insist, it can be done correctly either way. Notice that we were able to determine the force without knowing anything about the fringing fields that are ultimately responsible for it! Of course, it's built into the whole structure of electrostatics that \\curl \\vec{E} = 0 , and hence that the fringing fields must be present; we're not really getting something for nothing here - just cleverly exploiting the internal consistency of the theory. The energy stored in the fringing fields themselves (which was not accounted for in this derivation) stays constant, as the slab moves; what does change is the energy well inside the capacitor, where the field is nice and uniform.","title":"4.4.4: Forces on Dielectrics"},{"location":"ch5-1/","text":"5.1: The Lorentz Force Law 5.1.1: Magnetic Fields Remember the basic problem of classical electrodynamics: We have a collection of charges q_1, q_2, q_3, \\ldots (the \"source\" charges), and we want to calculate the force they exert on some other charge Q (the \"test\" charge). According to the principle of superposition, it is sufficient to find the force of a single source charge - the total is then the vector sum of all the individual forces. Up to now, we have confined our attention to the simplest case, electrostatics, in which the source charge is at rest (though the test charge need not be). The time has come to consider the forces between charges in motion. To give you some sense of what is in store, imagine that I set up the following demonstration: Two wires hang from the ceiling, a few centimeters apart; when I turn on a current, so that it passes up one wire and back down the other, the wires jump apart - they evidently repel one another (Fig. 5.2(a)). How do we explain this? You might suppose that the battery (or whatever drives the current) is actually charging up the wire, and that the force is simply due to the electrical repulsion of like charges. But this is incorrect. I could hold up a test charge near these wires, and there would be no force on it, for the wires are in fact electrically neutral. (It's true that electrons are flowing down the line - that's what a current is - but there are just as many stationary plus charges as moving minus charges on any given segment.) Moreover, if I hook up my demonstration so as to make the current flow up both wires (Fig. 5.2(b)), they are found to attract! What's going on here? Whatever force accounts for the attraction of parallel currents and the repulsion of anti-parallel ones is not electrostatic in nature. It is our first encounter with a magnetic force. Whereas a stationary charge produces only an electric field E in the space around it, a moving charge generates, in addition, a magnetic field B . In fact, magnetic fields are a lot easier to detect, in practice - all you need is a Boy Scout compass. How these devices work is irrelevant at the moment; it is enough to know that the needle points in the direction of the local magnetic field. Ordinarily, this means north, in response to the earth's magnetic field, but in the laboratory, where typical fields may be hundreds of times stronger than that, the compass indicates the direction of whatever magnetic field is present. Now, if you hold up a tiny compass in the vicinity of a current-carrying wire, you quickly discover a very peculiar thing: The field does not point toward the wire, nor away from it, but rather it circles around the wire. In fact, if you grab the wire with your right hand-thumb in the direction of the current-your fingers curl around in the direction of the magnetic field (Fig. 5.3). How can such a field lead to a force of attraction on a nearby parallel current? At the second wire, the magnetic field points into the page (Fig. 5.4), the current is upward, and yet the resulting force is to the left! It's going to take a strange law to account for these directions.","title":"5.1 - The Lorentz Force Law"},{"location":"ch5-1/#51-the-lorentz-force-law","text":"","title":"5.1: The Lorentz Force Law"},{"location":"ch5-1/#511-magnetic-fields","text":"Remember the basic problem of classical electrodynamics: We have a collection of charges q_1, q_2, q_3, \\ldots (the \"source\" charges), and we want to calculate the force they exert on some other charge Q (the \"test\" charge). According to the principle of superposition, it is sufficient to find the force of a single source charge - the total is then the vector sum of all the individual forces. Up to now, we have confined our attention to the simplest case, electrostatics, in which the source charge is at rest (though the test charge need not be). The time has come to consider the forces between charges in motion. To give you some sense of what is in store, imagine that I set up the following demonstration: Two wires hang from the ceiling, a few centimeters apart; when I turn on a current, so that it passes up one wire and back down the other, the wires jump apart - they evidently repel one another (Fig. 5.2(a)). How do we explain this? You might suppose that the battery (or whatever drives the current) is actually charging up the wire, and that the force is simply due to the electrical repulsion of like charges. But this is incorrect. I could hold up a test charge near these wires, and there would be no force on it, for the wires are in fact electrically neutral. (It's true that electrons are flowing down the line - that's what a current is - but there are just as many stationary plus charges as moving minus charges on any given segment.) Moreover, if I hook up my demonstration so as to make the current flow up both wires (Fig. 5.2(b)), they are found to attract! What's going on here? Whatever force accounts for the attraction of parallel currents and the repulsion of anti-parallel ones is not electrostatic in nature. It is our first encounter with a magnetic force. Whereas a stationary charge produces only an electric field E in the space around it, a moving charge generates, in addition, a magnetic field B . In fact, magnetic fields are a lot easier to detect, in practice - all you need is a Boy Scout compass. How these devices work is irrelevant at the moment; it is enough to know that the needle points in the direction of the local magnetic field. Ordinarily, this means north, in response to the earth's magnetic field, but in the laboratory, where typical fields may be hundreds of times stronger than that, the compass indicates the direction of whatever magnetic field is present. Now, if you hold up a tiny compass in the vicinity of a current-carrying wire, you quickly discover a very peculiar thing: The field does not point toward the wire, nor away from it, but rather it circles around the wire. In fact, if you grab the wire with your right hand-thumb in the direction of the current-your fingers curl around in the direction of the magnetic field (Fig. 5.3). How can such a field lead to a force of attraction on a nearby parallel current? At the second wire, the magnetic field points into the page (Fig. 5.4), the current is upward, and yet the resulting force is to the left! It's going to take a strange law to account for these directions.","title":"5.1.1: Magnetic Fields"},{"location":"problems-ch3/","text":"Problem 3.24 Solution Since we are in cylindrical coordinates, we will write Laplace's equation in cylindrical coordinates (s, \\phi, z) : \\laplacian V = \\frac{1}{s} \\pdv{}{s} \\left( s \\pdv{V}{s} \\right) + \\frac{1}{s^2} \\frac{\\partial ^2 V}{\\partial \\phi ^2} = 0 We'll try the method of separation of variables on s and \\phi by searching for solutions which are products of the form V(s, \\phi) = S(s) + \\Phi(\\phi) \\frac{1}{s} \\Phi \\dv{}{s} \\left( s \\dv{S}{s} \\right) + \\frac{1}{s^2} S \\frac{d^2 \\Phi}{d\\phi ^2} = 0 to separate the variables, we need to divide by V and multiply by s^2 \\frac{s}{S} \\dv{}{s} \\left( s \\dv{S}{s} \\right) + \\frac{1}{\\Phi} \\frac{d^2 \\Phi}{d \\phi ^2} = 0 We define f(s) = \\frac{s}{S} \\dv{}{s} \\left( s \\dv{S}{s} \\right) and g(\\phi) = \\frac{1}{\\Phi} \\frac{d^2 \\Phi}{d \\phi ^2} Since we have separated our independent variables and the sum is equal to zero, they must both be constant f(s) = C_1 \\qquad g(\\phi) = C_2 \\qquad C_1 + C_2 = 0 Cylindrical symmetry implies that \\text{ when } \\phi \\rightarrow \\phi + 2 \\pi : \\qquad \\Phi(\\phi + 2 \\pi) = \\Phi(\\phi) So C_2 must be the positive one, since we know that will give us the periodic solutions to Laplace's equation. We write our constant as k^2 so \\frac{d^2 \\Phi}{d \\phi ^2} = - k^2 \\Phi \\\\ \\rightarrow \\Phi(\\phi) = A \\cos (k \\phi) + B \\sin (k \\phi), \\quad k = 0, 1, 2, 3, \\ldots Back to the S part, we need a solution to s \\dv{}{s} \\left( s \\dv{S}{s} \\right) = k^2 S A convenient solution would be a power function, S(s) = s^n if we choose the power n appropriately \\begin{align*} s \\dv{}{s} \\left( s \\dv{s^n}{s} \\right) & = s \\dv{}{s} \\left( s n s^{n-1} \\right) \\\\ & = s \\dv{}{s} (n s^n) \\\\ & = s n^2 s^{n-1} \\\\ & = n^2 s^n \\\\ & = k^2 S = k^2 s^n \\\\ & \\rightarrow n = \\pm k \\end{align*} So, our general solution for S is S(s) = C s^k + D s^{-k} And our general solution will be an infinite series over k . But we have to now be careful, because previously we've expressed our general solution in terms of strictly non-zero k , but here we have k = 0 , which gives us a constant solution k = 0: \\qquad S(s) = C s^0 + D s^0 = \\text{const.} But we should get two solutions for a second-order ordinary differential equation. If we go back to the differential equation for S, s \\dv{}{s} \\left( s \\dv{S}{s} \\right) = k^2 S \\\\ \\rightarrow s \\dv{S}{s} = \\text{ const. } = C \\\\ \\rightarrow \\dv{S}{s} = \\frac{c}{s} \\\\ \\rightarrow \\dd S = C \\frac{ds}{s} \\\\ S(s) = C \\ln s + D This gives us our second solution for S for k = 0 . Now what about for \\Phi ? Looking at the k = 0 case for the \\Phi ODE, \\frac{d^2 \\Phi}{d \\phi ^2} = - k^2 \\Phi = 0 \\quad \\text{ for } k = 0 \\\\ \\frac{d \\Phi}{d \\phi} = \\text{ const. } = B \\\\ \\rightarrow \\Phi(\\phi) = B \\phi + A But this doesn't meet our periodicity requirement! This isn't a physically acceptable solution. For k = 0, \\Phi = B is the only 'physically acceptable' solution (we discard B \\phi + A out of hand.) Finally, our general solution looks like V(s, \\phi) = a_0 + b_0 \\ln s + \\sum_{k=1} ^\\infty \\left[ s^k (a_k \\cos k \\phi + b_k \\sin k \\phi) + s^{-k} (a_k \\cos k \\phi + b_k \\sin k \\phi) \\right] We've only been asked for the general solution in cylindrical coordinates (from which we can tell that our solution is independent of a ), and we must be given boundary conditions in order to solve for the constants a_k, b_k . Problem 3.27 A sphere of radius R, centered at the origin, carries charge density \\rho(r, \\theta) = k \\frac{R}{r^2} (R - 2r) \\sin \\theta where k is a constant, and r, \\theta are the usual spherical coordinates. Find the approximate potential for points on the z axis, far from the sphere. We are asked for the approximate potential for points on the z-axis far from the charge distribution, so we'll calculate the terms of our potential from Eq 3.95, and stop when we find the first non-zero term, replacing \\theta for \\alpha and z for r as we go. V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{n=0} ^\\infty \\frac{1}{r^{(n+1)}} \\int (r') P_n(\\cos \\alpha) \\rho(\\vec{r'}) \\dd{\\tau'} Let's start with the monopole term. The integral we have to calculate is simply the charge density integrated over the charge distribution \\int \\rho(r) \\dd \\tau = k R \\int_0 ^R \\int _0 ^{\\pi} \\int_{0} ^{2 \\pi} \\frac{1}{r^2} (R - 2r) \\sin \\theta (r^2 \\sin \\theta ) \\dd r \\dd \\theta \\dd \\phi \\\\ \\int _{0} ^R (R - 2r) \\dd r = \\left.(R r - r^2)\\right|_{0} ^R = 0 So the monopole term comes out to zero. Next, we try calculating the dipole term: \\int r \\cos \\theta \\rho(r) \\dd \\tau = k R \\iiint r \\cos \\theta \\frac{1}{r^2} (R - 2r) \\sin \\theta (r^2 \\sin \\theta) \\dd r \\dd \\theta \\dd \\phi The \\theta integral will come out to \\int_0 ^{\\pi} \\sin ^2 \\cos \\theta \\dd \\theta = \\int _0 ^\\pi \\sin ^2 \\theta \\dd (\\sin \\theta) = \\left. \\frac{1}{3} \\sin ^3 \\theta \\right|_0 ^\\pi = 0 Well dangit, we still don't have the first non-zero term! On to the quadrupole term: \\begin{align*} & \\int r^2 \\left( \\frac{3}{2} \\cos ^2 \\theta - \\frac{1}{2} \\right) \\rho \\dd \\tau \\\\ = & \\iiint r^2 \\left( \\frac{3}{2} \\cos ^2 \\theta - \\frac{1}{2} \\right) \\frac{kR}{r^2} (R - 2r) \\sin \\theta r^2 \\sin \\theta \\dd r \\dd \\theta \\dd \\phi \\\\ = & \\frac{1}{2} kR \\iiint r^2 (3 \\cos ^2 \\theta - 1)(R - 2r) \\sin ^2 \\theta \\dd r \\dd \\theta \\dd \\phi \\end{align*} Thankfully we don't have any cross-terms, so we can do the integrals separately. The integral in r is \\int_0 ^R r^2 (R - 2r) \\dd r = - \\frac{R^4}{6} The integral in \\theta is \\int_0 ^\\pi (3 \\cos ^2 \\theta - 1) \\sin ^2 \\theta \\dd \\theta = \\int _0 ^\\pi \\left[ 3 (1 - \\sin ^2 \\theta) - 1 \\right] \\sin ^2 \\theta \\dd \\theta = - \\frac{\\pi}{8} And we just get a 2 \\pi from the \\phi integral, so converting our r to z in our coordinate system, the whole quadrupole potential is V(\\vec{r}) \\approx \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{z^3} \\frac{1}{2} k R \\left( - \\frac{R^4}{6} \\right) \\left( - \\frac{\\pi}{8} \\right) (2 \\pi) \\\\ = \\frac{1}{4 \\pi \\epsilon_0} \\frac{k \\pi ^2 R ^5}{48 z^3} \\quad \\text{(Quadrupole)} Problem 3.31 In Ex. 3.9, we derived the exact potential for a spherical shell of radius R , which carries a surface charge \\sigma = k \\cos \\theta . a) Calculate the dipole moment of this charge distribution. b) Find the approximate potential, at points far from the sphere, and compare the exact answer (Eq 3.87). What can you conclude about the higher multipoles? By the symmetry of the problem, p is going to be in the z-direction: \\vec{p} = p \\vu{z}; \\, p = \\int z \\rho \\dd \\tau \\rightarrow \\int z \\sigma \\dd a . \\begin{align*} p & = \\int (R \\cos \\theta)(k \\cos \\theta) R^3 \\sin \\theta \\dd \\theta \\dd \\phi \\\\ & = 2 \\pi R^3 k \\int _0 ^\\pi \\cos ^2 \\theta \\sin \\theta \\dd \\theta \\\\ & = 2 \\pi R^3 k \\left. \\left( - \\frac{\\cos ^3 \\theta}{3} \\right) \\right|_0 ^\\pi \\\\ & = \\frac{2}{3} \\pi R^3 k [ 1 - (-1) ] \\\\ & = \\frac{4 \\pi R^3 k}{3} \\end{align*} \\tag{a} \\vec{p} = \\frac{4 \\pi R^3 k}{3} \\vu{z} The associated dipole potential is just V_{dip} \\approx \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vu{r} \\cdot \\vec{p}}{r^2} = \\frac{k R^3}{3 \\epsilon_0} \\frac{\\cos \\theta}{r^2} Problem 3.33 Show that the electric field of a 'pure' dipole can be written in the coordinate-free form \\vec{E_{dip}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^3} [3 (\\vec{p} \\cdot \\vu{r}) \\vu{r} - \\vec{p} ] We still assume the dipole is pointing in the z-direction and start with spherical coordinates, and then move to a coordinate-free system \\vec{p} = p \\vu{z} \\vec{p} = p_r \\vu{r} + p_\\theta \\vu{\\theta} + p_{\\phi} \\vu{\\phi} Since p is in the z-direction, we can safely say p_\\phi = 0 p_r = \\vec{p} \\cdot \\vu{r} = p \\cos \\theta \\\\ p_\\theta = \\vec{p} \\cdot \\vu{\\theta} = - p \\sin \\theta \\\\ \\vec{p} = p \\cos \\theta \\vu{r} - p \\sin \\theta \\vu{\\theta} So we can directly check this expression against the expression we got as Eqn 3.103 (\\vec{E_{dip}}(r, \\theta) = \\frac{p}{4 \\pi \\epsilon_0 r^3} (2 \\cos \\theta \\vu{r} + \\sin \\theta \\vu{\\theta} ) : \\begin{align*} 3 ( \\vec{p} \\cdot \\vu{r}) \\vu{r} - \\vec{p} = & 3 p \\cos \\theta \\vu{r} - p \\cos \\theta \\vu{r} + p \\sin \\theta \\vu{\\theta} \\\\ & = 2 p \\cos \\theta \\vu{r} + p \\sin \\theta \\vu{\\theta} \\\\ \\rightarrow \\vec{E_{dip}} & = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^3} [3 (\\vec{p} \\cdot \\vu{r}) \\vu{r} - \\vec{p} ] \\end{align*} So it all checks out. Problem 3.34 Three point charges are located as shown in Fig 3.38, each a distance a from the origin. Find the approximate electric field at points far from the origin. Express your answer in spherical coordinates, and include the two lowest orders in the multipole expansion. We'll get to the electric field by writing down the multipole expansion of the potential, and then using the approximate potential to get the electric field. The total charge is -q, so the monopole term will be V_{mon} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r} (-q) The dipole moment is given by \\begin{align*} \\vec{p} & = \\sum_{i=1} ^3 q_i \\vec{r_i} \\\\ & = (-q) a \\vu{y} + (-q) a (-\\vu{y}) + q a \\vu{z} \\\\ & = qa \\vu{z} \\end{align*} The dipole term in the multipole expansion of V is then \\begin{align*} V_{dip} & = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} \\\\ & = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q a \\vu{z} \\cdot \\vu{r}}{r^2} \\\\ & = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q a \\cos \\theta}{r^2} \\end{align*} V(r, \\theta) \\approx \\frac{q}{4 \\pi \\epsilon_0 } \\left( - \\frac{1}{r} + \\frac{a \\cos \\theta}{r^2} \\right) The field is given by \\vec{E} = - \\grad V \\approx \\frac{q}{4 \\pi \\epsilon_0} \\left( - \\frac{1}{r^2} \\vu{r} + \\frac{2 a \\cos \\theta \\vu{r}}{r^3} \\vu{r} + \\frac{a}{r^3} \\sin \\theta \\vu{\\theta} \\right)","title":"Problems"},{"location":"problems-ch3/#problem-324","text":"Solution Since we are in cylindrical coordinates, we will write Laplace's equation in cylindrical coordinates (s, \\phi, z) : \\laplacian V = \\frac{1}{s} \\pdv{}{s} \\left( s \\pdv{V}{s} \\right) + \\frac{1}{s^2} \\frac{\\partial ^2 V}{\\partial \\phi ^2} = 0 We'll try the method of separation of variables on s and \\phi by searching for solutions which are products of the form V(s, \\phi) = S(s) + \\Phi(\\phi) \\frac{1}{s} \\Phi \\dv{}{s} \\left( s \\dv{S}{s} \\right) + \\frac{1}{s^2} S \\frac{d^2 \\Phi}{d\\phi ^2} = 0 to separate the variables, we need to divide by V and multiply by s^2 \\frac{s}{S} \\dv{}{s} \\left( s \\dv{S}{s} \\right) + \\frac{1}{\\Phi} \\frac{d^2 \\Phi}{d \\phi ^2} = 0 We define f(s) = \\frac{s}{S} \\dv{}{s} \\left( s \\dv{S}{s} \\right) and g(\\phi) = \\frac{1}{\\Phi} \\frac{d^2 \\Phi}{d \\phi ^2} Since we have separated our independent variables and the sum is equal to zero, they must both be constant f(s) = C_1 \\qquad g(\\phi) = C_2 \\qquad C_1 + C_2 = 0 Cylindrical symmetry implies that \\text{ when } \\phi \\rightarrow \\phi + 2 \\pi : \\qquad \\Phi(\\phi + 2 \\pi) = \\Phi(\\phi) So C_2 must be the positive one, since we know that will give us the periodic solutions to Laplace's equation. We write our constant as k^2 so \\frac{d^2 \\Phi}{d \\phi ^2} = - k^2 \\Phi \\\\ \\rightarrow \\Phi(\\phi) = A \\cos (k \\phi) + B \\sin (k \\phi), \\quad k = 0, 1, 2, 3, \\ldots Back to the S part, we need a solution to s \\dv{}{s} \\left( s \\dv{S}{s} \\right) = k^2 S A convenient solution would be a power function, S(s) = s^n if we choose the power n appropriately \\begin{align*} s \\dv{}{s} \\left( s \\dv{s^n}{s} \\right) & = s \\dv{}{s} \\left( s n s^{n-1} \\right) \\\\ & = s \\dv{}{s} (n s^n) \\\\ & = s n^2 s^{n-1} \\\\ & = n^2 s^n \\\\ & = k^2 S = k^2 s^n \\\\ & \\rightarrow n = \\pm k \\end{align*} So, our general solution for S is S(s) = C s^k + D s^{-k} And our general solution will be an infinite series over k . But we have to now be careful, because previously we've expressed our general solution in terms of strictly non-zero k , but here we have k = 0 , which gives us a constant solution k = 0: \\qquad S(s) = C s^0 + D s^0 = \\text{const.} But we should get two solutions for a second-order ordinary differential equation. If we go back to the differential equation for S, s \\dv{}{s} \\left( s \\dv{S}{s} \\right) = k^2 S \\\\ \\rightarrow s \\dv{S}{s} = \\text{ const. } = C \\\\ \\rightarrow \\dv{S}{s} = \\frac{c}{s} \\\\ \\rightarrow \\dd S = C \\frac{ds}{s} \\\\ S(s) = C \\ln s + D This gives us our second solution for S for k = 0 . Now what about for \\Phi ? Looking at the k = 0 case for the \\Phi ODE, \\frac{d^2 \\Phi}{d \\phi ^2} = - k^2 \\Phi = 0 \\quad \\text{ for } k = 0 \\\\ \\frac{d \\Phi}{d \\phi} = \\text{ const. } = B \\\\ \\rightarrow \\Phi(\\phi) = B \\phi + A But this doesn't meet our periodicity requirement! This isn't a physically acceptable solution. For k = 0, \\Phi = B is the only 'physically acceptable' solution (we discard B \\phi + A out of hand.) Finally, our general solution looks like V(s, \\phi) = a_0 + b_0 \\ln s + \\sum_{k=1} ^\\infty \\left[ s^k (a_k \\cos k \\phi + b_k \\sin k \\phi) + s^{-k} (a_k \\cos k \\phi + b_k \\sin k \\phi) \\right] We've only been asked for the general solution in cylindrical coordinates (from which we can tell that our solution is independent of a ), and we must be given boundary conditions in order to solve for the constants a_k, b_k .","title":"Problem 3.24"},{"location":"problems-ch3/#problem-327","text":"A sphere of radius R, centered at the origin, carries charge density \\rho(r, \\theta) = k \\frac{R}{r^2} (R - 2r) \\sin \\theta where k is a constant, and r, \\theta are the usual spherical coordinates. Find the approximate potential for points on the z axis, far from the sphere. We are asked for the approximate potential for points on the z-axis far from the charge distribution, so we'll calculate the terms of our potential from Eq 3.95, and stop when we find the first non-zero term, replacing \\theta for \\alpha and z for r as we go. V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{n=0} ^\\infty \\frac{1}{r^{(n+1)}} \\int (r') P_n(\\cos \\alpha) \\rho(\\vec{r'}) \\dd{\\tau'} Let's start with the monopole term. The integral we have to calculate is simply the charge density integrated over the charge distribution \\int \\rho(r) \\dd \\tau = k R \\int_0 ^R \\int _0 ^{\\pi} \\int_{0} ^{2 \\pi} \\frac{1}{r^2} (R - 2r) \\sin \\theta (r^2 \\sin \\theta ) \\dd r \\dd \\theta \\dd \\phi \\\\ \\int _{0} ^R (R - 2r) \\dd r = \\left.(R r - r^2)\\right|_{0} ^R = 0 So the monopole term comes out to zero. Next, we try calculating the dipole term: \\int r \\cos \\theta \\rho(r) \\dd \\tau = k R \\iiint r \\cos \\theta \\frac{1}{r^2} (R - 2r) \\sin \\theta (r^2 \\sin \\theta) \\dd r \\dd \\theta \\dd \\phi The \\theta integral will come out to \\int_0 ^{\\pi} \\sin ^2 \\cos \\theta \\dd \\theta = \\int _0 ^\\pi \\sin ^2 \\theta \\dd (\\sin \\theta) = \\left. \\frac{1}{3} \\sin ^3 \\theta \\right|_0 ^\\pi = 0 Well dangit, we still don't have the first non-zero term! On to the quadrupole term: \\begin{align*} & \\int r^2 \\left( \\frac{3}{2} \\cos ^2 \\theta - \\frac{1}{2} \\right) \\rho \\dd \\tau \\\\ = & \\iiint r^2 \\left( \\frac{3}{2} \\cos ^2 \\theta - \\frac{1}{2} \\right) \\frac{kR}{r^2} (R - 2r) \\sin \\theta r^2 \\sin \\theta \\dd r \\dd \\theta \\dd \\phi \\\\ = & \\frac{1}{2} kR \\iiint r^2 (3 \\cos ^2 \\theta - 1)(R - 2r) \\sin ^2 \\theta \\dd r \\dd \\theta \\dd \\phi \\end{align*} Thankfully we don't have any cross-terms, so we can do the integrals separately. The integral in r is \\int_0 ^R r^2 (R - 2r) \\dd r = - \\frac{R^4}{6} The integral in \\theta is \\int_0 ^\\pi (3 \\cos ^2 \\theta - 1) \\sin ^2 \\theta \\dd \\theta = \\int _0 ^\\pi \\left[ 3 (1 - \\sin ^2 \\theta) - 1 \\right] \\sin ^2 \\theta \\dd \\theta = - \\frac{\\pi}{8} And we just get a 2 \\pi from the \\phi integral, so converting our r to z in our coordinate system, the whole quadrupole potential is V(\\vec{r}) \\approx \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{z^3} \\frac{1}{2} k R \\left( - \\frac{R^4}{6} \\right) \\left( - \\frac{\\pi}{8} \\right) (2 \\pi) \\\\ = \\frac{1}{4 \\pi \\epsilon_0} \\frac{k \\pi ^2 R ^5}{48 z^3} \\quad \\text{(Quadrupole)}","title":"Problem 3.27"},{"location":"problems-ch3/#problem-331","text":"In Ex. 3.9, we derived the exact potential for a spherical shell of radius R , which carries a surface charge \\sigma = k \\cos \\theta . a) Calculate the dipole moment of this charge distribution. b) Find the approximate potential, at points far from the sphere, and compare the exact answer (Eq 3.87). What can you conclude about the higher multipoles? By the symmetry of the problem, p is going to be in the z-direction: \\vec{p} = p \\vu{z}; \\, p = \\int z \\rho \\dd \\tau \\rightarrow \\int z \\sigma \\dd a . \\begin{align*} p & = \\int (R \\cos \\theta)(k \\cos \\theta) R^3 \\sin \\theta \\dd \\theta \\dd \\phi \\\\ & = 2 \\pi R^3 k \\int _0 ^\\pi \\cos ^2 \\theta \\sin \\theta \\dd \\theta \\\\ & = 2 \\pi R^3 k \\left. \\left( - \\frac{\\cos ^3 \\theta}{3} \\right) \\right|_0 ^\\pi \\\\ & = \\frac{2}{3} \\pi R^3 k [ 1 - (-1) ] \\\\ & = \\frac{4 \\pi R^3 k}{3} \\end{align*} \\tag{a} \\vec{p} = \\frac{4 \\pi R^3 k}{3} \\vu{z} The associated dipole potential is just V_{dip} \\approx \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vu{r} \\cdot \\vec{p}}{r^2} = \\frac{k R^3}{3 \\epsilon_0} \\frac{\\cos \\theta}{r^2}","title":"Problem 3.31"},{"location":"problems-ch3/#problem-333","text":"Show that the electric field of a 'pure' dipole can be written in the coordinate-free form \\vec{E_{dip}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^3} [3 (\\vec{p} \\cdot \\vu{r}) \\vu{r} - \\vec{p} ] We still assume the dipole is pointing in the z-direction and start with spherical coordinates, and then move to a coordinate-free system \\vec{p} = p \\vu{z} \\vec{p} = p_r \\vu{r} + p_\\theta \\vu{\\theta} + p_{\\phi} \\vu{\\phi} Since p is in the z-direction, we can safely say p_\\phi = 0 p_r = \\vec{p} \\cdot \\vu{r} = p \\cos \\theta \\\\ p_\\theta = \\vec{p} \\cdot \\vu{\\theta} = - p \\sin \\theta \\\\ \\vec{p} = p \\cos \\theta \\vu{r} - p \\sin \\theta \\vu{\\theta} So we can directly check this expression against the expression we got as Eqn 3.103 (\\vec{E_{dip}}(r, \\theta) = \\frac{p}{4 \\pi \\epsilon_0 r^3} (2 \\cos \\theta \\vu{r} + \\sin \\theta \\vu{\\theta} ) : \\begin{align*} 3 ( \\vec{p} \\cdot \\vu{r}) \\vu{r} - \\vec{p} = & 3 p \\cos \\theta \\vu{r} - p \\cos \\theta \\vu{r} + p \\sin \\theta \\vu{\\theta} \\\\ & = 2 p \\cos \\theta \\vu{r} + p \\sin \\theta \\vu{\\theta} \\\\ \\rightarrow \\vec{E_{dip}} & = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^3} [3 (\\vec{p} \\cdot \\vu{r}) \\vu{r} - \\vec{p} ] \\end{align*} So it all checks out.","title":"Problem 3.33"},{"location":"problems-ch3/#problem-334","text":"Three point charges are located as shown in Fig 3.38, each a distance a from the origin. Find the approximate electric field at points far from the origin. Express your answer in spherical coordinates, and include the two lowest orders in the multipole expansion. We'll get to the electric field by writing down the multipole expansion of the potential, and then using the approximate potential to get the electric field. The total charge is -q, so the monopole term will be V_{mon} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r} (-q) The dipole moment is given by \\begin{align*} \\vec{p} & = \\sum_{i=1} ^3 q_i \\vec{r_i} \\\\ & = (-q) a \\vu{y} + (-q) a (-\\vu{y}) + q a \\vu{z} \\\\ & = qa \\vu{z} \\end{align*} The dipole term in the multipole expansion of V is then \\begin{align*} V_{dip} & = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} \\\\ & = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q a \\vu{z} \\cdot \\vu{r}}{r^2} \\\\ & = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q a \\cos \\theta}{r^2} \\end{align*} V(r, \\theta) \\approx \\frac{q}{4 \\pi \\epsilon_0 } \\left( - \\frac{1}{r} + \\frac{a \\cos \\theta}{r^2} \\right) The field is given by \\vec{E} = - \\grad V \\approx \\frac{q}{4 \\pi \\epsilon_0} \\left( - \\frac{1}{r^2} \\vu{r} + \\frac{2 a \\cos \\theta \\vu{r}}{r^3} \\vu{r} + \\frac{a}{r^3} \\sin \\theta \\vu{\\theta} \\right)","title":"Problem 3.34"}]}