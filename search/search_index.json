{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Griffiths - Introduction to Electrodynamics This is basically just a web-friendly version of David Griffiths' Introduction to Electrodynamics, 4th Ed. . These are my class notes for the University of Washington's PHYS 543. This is mostly an exercise for myself in learning about Mkdocs, MathJax, and physics! Click around in the sidebar to find a chapter to read, or follow the links at the bottom of the page to read in order! Don't forget to try out the interface on mobile, it's very slick ;) Table of Contents 1 - Vector Analysis 1.1 - Vector Algebra 1.2 - Differential Calculus 1.3 - Integral Calculus 1.4 - Curvilinear Coordinates 1.5 - The Dirac Delta Function 1.6 - The Theory of Vector Fields 2 - Electrostatics 2.1 - The Electric Field 2.2 - Divergence and Curl of Electrostatic Fields 2.3 - Electric Potential 2.4 - Work and Energy in Electrostatics 2.5 - Conductors 3 - Potentials 3.1 - Laplace's Equation 3.2 - The Method of Images 3.3 - Separation of Variables 3.4 - Multipole Expansion Chapter 3 Problems 4 - Electric Fields in Matter 4.1 - Polarization 4.2 - The Field of a Polarized Object 4.3 - The Electric Displacement 4.4 - Linear Dielectrics 5 - Magnetostatics 5.1 - The Lorentz Force Law 5.2 - The Biot-Savart Law 5.3 - The Divergence and Curl of B 5.4 - Magnetic Vector Potential Chapter 5 Problems 6 - Magnetic Fields in Matter 6.1 - Magnetization 6.2 - The Field of a Magnetized Object 6.3 - The Auxiliary Field 6.4 - Linear and Nonlinear Media 7 - Electrodynamics 7.1 - Electromotive Force 7.2 - Electromagnetic Induction 7.3 - Maxwell's Equations Chapter 7 Problems 8 - Conservation Laws 8.0 - Phys 544 Introduction 8.1 - Charge and Energy 8.2 - Momentum 9 - Electromagnetic Waves 9.1 - Waves in One Dimension 9.2 - Electromagnetic Waves 9.3 - Electromagnetic Waves in Matter 9.4 - Absorption and Dispersion Internals All content is written in Markdown and rendered to a static site using MkDocs . The theme for the site is Material for MkDocs . I use python-markdown-math to turn any LaTeX in my source into full-blown MathJax to be rendered in the browser (and in a mobile-friendly format!).","title":"Home"},{"location":"#griffiths-introduction-to-electrodynamics","text":"This is basically just a web-friendly version of David Griffiths' Introduction to Electrodynamics, 4th Ed. . These are my class notes for the University of Washington's PHYS 543. This is mostly an exercise for myself in learning about Mkdocs, MathJax, and physics! Click around in the sidebar to find a chapter to read, or follow the links at the bottom of the page to read in order! Don't forget to try out the interface on mobile, it's very slick ;)","title":"Griffiths - Introduction to Electrodynamics"},{"location":"#table-of-contents","text":"1 - Vector Analysis 1.1 - Vector Algebra 1.2 - Differential Calculus 1.3 - Integral Calculus 1.4 - Curvilinear Coordinates 1.5 - The Dirac Delta Function 1.6 - The Theory of Vector Fields 2 - Electrostatics 2.1 - The Electric Field 2.2 - Divergence and Curl of Electrostatic Fields 2.3 - Electric Potential 2.4 - Work and Energy in Electrostatics 2.5 - Conductors 3 - Potentials 3.1 - Laplace's Equation 3.2 - The Method of Images 3.3 - Separation of Variables 3.4 - Multipole Expansion Chapter 3 Problems 4 - Electric Fields in Matter 4.1 - Polarization 4.2 - The Field of a Polarized Object 4.3 - The Electric Displacement 4.4 - Linear Dielectrics 5 - Magnetostatics 5.1 - The Lorentz Force Law 5.2 - The Biot-Savart Law 5.3 - The Divergence and Curl of B 5.4 - Magnetic Vector Potential Chapter 5 Problems 6 - Magnetic Fields in Matter 6.1 - Magnetization 6.2 - The Field of a Magnetized Object 6.3 - The Auxiliary Field 6.4 - Linear and Nonlinear Media 7 - Electrodynamics 7.1 - Electromotive Force 7.2 - Electromagnetic Induction 7.3 - Maxwell's Equations Chapter 7 Problems 8 - Conservation Laws 8.0 - Phys 544 Introduction 8.1 - Charge and Energy 8.2 - Momentum 9 - Electromagnetic Waves 9.1 - Waves in One Dimension 9.2 - Electromagnetic Waves 9.3 - Electromagnetic Waves in Matter 9.4 - Absorption and Dispersion","title":"Table of Contents"},{"location":"#internals","text":"All content is written in Markdown and rendered to a static site using MkDocs . The theme for the site is Material for MkDocs . I use python-markdown-math to turn any LaTeX in my source into full-blown MathJax to be rendered in the browser (and in a mobile-friendly format!).","title":"Internals"},{"location":"ch1-1/","text":"1.1 Vector Algebra 1.1.1 Vector Operations If you walk 4 miles due north and then 3 miles due east (Fig. 1.1), you will have gone a total of 7 miles, but you're not 7 miles from where you set out-you're only 5. We need an arithmetic to describe quantities like this, which evidently do not add in the ordinary way. The reason they don't, of course, is that displacements (straight line segments going from one point to another) have direction as well as magnitude (length), and it is essential to take both into account when you combine them. Such objects are called vectors: velocity, acceleration, force and momentum are other examples. By contrast, quantities that have magnitude but no direction are called scalars: examples include mass, charge, density, and temperature. I shall use boldface ( \\vec{A} , \\vec{B} , and so on) for vectors and ordinary type for scalars. The magnitude of a vector \\vec{A} is written |\\vec{A}| or, more simply, A . In diagrams, vectors are denoted by arrows: the length of the arrow is proportional to the magnitude of the vector, and the arrowhead indicates its direction. Minus \\vec{A} ( - \\vec{A} ) is a vector with the same magnitude as A but of opposite direction (Fig. 1.2). Note that vectors have magnitude and direction but not location: a displacement of 4 miles due north from Washington is represented by the same vector as a displacement 4 miles north from Baltimore (neglecting, of course, the curvature of the earth). On a diagram, therefore, you can slide the arrow around at will, as long as you don't change its length or direction. We define four vector operations: addition and three kinds of multiplication. (i) Addition of two vectors. . Place the tail of \\vec{B} at the head of \\vec{A} ; the sum, \\vec{A} + \\vec{B} , is the vector from the tail of \\vec{A} to the head of \\vec{B} (Fig 1.3). This rule generalizes the obvious procedure for combining two displacements. Addition is commutative : \\vec{A} + \\vec{B} = \\vec{B} + \\vec{A} 3 miles east followed by 4 miles north gets you to the same place as 4 miles north followed by 3 miles east. Addition is also associative: (\\vec{A} + \\vec{B}) + \\vec{C} = \\vec{A} + (\\vec{B} + \\vec{C}) To subtract a vector, add its opposite (Fig. 1.4): \\vec{A} - \\vec{B} = \\vec{A} + (- \\vec{B}) (ii) Multiplication by a scalar. Multiplication of a vector by a positive scalar a multiplies the magnitude but leaves the direction unchanged (Fig. 1.5). (If a is negative, the direction is reversed.) Scalar multiplication is distributive: a(\\vec{A} + \\vec{B}) = a \\vec{A} + a \\vec{B} (iii) Dot product of two vectors. The dot product of two vectors is defined by \\vec{A} \\cdot \\vec{B} = A B \\cos \\theta \\tag{1.1} where \\theta is the angle they form when placed tail-to-tail (Fig. 1.6). Note that \\vec{A} \\cdot \\vec{B} is itself a scalar (hence the alternative name scalar product ). The dot product is commutative, \\vec{A} \\cdot \\vec{B} = \\vec{B} \\cdot \\vec{A} and distributive \\vec{A} \\cdot (\\vec{B} + \\vec{C}) = \\vec{A} \\cdot \\vec{B} + \\vec{A} \\cdot \\vec{C} \\tag{1.2} Geometrically, \\vec{A} \\cdot \\vec{B} is the product of A times the projection of B along A (or the product of B times the projection of A along B). If the two vectors are parallel, then \\vec{A} \\cdot \\vec{B} = AB . In particular, for any vector A \\vec{A} \\cdot \\vec{A} = A^2 \\tag{1.3} If A and B are perpendicular, then \\vec{A} \\cdot \\vec{B} = 0 Example 1.1 Let \\vec{C} = \\vec{A} - \\vec{B} (Fig 1.7), and calculate the dot product of \\vec{C} with itself. Solution \\vec{C} \\cdot \\vec{C} = ( \\vec{A} - \\vec{B} ) \\cdot (\\vec{A} - \\vec{B}) = \\vec{A} \\cdot \\vec{A} - \\vec{A} \\cdot \\vec{B} - \\vec{B} \\cdot \\vec{A} + \\vec{B} \\cdot \\vec{B} or C^2 = A^2 + B^2 - 2AB\\cos \\theta This is the law of cosines. (iv) Cross product of two vectors. The cross product of two vectors is defined by \\vec{A} \\cross \\vec{B} = AB \\sin \\theta \\vu{n} \\tag{1.4} where \\vu{n} is a unit vector (vector of magnitude 1) pointing perpendicular to the plane of A and B. (I shall use a hat \\vu{} to denote unit vectors.) Of course, there are two directions perpendicular to any plane: \"in\" and \"out.\" The ambiguity is resolved by the right-hand rule: let your fingers point in the direction of the first vector and curl around (via the smaller angle) toward the second; then your thumb indicates the direction of \\vu{n} . (In Fig. 1.8, \\vec{A} \\cross \\vec{B} points into the page; \\vec{B} \\cross \\vec{A} points out of the page.) Note that \\vec{A} \\cross \\vec{B} is itself a vector (hence the alternative name vector product). The cross product is distributive \\vec{A} \\cross ( \\vec{B} + \\vec{C}) = ( \\vec{A} \\cross \\vec{B}) + (\\vec{A} \\cross \\vec{C}) but not commutative . In fact, (\\vec{B} \\cross \\vec{A}) = - (\\vec{A} \\cross \\vec{B}) \\tagl{1.6} Geometrically, | \\vec{A} \\cross \\vec{B} | is the area of the parallelogram generated by \\vec{A} and \\vec{B} (Fig 1.8). If two vectors are parallel, their cross product is zero. In particular, \\vec{A} \\cross \\vec{A} = 0 for any vector A. 1.1.2: Vector Algebra: Component Form In the previous section, I defined the four vector operations (addition, scalar multiplication, dot product, and cross product) in \"abstract\" form-that is, without reference to any particular coordinate system. In practice, it is often easier to set up Cartesian coordinates x, y, z and work with vector components. Let \\vu{x} , \\vu{y} , and \\vu{z} be unit vectors parallel to the x, y, and z axes, respectively (Fig. 1.9(a)). An arbitrary vector A can be expanded in terms ofthese basis vectors (Fig. 1.9(b)): \\vec{A} = A_x \\vu{x} + A_y \\vu{y} + A_z \\vu{z} The numbers A_x , A_y , and A_z are the \"components\" of A; geometrically, they are the projections of A along the three coordinate axes ( A_x = \\vec{A} \\cdot \\vu{x}, A_y = \\vec{A} \\cdot \\vu{y}, A_z = \\vec{A} \\cdot \\vu{z} ). We can now reformulate each of the four vector operations as a rule for manipulating components: \\vec{A} + \\vec{B} = (A_x \\vu{x} + A_y \\vu{y} + A_z \\vu{z}) + (B_x \\vu{x} + B_y \\vu{y} + B_z \\vu{z}) \\\\ = (A_x + B_x) \\vu{x} + (A_y + B_y) \\vu{y} + (A_z + B_z) \\vu{z} \\tag{1.7} Rule (i): To add vectors, add like components. a\\vec{A} = (a A_x) \\vu{x} + (a A_y) \\vu{y} + (a A_z)\\vu{z} \\tag{1.8} Rule (ii): To multiply by a scalar, multiply each component. Because \\vu{x}, \\vu{y} , and \\vu{z} are mutually perpendicular unit vectors \\vu{x} \\cdot \\vu{x} = \\vu{y} \\cdot \\vu{y} = \\vu{z} \\cdot \\vu{z} = 1; \\qquad \\vu{x} \\cdot \\vu{y} = \\vu{x} \\cdot \\vu{z} = \\vu{y} \\cdot \\vu{z} = 0 \\tag{1.9} Accordingly, \\vec{A} \\cdot \\vec{B} = (A_x \\vu{x} + A_y \\vu{y} + A_z \\vu{z}) \\cdot (B_x \\vu{x} + B_y \\vu{y} + B_z \\vu{z}) \\\\ = A_x B_x + A_y B_y + A_z B_z \\tag{1.10} Rule (iii): To calculate the dot product, multiply like components and add. In particular, \\vec{A} \\cdot \\vec{A} = A_x ^2 + A_y ^2 + A_z ^2 so A = \\sqrt{A_x ^2 + A_y ^2 + A_z ^2} \\tag{1.11} Similarly, \\begin{align} \\vu{x} \\cross \\vu{x} & = & \\vu{y} \\cross \\vu{y} & = & \\vu{z} \\cross \\vu{z} = 0 \\\\ \\vu{x} \\cross \\vu{y} & = & - \\vu{y} \\cross \\vu{x} & = & \\vu{z} \\\\ \\vu{y} \\cross \\vu{z} & = & - \\vu{z} \\cross \\vu{y} & = & \\vu{x} \\\\ \\vu{z} \\cross \\vu{x} & = & - \\vu{x} \\cross \\vu{z} & = & \\vu{y} \\end{align} Therefore, \\vec{A} \\cross \\vec{B} = (A_x \\vu{x} + A_y \\vu{y} + A_z \\vu{z}) \\cross (B_x \\vu{x} + B_y \\vu{y} + B_z \\vu{z}) \\\\ = (A_y B_z - A_z B_y) \\vu{x} + (A_z B_x - A_x B_z)\\vu{y} + (A_x B_y - A_y B_x) \\vu{z} \\tag{1.13} This cumbersome expression can be written more neatly as a determinant: \\vec{A} \\cross \\vec{B} = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ A_x & A_y & A_z \\\\ B_x & B_y & B_z \\end{vmatrix} Rule (iv): To calculate the cross product, form the determinant whose first row is \\vu{x}, \\vu{y}, \\vu{z} , whose second row is A, and whose third row is B. Example 1.2 Find the angle between the face diagonals of a cube. Solution We might as well use a cube of side 1, and place it as shown in Fig 1.10, with one corner at the origin. The face diagonals \\vec{A} and \\vec{B} are \\vec{A} = 1 \\vu{x} + 0 \\vu{y} + 1 \\vu{z}; \\qquad \\vec{B} = 0 \\vu{x} + 1 \\vu{y} + 1 \\vu{z} So, in component form, \\vec{A} \\cdot \\vec{B} = 1 \\cdot 0 + 0 \\cdot 1 + 1 \\cdot 1 = 1 On the other hand, in \"abstract\" form, \\vec{A} \\cdot \\vec{B} = AB \\cos \\theta = \\sqrt{2} \\sqrt{2} \\cos \\theta = 2 \\cos \\theta Therefore, \\cos \\theta = 1/2 \\quad \\text{ or } \\quad \\theta = 60^{\\circ} Of course, you can get the answer more easily by drawing in a diagonal across the top of the cube, completing the equilateral triangle. But in cases where the geometry is not so simple, this device of comparing the abstract and component forms of the dot product can be a very efficient means of finding angles. 1.1.3: Triple Products Since the cross product of two vectors is itself a vector, it can be dotted or crossed with a third vector to form a triple product. (i) Scalar triple product: \\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) . Geometrically, |\\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) | is the volume of the parallelpiped generated by A , B , and C , since |\\vec{B} \\cross \\vec{C}| is the area of the base, and | \\vec{A} \\cos \\theta | is the altitude (Fig. 1.12). Evidently, \\vec{A} \\cdot(\\vec{B} \\cross \\vec{C}) = \\vec{B} \\cdot (\\vec{C} \\cross \\vec{A}) = \\vec{C} \\cdot (\\vec{A} \\cross \\vec{B}) \\tagl{1.15} for they all correspond to the same figure. Note that \"alphabetical\" order is preserved - in view of \\eqref{1.6} , the \"nonalphabetical\" triple products \\vec{A} \\cdot(\\vec{C} \\cross \\vec{B}) = \\vec{B} \\cdot (\\vec{A} \\cross \\vec{C}) = \\vec{C} \\cdot (\\vec{B} \\cross \\vec{A}) have the opposite sign. In component form, \\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) = \\begin{vmatrix} A_x & A_y & A_z \\\\ B_x & B_y & B_z \\\\ C_x & C_y & C_z \\end{vmatrix} \\tagl{1.16} Note that the dot and cross can be interchanged: \\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) = (\\vec{A} \\cross \\vec{B}) \\cdot \\vec{C} (this follows immediately from Eq. 1.15); however, the placement of the parentheses is critical: (\\vec{A} \\cdot \\vec{B}) \\cdot \\vec{C} is a meaningless expression - you can't make a cross product from a scalar and a vector. (ii) Vector triple product: \\vec{A} \\cross (\\vec{B} \\cross \\vec{C}) . The vector triple product can be simplified by the so-called BAC-CAB rule: \\vec{A} \\cross (\\vec{B} \\cross \\vec{C}) = \\vec{B}(\\vec{A} \\cdot \\vec{C}) - \\vec{C} (\\vec{A} \\cdot \\vec{B}) \\tagl{1.17} Notice that (\\vec{A} \\cross \\vec{B}) \\cross \\vec{C} = - \\vec{C} \\cross (\\vec{A} \\cross \\vec{B}) = -\\vec{A}(\\vec{B} \\cdot \\vec{C}) + \\vec{B}(\\vec{A} \\cdot \\vec{C}) is an entirely different vector (cross-products are not associative). All higher vector products can be similarly reduced, often by repeated application of \\eqref{1.17} , so it is never necessary for an expression to contain more than one cross product in any term. For instance, (\\vec{A} \\cross \\vec{B}) \\cdot (\\vec{C} \\cross \\vec{D}) = (\\vec{A} \\cdot \\vec{C})(\\vec{B} \\cdot \\vec{D}) - (\\vec{A} \\cdot \\vec{D}) (\\vec{B} \\cdot \\vec{C}) \\vec{A} \\cross [\\vec{B} \\cross (\\vec{C} \\cross \\vec{D})] = \\vec{B}[ \\vec{A} \\cdot (\\vec{C} \\cross \\vec{D})] - (\\vec{A} \\cdot \\vec{B})(\\vec{C} \\cross \\vec{D}) \\tagl{1.18} 1.1.4: Position, Displacement, and Separation Vectors The location of a point in three dimensions can be described by listing its Cartesian coordinates (x, y, z). The vector to that point from the origin ( \\mathscr{O} ) is called the position vector (Fig 1.13): \\vec{r} \\equiv x \\vu{x} + y \\vu{y} + z \\vu{z} \\tagl{1.19} I will reserve the letter \\vec{r} for this purpose. Its magnitude, r = \\sqrt{x^2 + y^2 + z^2} \\tagl{1.20} is the distance from the origin, and \\vu{r} = \\frac{\\vec{r}}{r} = \\frac{x \\vu{x} + y \\vu{y} + z \\vu{z}}{ \\sqrt{x^2 + y^2 + z^2}} \\tagl{1.21} is a unit vector pointing radially outward. The infinitesimal displacement vector from (x, y, z) to x + \\dd{x}, y + \\dd{y}, z + \\dd{z} is \\dd{\\vec{l}} = \\dd{x} \\vu{x} + \\dd{y} \\vu{y} + \\dd{z} \\vu{z} \\tagl{1.22} (We could call this \\dd{\\vec{r}} , since that's what it is, but it is useful to have a special notation for infinitesimal displacements.) In electrodynamics, one frequently encounters problems involving two points - typically a source point , \\vec{r'} , where an electric charge is located, and a field point \\vec{r} at which you are calculating the electric or magnetic field (Fig 1.14). It pays to adopt right from the start some short-hand notation for the separation vector from the source point to the field point. I shall use for this purpose the letter \\gr : \\vec{\\gr} \\equiv \\vec{r} - \\vec{r'} \\tagl{1.23} Its magnitude is |\\gr| = | \\vu{r} - \\vu{r'} | \\tagl{1.24} and a unit vector in the direction from \\vec{r'} to \\vec{r} is \\vu{\\gr} = \\frac{\\gr}{|\\gr|} = \\frac{\\vec{r} - \\vec{r'}}{|\\vec{r} - \\vec{r'}|} \\tagl{1.25} In Cartesian coordinates, \\gr = (x - x') \\vu{x} + (y-y') \\vu{y} + (z-z') \\vu{z} \\tagl{1.26} |\\gr| = \\sqrt{(x - x')^2 + (y-y')^2 + (z-z')^2 } \\tagl{1.27} \\vu{\\gr} = \\frac{(x - x') \\vu{x} + (y-y') \\vu{y} + (z-z') \\vu{z}}{\\sqrt{(x - x')^2 + (y-y')^2 + (z-z')^2 }} (from which you can appreciate the economy of the \\gr notation). 1.1.5: How Vectors Transform The definition of a vector as \"a quantity with a magnitude and direction\" is not altogether satisfactory: What precisely does \"direction\" mean? This may seem a pedantic question, but we shall soon encounter a species of derivative that looks rather like a vector, and we'll want to know for sure whether it is one. You might be inclined to say that a vector is anything that has three components that combine properly under addition. Well, how about this: We have a barrel of fruit that contains N_x pears, N_y apples, and N_z bananas. Is \\vec{N} = N_x \\vu{x} + N_y \\vu{y} + N_z \\vu{z} a vector? It has three components, and when you add another barrel with M_x pears, M_y apples, and M_z bananas the result is N_x + M_x pears, N_y + M_y apples, N_z + M_z bananas. So it does add like a vector. Yet it's obviously not a vector, in the physicist's sense of the word, because it doesn't really have a direction. What exactly is wrong with it? The answer is that \\vec{N} does not transform properly when you change coordinates . The coordinate frame we use to describe positions in space is of course entirely arbitrary, but there is a specific geometrical transformation law for converting vector components from one frame to another. Suppose, for instance, the \\overline{x}, \\overline{y}, \\overline{z} system is rotated by angle \\phi , relative to x, y, z , about the common x = \\overline{x} axes. From Fig. 1.15, A_y = A \\cos \\theta, \\qquad A_z = A \\sin \\theta while \\begin{align*} \\overline{A_y} & = A \\cos \\overline{\\theta} = A \\cos (\\theta - \\phi) = A (\\cos \\theta \\cos \\phi + \\sin \\theta \\sin \\phi) \\\\ & = \\cos \\phi A_y + \\sin \\phi A_z \\\\ \\overline{A_z} & = A \\sin \\overline{\\theta} = A \\sin (\\theta - \\phi) = A (\\sin \\theta \\cos \\phi - \\cos \\theta \\sin \\phi) \\\\ & = - \\sin \\phi A_y + \\cos \\phi A_z \\end{align*} We might express this conclusion in matrix notation: \\begin{pmatrix} \\overline{A_y} \\\\ \\overline{A_z} \\end{pmatrix} = \\begin{pmatrix} \\cos \\phi & \\sin \\phi \\\\ - \\sin \\phi & \\cos \\phi \\end{pmatrix} \\begin{pmatrix} A_y \\\\ A_z \\end{pmatrix} \\tagl{1.29} More generally, for rotation about an arbitrary axis in three dimensions, the transformation law takes the form \\begin{pmatrix} \\overline{A_x} \\\\ \\overline{A_y} \\\\ \\overline{A_z} \\end{pmatrix} = \\begin{pmatrix} R_{xx} & R_{xy} & R_{xz} \\\\ R_{yx} & R_{yy} & R_{yz} \\\\ R_{zx} & R_{zy} & R_{zz} \\end{pmatrix} \\begin{pmatrix} A_x \\\\ A_y \\\\ A_z \\end{pmatrix} \\tagl{1.30} or, more compactly, \\overline{A_i} = \\sum_{j=1}^3 R_{ij} A_j \\tagl{1.31} where index 1 stands for x, 2 for y, and 3 for z. The elements of the matrix R can be ascertained, for a given rotation, by the same sort of trigonometric arguments as we used for a rotation about the x axis. Now: Do the components of \\vec{N} transform this way? Of course not - it doesn't matter what coordinates you use to represent positions in space; there are still just as many apples in the barrel. You can't convert a pear into a banana by choosing a different set of axes, but you can turn in A_x into \\overline{A_y} . Formally, then, a vector is any set of three components that transforms in the same manner as a displacement when you change coordinates . As always, displacement is the model for the behavior of vectors. By the way, a (second-rank) tensor is a quantity with nine components, T_{xx}, T_{xy}, T_{xz}, T_{yx}, \\ldots T_{zz} which transform with two factors of R : \\begin{align*} \\overline{T}_{xx} & = R_{xx}(R_{xx} T_{xx} + R_{xy} T_{xy} + R_{xz} T_{xz}) \\\\ & + R_{xy}(R_{xx} T_{yx} + R_{xy} T_{yy} + R_{xz} T_{yz}) \\\\ & + R_{xz}(R_{xx} T_{zx} + R_{xy} T_{zy} + R_{xz} T_{zz}), \\ldots \\end{align*} or, more compactly, \\overline{T}_{ij} = \\sum_{k=1}^3 \\sum_{l=1} ^3 R_{ik} R_{jl} T_{kl} \\tagl{1.32} In general, an n-th rank tensor has n indices and 3^n components, and transforms with n factors of R . In this hierarchy, a vector is a tensor of rank 1, and a scalar is a tensor of rank zero.","title":"1.1 - Vector Algebra"},{"location":"ch1-1/#11-vector-algebra","text":"","title":"1.1 Vector Algebra"},{"location":"ch1-1/#111-vector-operations","text":"If you walk 4 miles due north and then 3 miles due east (Fig. 1.1), you will have gone a total of 7 miles, but you're not 7 miles from where you set out-you're only 5. We need an arithmetic to describe quantities like this, which evidently do not add in the ordinary way. The reason they don't, of course, is that displacements (straight line segments going from one point to another) have direction as well as magnitude (length), and it is essential to take both into account when you combine them. Such objects are called vectors: velocity, acceleration, force and momentum are other examples. By contrast, quantities that have magnitude but no direction are called scalars: examples include mass, charge, density, and temperature. I shall use boldface ( \\vec{A} , \\vec{B} , and so on) for vectors and ordinary type for scalars. The magnitude of a vector \\vec{A} is written |\\vec{A}| or, more simply, A . In diagrams, vectors are denoted by arrows: the length of the arrow is proportional to the magnitude of the vector, and the arrowhead indicates its direction. Minus \\vec{A} ( - \\vec{A} ) is a vector with the same magnitude as A but of opposite direction (Fig. 1.2). Note that vectors have magnitude and direction but not location: a displacement of 4 miles due north from Washington is represented by the same vector as a displacement 4 miles north from Baltimore (neglecting, of course, the curvature of the earth). On a diagram, therefore, you can slide the arrow around at will, as long as you don't change its length or direction. We define four vector operations: addition and three kinds of multiplication. (i) Addition of two vectors. . Place the tail of \\vec{B} at the head of \\vec{A} ; the sum, \\vec{A} + \\vec{B} , is the vector from the tail of \\vec{A} to the head of \\vec{B} (Fig 1.3). This rule generalizes the obvious procedure for combining two displacements. Addition is commutative : \\vec{A} + \\vec{B} = \\vec{B} + \\vec{A} 3 miles east followed by 4 miles north gets you to the same place as 4 miles north followed by 3 miles east. Addition is also associative: (\\vec{A} + \\vec{B}) + \\vec{C} = \\vec{A} + (\\vec{B} + \\vec{C}) To subtract a vector, add its opposite (Fig. 1.4): \\vec{A} - \\vec{B} = \\vec{A} + (- \\vec{B}) (ii) Multiplication by a scalar. Multiplication of a vector by a positive scalar a multiplies the magnitude but leaves the direction unchanged (Fig. 1.5). (If a is negative, the direction is reversed.) Scalar multiplication is distributive: a(\\vec{A} + \\vec{B}) = a \\vec{A} + a \\vec{B} (iii) Dot product of two vectors. The dot product of two vectors is defined by \\vec{A} \\cdot \\vec{B} = A B \\cos \\theta \\tag{1.1} where \\theta is the angle they form when placed tail-to-tail (Fig. 1.6). Note that \\vec{A} \\cdot \\vec{B} is itself a scalar (hence the alternative name scalar product ). The dot product is commutative, \\vec{A} \\cdot \\vec{B} = \\vec{B} \\cdot \\vec{A} and distributive \\vec{A} \\cdot (\\vec{B} + \\vec{C}) = \\vec{A} \\cdot \\vec{B} + \\vec{A} \\cdot \\vec{C} \\tag{1.2} Geometrically, \\vec{A} \\cdot \\vec{B} is the product of A times the projection of B along A (or the product of B times the projection of A along B). If the two vectors are parallel, then \\vec{A} \\cdot \\vec{B} = AB . In particular, for any vector A \\vec{A} \\cdot \\vec{A} = A^2 \\tag{1.3} If A and B are perpendicular, then \\vec{A} \\cdot \\vec{B} = 0","title":"1.1.1 Vector Operations"},{"location":"ch1-1/#example-11","text":"Let \\vec{C} = \\vec{A} - \\vec{B} (Fig 1.7), and calculate the dot product of \\vec{C} with itself. Solution \\vec{C} \\cdot \\vec{C} = ( \\vec{A} - \\vec{B} ) \\cdot (\\vec{A} - \\vec{B}) = \\vec{A} \\cdot \\vec{A} - \\vec{A} \\cdot \\vec{B} - \\vec{B} \\cdot \\vec{A} + \\vec{B} \\cdot \\vec{B} or C^2 = A^2 + B^2 - 2AB\\cos \\theta This is the law of cosines. (iv) Cross product of two vectors. The cross product of two vectors is defined by \\vec{A} \\cross \\vec{B} = AB \\sin \\theta \\vu{n} \\tag{1.4} where \\vu{n} is a unit vector (vector of magnitude 1) pointing perpendicular to the plane of A and B. (I shall use a hat \\vu{} to denote unit vectors.) Of course, there are two directions perpendicular to any plane: \"in\" and \"out.\" The ambiguity is resolved by the right-hand rule: let your fingers point in the direction of the first vector and curl around (via the smaller angle) toward the second; then your thumb indicates the direction of \\vu{n} . (In Fig. 1.8, \\vec{A} \\cross \\vec{B} points into the page; \\vec{B} \\cross \\vec{A} points out of the page.) Note that \\vec{A} \\cross \\vec{B} is itself a vector (hence the alternative name vector product). The cross product is distributive \\vec{A} \\cross ( \\vec{B} + \\vec{C}) = ( \\vec{A} \\cross \\vec{B}) + (\\vec{A} \\cross \\vec{C}) but not commutative . In fact, (\\vec{B} \\cross \\vec{A}) = - (\\vec{A} \\cross \\vec{B}) \\tagl{1.6} Geometrically, | \\vec{A} \\cross \\vec{B} | is the area of the parallelogram generated by \\vec{A} and \\vec{B} (Fig 1.8). If two vectors are parallel, their cross product is zero. In particular, \\vec{A} \\cross \\vec{A} = 0 for any vector A.","title":"Example 1.1"},{"location":"ch1-1/#112-vector-algebra-component-form","text":"In the previous section, I defined the four vector operations (addition, scalar multiplication, dot product, and cross product) in \"abstract\" form-that is, without reference to any particular coordinate system. In practice, it is often easier to set up Cartesian coordinates x, y, z and work with vector components. Let \\vu{x} , \\vu{y} , and \\vu{z} be unit vectors parallel to the x, y, and z axes, respectively (Fig. 1.9(a)). An arbitrary vector A can be expanded in terms ofthese basis vectors (Fig. 1.9(b)): \\vec{A} = A_x \\vu{x} + A_y \\vu{y} + A_z \\vu{z} The numbers A_x , A_y , and A_z are the \"components\" of A; geometrically, they are the projections of A along the three coordinate axes ( A_x = \\vec{A} \\cdot \\vu{x}, A_y = \\vec{A} \\cdot \\vu{y}, A_z = \\vec{A} \\cdot \\vu{z} ). We can now reformulate each of the four vector operations as a rule for manipulating components: \\vec{A} + \\vec{B} = (A_x \\vu{x} + A_y \\vu{y} + A_z \\vu{z}) + (B_x \\vu{x} + B_y \\vu{y} + B_z \\vu{z}) \\\\ = (A_x + B_x) \\vu{x} + (A_y + B_y) \\vu{y} + (A_z + B_z) \\vu{z} \\tag{1.7} Rule (i): To add vectors, add like components. a\\vec{A} = (a A_x) \\vu{x} + (a A_y) \\vu{y} + (a A_z)\\vu{z} \\tag{1.8} Rule (ii): To multiply by a scalar, multiply each component. Because \\vu{x}, \\vu{y} , and \\vu{z} are mutually perpendicular unit vectors \\vu{x} \\cdot \\vu{x} = \\vu{y} \\cdot \\vu{y} = \\vu{z} \\cdot \\vu{z} = 1; \\qquad \\vu{x} \\cdot \\vu{y} = \\vu{x} \\cdot \\vu{z} = \\vu{y} \\cdot \\vu{z} = 0 \\tag{1.9} Accordingly, \\vec{A} \\cdot \\vec{B} = (A_x \\vu{x} + A_y \\vu{y} + A_z \\vu{z}) \\cdot (B_x \\vu{x} + B_y \\vu{y} + B_z \\vu{z}) \\\\ = A_x B_x + A_y B_y + A_z B_z \\tag{1.10} Rule (iii): To calculate the dot product, multiply like components and add. In particular, \\vec{A} \\cdot \\vec{A} = A_x ^2 + A_y ^2 + A_z ^2 so A = \\sqrt{A_x ^2 + A_y ^2 + A_z ^2} \\tag{1.11} Similarly, \\begin{align} \\vu{x} \\cross \\vu{x} & = & \\vu{y} \\cross \\vu{y} & = & \\vu{z} \\cross \\vu{z} = 0 \\\\ \\vu{x} \\cross \\vu{y} & = & - \\vu{y} \\cross \\vu{x} & = & \\vu{z} \\\\ \\vu{y} \\cross \\vu{z} & = & - \\vu{z} \\cross \\vu{y} & = & \\vu{x} \\\\ \\vu{z} \\cross \\vu{x} & = & - \\vu{x} \\cross \\vu{z} & = & \\vu{y} \\end{align} Therefore, \\vec{A} \\cross \\vec{B} = (A_x \\vu{x} + A_y \\vu{y} + A_z \\vu{z}) \\cross (B_x \\vu{x} + B_y \\vu{y} + B_z \\vu{z}) \\\\ = (A_y B_z - A_z B_y) \\vu{x} + (A_z B_x - A_x B_z)\\vu{y} + (A_x B_y - A_y B_x) \\vu{z} \\tag{1.13} This cumbersome expression can be written more neatly as a determinant: \\vec{A} \\cross \\vec{B} = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ A_x & A_y & A_z \\\\ B_x & B_y & B_z \\end{vmatrix} Rule (iv): To calculate the cross product, form the determinant whose first row is \\vu{x}, \\vu{y}, \\vu{z} , whose second row is A, and whose third row is B.","title":"1.1.2: Vector Algebra: Component Form"},{"location":"ch1-1/#example-12","text":"Find the angle between the face diagonals of a cube. Solution We might as well use a cube of side 1, and place it as shown in Fig 1.10, with one corner at the origin. The face diagonals \\vec{A} and \\vec{B} are \\vec{A} = 1 \\vu{x} + 0 \\vu{y} + 1 \\vu{z}; \\qquad \\vec{B} = 0 \\vu{x} + 1 \\vu{y} + 1 \\vu{z} So, in component form, \\vec{A} \\cdot \\vec{B} = 1 \\cdot 0 + 0 \\cdot 1 + 1 \\cdot 1 = 1 On the other hand, in \"abstract\" form, \\vec{A} \\cdot \\vec{B} = AB \\cos \\theta = \\sqrt{2} \\sqrt{2} \\cos \\theta = 2 \\cos \\theta Therefore, \\cos \\theta = 1/2 \\quad \\text{ or } \\quad \\theta = 60^{\\circ} Of course, you can get the answer more easily by drawing in a diagonal across the top of the cube, completing the equilateral triangle. But in cases where the geometry is not so simple, this device of comparing the abstract and component forms of the dot product can be a very efficient means of finding angles.","title":"Example 1.2"},{"location":"ch1-1/#113-triple-products","text":"Since the cross product of two vectors is itself a vector, it can be dotted or crossed with a third vector to form a triple product. (i) Scalar triple product: \\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) . Geometrically, |\\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) | is the volume of the parallelpiped generated by A , B , and C , since |\\vec{B} \\cross \\vec{C}| is the area of the base, and | \\vec{A} \\cos \\theta | is the altitude (Fig. 1.12). Evidently, \\vec{A} \\cdot(\\vec{B} \\cross \\vec{C}) = \\vec{B} \\cdot (\\vec{C} \\cross \\vec{A}) = \\vec{C} \\cdot (\\vec{A} \\cross \\vec{B}) \\tagl{1.15} for they all correspond to the same figure. Note that \"alphabetical\" order is preserved - in view of \\eqref{1.6} , the \"nonalphabetical\" triple products \\vec{A} \\cdot(\\vec{C} \\cross \\vec{B}) = \\vec{B} \\cdot (\\vec{A} \\cross \\vec{C}) = \\vec{C} \\cdot (\\vec{B} \\cross \\vec{A}) have the opposite sign. In component form, \\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) = \\begin{vmatrix} A_x & A_y & A_z \\\\ B_x & B_y & B_z \\\\ C_x & C_y & C_z \\end{vmatrix} \\tagl{1.16} Note that the dot and cross can be interchanged: \\vec{A} \\cdot (\\vec{B} \\cross \\vec{C}) = (\\vec{A} \\cross \\vec{B}) \\cdot \\vec{C} (this follows immediately from Eq. 1.15); however, the placement of the parentheses is critical: (\\vec{A} \\cdot \\vec{B}) \\cdot \\vec{C} is a meaningless expression - you can't make a cross product from a scalar and a vector. (ii) Vector triple product: \\vec{A} \\cross (\\vec{B} \\cross \\vec{C}) . The vector triple product can be simplified by the so-called BAC-CAB rule: \\vec{A} \\cross (\\vec{B} \\cross \\vec{C}) = \\vec{B}(\\vec{A} \\cdot \\vec{C}) - \\vec{C} (\\vec{A} \\cdot \\vec{B}) \\tagl{1.17} Notice that (\\vec{A} \\cross \\vec{B}) \\cross \\vec{C} = - \\vec{C} \\cross (\\vec{A} \\cross \\vec{B}) = -\\vec{A}(\\vec{B} \\cdot \\vec{C}) + \\vec{B}(\\vec{A} \\cdot \\vec{C}) is an entirely different vector (cross-products are not associative). All higher vector products can be similarly reduced, often by repeated application of \\eqref{1.17} , so it is never necessary for an expression to contain more than one cross product in any term. For instance, (\\vec{A} \\cross \\vec{B}) \\cdot (\\vec{C} \\cross \\vec{D}) = (\\vec{A} \\cdot \\vec{C})(\\vec{B} \\cdot \\vec{D}) - (\\vec{A} \\cdot \\vec{D}) (\\vec{B} \\cdot \\vec{C}) \\vec{A} \\cross [\\vec{B} \\cross (\\vec{C} \\cross \\vec{D})] = \\vec{B}[ \\vec{A} \\cdot (\\vec{C} \\cross \\vec{D})] - (\\vec{A} \\cdot \\vec{B})(\\vec{C} \\cross \\vec{D}) \\tagl{1.18}","title":"1.1.3: Triple Products"},{"location":"ch1-1/#114-position-displacement-and-separation-vectors","text":"The location of a point in three dimensions can be described by listing its Cartesian coordinates (x, y, z). The vector to that point from the origin ( \\mathscr{O} ) is called the position vector (Fig 1.13): \\vec{r} \\equiv x \\vu{x} + y \\vu{y} + z \\vu{z} \\tagl{1.19} I will reserve the letter \\vec{r} for this purpose. Its magnitude, r = \\sqrt{x^2 + y^2 + z^2} \\tagl{1.20} is the distance from the origin, and \\vu{r} = \\frac{\\vec{r}}{r} = \\frac{x \\vu{x} + y \\vu{y} + z \\vu{z}}{ \\sqrt{x^2 + y^2 + z^2}} \\tagl{1.21} is a unit vector pointing radially outward. The infinitesimal displacement vector from (x, y, z) to x + \\dd{x}, y + \\dd{y}, z + \\dd{z} is \\dd{\\vec{l}} = \\dd{x} \\vu{x} + \\dd{y} \\vu{y} + \\dd{z} \\vu{z} \\tagl{1.22} (We could call this \\dd{\\vec{r}} , since that's what it is, but it is useful to have a special notation for infinitesimal displacements.) In electrodynamics, one frequently encounters problems involving two points - typically a source point , \\vec{r'} , where an electric charge is located, and a field point \\vec{r} at which you are calculating the electric or magnetic field (Fig 1.14). It pays to adopt right from the start some short-hand notation for the separation vector from the source point to the field point. I shall use for this purpose the letter \\gr : \\vec{\\gr} \\equiv \\vec{r} - \\vec{r'} \\tagl{1.23} Its magnitude is |\\gr| = | \\vu{r} - \\vu{r'} | \\tagl{1.24} and a unit vector in the direction from \\vec{r'} to \\vec{r} is \\vu{\\gr} = \\frac{\\gr}{|\\gr|} = \\frac{\\vec{r} - \\vec{r'}}{|\\vec{r} - \\vec{r'}|} \\tagl{1.25} In Cartesian coordinates, \\gr = (x - x') \\vu{x} + (y-y') \\vu{y} + (z-z') \\vu{z} \\tagl{1.26} |\\gr| = \\sqrt{(x - x')^2 + (y-y')^2 + (z-z')^2 } \\tagl{1.27} \\vu{\\gr} = \\frac{(x - x') \\vu{x} + (y-y') \\vu{y} + (z-z') \\vu{z}}{\\sqrt{(x - x')^2 + (y-y')^2 + (z-z')^2 }} (from which you can appreciate the economy of the \\gr notation).","title":"1.1.4: Position, Displacement, and Separation Vectors"},{"location":"ch1-1/#115-how-vectors-transform","text":"The definition of a vector as \"a quantity with a magnitude and direction\" is not altogether satisfactory: What precisely does \"direction\" mean? This may seem a pedantic question, but we shall soon encounter a species of derivative that looks rather like a vector, and we'll want to know for sure whether it is one. You might be inclined to say that a vector is anything that has three components that combine properly under addition. Well, how about this: We have a barrel of fruit that contains N_x pears, N_y apples, and N_z bananas. Is \\vec{N} = N_x \\vu{x} + N_y \\vu{y} + N_z \\vu{z} a vector? It has three components, and when you add another barrel with M_x pears, M_y apples, and M_z bananas the result is N_x + M_x pears, N_y + M_y apples, N_z + M_z bananas. So it does add like a vector. Yet it's obviously not a vector, in the physicist's sense of the word, because it doesn't really have a direction. What exactly is wrong with it? The answer is that \\vec{N} does not transform properly when you change coordinates . The coordinate frame we use to describe positions in space is of course entirely arbitrary, but there is a specific geometrical transformation law for converting vector components from one frame to another. Suppose, for instance, the \\overline{x}, \\overline{y}, \\overline{z} system is rotated by angle \\phi , relative to x, y, z , about the common x = \\overline{x} axes. From Fig. 1.15, A_y = A \\cos \\theta, \\qquad A_z = A \\sin \\theta while \\begin{align*} \\overline{A_y} & = A \\cos \\overline{\\theta} = A \\cos (\\theta - \\phi) = A (\\cos \\theta \\cos \\phi + \\sin \\theta \\sin \\phi) \\\\ & = \\cos \\phi A_y + \\sin \\phi A_z \\\\ \\overline{A_z} & = A \\sin \\overline{\\theta} = A \\sin (\\theta - \\phi) = A (\\sin \\theta \\cos \\phi - \\cos \\theta \\sin \\phi) \\\\ & = - \\sin \\phi A_y + \\cos \\phi A_z \\end{align*} We might express this conclusion in matrix notation: \\begin{pmatrix} \\overline{A_y} \\\\ \\overline{A_z} \\end{pmatrix} = \\begin{pmatrix} \\cos \\phi & \\sin \\phi \\\\ - \\sin \\phi & \\cos \\phi \\end{pmatrix} \\begin{pmatrix} A_y \\\\ A_z \\end{pmatrix} \\tagl{1.29} More generally, for rotation about an arbitrary axis in three dimensions, the transformation law takes the form \\begin{pmatrix} \\overline{A_x} \\\\ \\overline{A_y} \\\\ \\overline{A_z} \\end{pmatrix} = \\begin{pmatrix} R_{xx} & R_{xy} & R_{xz} \\\\ R_{yx} & R_{yy} & R_{yz} \\\\ R_{zx} & R_{zy} & R_{zz} \\end{pmatrix} \\begin{pmatrix} A_x \\\\ A_y \\\\ A_z \\end{pmatrix} \\tagl{1.30} or, more compactly, \\overline{A_i} = \\sum_{j=1}^3 R_{ij} A_j \\tagl{1.31} where index 1 stands for x, 2 for y, and 3 for z. The elements of the matrix R can be ascertained, for a given rotation, by the same sort of trigonometric arguments as we used for a rotation about the x axis. Now: Do the components of \\vec{N} transform this way? Of course not - it doesn't matter what coordinates you use to represent positions in space; there are still just as many apples in the barrel. You can't convert a pear into a banana by choosing a different set of axes, but you can turn in A_x into \\overline{A_y} . Formally, then, a vector is any set of three components that transforms in the same manner as a displacement when you change coordinates . As always, displacement is the model for the behavior of vectors. By the way, a (second-rank) tensor is a quantity with nine components, T_{xx}, T_{xy}, T_{xz}, T_{yx}, \\ldots T_{zz} which transform with two factors of R : \\begin{align*} \\overline{T}_{xx} & = R_{xx}(R_{xx} T_{xx} + R_{xy} T_{xy} + R_{xz} T_{xz}) \\\\ & + R_{xy}(R_{xx} T_{yx} + R_{xy} T_{yy} + R_{xz} T_{yz}) \\\\ & + R_{xz}(R_{xx} T_{zx} + R_{xy} T_{zy} + R_{xz} T_{zz}), \\ldots \\end{align*} or, more compactly, \\overline{T}_{ij} = \\sum_{k=1}^3 \\sum_{l=1} ^3 R_{ik} R_{jl} T_{kl} \\tagl{1.32} In general, an n-th rank tensor has n indices and 3^n components, and transforms with n factors of R . In this hierarchy, a vector is a tensor of rank 1, and a scalar is a tensor of rank zero.","title":"1.1.5: How Vectors Transform"},{"location":"ch1-2/","text":"1.2: Differential Calculus 1.2.1: \"Ordinary\" Derivatives Suppose we have a function of one variable, f(x) . Question : what does the derivative \\dv{f}{x} do for us? Answer : It tells us how rapidly the function f(x) varies when we change the argument x by a tiny amount, \\dd{x} \\dd{f} = \\left( \\dv{f}{x} \\right) \\dd{x} \\tagl{1.33} In words: If we increment x by an infinitesimal amount \\dd{x} , then f changes by an amount \\dd{f} ; the derivative is the proportionality factor. Foe example, in Fig. 1.17(a), the function varies slowly with x , and the derivative is correspondingly small. In Fig 1.17(b), f increases rapidly with x , and the derivative is large as you move away from x = 0 . Geometrical interpretation : The derivative \\dv{f}{x} is the slope of the graph of f versus x . 1.2.2: Gradient Suppose, now, that we have a function of three variables-say, the temperature T (x, y, z) in this room. (Start out in one comer, and set up a system of axes; then for each point (x, y, z) in the room, T gives the temperature at that spot.) We want to generalize the notion of \"derivative\" to functions like T, which depend not on one but on three variables. A derivative is supposed to tell us how fast the function varies, if we move a little distance. But this time the situation is more complicated, because it depends on what direction we move: If we go straight up, then the temperature will prob- ably increase fairly rapidly, but if we move horizontally, it may not change much at all. In fact, the question \"How fast does T vary?\" has an infinite number of answers, one for each direction we might choose to explore. Fortunately, the problem is not as bad as it looks. A theorem on partial derivatives states that \\dd{T} = \\left( \\pdv{T}{x} \\right)\\dd{x} + \\left( \\pdv{T}{y} \\right) \\dd{y} + \\left( \\pdv{T}{z} \\right) \\dd{z} \\tagl{1.34} This tells us how T changes when we alter all three variables by the infinitesimal amounts dx, dy, dz. Notice that we do not require an infinite number of derivatives - three will suffice: the partial derivatives along each of the three coordinate directions. \\eqref{1.34} is reminiscent of a dot product: \\dd{T} = \\left( \\pdv{T}{x} \\vu{x} + \\pdv{T}{y} \\vu{y} + \\pdv{T}{z} \\vu{z} \\right)\\cdot ( \\dd{x} \\vu{x} + \\dd{y} \\vu{y} + \\dd{z} \\vu{z} \\\\ = (\\grad{T}) \\cdot (\\dd{\\vec{l}}) \\tagl{1.35} where \\grad{T} \\equiv \\pdv{T}{x} \\vu{x} + \\pdv{T}{y} \\vu{y} + \\pdv{T}{z} \\vu{z} \\tagl{1.36} is the gradient of T. Note that \\grad{T} is a vector quantity, with three components; it is the generalized derivative we have been looking for. \\eqref{1.35} is the three-dimensional version of \\eqref{1.33} . Geometrical interpretation of the Gradient : Like any vector, the gradient has magnitude and direction . To determine its geometrical meaning, let's rewrite the dot product using Eq. 1.1 \\dd{T} = \\grad{T} \\cdot \\dd{\\vec{l}} = |\\grad{T}| |\\dd{\\vec{l}}| \\cos \\theta \\tagl{1.37} where \\theta is the angle between \\grad{T} and \\dd{\\vec{l}} . Now if we fix the magnitude |\\dd{\\vec{l}}| and search around in various directions, the maximum change in T evidently occurs when \\theta = 0 (for then \\cos \\theta = 1 ). That is, for a fixed distance, dT is greatest when I move in the same direction as \\grad{T} . Thus: The gradient \\grad{T} points in the direction of maximum increase of the function T. Moreover: The magnitude | \\grad{T} | gives the slope (rate of increase) along this maximal direction Imagine you are standing on a hillside. Look all around you, and find the direction of steepest ascent. That is the direction of the gradient. Now measure the slope in that direction (rise over run). That is the magnitude of the gradient. (Here the function we're talking about is the height of the hill, and the coordinates it depends on are positions-latitude and longitude, say. This function depends on only two variables, not three, but the geometrical meaning of the gradient is easier to grasp in two dimensions.) Notice from Eq. 1.37 that the direction of maximum descent is opposite to the direction of maximum ascent, while at right angles (\\theta = 90^{\\circ}) the slope is zero (the gradient is perpendicular to the contour lines). You can conceive of surfaces that do not have these properties, but they always have \"kinks\" in them, and correspond to non-differentiable functions. What would it mean for the gradient to vanish? If \\grad{T} = 0 at (x, y, z), then \\dd{T} = 0 for small displacements about the point (x, y, z). This is, then, a stationary point of the function T(x, y, z). It could be a maximum (a summit), a minimum (a valley), a saddle point (a pass), or a \"shoulder.\" This is analogous to the situation for functions of one variable, where a vanishing derivative signals a maximum, a minimum, or an inflection. In particular, if you want to locate the extrema of a function of three variables, set its gradient equal to zero. Example 1.3 Find the gradient of r = \\sqrt{x^2 + y^2 + z^2} (the magnitude of the position vector) Solution \\begin{align*} \\grad{r} & = \\pdv{r}{x} \\vu{x} + \\pdv{r}{y} \\vu{y} + \\pdv{r}{z} \\vu{z} \\\\ & = \\frac{1}{2} \\frac{2x}{\\sqrt{x^2 + y^2 + z^2}}\\vu{x} + \\frac{1}{2} \\frac{2y}{\\sqrt{x^2 + y^2 + z^2}}\\vu{y} + \\frac{1}{2} \\frac{2z}{\\sqrt{x^2 + y^2 + z^2}}\\vu{z} \\\\ & = \\frac{x \\vu{x} + y \\vu{y} + z \\vu{z}}{\\sqrt{x^2 + y^2 + z^2}} = \\frac{\\vec{r}}{r} = \\vu{r} \\end{align*} Does this makes sense? Well, it says that the distance from the origin increases most rapidly in the radial direction, and that its rate of increase in that direction is 1... just what you'd expect. 1.2.3: The Del Operator The gradient has the formal appearance of a vector, \\nabla , \"multiplying\" a scalar T: \\grad{T} = \\left( \\vu{x} \\pdv{}{x} + \\vu{y} \\pdv{}{y} + \\vu{z} \\pdv{}{z} \\right) T \\tagl{1.38} (For once, I write the unit vectors to the left, just so no one will think that this means \\pdv{\\vu{x}}{x} and so on, which would be zero since the coordinate directions are constant.) The term in parentheses is called del : \\grad{} = \\vu{x} \\pdv{}{x} + \\vu{y} \\pdv{}{y} + \\vu{z} \\pdv{}{z} \\tagl{1.39} Of course, del is not a vector, in the usual sense. Indeed, it doesn't mean much until we provide it with a function to act upon. Furthermore, it does not \"multiply\" T; rather, it is an instruction to differentiate what follows. To be precise, then, we say that \\grad{} is a vector operator that acts upon T, not a vector that multiplies T. With this qualification, though, \\grad{} mimics the behavior of an ordinary vector in virtually every way; almost anything that can be done with other vectors can also be done with \\grad{} , if we merely translate \"multiply\" by \"act upon.\" So by all means take the vector appearance of \\grad{} seriously: it is a marvelous piece of notational simplification, as you will appreciate if you ever consult Maxwell's original work on electromagnetism, written without the benefit of \\grad{} . Now, an ordinary vector \\vec{A} can multiply in three ways: By a scalar a : \\vec{A}a By a vector \\vec{B} , via the dot product: \\vec{A} \\cdot \\vec{B} By a vector \\vec{B} , via the cross product: \\vec{A} \\cross \\vec{B} Correspondingly, there are three ways the operator \\grad{} can act: On a scalar function T: \\grad{T} (the gradient) On a vector function \\vec{v} , via the dot product: \\div{\\vec{v}} (the divergence ) On a vector function \\vec{v} , via the cross product: \\curl{\\vec{v}} (the curl ) We have already discussed the gradient. In the following sections we examine the other two vector derivatives: divergence and curl. 1.2.4: The Divergence From the definition of \\grad{} we construct the divergence: \\begin{align*} \\div{\\vec{v}} & = \\left( \\vu{x} \\pdv{}{x} + \\vu{y} \\pdv{}{y} + \\vu{z} \\pdv{}{z} \\right) \\cdot (v_x \\vu{x} + v_y \\vu{y} + v_z \\vu{z}) \\\\ & = \\pdv{v_x}{x} + \\pdv{v_y}{y} + \\pdv{v_z}{z} \\tagl{1.40} \\end{align*} Observe that the divergence of a vector function is itself a scalar. Geometrical interpretation : The name divergence is well chosen, for \\div{\\vec{v}} is a measure of how much the vector \\vec{v} spreads out (diverges) from the point in question. For example, the vector function in Fig. 1.18a has a large (positive) divergence (if the arrows pointed in, it would be a negative divergence), the function in Fig. 1.18b has zero divergence, and the function in Fig. 1.18c again has a positive divergence. (Please understand that \\vec{v} here is a function - there's a different vector associated with every point in space. In the diagrams, of course, I can only draw the arrows at a few representative locations.) Imagine standing at the edge of a pond. Sprinkle some sawdust or pine needles on the surface. If the material spreads out, then you dropped it at a point of positive divergence; if it collects together, you dropped it at a point of negative divergence. (The vector function \\vec{v} in this model is the velocity of the water at the surface - this is a two-dimensional example, but it helps give one a \"feel\" for what the divergence means. A point of positive divergence is a source, or \"faucet\"; a point of negative divergence is a sink, or \"drain.\") Example 1.4 Suppose the functions in Fig 1.18 are \\vec{v_a} = \\vec{r} = x \\vu{x} + y \\vu{y} + z \\vu{z} , \\vec{v_b} = \\vu{z} , and \\vec{v_c} = z\\vu{z} . Calculate their divergences. Solution \\div{\\vec{v_a}} = \\pdv{}{x} (x) + \\pdv{}{y} (x) + \\pdv{}{z} (z) = 1 + 1 + 1 = 3 As anticipated, this function has a positive divergence. \\div{\\vec{v_b}} = \\pdv{}{x} (0) + \\pdv{}{0} (x) + \\pdv{}{z} (1) = 0 + 0 + 0 = 0 as expected. \\div{\\vec{v_b}} = \\pdv{}{x} (0) + \\pdv{}{0} (x) + \\pdv{}{z} (z) = 0 + 0 + 1 = 1 1.2.5: The Curl From the definition of \\grad{} we construct the curl: \\begin{align*} \\curl{\\vec{v}} & = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ \\pdv{}{x} & \\pdv{}{y} & \\pdv{}{z} \\\\ v_x & v_y & v_z \\end{vmatrix} \\\\ & + \\vu{x} \\left( \\pdv{v_z}{y} - \\pdv{v_y}{z} \\right) + \\vu{y} \\left( \\pdv{v_x}{z} - \\pdv{v_z}{x} \\right) + \\vu{z} \\left( \\pdv{v_y}{x} - \\pdv{v_x}{y} \\right) \\tagl{1.41} \\end{align*} Notice that the curl of a vector function is, like any cross product, a vector. Geometrical Interpretation : The name curl is also well chosen, for \\curl{\\vec{v}} is a measure of how much the vector \\vec{v} swirls around the point in question. Thus the three functions in Fig. 1.18 all have zero curl (as you can easily check for yourself), whereas the functions in Fig. 1.19 have a substantial curl, pointing in the z direction, as the natural right-hand rule would suggest. Imagine (again) you are standing at the edge of a pond. Float a small paddlewheel (a cork with toothpicks pointing out radially would do); if it starts to rotate, then you placed it at a point of nonzero curl. A whirlpool would be a region of large curl. Example 1.5 Suppose the function sketched in Fig 1.19a is \\vec{v_a} = -y \\vu{x} + x \\vu{y} , and that in Fig 1.19b is \\vec{v_b} = x \\vu{y} . Calculate their curls. Solution \\curl{\\vec{v_a}} = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ \\pdv{}{x} & \\pdv{}{y} & \\pdv{}{z} \\\\ -y & x & 0 \\end{vmatrix} = 2 \\vu{z} and \\curl{\\vec{v_b}} = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ \\pdv{}{x} & \\pdv{}{y} & \\pdv{}{z} \\\\ 0 & x & 0 \\end{vmatrix} = \\vu{z} As expected, these curls point in the +z direction. (Incidentally, they both have zero divergence, as you might guess from the pictures: nothing is \"spreading out\"... it just \"swirls around.\") 1.2.6: Product Rules The calculation of ordinary derivatives is facilitated by a number of rules, such as the sum rule \\dv{}{x} (f + g) = \\dv{f}{x} + \\dv{g}{x} the rule for multiplying a constant: \\dv{}{x} (kf) = k \\dv{f}{x} the product rule: \\dv{}{x}(fg) = f \\dv{g}{x} + g \\dv{f}{x} and the quotient rule \\dv{}{x} \\left( \\frac{f}{g} \\right) = \\frac{g \\dv{f}{x} - f \\dv{g}{x}}{g^2} Similar relations hold for the vector derivatives. Thus, \\grad{(f + g)} = \\grad{f} + \\grad{g}, \\qquad \\div{(\\vec{A} + \\vec{B})} = (\\div{\\vec{A}}) + (\\div{\\vec{B}}) \\curl{(\\vec{A} + \\vec{B})} = (\\curl{\\vec{A}}) + (\\curl{\\vec{B}}) and \\grad{(kf)} = k \\grad f, \\quad \\div{(k\\vec{A})} = k (\\div{\\vec{A}}), \\quad \\curl{(k\\vec{A})} = k(\\curl{\\vec{A}}) as you can check for yourself. The product rules are not quite so simple. There are two ways to construct a scalar as the product of two functions: fg \\quad \\text{(product of two scalar functions),} \\\\ \\vec{A} \\cdot \\vec{B} \\quad \\text{(dot product of two vector functions),} and two ways to make a vector: f \\vec{A} \\quad \\text{(scalar times vector),} \\\\ \\vec{A} \\cross \\vec{B} \\quad \\text{(cross product of two vectors).} Accordingly, there are six product rules, two for gradients: \\grad{(fg)} = f \\grad{g} + g \\grad{f} \\tag{i} \\grad( \\vec{A} \\cdot \\vec{B}) = \\vec{A} \\cross (\\curl{\\vec{B}}) + \\vec{B} \\cross (\\curl{\\vec{A}}) + (\\vec{A} \\cdot \\grad{})\\vec{B} + (\\vec{B} \\cdot \\grad{}) \\vec{A} \\tag{ii} two for divergences: \\div{(f\\vec{A})} = f(\\div{\\vec{A}}) + \\vec{A} \\cdot (\\grad{f}) \\tag{iii} \\div{(\\vec{A} \\cross \\vec{B})} = \\vec{B} \\cdot (\\curl{\\vec{A}}) - \\vec{A} \\cdot (\\curl{\\vec{B}}) \\tag{iv} and two for curls: \\curl{(f\\vec{A})} = f(\\curl{\\vec{A}}) - \\vec{A} \\cross (\\grad{f}) \\tag{v} \\curl{(\\vec{A} \\cross \\vec{B})} = (\\vec{B} \\cdot \\grad{})\\vec{A} - (\\vec{A} \\cdot \\grad{}) \\vec{B} + \\vec{A}(\\div{\\vec{B}}) - \\vec{B}(\\div{\\vec{A}}) \\tag{vi} If there's anything in this chapter that's worth memorizing, it is this set of identities. The proofs come straight from the product rule for ordinary derivatives. For instance, \\begin{align*} \\div{(f\\vec{A})} & = \\pdv{}{x} (f A_x) + \\pdv{}{y} (fA_y) + \\pdv{}{z}(f A_z) \\\\ & = \\left( \\pdv{f}{x} A_x + f \\pdv{A_x}{x} \\right) + \\left( \\pdv{f}{y} A_y + f \\pdv{A_y}{y} \\right) + \\left( \\pdv{f}{z}A_z + f \\pdv{A_z}{z} \\right) \\\\ & = (\\grad{f}) \\cdot \\vec{A} + f(\\div{\\vec{A}}) \\end{align*} It is also possible to formulate three quotient rules: \\grad \\left( \\frac{f}{g} \\right) = \\frac{g \\grad{f} - f \\grad{g}}{g^2} \\div{\\left( \\frac{\\vec{A}}{g} \\right)} = \\frac{g(\\div{\\vec{A}}) - \\vec{A} \\cdot (\\grad{g})}{g^2} \\curl \\left( \\frac{\\vec{A}}{g} \\right) = \\frac{g(\\curl{\\vec{A}}) + \\vec{A} \\cross (\\grad{g})}{g^2} However, since these can be obtained quickly from the corresponding product rules, there is no point in listing them separately. 1.2.7: Second Derivatives The gradient, the divergence, and the curl are the only first derivatives we can make with \\grad ; by applying \\grad twice, we can construct five species of second derivatives. The gradient \\grad{T} is a vector, so we can take the divergence and curl of it: Divergence of gradient: \\div (\\grad{T}) Curl of gradient: \\curl (\\grad{T}) The divergence \\div{\\vec{v}} is a scalar - all we can do is take its gradient: Gradient of divergence: \\grad (\\div{\\vec{v}}) The curl \\curl \\vec{v} is a vector, so we can take its divergence and curl: Divergence of curl: \\div (\\curl \\vec{v}) Curl of curl: \\curl (\\curl \\vec{v}) This exhausts the possibilities, and in fact not all of them give anything new. Let's consider them one at a time: \\begin{align*} \\div (\\grad{T}) & = \\left( \\vu{x} \\pdv{}{x} + \\vu{y} \\pdv{}{y} + \\vu{z} \\pdv{}{z} \\right) \\cdot \\left( \\pdv{T}{x} \\vu{x} + \\pdv{T}{y} \\vu{y} + \\pdv{T}{z} \\vu{z} \\right) \\\\ & = \\frac{\\partial ^2 T}{\\partial x^2} + \\frac{\\partial ^2 T}{\\partial y^2} + \\frac{\\partial ^2 T}{\\partial z^2} \\end{align*} \\tagl{1.42} This object, which we write as \\laplacian T for short, is called the Laplacian of T; we shall be studying it in great detail later on. Notice that the Laplacian of a scalar T is a scalar. Occasionally we shall speak of the laplacian of a vector, \\laplacian \\vec{v} . By this we mean a vector quantity whose x-component is the Laplacian of v_x , and so in: \\laplacian \\vec{v} \\equiv (\\laplacian v_x) \\vu{x} + (\\laplacian v_y) \\vu{y} + (\\laplacian v_z) \\vu{z} \\tagl{1.43} This is nothing more than a convenient extension of the meaning of \\laplacian . The curl of a gradient is always zero : \\curl (\\grad{T}) = 0 \\tagl{1.44} This is an important fact, which we shall use repeatedly; you can easily prove it from the definition of \\grad , hinging on the equality of cross-derivatives: \\pdv{}{x} \\left( \\pdv{T}{y} \\right) = \\pdv{}{y} \\left( \\pdv{T}{x} \\right) \\tagl{1.45} \\grad(\\div{\\vec{v}}) seldom occurs in physical applications, and it has not been given any special name of its own - it's just the gradient of the divergence . Notice that \\grad (\\div \\vec{v}) is not the same as the Laplacian of a vector: \\laplacian \\vec{v} = (\\div \\grad) \\vec{v} \\neq \\grad( \\div \\vec{v}) . The divergence of a curl, like the curl of a gradient, is always zero: \\div (\\curl \\vec{v}) = 0 \\tagl{1.46} You can prove this for yourself. As you can check from the definition of \\grad : \\curl (\\curl \\vec{v}) = \\grad(\\div \\vec{v}) - \\laplacian \\vec{v} \\tagl{1.47} So curl-of-curl gives nothing new; the first term is just gradient of divergence, and the second is the Laplacian (of a vector). (In fact, this is often used to define the Laplacian of a vector, in preference to \\eqref{1.43} which makes explicit reference to Cartesian coordinates.) Really, then, there are just two kinds of second derivatives: the Laplacian (which is of fundamental importance) and the gradient-of-divergence (which we seldom encounter). We could go through a similar ritual to work out third derivatives, but fortunately second derivatives suffice for practically all physical applications. A final word on vector differential calculus: It all flows from the operator \\grad , and from taking seriously its vectorial character. Even if you remembered only the definition of \\grad , you could easily reconstruct all the rest.","title":"1.2 - Differential Calculus"},{"location":"ch1-2/#12-differential-calculus","text":"","title":"1.2: Differential Calculus"},{"location":"ch1-2/#121-ordinary-derivatives","text":"Suppose we have a function of one variable, f(x) . Question : what does the derivative \\dv{f}{x} do for us? Answer : It tells us how rapidly the function f(x) varies when we change the argument x by a tiny amount, \\dd{x} \\dd{f} = \\left( \\dv{f}{x} \\right) \\dd{x} \\tagl{1.33} In words: If we increment x by an infinitesimal amount \\dd{x} , then f changes by an amount \\dd{f} ; the derivative is the proportionality factor. Foe example, in Fig. 1.17(a), the function varies slowly with x , and the derivative is correspondingly small. In Fig 1.17(b), f increases rapidly with x , and the derivative is large as you move away from x = 0 . Geometrical interpretation : The derivative \\dv{f}{x} is the slope of the graph of f versus x .","title":"1.2.1: \"Ordinary\" Derivatives"},{"location":"ch1-2/#122-gradient","text":"Suppose, now, that we have a function of three variables-say, the temperature T (x, y, z) in this room. (Start out in one comer, and set up a system of axes; then for each point (x, y, z) in the room, T gives the temperature at that spot.) We want to generalize the notion of \"derivative\" to functions like T, which depend not on one but on three variables. A derivative is supposed to tell us how fast the function varies, if we move a little distance. But this time the situation is more complicated, because it depends on what direction we move: If we go straight up, then the temperature will prob- ably increase fairly rapidly, but if we move horizontally, it may not change much at all. In fact, the question \"How fast does T vary?\" has an infinite number of answers, one for each direction we might choose to explore. Fortunately, the problem is not as bad as it looks. A theorem on partial derivatives states that \\dd{T} = \\left( \\pdv{T}{x} \\right)\\dd{x} + \\left( \\pdv{T}{y} \\right) \\dd{y} + \\left( \\pdv{T}{z} \\right) \\dd{z} \\tagl{1.34} This tells us how T changes when we alter all three variables by the infinitesimal amounts dx, dy, dz. Notice that we do not require an infinite number of derivatives - three will suffice: the partial derivatives along each of the three coordinate directions. \\eqref{1.34} is reminiscent of a dot product: \\dd{T} = \\left( \\pdv{T}{x} \\vu{x} + \\pdv{T}{y} \\vu{y} + \\pdv{T}{z} \\vu{z} \\right)\\cdot ( \\dd{x} \\vu{x} + \\dd{y} \\vu{y} + \\dd{z} \\vu{z} \\\\ = (\\grad{T}) \\cdot (\\dd{\\vec{l}}) \\tagl{1.35} where \\grad{T} \\equiv \\pdv{T}{x} \\vu{x} + \\pdv{T}{y} \\vu{y} + \\pdv{T}{z} \\vu{z} \\tagl{1.36} is the gradient of T. Note that \\grad{T} is a vector quantity, with three components; it is the generalized derivative we have been looking for. \\eqref{1.35} is the three-dimensional version of \\eqref{1.33} . Geometrical interpretation of the Gradient : Like any vector, the gradient has magnitude and direction . To determine its geometrical meaning, let's rewrite the dot product using Eq. 1.1 \\dd{T} = \\grad{T} \\cdot \\dd{\\vec{l}} = |\\grad{T}| |\\dd{\\vec{l}}| \\cos \\theta \\tagl{1.37} where \\theta is the angle between \\grad{T} and \\dd{\\vec{l}} . Now if we fix the magnitude |\\dd{\\vec{l}}| and search around in various directions, the maximum change in T evidently occurs when \\theta = 0 (for then \\cos \\theta = 1 ). That is, for a fixed distance, dT is greatest when I move in the same direction as \\grad{T} . Thus: The gradient \\grad{T} points in the direction of maximum increase of the function T. Moreover: The magnitude | \\grad{T} | gives the slope (rate of increase) along this maximal direction Imagine you are standing on a hillside. Look all around you, and find the direction of steepest ascent. That is the direction of the gradient. Now measure the slope in that direction (rise over run). That is the magnitude of the gradient. (Here the function we're talking about is the height of the hill, and the coordinates it depends on are positions-latitude and longitude, say. This function depends on only two variables, not three, but the geometrical meaning of the gradient is easier to grasp in two dimensions.) Notice from Eq. 1.37 that the direction of maximum descent is opposite to the direction of maximum ascent, while at right angles (\\theta = 90^{\\circ}) the slope is zero (the gradient is perpendicular to the contour lines). You can conceive of surfaces that do not have these properties, but they always have \"kinks\" in them, and correspond to non-differentiable functions. What would it mean for the gradient to vanish? If \\grad{T} = 0 at (x, y, z), then \\dd{T} = 0 for small displacements about the point (x, y, z). This is, then, a stationary point of the function T(x, y, z). It could be a maximum (a summit), a minimum (a valley), a saddle point (a pass), or a \"shoulder.\" This is analogous to the situation for functions of one variable, where a vanishing derivative signals a maximum, a minimum, or an inflection. In particular, if you want to locate the extrema of a function of three variables, set its gradient equal to zero.","title":"1.2.2: Gradient"},{"location":"ch1-2/#example-13","text":"Find the gradient of r = \\sqrt{x^2 + y^2 + z^2} (the magnitude of the position vector) Solution \\begin{align*} \\grad{r} & = \\pdv{r}{x} \\vu{x} + \\pdv{r}{y} \\vu{y} + \\pdv{r}{z} \\vu{z} \\\\ & = \\frac{1}{2} \\frac{2x}{\\sqrt{x^2 + y^2 + z^2}}\\vu{x} + \\frac{1}{2} \\frac{2y}{\\sqrt{x^2 + y^2 + z^2}}\\vu{y} + \\frac{1}{2} \\frac{2z}{\\sqrt{x^2 + y^2 + z^2}}\\vu{z} \\\\ & = \\frac{x \\vu{x} + y \\vu{y} + z \\vu{z}}{\\sqrt{x^2 + y^2 + z^2}} = \\frac{\\vec{r}}{r} = \\vu{r} \\end{align*} Does this makes sense? Well, it says that the distance from the origin increases most rapidly in the radial direction, and that its rate of increase in that direction is 1... just what you'd expect.","title":"Example 1.3"},{"location":"ch1-2/#123-the-del-operator","text":"The gradient has the formal appearance of a vector, \\nabla , \"multiplying\" a scalar T: \\grad{T} = \\left( \\vu{x} \\pdv{}{x} + \\vu{y} \\pdv{}{y} + \\vu{z} \\pdv{}{z} \\right) T \\tagl{1.38} (For once, I write the unit vectors to the left, just so no one will think that this means \\pdv{\\vu{x}}{x} and so on, which would be zero since the coordinate directions are constant.) The term in parentheses is called del : \\grad{} = \\vu{x} \\pdv{}{x} + \\vu{y} \\pdv{}{y} + \\vu{z} \\pdv{}{z} \\tagl{1.39} Of course, del is not a vector, in the usual sense. Indeed, it doesn't mean much until we provide it with a function to act upon. Furthermore, it does not \"multiply\" T; rather, it is an instruction to differentiate what follows. To be precise, then, we say that \\grad{} is a vector operator that acts upon T, not a vector that multiplies T. With this qualification, though, \\grad{} mimics the behavior of an ordinary vector in virtually every way; almost anything that can be done with other vectors can also be done with \\grad{} , if we merely translate \"multiply\" by \"act upon.\" So by all means take the vector appearance of \\grad{} seriously: it is a marvelous piece of notational simplification, as you will appreciate if you ever consult Maxwell's original work on electromagnetism, written without the benefit of \\grad{} . Now, an ordinary vector \\vec{A} can multiply in three ways: By a scalar a : \\vec{A}a By a vector \\vec{B} , via the dot product: \\vec{A} \\cdot \\vec{B} By a vector \\vec{B} , via the cross product: \\vec{A} \\cross \\vec{B} Correspondingly, there are three ways the operator \\grad{} can act: On a scalar function T: \\grad{T} (the gradient) On a vector function \\vec{v} , via the dot product: \\div{\\vec{v}} (the divergence ) On a vector function \\vec{v} , via the cross product: \\curl{\\vec{v}} (the curl ) We have already discussed the gradient. In the following sections we examine the other two vector derivatives: divergence and curl.","title":"1.2.3: The Del Operator"},{"location":"ch1-2/#124-the-divergence","text":"From the definition of \\grad{} we construct the divergence: \\begin{align*} \\div{\\vec{v}} & = \\left( \\vu{x} \\pdv{}{x} + \\vu{y} \\pdv{}{y} + \\vu{z} \\pdv{}{z} \\right) \\cdot (v_x \\vu{x} + v_y \\vu{y} + v_z \\vu{z}) \\\\ & = \\pdv{v_x}{x} + \\pdv{v_y}{y} + \\pdv{v_z}{z} \\tagl{1.40} \\end{align*} Observe that the divergence of a vector function is itself a scalar. Geometrical interpretation : The name divergence is well chosen, for \\div{\\vec{v}} is a measure of how much the vector \\vec{v} spreads out (diverges) from the point in question. For example, the vector function in Fig. 1.18a has a large (positive) divergence (if the arrows pointed in, it would be a negative divergence), the function in Fig. 1.18b has zero divergence, and the function in Fig. 1.18c again has a positive divergence. (Please understand that \\vec{v} here is a function - there's a different vector associated with every point in space. In the diagrams, of course, I can only draw the arrows at a few representative locations.) Imagine standing at the edge of a pond. Sprinkle some sawdust or pine needles on the surface. If the material spreads out, then you dropped it at a point of positive divergence; if it collects together, you dropped it at a point of negative divergence. (The vector function \\vec{v} in this model is the velocity of the water at the surface - this is a two-dimensional example, but it helps give one a \"feel\" for what the divergence means. A point of positive divergence is a source, or \"faucet\"; a point of negative divergence is a sink, or \"drain.\")","title":"1.2.4: The Divergence"},{"location":"ch1-2/#example-14","text":"Suppose the functions in Fig 1.18 are \\vec{v_a} = \\vec{r} = x \\vu{x} + y \\vu{y} + z \\vu{z} , \\vec{v_b} = \\vu{z} , and \\vec{v_c} = z\\vu{z} . Calculate their divergences. Solution \\div{\\vec{v_a}} = \\pdv{}{x} (x) + \\pdv{}{y} (x) + \\pdv{}{z} (z) = 1 + 1 + 1 = 3 As anticipated, this function has a positive divergence. \\div{\\vec{v_b}} = \\pdv{}{x} (0) + \\pdv{}{0} (x) + \\pdv{}{z} (1) = 0 + 0 + 0 = 0 as expected. \\div{\\vec{v_b}} = \\pdv{}{x} (0) + \\pdv{}{0} (x) + \\pdv{}{z} (z) = 0 + 0 + 1 = 1","title":"Example 1.4"},{"location":"ch1-2/#125-the-curl","text":"From the definition of \\grad{} we construct the curl: \\begin{align*} \\curl{\\vec{v}} & = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ \\pdv{}{x} & \\pdv{}{y} & \\pdv{}{z} \\\\ v_x & v_y & v_z \\end{vmatrix} \\\\ & + \\vu{x} \\left( \\pdv{v_z}{y} - \\pdv{v_y}{z} \\right) + \\vu{y} \\left( \\pdv{v_x}{z} - \\pdv{v_z}{x} \\right) + \\vu{z} \\left( \\pdv{v_y}{x} - \\pdv{v_x}{y} \\right) \\tagl{1.41} \\end{align*} Notice that the curl of a vector function is, like any cross product, a vector. Geometrical Interpretation : The name curl is also well chosen, for \\curl{\\vec{v}} is a measure of how much the vector \\vec{v} swirls around the point in question. Thus the three functions in Fig. 1.18 all have zero curl (as you can easily check for yourself), whereas the functions in Fig. 1.19 have a substantial curl, pointing in the z direction, as the natural right-hand rule would suggest. Imagine (again) you are standing at the edge of a pond. Float a small paddlewheel (a cork with toothpicks pointing out radially would do); if it starts to rotate, then you placed it at a point of nonzero curl. A whirlpool would be a region of large curl.","title":"1.2.5: The Curl"},{"location":"ch1-2/#example-15","text":"Suppose the function sketched in Fig 1.19a is \\vec{v_a} = -y \\vu{x} + x \\vu{y} , and that in Fig 1.19b is \\vec{v_b} = x \\vu{y} . Calculate their curls. Solution \\curl{\\vec{v_a}} = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ \\pdv{}{x} & \\pdv{}{y} & \\pdv{}{z} \\\\ -y & x & 0 \\end{vmatrix} = 2 \\vu{z} and \\curl{\\vec{v_b}} = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ \\pdv{}{x} & \\pdv{}{y} & \\pdv{}{z} \\\\ 0 & x & 0 \\end{vmatrix} = \\vu{z} As expected, these curls point in the +z direction. (Incidentally, they both have zero divergence, as you might guess from the pictures: nothing is \"spreading out\"... it just \"swirls around.\")","title":"Example 1.5"},{"location":"ch1-2/#126-product-rules","text":"The calculation of ordinary derivatives is facilitated by a number of rules, such as the sum rule \\dv{}{x} (f + g) = \\dv{f}{x} + \\dv{g}{x} the rule for multiplying a constant: \\dv{}{x} (kf) = k \\dv{f}{x} the product rule: \\dv{}{x}(fg) = f \\dv{g}{x} + g \\dv{f}{x} and the quotient rule \\dv{}{x} \\left( \\frac{f}{g} \\right) = \\frac{g \\dv{f}{x} - f \\dv{g}{x}}{g^2} Similar relations hold for the vector derivatives. Thus, \\grad{(f + g)} = \\grad{f} + \\grad{g}, \\qquad \\div{(\\vec{A} + \\vec{B})} = (\\div{\\vec{A}}) + (\\div{\\vec{B}}) \\curl{(\\vec{A} + \\vec{B})} = (\\curl{\\vec{A}}) + (\\curl{\\vec{B}}) and \\grad{(kf)} = k \\grad f, \\quad \\div{(k\\vec{A})} = k (\\div{\\vec{A}}), \\quad \\curl{(k\\vec{A})} = k(\\curl{\\vec{A}}) as you can check for yourself. The product rules are not quite so simple. There are two ways to construct a scalar as the product of two functions: fg \\quad \\text{(product of two scalar functions),} \\\\ \\vec{A} \\cdot \\vec{B} \\quad \\text{(dot product of two vector functions),} and two ways to make a vector: f \\vec{A} \\quad \\text{(scalar times vector),} \\\\ \\vec{A} \\cross \\vec{B} \\quad \\text{(cross product of two vectors).} Accordingly, there are six product rules, two for gradients: \\grad{(fg)} = f \\grad{g} + g \\grad{f} \\tag{i} \\grad( \\vec{A} \\cdot \\vec{B}) = \\vec{A} \\cross (\\curl{\\vec{B}}) + \\vec{B} \\cross (\\curl{\\vec{A}}) + (\\vec{A} \\cdot \\grad{})\\vec{B} + (\\vec{B} \\cdot \\grad{}) \\vec{A} \\tag{ii} two for divergences: \\div{(f\\vec{A})} = f(\\div{\\vec{A}}) + \\vec{A} \\cdot (\\grad{f}) \\tag{iii} \\div{(\\vec{A} \\cross \\vec{B})} = \\vec{B} \\cdot (\\curl{\\vec{A}}) - \\vec{A} \\cdot (\\curl{\\vec{B}}) \\tag{iv} and two for curls: \\curl{(f\\vec{A})} = f(\\curl{\\vec{A}}) - \\vec{A} \\cross (\\grad{f}) \\tag{v} \\curl{(\\vec{A} \\cross \\vec{B})} = (\\vec{B} \\cdot \\grad{})\\vec{A} - (\\vec{A} \\cdot \\grad{}) \\vec{B} + \\vec{A}(\\div{\\vec{B}}) - \\vec{B}(\\div{\\vec{A}}) \\tag{vi} If there's anything in this chapter that's worth memorizing, it is this set of identities. The proofs come straight from the product rule for ordinary derivatives. For instance, \\begin{align*} \\div{(f\\vec{A})} & = \\pdv{}{x} (f A_x) + \\pdv{}{y} (fA_y) + \\pdv{}{z}(f A_z) \\\\ & = \\left( \\pdv{f}{x} A_x + f \\pdv{A_x}{x} \\right) + \\left( \\pdv{f}{y} A_y + f \\pdv{A_y}{y} \\right) + \\left( \\pdv{f}{z}A_z + f \\pdv{A_z}{z} \\right) \\\\ & = (\\grad{f}) \\cdot \\vec{A} + f(\\div{\\vec{A}}) \\end{align*} It is also possible to formulate three quotient rules: \\grad \\left( \\frac{f}{g} \\right) = \\frac{g \\grad{f} - f \\grad{g}}{g^2} \\div{\\left( \\frac{\\vec{A}}{g} \\right)} = \\frac{g(\\div{\\vec{A}}) - \\vec{A} \\cdot (\\grad{g})}{g^2} \\curl \\left( \\frac{\\vec{A}}{g} \\right) = \\frac{g(\\curl{\\vec{A}}) + \\vec{A} \\cross (\\grad{g})}{g^2} However, since these can be obtained quickly from the corresponding product rules, there is no point in listing them separately.","title":"1.2.6: Product Rules"},{"location":"ch1-2/#127-second-derivatives","text":"The gradient, the divergence, and the curl are the only first derivatives we can make with \\grad ; by applying \\grad twice, we can construct five species of second derivatives. The gradient \\grad{T} is a vector, so we can take the divergence and curl of it: Divergence of gradient: \\div (\\grad{T}) Curl of gradient: \\curl (\\grad{T}) The divergence \\div{\\vec{v}} is a scalar - all we can do is take its gradient: Gradient of divergence: \\grad (\\div{\\vec{v}}) The curl \\curl \\vec{v} is a vector, so we can take its divergence and curl: Divergence of curl: \\div (\\curl \\vec{v}) Curl of curl: \\curl (\\curl \\vec{v}) This exhausts the possibilities, and in fact not all of them give anything new. Let's consider them one at a time: \\begin{align*} \\div (\\grad{T}) & = \\left( \\vu{x} \\pdv{}{x} + \\vu{y} \\pdv{}{y} + \\vu{z} \\pdv{}{z} \\right) \\cdot \\left( \\pdv{T}{x} \\vu{x} + \\pdv{T}{y} \\vu{y} + \\pdv{T}{z} \\vu{z} \\right) \\\\ & = \\frac{\\partial ^2 T}{\\partial x^2} + \\frac{\\partial ^2 T}{\\partial y^2} + \\frac{\\partial ^2 T}{\\partial z^2} \\end{align*} \\tagl{1.42} This object, which we write as \\laplacian T for short, is called the Laplacian of T; we shall be studying it in great detail later on. Notice that the Laplacian of a scalar T is a scalar. Occasionally we shall speak of the laplacian of a vector, \\laplacian \\vec{v} . By this we mean a vector quantity whose x-component is the Laplacian of v_x , and so in: \\laplacian \\vec{v} \\equiv (\\laplacian v_x) \\vu{x} + (\\laplacian v_y) \\vu{y} + (\\laplacian v_z) \\vu{z} \\tagl{1.43} This is nothing more than a convenient extension of the meaning of \\laplacian . The curl of a gradient is always zero : \\curl (\\grad{T}) = 0 \\tagl{1.44} This is an important fact, which we shall use repeatedly; you can easily prove it from the definition of \\grad , hinging on the equality of cross-derivatives: \\pdv{}{x} \\left( \\pdv{T}{y} \\right) = \\pdv{}{y} \\left( \\pdv{T}{x} \\right) \\tagl{1.45} \\grad(\\div{\\vec{v}}) seldom occurs in physical applications, and it has not been given any special name of its own - it's just the gradient of the divergence . Notice that \\grad (\\div \\vec{v}) is not the same as the Laplacian of a vector: \\laplacian \\vec{v} = (\\div \\grad) \\vec{v} \\neq \\grad( \\div \\vec{v}) . The divergence of a curl, like the curl of a gradient, is always zero: \\div (\\curl \\vec{v}) = 0 \\tagl{1.46} You can prove this for yourself. As you can check from the definition of \\grad : \\curl (\\curl \\vec{v}) = \\grad(\\div \\vec{v}) - \\laplacian \\vec{v} \\tagl{1.47} So curl-of-curl gives nothing new; the first term is just gradient of divergence, and the second is the Laplacian (of a vector). (In fact, this is often used to define the Laplacian of a vector, in preference to \\eqref{1.43} which makes explicit reference to Cartesian coordinates.) Really, then, there are just two kinds of second derivatives: the Laplacian (which is of fundamental importance) and the gradient-of-divergence (which we seldom encounter). We could go through a similar ritual to work out third derivatives, but fortunately second derivatives suffice for practically all physical applications. A final word on vector differential calculus: It all flows from the operator \\grad , and from taking seriously its vectorial character. Even if you remembered only the definition of \\grad , you could easily reconstruct all the rest.","title":"1.2.7: Second Derivatives"},{"location":"ch1-3/","text":"1.3: Integral Calculus 1.3.1: Line, Surface, and Volume Integrals In electrodynamics, we encounter several different kinds of integrals, among which the most important are line (or path) integrals , surface integrals , and volume integrals . Line Integrals A line integral is an expression of the form \\int_a ^b \\vec{v} \\cdot \\dd{\\vec{l}} \\tagl{1.48} where v is a vector function, \\dd \\vec{l} is the infinitesimal displacement vector and the integral is to be carried out along a prescribed path P from point a to point b . If the path in question forms a closed loop (that is, if \\vec{b} = \\vec{a} , then put a circle on the integral sign: \\oint \\vec{v} \\cdot \\dd \\vec{l} \\tagl{1.49} At each point on the path, we take the dot product of v (evaluated at that point) with the displacement to the next point on the pat. To a physicist, the most familiar example of a line integral is the work done by a force \\vec{F} : W = \\int \\vec{F} \\cdot \\dd \\vec{l} Ordinarily, the value of a line integral depends critically on the path taken from a to b, but there is an important special class of vector functions for which the line integral is independent of path and is determined entirely by the end points. It will be our business in due course to characterize this special class of vectors. (A force that has this property is called conservative. ) Example 1.6 Calculate the line integral of the function \\vec{v} = y^2 \\vu{x} + 2x (y+1) \\vu{y} from the point a = (1, 1, 0) to the point b = (2, 2, 0), along the paths (1) and (2) in Fig 1.21. What is \\oint \\vec{v} \\cdot \\dd \\vec{l} for the loop that goes from a to b along (1) and returns to a along (2)? Solution As always, \\dd \\vec{l} = \\dd x \\vu{x} + \\dd y \\vu{y} + \\dd z \\vu{z} . Path (1) consists of two parts. Along the \"horizontal\" segment, dy = dz = 0 so \\dd \\vec{l} = \\dd x \\vu{x} , y = 1, \\vec{v} \\cdot \\dd{\\vec{l}} = y^2 \\dd x = \\dd x, \\text{ so } \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int_1 ^2 \\dd x = 1 \\tag{i} On the \"vertical\" stretch, dx = dz = 0, so \\dd \\vec{l} = \\dd y \\vu{y}, x = 2, \\vec{v} \\cdot \\dd \\vec{l} = 2x(y+1) \\dd y = 4(y+1) \\dd y, \\text{ so } \\tag{ii} \\int \\vec{v} \\dd \\vec{l} = 4 \\int_1 ^2 (y+1) \\dd y = 10 By path (1), then \\int _{\\vec{a}} ^{\\vec{b}} \\vec{v} \\cdot \\dd \\vec{l} = 1 + 10 = 11 Meanwhile on path (2), x = y, \\dd x = \\dd y, and \\dd z = 0 , so \\dd \\vec{l} = \\dd x \\vu{x} + \\dd x \\vu{y}, \\vec{v} \\cdot \\dd \\vec{l} = x^2 \\dd x + 2x(x+1) \\dd x = (3x^2 + 2x) \\dd x and \\int_{\\vec{a}} ^{\\vec{b}} \\vec{v} \\cdot \\dd \\vec{l} = \\int_1 ^2 (3x^2 + 2x) \\dd x = \\left. (x^3 + x^2)\\right|_{1} ^2 = 10 (The strategy here is to get everything in terms of one variable; I could just as well have eliminated x in favor of y.) For the loop that goes out (1) and back (2), then \\oint \\vec{v} \\cdot \\dd \\vec{l} = 11 - 10 = 1 Surface Integrals A surface integral is an expression of the form \\int_{\\mathscr{S}} \\vec{v} \\cdot \\dd \\vec{a} \\tagl{1.50} where v is again some vector function, and the integral is over a specified surface \\mathscr{S} . Here \\dd \\vec{a} is an infinitesimal patch of area, with direction perpendicular to the surface (Fig 1.22). There are, of course, two directions perpendicular to any surface, so the sign of a surface integral is intrinsically ambiguous. If the surface is closed (forming a \"balloon\"), in which case I again put a circle on the integral sign \\oint \\vec{v} \\cdot \\dd \\vec{a} then tradition dictates that \"outward\" is positive, but for open surfaces it's arbitrary. If v describes the flow of a fluid (mass per unit area per unit time), then \\int \\vec{v} \\cdot \\dd \\vec{a} represents the total mass per unit time passing through the surface - hence the alternative name, \"flux.\" Ordinarily, the value of a surface integral depends on the particular surface chosen, but there is a special class of vector functions for which it is independent of the surface and is determined entirely by the boundary line. An important task will be to characterize this special class of functions. Example 1.7 Calculate the surface integral of \\vec{v} = 2xz \\vu{x} + (x+2) \\vu{y} + y(z^2 -3) \\vu{z} over five sides (excluding the bottom) of the cubical box (side 2) in Fig 1.23. Let 'upward and outward' be the positive direction, as indicated by the arrows. Solution Taking the sides one at a time (i) x = 2, \\dd \\vec{a} = \\dd y \\dd z \\, \\vu{x}, \\vec{v} \\cdot \\dd \\vec{a} = 2xyz \\, \\dd y \\dd z = 4z \\dd y \\dd z , so \\int \\vec{v} \\cdot \\dd \\vec{a} = 4 \\int_0 ^2 \\dd y \\int_0 ^2 z \\dd z = 16 (ii) x = 0, \\dd \\vec{a} = -\\dd y \\dd z \\, \\vu{x}, \\vec{v} \\cdot \\dd \\vec{a} = -2xyz \\, \\dd y \\dd z = 4z \\dd y \\dd z = 0 , so \\int \\vec{v} \\cdot \\dd \\vec{a} = 0 (iii) y = 2, \\dd \\vec{a} = \\dd x \\dd z \\, \\vu{y}, \\vec{v} \\cdot \\dd \\vec{a} = (x+2) \\, \\dd x \\dd z , so \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int _0 ^2 (x + 2) \\dd x \\int _0 ^2 \\dd z = 12 (iv) y = 0, \\dd \\vec{a} = - \\dd x \\dd z \\, \\vu{y}, \\vec{v} \\cdot \\dd \\vec{a} = -(x+2) \\, \\dd x \\dd z , so \\int \\vec{v} \\cdot \\dd \\vec{a} = -\\int _0 ^2 (x + 2) \\dd x \\int _0 ^2 \\dd z = -12 (v) z = 2, \\dd \\vec{a} = \\dd x \\dd y \\, \\vu{z}, \\vec{v} \\cdot \\dd \\vec{a} = y(z^2 -3) \\, \\dd x \\dd y = y \\, \\dd x \\dd y , so \\int \\vec{v} \\cdot \\dd{a} = \\int_0 ^2 \\dd x \\int_0 ^2 y \\dd y = 4 The total flux is \\int _{surface} \\vec{v} \\cdot \\dd \\vec{a} = 16 + 0 + 12 - 12 + 4 = 20 Volume Integrals A volume integral is an expression of the form \\int_{V} T \\dd \\tau \\tagl{1.51} where T is a scalar function and \\dd \\tau is an infinitesimal volume element. In Cartesian coordinates, \\dd \\tau = \\dd x \\, \\dd y \\, \\dd z \\tagl{1.52} For example, if T is the density of a substance (which might vary from point to point), then the volume integral would give the total mass. Occasionally we shall encounter volume integrals of vector functions: \\int \\vec{v} \\dd \\tau = \\int (v_x \\vu{x} + v_y \\vu{y} + v_z \\vu{z}) \\dd \\tau = \\vu{x} \\int v_x \\dd \\tau + \\vu{y} \\int v_y \\dd \\tau + \\vu{z} \\int v_z \\dd \\tau \\tagl{1.53} Because the unit vectors are constants, they come outside the integral. Example 1.8 Calculate the volume integral of T = xyz^2 over the prism in Fig 1.24. Solution \\begin{align*} \\int T \\dd \\tau & = \\int _0 ^3 z^2 \\left( \\int _0 ^1 y \\left[ \\int_0 ^{1-y} x \\, \\dd x \\right] \\dd y \\right) \\dd z \\\\ & = \\frac{1}{2} \\int_0 ^3 z^2 \\, \\dd z \\int_0 ^1 (1-y)^2 y \\, \\dd \\y = \\frac{1}{2} (9) \\left( \\frac{1}{12} \\right) = \\frac{3}{8} \\end{align*} 1.3.2: The Fundamental Theorem of Calculus Suppose f(x) is a function in one variable. The fundamental theorem of calculus says \\int_a ^b \\left( \\dv{f}{x} \\right) \\dd x = f(b) - f(a) \\tagl{1.54} In case this doesn't look familiar, I'll write it another way: \\int_a ^b F(x) \\dd x = f(b) - f(a) where df / dx = F(x) . The fundamental theorem tells you how to integrate F(x) : you think up a function f(x) whose derivative is equal to F. Geometrical interpretation : According to Eq. 1.33, df = (df / dx) dx is the infinitesimal change in f when you go from (x) to (x + dx). The fundamental theorem (Eq. 1.54) says that if you chop the interval from a to b (Fig. 1.25) into many tiny pieces, dx, and add up the increments df from each little piece, the result is (not surprisingly) equal to the total change in f: f(b) - f(a) . In other words, there are two ways to determine the total change in the function: either subtract the values at the ends or go step-by-step, adding up all the tiny increments as you go. You'll get the same answer either way. Notice the basic format of the fundamental theorem: the integral of a derivative over some region is given by the value of the function at the end points (boundaries). In vector calculus there are three species of derivative (gradient, divergence, and curl), and each has its own \"fundamental theorem,\" with essentially the same format. I don't plan to prove these theorems here; rather, I will explain what they mean, and try to make them plausible. 1.3.3: The Fundamental Theorem for Gradients Suppose we have a scalar function of three variables T(x, y, z). Starting at point a , we move a small distance \\dd \\vec{l}_1 (Fig 1.26). According to Eq. 1.37, the function T will change by an amount \\dd T = (\\grad T) \\cdot \\dd \\vec{l}_1 Now we move a little further, by an additional small displacement \\dd \\vec{l}_2 ; the incremental change in T will be (\\grad T) \\cdot \\dd \\vec{l}_2 . In this manner, proceeding by infinitesimal steps, we make the journey to point b. At each step we compute the gradient of T (at that point) and dot it into the displacement dl... this gives us the change in T. Evidently the total change in Tin going from a to b (along the path selected) is \\int_{\\vec{a}} ^{\\vec{b}} (\\grad T) \\cdot \\dd \\vec{l} = T(\\vec{b}) - T(\\vec{a}) \\tagl{1.55} This is the fundamental theorem for gradients; like the \"ordinary\" fundamental theorem, it says that the integral (here a line integral) of a derivative (here the gradient) is given by the value of the function at the boundaries (a and b). Geometrical Interpretation: Suppose you wanted to determine the height of the Eiffel Tower. You could climb the stairs, using a ruler to measure the rise at each step, and adding them all up (that's the left side of Eq. 1.55), or you could place altimeters at the top and the bottom, and subtract the two readings (that's the right side); you should get the same answer either way (that's the fundamental theorem). Incidentally, as we found in Ex. 1.6, line integrals ordinarily depend on the path taken from a to b. But the right side of Eq. 1.55 makes no reference to the path - only to the end points. Evidently, gradients have the special property that their line integrals are path independent: Corollary 1: \\int_a ^b (\\grad T) \\cdot \\dd \\vec{l} is independent of the path taken from a to b. Corollary 2: \\oint (\\grad T) \\cdot \\dd \\vec{l} = 0 , since the beginning and end points are identical, and hence T(\\vec{b}) - T(\\vec{a}) = 0 . Example 1.9 Let T = xy^2 , and take point a to be the origin (0, 0, 0) and b to be the point (2, 1, 0). Check the fundamental theorem for gradients. Solution We know a priori that the integral should be independent of the path, but we must sill pick a specific path in order to evaluate it. Let's go out along the x axis, then up (Fig 1.27). As always, \\dd \\vec{l} = \\dd x \\vu{x} + \\dd y \\vu{y} + \\dd z \\vu{z}; \\grad T = y^2 \\vu{x} + 2xy \\vu{y} y = 0; \\dd \\vec{l} = \\dd x \\vu{x}; \\grad T \\cdot \\dd \\vec{l} = y^2 \\dd x = 0 \\rightarrow \\int_{i} \\grad T \\cdot \\dd \\vec{l} = 0 x = 2; \\dd \\vec{l} = \\dd y \\vu{y}; \\grad T \\cdot \\dd \\vec{l} = 4y \\dd y \\rightarrow \\int_{ii} \\grad T \\cdot \\dd \\vec{l} = \\left. 2y^2 \\right| _0 ^1 = 2 The total line integral is 2. So is this consistent with what we expect from the fundamental theorem? Well, T(b) - T(a) = 2 - 0 = 2 , so yes! 1.3.4: The Fundamental Theorem for Divergences The fundamental theorem for divergences states that \\int _{\\mathscr{V}} (\\div \\vec{v} ) \\dd \\tau = \\oint_{\\mathscr{S}} \\vec{v} \\cdot \\dd \\vec{a} \\tagl{1.56} In honor, I suppose, of its great importance, this theorem has at least three special names: Gauss's theorem , Green's theorem , or simply the divergence theorem . Like the other \"fundamental theorems,\" it says that the integral of a derivative (in this case the divergence) over a region (in this case a volume, V) is equal to the value of the function at the boundary (in this case the surface S that bounds the volume). Notice that the boundary term is itself an integral (specifically, a surface integral). This is reasonable: the \"boundary\" of a line is just two end points, but the boundary of a volume is a (closed) surface. Geometrical Interpretation: If v represents the flow of an incompressible fluid, then the flux of v (the right side of Eq. 1.56) is the total amount of fluid passing out through the surface, per unit time. Now, the divergence measures the \"spreading out\" of the vectors from a point-a place of high divergence is like a \"faucet,\" pouring out liquid. If we have a bunch of faucets in a region filled with incompressible fluid, an equal amount of liquid will be forced out through the boundaries of the region. In fact, there are two ways we could determine how much is being produced: (a) we could count up all the faucets, recording how much each puts out, or (b) we could go around the boundary, measuring the flow at each point, and add it all up. You get the same answer either way: \\int (\\text{faucets within the volume}) = \\oint (\\text{flow out through the surface}) This, in essence, is what the divergence theorem says. Example 1.10 Check the divergence theorem using the function \\vec{v} = y^2 \\vu{x} + (2xy + z^2) \\vu{y} + (2yz) \\vu{z} using a unit cube at the origin as the surface boundary (Fig 1.29). Solution In this case \\div \\vec{v} = 2(x + y) and \\int_V 2(x+y) \\dd \\tau = 2 \\int_0 ^1 \\dd x \\int _0 ^1 \\dd y \\int _0 ^1 \\dd z (x+y) \\int _0 ^1 \\dd x (x+y) = \\frac{1}{2} + y, \\quad \\int _0 ^1 \\dd y (\\frac{1}{2} + y) \\dd y = 1, \\quad \\int_0 ^1 \\dd z (1) = 1 Thus, \\int_{V} \\div \\vec{v} \\dd \\tau = 2 That takes care of the volume integral part of Gauss' Law, now how about the surface integral? We have to it in six parts, for each face of the cube: \\tag{i} \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd y \\int_0 ^1 \\dd z y^2 = \\frac{1}{3} \\tag{ii} \\int \\vec{v} \\cdot \\dd \\vec{a} = - \\int _0 ^1 \\dd y \\int _0 ^1 \\dd z y^2 = - \\frac{1}{3} \\tag{iii} \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd x \\int_0 ^1 \\dd z (2x + z^2) = \\frac{4}{3} \\tag{iv} \\int \\vec{v} \\cdot \\dd \\vec{a} = - \\int _0 ^1 \\dd x \\int _0 ^1 \\dd z (z^2) = - \\frac{1}{3} \\tag{v} \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd x \\int_0 ^1 \\dd y (2y) = 1 \\tag{vi} \\int \\vec{v} \\cdot \\dd \\vec{a} = - \\int _0 ^1 \\dd x \\int _0 ^1 \\dd y (0) = 0 So the total flux is \\oint _S \\vec{v} \\cdot \\dd \\vec{a} = \\frac{1}{3} - \\frac{1}{3} + \\frac{4}{3} - \\frac{1}{3} + 1 + 0 = 2 as we should expect. 1.3.5: The Fundamental Theorem for Curls The fundamental theorem for curls, which goes by the name Stokes' Theorem , states that \\int _S(\\curl \\vec{V}) \\cdot \\dd \\vec{a} = \\oint _P \\vec{v} \\cdot \\dd \\vec{l} \\tagl{1.57} As always, the integral of a derivative (here, the curl) over a region (here, a patch of surface, S) is equal to the value of the function at the boundary (here, the perimeter of the patch, P). As in the case of the divergence theorem, the boundary term is itself an integral-specifically, a closed line integral. Geometrical Interpretation: Recall that the curl measures the \"twist\" of the vectors v ; a region of high curl is a whirlpool - if you put a tiny paddle wheel there, it will rotate. Now, the integral of the curl over some surface (or, more precisely, the flux of the curl through that surface) represents the \"total amount of swirl,\" and we can determine that just as well by going around the edge and finding how much the flow is following the boundary (Fig. 1.31). Indeed, \\oint \\vec{v} \\cdot \\dd \\vec{l} is sometimes called the circulation of v. You may have noticed an apparent ambiguity in Stokes' theorem: concerning the boundary line integral, which way are we supposed to go around (clockwise or counterclockwise)? If we go the \"wrong\" way, we'll pick up an overall sign error. The answer is that it doesn't matter which way you go as long as you are consistent, for there is a compensating sign ambiguity in the surface integral: Which way does \\dd \\vec{a} point? For a closed surface (as in the divergence theorem), \\dd \\vec{a} points in the direction of the outward normal; but for an open surface, which way is \"out\"? Consistency in Stokes' theorem (as in all such matters) is given by the right-hand rule: if your fingers point in the direction of the line integral, then your thumb fixes the direction of \\dd \\vec{a} (Fig. 1.32). Now, there are plenty of surfaces (infinitely many) that share any given boundary line. Twist a paper clip into a loop, and dip it in soapy water. The soap film constitutes a surface, with the wire loop as its boundary. If you blow on it, the soap film will expand, making a larger surface, with the same boundary. Ordinarily, a flux integral depends critically on what surface you integrate over, but evidently this is not the case with curls. For Stokes' theorem says that \\int (\\curl \\vec{v}) \\cdot \\dd \\vec{a} is equal to the line integral of \\vec{v}\\ around the boundary, and the latter makes no reference to the specific surface you choose. Corollary 1: \\int (\\curl \\vec{v}) \\cdot \\dd \\vec{a} depends only on the boundary line, not on the particular surface used. Corollary 2: \\oint (\\curl \\vec{v}) \\cdot \\dd \\vec{a} = 0 for any closed surface, since the boundary line, like the mouth of a balloon, shrinks down to a point, and hence the right side of \\eqref{1.57} vanishes. Example 1.11 Suppose \\vec{v} = (2xz + 3y^2) \\vu{y} + (4yz^2) \\vu{z} . Check Stokes' theorem for the square surface shown in Fig 1.33. Solution Here \\curl \\vec{v} = (4z^2 - 2x) \\vu{x} + 2z \\vu{z} \\quad \\text{and} \\quad \\dd \\vec{a} = \\dd y \\, \\dd z \\, \\vu{x} (In saying that \\dd \\vec{a} points in the x direction, we are committing ourselves to a counterclockwise integral. We could as well write \\dd \\vec{a} pointing in the other direction ( \\dd \\vec{a} = - \\dd y \\, \\dd z\\, \\vu{x} ) and perform the integral in the clockwise direction.) Since x = 0 for this surface, \\int (\\curl \\vec{v}) \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd y \\int_0 ^1 \\dd z (4z^2) = \\frac{4}{3} Now for the line integral, which we of course break into 4 pieces: \\tag{i} x = 0 \\quad z = 0 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 3y^2 \\dd y, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _0 ^1 3y^2 \\dd y = 1 \\tag{ii} x = 0 \\quad y = 1 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 4z^2 \\dd z, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _0 ^1 4z^2 \\dd z = \\frac{4}{3} \\tag{iii} x = 0 \\quad z = 1 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 3y^2 \\dd y, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _1 ^0 3y^2 \\dd y = -1 \\tag{iv} x = 0 \\quad y = 0 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 0 \\dd z, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _1 ^0 0 \\dd z = 0 So \\oint \\vec{v} \\cdot \\dd \\vec{l} = 1 + \\frac{4}{3} - 1 + 0 = \\frac{4}{3} It all checks out! 1.3.6: Integration by Parts The technique known (awkwardly) as integration by parts exploits the product rule for derivatives: \\dv{}{x} (fg) = f \\left( \\dv{g}{x} \\right) + g \\left( \\dv{f}{x} \\right) Integrating both sides, and invoking the fundamental theorem, \\int_a ^b \\dv{}{x} (fg) \\dd x = \\left. fg \\right| ^b _a = \\int _a ^b f \\left( \\dv{g}{x} \\right) \\dd x + \\int_a ^b g \\left( \\dv{f}{x} \\right) \\dd x or \\int_a ^b f \\left( \\dv{g}{x} \\right) \\dd x = - \\int_a ^b \\left( \\dv{f}{x} \\right) \\dd x + \\left. fg \\right| ^b _a \\tagl{1.58} That's \"integration by parts.\" It applies to the situation in which you are called upon to integrate the product of one function (f) and the derivative of another (g); it says you can transfer the derivative from g to f, at the cost of a minus sign and a boundary term. Example 1.12 Evaluate the integral $$ \\int _0 ^\\infty x e^{-x} \\dd x Solution The exponential can be expressed as a derivative: e^{-x} = \\dv{}{x} \\left( - e^{-x} \\right) in this case, then, f(x) = x , g(x) = - e^{-x} , and df /dx = 1 , so \\int_0 ^\\infty x e^{-x} \\dd x = \\int _0 ^{\\infty} e^{-x} \\dd x - \\left. x e^{-x} \\right| _{0} ^{\\infty} = - \\left. e^{-x} \\right| _0 ^{\\infty} = 1 We can exploit the product rules of vector calculus, together with the appropriate fundamental theorems, in exactly the same way. For example, integrating \\div (f\\vec{A}) = f(\\div \\vec{A}) + \\vec{A} \\cdot (\\grad f) over a volume, and invoking the divergence theorem, yileds \\int \\div (f \\vec{A}) \\dd \\tau = \\int f(\\div \\vec{A}) \\dd \\tau + \\int \\vec{A} \\cdot (\\grad f) \\dd \\tau \\ \\oint f \\vec{A} \\cdot \\dd \\vec{a} or \\int _V f(\\div \\vec{A}) \\dd \\tau = - \\int _V \\vec{A} \\cdot (\\grad f) \\dd \\tau + \\oint _S f \\vec{A} \\cdot \\dd \\vec{a} \\tagl{1.59} Here again the integrand is the product of one function (f) and the derivative (in this case the divergence) of another ( A ), and integration by parts licenses us to transfer the derivative from A to f (where it becomes a gradient), at the cost of a minus sign and a boundary term (in this case a surface integral). In practice, this turns out to be one of the most useful tools at our disposal in vector calculus. Though you might wonder how often you're really likely to encounter an integral involving the product of one function and the derivative of another, the answer is surprisingly often .","title":"1.3 - Integral Calculus"},{"location":"ch1-3/#13-integral-calculus","text":"","title":"1.3: Integral Calculus"},{"location":"ch1-3/#131-line-surface-and-volume-integrals","text":"In electrodynamics, we encounter several different kinds of integrals, among which the most important are line (or path) integrals , surface integrals , and volume integrals .","title":"1.3.1: Line, Surface, and Volume Integrals"},{"location":"ch1-3/#line-integrals","text":"A line integral is an expression of the form \\int_a ^b \\vec{v} \\cdot \\dd{\\vec{l}} \\tagl{1.48} where v is a vector function, \\dd \\vec{l} is the infinitesimal displacement vector and the integral is to be carried out along a prescribed path P from point a to point b . If the path in question forms a closed loop (that is, if \\vec{b} = \\vec{a} , then put a circle on the integral sign: \\oint \\vec{v} \\cdot \\dd \\vec{l} \\tagl{1.49} At each point on the path, we take the dot product of v (evaluated at that point) with the displacement to the next point on the pat. To a physicist, the most familiar example of a line integral is the work done by a force \\vec{F} : W = \\int \\vec{F} \\cdot \\dd \\vec{l} Ordinarily, the value of a line integral depends critically on the path taken from a to b, but there is an important special class of vector functions for which the line integral is independent of path and is determined entirely by the end points. It will be our business in due course to characterize this special class of vectors. (A force that has this property is called conservative. )","title":"Line Integrals"},{"location":"ch1-3/#example-16","text":"Calculate the line integral of the function \\vec{v} = y^2 \\vu{x} + 2x (y+1) \\vu{y} from the point a = (1, 1, 0) to the point b = (2, 2, 0), along the paths (1) and (2) in Fig 1.21. What is \\oint \\vec{v} \\cdot \\dd \\vec{l} for the loop that goes from a to b along (1) and returns to a along (2)? Solution As always, \\dd \\vec{l} = \\dd x \\vu{x} + \\dd y \\vu{y} + \\dd z \\vu{z} . Path (1) consists of two parts. Along the \"horizontal\" segment, dy = dz = 0 so \\dd \\vec{l} = \\dd x \\vu{x} , y = 1, \\vec{v} \\cdot \\dd{\\vec{l}} = y^2 \\dd x = \\dd x, \\text{ so } \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int_1 ^2 \\dd x = 1 \\tag{i} On the \"vertical\" stretch, dx = dz = 0, so \\dd \\vec{l} = \\dd y \\vu{y}, x = 2, \\vec{v} \\cdot \\dd \\vec{l} = 2x(y+1) \\dd y = 4(y+1) \\dd y, \\text{ so } \\tag{ii} \\int \\vec{v} \\dd \\vec{l} = 4 \\int_1 ^2 (y+1) \\dd y = 10 By path (1), then \\int _{\\vec{a}} ^{\\vec{b}} \\vec{v} \\cdot \\dd \\vec{l} = 1 + 10 = 11 Meanwhile on path (2), x = y, \\dd x = \\dd y, and \\dd z = 0 , so \\dd \\vec{l} = \\dd x \\vu{x} + \\dd x \\vu{y}, \\vec{v} \\cdot \\dd \\vec{l} = x^2 \\dd x + 2x(x+1) \\dd x = (3x^2 + 2x) \\dd x and \\int_{\\vec{a}} ^{\\vec{b}} \\vec{v} \\cdot \\dd \\vec{l} = \\int_1 ^2 (3x^2 + 2x) \\dd x = \\left. (x^3 + x^2)\\right|_{1} ^2 = 10 (The strategy here is to get everything in terms of one variable; I could just as well have eliminated x in favor of y.) For the loop that goes out (1) and back (2), then \\oint \\vec{v} \\cdot \\dd \\vec{l} = 11 - 10 = 1","title":"Example 1.6"},{"location":"ch1-3/#surface-integrals","text":"A surface integral is an expression of the form \\int_{\\mathscr{S}} \\vec{v} \\cdot \\dd \\vec{a} \\tagl{1.50} where v is again some vector function, and the integral is over a specified surface \\mathscr{S} . Here \\dd \\vec{a} is an infinitesimal patch of area, with direction perpendicular to the surface (Fig 1.22). There are, of course, two directions perpendicular to any surface, so the sign of a surface integral is intrinsically ambiguous. If the surface is closed (forming a \"balloon\"), in which case I again put a circle on the integral sign \\oint \\vec{v} \\cdot \\dd \\vec{a} then tradition dictates that \"outward\" is positive, but for open surfaces it's arbitrary. If v describes the flow of a fluid (mass per unit area per unit time), then \\int \\vec{v} \\cdot \\dd \\vec{a} represents the total mass per unit time passing through the surface - hence the alternative name, \"flux.\" Ordinarily, the value of a surface integral depends on the particular surface chosen, but there is a special class of vector functions for which it is independent of the surface and is determined entirely by the boundary line. An important task will be to characterize this special class of functions.","title":"Surface Integrals"},{"location":"ch1-3/#example-17","text":"Calculate the surface integral of \\vec{v} = 2xz \\vu{x} + (x+2) \\vu{y} + y(z^2 -3) \\vu{z} over five sides (excluding the bottom) of the cubical box (side 2) in Fig 1.23. Let 'upward and outward' be the positive direction, as indicated by the arrows. Solution Taking the sides one at a time (i) x = 2, \\dd \\vec{a} = \\dd y \\dd z \\, \\vu{x}, \\vec{v} \\cdot \\dd \\vec{a} = 2xyz \\, \\dd y \\dd z = 4z \\dd y \\dd z , so \\int \\vec{v} \\cdot \\dd \\vec{a} = 4 \\int_0 ^2 \\dd y \\int_0 ^2 z \\dd z = 16 (ii) x = 0, \\dd \\vec{a} = -\\dd y \\dd z \\, \\vu{x}, \\vec{v} \\cdot \\dd \\vec{a} = -2xyz \\, \\dd y \\dd z = 4z \\dd y \\dd z = 0 , so \\int \\vec{v} \\cdot \\dd \\vec{a} = 0 (iii) y = 2, \\dd \\vec{a} = \\dd x \\dd z \\, \\vu{y}, \\vec{v} \\cdot \\dd \\vec{a} = (x+2) \\, \\dd x \\dd z , so \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int _0 ^2 (x + 2) \\dd x \\int _0 ^2 \\dd z = 12 (iv) y = 0, \\dd \\vec{a} = - \\dd x \\dd z \\, \\vu{y}, \\vec{v} \\cdot \\dd \\vec{a} = -(x+2) \\, \\dd x \\dd z , so \\int \\vec{v} \\cdot \\dd \\vec{a} = -\\int _0 ^2 (x + 2) \\dd x \\int _0 ^2 \\dd z = -12 (v) z = 2, \\dd \\vec{a} = \\dd x \\dd y \\, \\vu{z}, \\vec{v} \\cdot \\dd \\vec{a} = y(z^2 -3) \\, \\dd x \\dd y = y \\, \\dd x \\dd y , so \\int \\vec{v} \\cdot \\dd{a} = \\int_0 ^2 \\dd x \\int_0 ^2 y \\dd y = 4 The total flux is \\int _{surface} \\vec{v} \\cdot \\dd \\vec{a} = 16 + 0 + 12 - 12 + 4 = 20","title":"Example 1.7"},{"location":"ch1-3/#volume-integrals","text":"A volume integral is an expression of the form \\int_{V} T \\dd \\tau \\tagl{1.51} where T is a scalar function and \\dd \\tau is an infinitesimal volume element. In Cartesian coordinates, \\dd \\tau = \\dd x \\, \\dd y \\, \\dd z \\tagl{1.52} For example, if T is the density of a substance (which might vary from point to point), then the volume integral would give the total mass. Occasionally we shall encounter volume integrals of vector functions: \\int \\vec{v} \\dd \\tau = \\int (v_x \\vu{x} + v_y \\vu{y} + v_z \\vu{z}) \\dd \\tau = \\vu{x} \\int v_x \\dd \\tau + \\vu{y} \\int v_y \\dd \\tau + \\vu{z} \\int v_z \\dd \\tau \\tagl{1.53} Because the unit vectors are constants, they come outside the integral.","title":"Volume Integrals"},{"location":"ch1-3/#example-18","text":"Calculate the volume integral of T = xyz^2 over the prism in Fig 1.24. Solution \\begin{align*} \\int T \\dd \\tau & = \\int _0 ^3 z^2 \\left( \\int _0 ^1 y \\left[ \\int_0 ^{1-y} x \\, \\dd x \\right] \\dd y \\right) \\dd z \\\\ & = \\frac{1}{2} \\int_0 ^3 z^2 \\, \\dd z \\int_0 ^1 (1-y)^2 y \\, \\dd \\y = \\frac{1}{2} (9) \\left( \\frac{1}{12} \\right) = \\frac{3}{8} \\end{align*}","title":"Example 1.8"},{"location":"ch1-3/#132-the-fundamental-theorem-of-calculus","text":"Suppose f(x) is a function in one variable. The fundamental theorem of calculus says \\int_a ^b \\left( \\dv{f}{x} \\right) \\dd x = f(b) - f(a) \\tagl{1.54} In case this doesn't look familiar, I'll write it another way: \\int_a ^b F(x) \\dd x = f(b) - f(a) where df / dx = F(x) . The fundamental theorem tells you how to integrate F(x) : you think up a function f(x) whose derivative is equal to F. Geometrical interpretation : According to Eq. 1.33, df = (df / dx) dx is the infinitesimal change in f when you go from (x) to (x + dx). The fundamental theorem (Eq. 1.54) says that if you chop the interval from a to b (Fig. 1.25) into many tiny pieces, dx, and add up the increments df from each little piece, the result is (not surprisingly) equal to the total change in f: f(b) - f(a) . In other words, there are two ways to determine the total change in the function: either subtract the values at the ends or go step-by-step, adding up all the tiny increments as you go. You'll get the same answer either way. Notice the basic format of the fundamental theorem: the integral of a derivative over some region is given by the value of the function at the end points (boundaries). In vector calculus there are three species of derivative (gradient, divergence, and curl), and each has its own \"fundamental theorem,\" with essentially the same format. I don't plan to prove these theorems here; rather, I will explain what they mean, and try to make them plausible.","title":"1.3.2: The Fundamental Theorem of Calculus"},{"location":"ch1-3/#133-the-fundamental-theorem-for-gradients","text":"Suppose we have a scalar function of three variables T(x, y, z). Starting at point a , we move a small distance \\dd \\vec{l}_1 (Fig 1.26). According to Eq. 1.37, the function T will change by an amount \\dd T = (\\grad T) \\cdot \\dd \\vec{l}_1 Now we move a little further, by an additional small displacement \\dd \\vec{l}_2 ; the incremental change in T will be (\\grad T) \\cdot \\dd \\vec{l}_2 . In this manner, proceeding by infinitesimal steps, we make the journey to point b. At each step we compute the gradient of T (at that point) and dot it into the displacement dl... this gives us the change in T. Evidently the total change in Tin going from a to b (along the path selected) is \\int_{\\vec{a}} ^{\\vec{b}} (\\grad T) \\cdot \\dd \\vec{l} = T(\\vec{b}) - T(\\vec{a}) \\tagl{1.55} This is the fundamental theorem for gradients; like the \"ordinary\" fundamental theorem, it says that the integral (here a line integral) of a derivative (here the gradient) is given by the value of the function at the boundaries (a and b). Geometrical Interpretation: Suppose you wanted to determine the height of the Eiffel Tower. You could climb the stairs, using a ruler to measure the rise at each step, and adding them all up (that's the left side of Eq. 1.55), or you could place altimeters at the top and the bottom, and subtract the two readings (that's the right side); you should get the same answer either way (that's the fundamental theorem). Incidentally, as we found in Ex. 1.6, line integrals ordinarily depend on the path taken from a to b. But the right side of Eq. 1.55 makes no reference to the path - only to the end points. Evidently, gradients have the special property that their line integrals are path independent: Corollary 1: \\int_a ^b (\\grad T) \\cdot \\dd \\vec{l} is independent of the path taken from a to b. Corollary 2: \\oint (\\grad T) \\cdot \\dd \\vec{l} = 0 , since the beginning and end points are identical, and hence T(\\vec{b}) - T(\\vec{a}) = 0 .","title":"1.3.3: The Fundamental Theorem for Gradients"},{"location":"ch1-3/#example-19","text":"Let T = xy^2 , and take point a to be the origin (0, 0, 0) and b to be the point (2, 1, 0). Check the fundamental theorem for gradients. Solution We know a priori that the integral should be independent of the path, but we must sill pick a specific path in order to evaluate it. Let's go out along the x axis, then up (Fig 1.27). As always, \\dd \\vec{l} = \\dd x \\vu{x} + \\dd y \\vu{y} + \\dd z \\vu{z}; \\grad T = y^2 \\vu{x} + 2xy \\vu{y} y = 0; \\dd \\vec{l} = \\dd x \\vu{x}; \\grad T \\cdot \\dd \\vec{l} = y^2 \\dd x = 0 \\rightarrow \\int_{i} \\grad T \\cdot \\dd \\vec{l} = 0 x = 2; \\dd \\vec{l} = \\dd y \\vu{y}; \\grad T \\cdot \\dd \\vec{l} = 4y \\dd y \\rightarrow \\int_{ii} \\grad T \\cdot \\dd \\vec{l} = \\left. 2y^2 \\right| _0 ^1 = 2 The total line integral is 2. So is this consistent with what we expect from the fundamental theorem? Well, T(b) - T(a) = 2 - 0 = 2 , so yes!","title":"Example 1.9"},{"location":"ch1-3/#134-the-fundamental-theorem-for-divergences","text":"The fundamental theorem for divergences states that \\int _{\\mathscr{V}} (\\div \\vec{v} ) \\dd \\tau = \\oint_{\\mathscr{S}} \\vec{v} \\cdot \\dd \\vec{a} \\tagl{1.56} In honor, I suppose, of its great importance, this theorem has at least three special names: Gauss's theorem , Green's theorem , or simply the divergence theorem . Like the other \"fundamental theorems,\" it says that the integral of a derivative (in this case the divergence) over a region (in this case a volume, V) is equal to the value of the function at the boundary (in this case the surface S that bounds the volume). Notice that the boundary term is itself an integral (specifically, a surface integral). This is reasonable: the \"boundary\" of a line is just two end points, but the boundary of a volume is a (closed) surface. Geometrical Interpretation: If v represents the flow of an incompressible fluid, then the flux of v (the right side of Eq. 1.56) is the total amount of fluid passing out through the surface, per unit time. Now, the divergence measures the \"spreading out\" of the vectors from a point-a place of high divergence is like a \"faucet,\" pouring out liquid. If we have a bunch of faucets in a region filled with incompressible fluid, an equal amount of liquid will be forced out through the boundaries of the region. In fact, there are two ways we could determine how much is being produced: (a) we could count up all the faucets, recording how much each puts out, or (b) we could go around the boundary, measuring the flow at each point, and add it all up. You get the same answer either way: \\int (\\text{faucets within the volume}) = \\oint (\\text{flow out through the surface}) This, in essence, is what the divergence theorem says.","title":"1.3.4: The Fundamental Theorem for Divergences"},{"location":"ch1-3/#example-110","text":"Check the divergence theorem using the function \\vec{v} = y^2 \\vu{x} + (2xy + z^2) \\vu{y} + (2yz) \\vu{z} using a unit cube at the origin as the surface boundary (Fig 1.29). Solution In this case \\div \\vec{v} = 2(x + y) and \\int_V 2(x+y) \\dd \\tau = 2 \\int_0 ^1 \\dd x \\int _0 ^1 \\dd y \\int _0 ^1 \\dd z (x+y) \\int _0 ^1 \\dd x (x+y) = \\frac{1}{2} + y, \\quad \\int _0 ^1 \\dd y (\\frac{1}{2} + y) \\dd y = 1, \\quad \\int_0 ^1 \\dd z (1) = 1 Thus, \\int_{V} \\div \\vec{v} \\dd \\tau = 2 That takes care of the volume integral part of Gauss' Law, now how about the surface integral? We have to it in six parts, for each face of the cube: \\tag{i} \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd y \\int_0 ^1 \\dd z y^2 = \\frac{1}{3} \\tag{ii} \\int \\vec{v} \\cdot \\dd \\vec{a} = - \\int _0 ^1 \\dd y \\int _0 ^1 \\dd z y^2 = - \\frac{1}{3} \\tag{iii} \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd x \\int_0 ^1 \\dd z (2x + z^2) = \\frac{4}{3} \\tag{iv} \\int \\vec{v} \\cdot \\dd \\vec{a} = - \\int _0 ^1 \\dd x \\int _0 ^1 \\dd z (z^2) = - \\frac{1}{3} \\tag{v} \\int \\vec{v} \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd x \\int_0 ^1 \\dd y (2y) = 1 \\tag{vi} \\int \\vec{v} \\cdot \\dd \\vec{a} = - \\int _0 ^1 \\dd x \\int _0 ^1 \\dd y (0) = 0 So the total flux is \\oint _S \\vec{v} \\cdot \\dd \\vec{a} = \\frac{1}{3} - \\frac{1}{3} + \\frac{4}{3} - \\frac{1}{3} + 1 + 0 = 2 as we should expect.","title":"Example 1.10"},{"location":"ch1-3/#135-the-fundamental-theorem-for-curls","text":"The fundamental theorem for curls, which goes by the name Stokes' Theorem , states that \\int _S(\\curl \\vec{V}) \\cdot \\dd \\vec{a} = \\oint _P \\vec{v} \\cdot \\dd \\vec{l} \\tagl{1.57} As always, the integral of a derivative (here, the curl) over a region (here, a patch of surface, S) is equal to the value of the function at the boundary (here, the perimeter of the patch, P). As in the case of the divergence theorem, the boundary term is itself an integral-specifically, a closed line integral. Geometrical Interpretation: Recall that the curl measures the \"twist\" of the vectors v ; a region of high curl is a whirlpool - if you put a tiny paddle wheel there, it will rotate. Now, the integral of the curl over some surface (or, more precisely, the flux of the curl through that surface) represents the \"total amount of swirl,\" and we can determine that just as well by going around the edge and finding how much the flow is following the boundary (Fig. 1.31). Indeed, \\oint \\vec{v} \\cdot \\dd \\vec{l} is sometimes called the circulation of v. You may have noticed an apparent ambiguity in Stokes' theorem: concerning the boundary line integral, which way are we supposed to go around (clockwise or counterclockwise)? If we go the \"wrong\" way, we'll pick up an overall sign error. The answer is that it doesn't matter which way you go as long as you are consistent, for there is a compensating sign ambiguity in the surface integral: Which way does \\dd \\vec{a} point? For a closed surface (as in the divergence theorem), \\dd \\vec{a} points in the direction of the outward normal; but for an open surface, which way is \"out\"? Consistency in Stokes' theorem (as in all such matters) is given by the right-hand rule: if your fingers point in the direction of the line integral, then your thumb fixes the direction of \\dd \\vec{a} (Fig. 1.32). Now, there are plenty of surfaces (infinitely many) that share any given boundary line. Twist a paper clip into a loop, and dip it in soapy water. The soap film constitutes a surface, with the wire loop as its boundary. If you blow on it, the soap film will expand, making a larger surface, with the same boundary. Ordinarily, a flux integral depends critically on what surface you integrate over, but evidently this is not the case with curls. For Stokes' theorem says that \\int (\\curl \\vec{v}) \\cdot \\dd \\vec{a} is equal to the line integral of \\vec{v}\\ around the boundary, and the latter makes no reference to the specific surface you choose. Corollary 1: \\int (\\curl \\vec{v}) \\cdot \\dd \\vec{a} depends only on the boundary line, not on the particular surface used. Corollary 2: \\oint (\\curl \\vec{v}) \\cdot \\dd \\vec{a} = 0 for any closed surface, since the boundary line, like the mouth of a balloon, shrinks down to a point, and hence the right side of \\eqref{1.57} vanishes.","title":"1.3.5: The Fundamental Theorem for Curls"},{"location":"ch1-3/#example-111","text":"Suppose \\vec{v} = (2xz + 3y^2) \\vu{y} + (4yz^2) \\vu{z} . Check Stokes' theorem for the square surface shown in Fig 1.33. Solution Here \\curl \\vec{v} = (4z^2 - 2x) \\vu{x} + 2z \\vu{z} \\quad \\text{and} \\quad \\dd \\vec{a} = \\dd y \\, \\dd z \\, \\vu{x} (In saying that \\dd \\vec{a} points in the x direction, we are committing ourselves to a counterclockwise integral. We could as well write \\dd \\vec{a} pointing in the other direction ( \\dd \\vec{a} = - \\dd y \\, \\dd z\\, \\vu{x} ) and perform the integral in the clockwise direction.) Since x = 0 for this surface, \\int (\\curl \\vec{v}) \\cdot \\dd \\vec{a} = \\int_0 ^1 \\dd y \\int_0 ^1 \\dd z (4z^2) = \\frac{4}{3} Now for the line integral, which we of course break into 4 pieces: \\tag{i} x = 0 \\quad z = 0 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 3y^2 \\dd y, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _0 ^1 3y^2 \\dd y = 1 \\tag{ii} x = 0 \\quad y = 1 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 4z^2 \\dd z, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _0 ^1 4z^2 \\dd z = \\frac{4}{3} \\tag{iii} x = 0 \\quad z = 1 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 3y^2 \\dd y, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _1 ^0 3y^2 \\dd y = -1 \\tag{iv} x = 0 \\quad y = 0 \\quad \\vec{v} \\cdot \\dd \\vec{l} = 0 \\dd z, \\quad \\int \\vec{v} \\cdot \\dd \\vec{l} = \\int _1 ^0 0 \\dd z = 0 So \\oint \\vec{v} \\cdot \\dd \\vec{l} = 1 + \\frac{4}{3} - 1 + 0 = \\frac{4}{3} It all checks out!","title":"Example 1.11"},{"location":"ch1-3/#136-integration-by-parts","text":"The technique known (awkwardly) as integration by parts exploits the product rule for derivatives: \\dv{}{x} (fg) = f \\left( \\dv{g}{x} \\right) + g \\left( \\dv{f}{x} \\right) Integrating both sides, and invoking the fundamental theorem, \\int_a ^b \\dv{}{x} (fg) \\dd x = \\left. fg \\right| ^b _a = \\int _a ^b f \\left( \\dv{g}{x} \\right) \\dd x + \\int_a ^b g \\left( \\dv{f}{x} \\right) \\dd x or \\int_a ^b f \\left( \\dv{g}{x} \\right) \\dd x = - \\int_a ^b \\left( \\dv{f}{x} \\right) \\dd x + \\left. fg \\right| ^b _a \\tagl{1.58} That's \"integration by parts.\" It applies to the situation in which you are called upon to integrate the product of one function (f) and the derivative of another (g); it says you can transfer the derivative from g to f, at the cost of a minus sign and a boundary term.","title":"1.3.6: Integration by Parts"},{"location":"ch1-3/#example-112","text":"Evaluate the integral $$ \\int _0 ^\\infty x e^{-x} \\dd x Solution The exponential can be expressed as a derivative: e^{-x} = \\dv{}{x} \\left( - e^{-x} \\right) in this case, then, f(x) = x , g(x) = - e^{-x} , and df /dx = 1 , so \\int_0 ^\\infty x e^{-x} \\dd x = \\int _0 ^{\\infty} e^{-x} \\dd x - \\left. x e^{-x} \\right| _{0} ^{\\infty} = - \\left. e^{-x} \\right| _0 ^{\\infty} = 1 We can exploit the product rules of vector calculus, together with the appropriate fundamental theorems, in exactly the same way. For example, integrating \\div (f\\vec{A}) = f(\\div \\vec{A}) + \\vec{A} \\cdot (\\grad f) over a volume, and invoking the divergence theorem, yileds \\int \\div (f \\vec{A}) \\dd \\tau = \\int f(\\div \\vec{A}) \\dd \\tau + \\int \\vec{A} \\cdot (\\grad f) \\dd \\tau \\ \\oint f \\vec{A} \\cdot \\dd \\vec{a} or \\int _V f(\\div \\vec{A}) \\dd \\tau = - \\int _V \\vec{A} \\cdot (\\grad f) \\dd \\tau + \\oint _S f \\vec{A} \\cdot \\dd \\vec{a} \\tagl{1.59} Here again the integrand is the product of one function (f) and the derivative (in this case the divergence) of another ( A ), and integration by parts licenses us to transfer the derivative from A to f (where it becomes a gradient), at the cost of a minus sign and a boundary term (in this case a surface integral). In practice, this turns out to be one of the most useful tools at our disposal in vector calculus. Though you might wonder how often you're really likely to encounter an integral involving the product of one function and the derivative of another, the answer is surprisingly often .","title":"Example 1.12"},{"location":"ch1-4/","text":"1.4: Curvilinear Coordinates 1.4.1: Spherical Coordinates You can label a point P by its Cartesian coordinates (x, y, z), but sometimes it is more convenient to use spherical coordinates (r, \\theta, \\phi) ; r is the distance from the origin (the magnitude of the position vector r ), \\theta (the angle down from the z axis) is called the polar angle , and \\phi (the angle around from the x axis) is the azimuthal angle . Their relation to Cartesian coordinates can be read trigonometrically from Fig 1.36: x = r \\sin \\theta \\cos \\phi, \\qquad y = r \\sin \\theta \\sin \\phi, \\qquad z = r \\cos \\theta \\tagl{1.62} Figure 1.36 also shows three unit vectors, \\vu{r}, \\vu{\\theta}, \\vu{\\phi} , pointing in the direction of increase of the corresponding coordinates. They constitute an orthogonal (mutually perpendicular) basis set (just like \\vu{x}, \\vu{y}, \\vu{z} ), and any vector A can be expressed in terms of them, in the usual way: \\vec{A} = A_r \\vu{r} + A_\\theta \\vu{\\theta} + A_\\phi \\vu{\\phi} \\tagl{1.63} A_r, A_{\\theta}, A_{\\phi} are the radial, polar, and azimuthal components of A . In terms of the Cartesian unit vectors, \\begin{align*} \\vu{r} & = \\sin \\theta \\cos \\phi \\vu{x} + \\sin \\theta \\sin \\phi \\vu{y} + \\cos \\theta \\vu{z} \\\\ \\vu{\\theta} & = \\cos \\theta \\cos \\phi \\vu{x} + \\cos \\theta \\sin \\phi \\vu{y} - \\sin \\theta \\vu{z} \\\\ \\vu{\\phi} & = - \\sin \\phi \\vu{x} + \\cos \\phi \\vu{y} \\end{align*} \\tagl{1.64} as you can check for yourself (Prob 1.38). There is a poisonous snake lurking here that I'd better warn you about: \\vu{r} , \\vu{\\theta} , and \\vu{\\phi} are associated with a particular point P, and they change direction as P moves around. For example, \\vu{r} always points radially outward, but \"radially outward\" can be in the x direction, the y direction, or any other direction, depending on where you are. In Fig. 1.37, \\vec{A} = \\vu{y} and \\vec{B} = - \\vu{y} , and yet both of them would be written as \\vu{r} in spherical coordinates. One could take account of this by explicitly indicating the point of reference: \\vu{r}(\\theta, \\phi), \\vu{\\theta}(\\theta, \\phi), \\vu{\\phi}(\\theta, \\phi) , but this would be cumbersome, and as long as you are alert to the problem, I don't think it will cause difficulties. In particular, do not naively combine the spherical components of vectors associated with different points (in Fig. 1.37, \\vec{A} + \\vec{B} = 0 , not 2 \\vu{r} , and \\vec{A} \\cdot \\vec{B} = -1 , not +1 ). Beware of differentiating a vector that is expressed in spherical coordinates, since the unit vectors themselves are functions of position ( \\partial \\vu{r} / \\partial \\theta = \\vu{\\theta} , for example). And do not take \\vu{r}, \\vu{\\theta}, \\vu{\\phi} outside an integral, as I did with the Cartesian unit vectors. In general, if you're uncertain about the validity of an operation, rewrite the problem using Cartesian coordinates, for which this difficulty does not arise. An infinitesimal displacement in the \\vu{r} direction is simply dr (Fig. 1.38a), just as an infinitesimal element of length in the x direction is dx: \\dd l_r = dr \\tagl{1.65} On the other hand, an infinitesimal element of length in the \\vu{\\theta} direction (Fig 1.38b) is not just \\dd \\theta - that doesn't even have the right units for a length! Rather, \\dd l_{\\theta} = r \\dd \\theta \\tagl{1.66} Similarly, an infinitesimal element of length in the \\vu{\\phi} direction (Fig 1.38c) is \\dd l_{\\phi} = r \\sin \\theta \\dd \\phi \\tagl{1.67} so that we can write the general infinitesimal displacement as \\dd \\vec{l} = \\dd r \\vu{r} + r \\dd \\theta \\vu{\\theta} + r \\sin \\theta \\dd \\phi \\vu{\\phi} \\tagl{1.68} This plays the role that \\dd x \\vu{x} + \\dd y \\vu{y} + \\dd z \\vu{z} played in Cartesian coordinates. The infinitesimal volume element \\dd \\tau in spherical coordinates, is the product of the three infinitesimal displacements: \\dd \\tau = \\dd l_r \\dd l_{\\theta} \\dd l_{\\phi} = r^2 \\sin \\theta \\dd r \\dd \\theta \\dd \\phi \\tagl{1.69} I cannot give you a general expression for surface elements \\dd \\vec{a} , since these depend on the orientation of the surface. You simply have to analyze the geometry for any given case (this goes for Cartesian and curvilinear coordinates alike). If you are integrating over the surface of a sphere, for instance, then r is constant, whereas \\theta and \\phi change (Fig. 1.39), so \\dd \\vec{a}_1 = \\dd l_\\theta \\dd l_\\phi \\vu{r} = r^2 \\sin \\theta \\dd \\theta \\dd \\phi \\vu{r} On the other hand, if the surface lies in the xy plane, say, so that \\theta is constant ( \\pi / 2 ) while r and \\phi may vary, then \\dd \\vec{a}_2 = \\dd l_r \\dd l_{\\phi} \\vu{\\theta} = r \\dd r \\dd \\phi \\vu{\\theta} Notice, finally, that r ranges from 0 to \\infty , \\phi from 0 to 2 \\pi , and \\theta from 0 to \\pi . Example 1.13 Find the volume of a sphere of radius R Solution Well, we know that we should get \\frac{4}{3} \\pi R^3 . Let's see what happens... \\begin{align*} V & = \\int \\dd \\tau = \\int_{r = 0} ^R \\dd r \\int_{\\theta = 0} ^{\\pi} r \\dd \\theta \\int_{\\phi = 0} ^{2 \\pi} r \\sin \\theta \\dd \\phi \\\\ & = \\left( \\int_{0} ^R r^2 \\dd r \\right) \\left( \\int_0 ^\\pi \\sin \\theta \\dd \\theta \\right) \\left( \\int_0 ^{2 \\pi} \\dd \\phi \\right) \\\\ & = \\left( \\frac{R^3}{3} \\right)(2)(2 \\pi) = \\frac{4}{3} \\pi R^3 \\end{align*} Great! So far we have talked only about the geometry of spherical coordinates. Now I would like to \"translate\" the vector derivatives (gradient, divergence, curl, and Laplacian) into r, \\theta, \\phi notation. In principle, this is entirely straightforward: in the case of the gradient, \\grad T = \\pdv{T}{x} \\vu{x} + \\pdv{T}{y} \\vu{y} + \\pdv{T}{z} \\vu{z} for instance, we would first use the chain rule to expand the partials \\pdv{T}{x} = \\pdv{T}{r} \\left( \\pdv{r}{x} \\right) + \\pdv{T}{\\theta} \\left( \\pdv{\\theta}{x} \\right) + \\pdv{T}{\\phi} \\left( \\pdv{\\phi}{x} \\right) The terms in parentheses could be worked out from \\eqref{1.62} - or rather, their inverse. Then we'd do the same for y and z, and then substitute in the formulas for \\vu{x}, \\vu{y}, \\vu{z} in terms of \\vu{r}, \\vu{\\theta}, \\vu{\\phi} . It would take an hour to carry out this very brute-force approach, and I suppose this is how it was originally done, but there is a much more efficient indirect approach, which has the extra advantage of treating all coordinate systems at once. I describe the \"straightforward\" method only to show you that there is nothing subtle or mysterious about transforming to spherical coordinates: you're expressing the same quantity in different notation, that's all. The indirect method is relegated to one of the appendices, which I may add later. Here, then, are the vector derivatives in spherical coordinates: Gradient : \\grad T = \\pdv{T}{r} \\vu{r} + \\frac{1}{r} \\pdv{T}{\\theta} \\vu{\\theta} + \\frac{1}{r \\sin \\theta} \\pdv{T}{\\phi} \\vu{\\phi} \\tagl{1.70} Divergence : \\div \\vec{v} = \\frac{1}{r^2} \\pdv{}{r} (r^2 v_r) + \\frac{1}{r\\sin \\theta} \\pdv{}{\\theta} (\\sin \\theta v_{\\theta}) + \\frac{1}{r \\sin \\theta} \\pdv{v_{\\phi}}{\\phi} \\tagl{1.71} Curl : \\begin{align*} \\curl \\vec{v} = & \\frac{1}{r \\sin \\theta} \\left[ \\pdv{}{\\theta} (\\sin \\theta V_{\\phi}) - \\pdv{v_{\\theta}}{\\phi} \\right] \\vu{r} \\\\ & \\quad + \\frac{1}{r} \\left[ \\frac{1}{\\sin \\theta} \\pdv{v_r}{\\phi} - \\pdv{}{r} (r v_{\\phi}) \\right] \\vu{\\theta} \\\\ & \\quad + \\frac{1}{r} \\left[ \\pdv{}{r} (r v_{\\theta}) - \\pdv{v_r}{\\theta} \\right] \\vu{\\phi} \\end{align*} \\tagl{1.72} Laplacian : \\laplacian T = \\frac{1}{r^2} \\pdv{}{r} \\left( r^2 \\pdv{T}{r} \\right) + \\frac{1}{r^2 \\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{T}{\\theta} \\right) + \\frac{1}{r^2 \\sin ^2 \\theta} \\frac{\\partial ^2 T}{\\partial \\phi ^2} \\tagl{1.73} 1.4.2: Cylindrical Coordinates The cylindrical coordinates (s, \\phi, z) of a point P are defined in Fig 1.42. Notice that \\phi has the same meaning as in spherical coordinates, and z is the same as Cartesian. s is the distance to P from the z axis , whereas the spherical coordinate r is the distance from the origin . The relation to Cartesian coordinates is somewhat cleaner than the spherical sort x = s \\cos \\phi, \\qquad y = s \\sin \\phi, \\qquad z = z \\tagl{1.74} The unit vectors (Prob 1.42) are \\begin{align*} \\vu{s} & = \\cos \\phi \\vu{x} + \\sin \\phi \\vu{y} \\\\ \\vu{\\phi} & = - \\sin \\phi \\vu{x} + \\cos \\phi \\vu{y} \\\\ \\vu{z} & = \\vu{z} \\end{align*} \\tagl{1.75} The infinitesimal displacements are dl_s = ds, \\qquad dl_{\\phi} = s \\dd \\phi, \\qquad dl_z = dz \\tagl{1.76} so \\dd \\vec{l} = ds \\vu{s} + s \\dd \\phi \\vu{\\phi} + dz \\vu{z} \\tagl{1.77} and the volume element is \\dd \\tau = s \\, \\dd s \\, \\dd \\phi \\, \\dd z \\tagl{1.78} The range of s is 0 \\rightarrow \\infty , \\phi goes from 0 \\rightarrow 2\\pi , and z from -\\infty \\rightarrow \\infty . The vector derivatives in cylindrical coordinates are: Gradient : \\grad T = \\pdv{T}{s} \\vu{s} + \\frac{1}{s} \\pdv{T}{\\phi} \\vu{\\phi} + \\pdv{T}{z} \\vu{z} \\tagl{1.79} Divergence : \\div \\vec{v} = \\frac{1}{s} \\pdv{}{s} (s v_s) + \\frac{1}{s} \\pdv{v_\\phi}{\\phi} + \\pdv{v_z}{z} \\tagl{1.80} Curl : \\begin{align*} \\curl \\vec{v} = & \\left( \\frac{1}{s} \\pdv{v_z}{\\phi} - \\pdv{v_{\\phi}}{z} \\right) \\vu{s} \\\\ & + \\left( \\pdv{v_s}{z} - \\pdv{v_z}{s} \\right) \\vu{\\phi} \\\\ & + \\frac{1}{s} \\left[ \\pdv{}{s} (s v_{\\phi}) - \\pdv{v_s}{\\phi} \\right] \\vu{z} \\end{align*} \\tagl{1.81} Laplacian : \\laplacian T = \\frac{1}{s} \\pdv{}{s} \\left( s \\pdv{T}{s} \\right) + \\frac{1}{s^2} \\frac{\\partial ^2 T}{\\partial \\phi ^2} + \\frac{\\partial ^2 T}{\\partial z^2} \\tagl{1.82}","title":"1.4 - Curvilinear Coordinates"},{"location":"ch1-4/#14-curvilinear-coordinates","text":"","title":"1.4: Curvilinear Coordinates"},{"location":"ch1-4/#141-spherical-coordinates","text":"You can label a point P by its Cartesian coordinates (x, y, z), but sometimes it is more convenient to use spherical coordinates (r, \\theta, \\phi) ; r is the distance from the origin (the magnitude of the position vector r ), \\theta (the angle down from the z axis) is called the polar angle , and \\phi (the angle around from the x axis) is the azimuthal angle . Their relation to Cartesian coordinates can be read trigonometrically from Fig 1.36: x = r \\sin \\theta \\cos \\phi, \\qquad y = r \\sin \\theta \\sin \\phi, \\qquad z = r \\cos \\theta \\tagl{1.62} Figure 1.36 also shows three unit vectors, \\vu{r}, \\vu{\\theta}, \\vu{\\phi} , pointing in the direction of increase of the corresponding coordinates. They constitute an orthogonal (mutually perpendicular) basis set (just like \\vu{x}, \\vu{y}, \\vu{z} ), and any vector A can be expressed in terms of them, in the usual way: \\vec{A} = A_r \\vu{r} + A_\\theta \\vu{\\theta} + A_\\phi \\vu{\\phi} \\tagl{1.63} A_r, A_{\\theta}, A_{\\phi} are the radial, polar, and azimuthal components of A . In terms of the Cartesian unit vectors, \\begin{align*} \\vu{r} & = \\sin \\theta \\cos \\phi \\vu{x} + \\sin \\theta \\sin \\phi \\vu{y} + \\cos \\theta \\vu{z} \\\\ \\vu{\\theta} & = \\cos \\theta \\cos \\phi \\vu{x} + \\cos \\theta \\sin \\phi \\vu{y} - \\sin \\theta \\vu{z} \\\\ \\vu{\\phi} & = - \\sin \\phi \\vu{x} + \\cos \\phi \\vu{y} \\end{align*} \\tagl{1.64} as you can check for yourself (Prob 1.38). There is a poisonous snake lurking here that I'd better warn you about: \\vu{r} , \\vu{\\theta} , and \\vu{\\phi} are associated with a particular point P, and they change direction as P moves around. For example, \\vu{r} always points radially outward, but \"radially outward\" can be in the x direction, the y direction, or any other direction, depending on where you are. In Fig. 1.37, \\vec{A} = \\vu{y} and \\vec{B} = - \\vu{y} , and yet both of them would be written as \\vu{r} in spherical coordinates. One could take account of this by explicitly indicating the point of reference: \\vu{r}(\\theta, \\phi), \\vu{\\theta}(\\theta, \\phi), \\vu{\\phi}(\\theta, \\phi) , but this would be cumbersome, and as long as you are alert to the problem, I don't think it will cause difficulties. In particular, do not naively combine the spherical components of vectors associated with different points (in Fig. 1.37, \\vec{A} + \\vec{B} = 0 , not 2 \\vu{r} , and \\vec{A} \\cdot \\vec{B} = -1 , not +1 ). Beware of differentiating a vector that is expressed in spherical coordinates, since the unit vectors themselves are functions of position ( \\partial \\vu{r} / \\partial \\theta = \\vu{\\theta} , for example). And do not take \\vu{r}, \\vu{\\theta}, \\vu{\\phi} outside an integral, as I did with the Cartesian unit vectors. In general, if you're uncertain about the validity of an operation, rewrite the problem using Cartesian coordinates, for which this difficulty does not arise. An infinitesimal displacement in the \\vu{r} direction is simply dr (Fig. 1.38a), just as an infinitesimal element of length in the x direction is dx: \\dd l_r = dr \\tagl{1.65} On the other hand, an infinitesimal element of length in the \\vu{\\theta} direction (Fig 1.38b) is not just \\dd \\theta - that doesn't even have the right units for a length! Rather, \\dd l_{\\theta} = r \\dd \\theta \\tagl{1.66} Similarly, an infinitesimal element of length in the \\vu{\\phi} direction (Fig 1.38c) is \\dd l_{\\phi} = r \\sin \\theta \\dd \\phi \\tagl{1.67} so that we can write the general infinitesimal displacement as \\dd \\vec{l} = \\dd r \\vu{r} + r \\dd \\theta \\vu{\\theta} + r \\sin \\theta \\dd \\phi \\vu{\\phi} \\tagl{1.68} This plays the role that \\dd x \\vu{x} + \\dd y \\vu{y} + \\dd z \\vu{z} played in Cartesian coordinates. The infinitesimal volume element \\dd \\tau in spherical coordinates, is the product of the three infinitesimal displacements: \\dd \\tau = \\dd l_r \\dd l_{\\theta} \\dd l_{\\phi} = r^2 \\sin \\theta \\dd r \\dd \\theta \\dd \\phi \\tagl{1.69} I cannot give you a general expression for surface elements \\dd \\vec{a} , since these depend on the orientation of the surface. You simply have to analyze the geometry for any given case (this goes for Cartesian and curvilinear coordinates alike). If you are integrating over the surface of a sphere, for instance, then r is constant, whereas \\theta and \\phi change (Fig. 1.39), so \\dd \\vec{a}_1 = \\dd l_\\theta \\dd l_\\phi \\vu{r} = r^2 \\sin \\theta \\dd \\theta \\dd \\phi \\vu{r} On the other hand, if the surface lies in the xy plane, say, so that \\theta is constant ( \\pi / 2 ) while r and \\phi may vary, then \\dd \\vec{a}_2 = \\dd l_r \\dd l_{\\phi} \\vu{\\theta} = r \\dd r \\dd \\phi \\vu{\\theta} Notice, finally, that r ranges from 0 to \\infty , \\phi from 0 to 2 \\pi , and \\theta from 0 to \\pi .","title":"1.4.1: Spherical Coordinates"},{"location":"ch1-4/#example-113","text":"Find the volume of a sphere of radius R Solution Well, we know that we should get \\frac{4}{3} \\pi R^3 . Let's see what happens... \\begin{align*} V & = \\int \\dd \\tau = \\int_{r = 0} ^R \\dd r \\int_{\\theta = 0} ^{\\pi} r \\dd \\theta \\int_{\\phi = 0} ^{2 \\pi} r \\sin \\theta \\dd \\phi \\\\ & = \\left( \\int_{0} ^R r^2 \\dd r \\right) \\left( \\int_0 ^\\pi \\sin \\theta \\dd \\theta \\right) \\left( \\int_0 ^{2 \\pi} \\dd \\phi \\right) \\\\ & = \\left( \\frac{R^3}{3} \\right)(2)(2 \\pi) = \\frac{4}{3} \\pi R^3 \\end{align*} Great! So far we have talked only about the geometry of spherical coordinates. Now I would like to \"translate\" the vector derivatives (gradient, divergence, curl, and Laplacian) into r, \\theta, \\phi notation. In principle, this is entirely straightforward: in the case of the gradient, \\grad T = \\pdv{T}{x} \\vu{x} + \\pdv{T}{y} \\vu{y} + \\pdv{T}{z} \\vu{z} for instance, we would first use the chain rule to expand the partials \\pdv{T}{x} = \\pdv{T}{r} \\left( \\pdv{r}{x} \\right) + \\pdv{T}{\\theta} \\left( \\pdv{\\theta}{x} \\right) + \\pdv{T}{\\phi} \\left( \\pdv{\\phi}{x} \\right) The terms in parentheses could be worked out from \\eqref{1.62} - or rather, their inverse. Then we'd do the same for y and z, and then substitute in the formulas for \\vu{x}, \\vu{y}, \\vu{z} in terms of \\vu{r}, \\vu{\\theta}, \\vu{\\phi} . It would take an hour to carry out this very brute-force approach, and I suppose this is how it was originally done, but there is a much more efficient indirect approach, which has the extra advantage of treating all coordinate systems at once. I describe the \"straightforward\" method only to show you that there is nothing subtle or mysterious about transforming to spherical coordinates: you're expressing the same quantity in different notation, that's all. The indirect method is relegated to one of the appendices, which I may add later. Here, then, are the vector derivatives in spherical coordinates: Gradient : \\grad T = \\pdv{T}{r} \\vu{r} + \\frac{1}{r} \\pdv{T}{\\theta} \\vu{\\theta} + \\frac{1}{r \\sin \\theta} \\pdv{T}{\\phi} \\vu{\\phi} \\tagl{1.70} Divergence : \\div \\vec{v} = \\frac{1}{r^2} \\pdv{}{r} (r^2 v_r) + \\frac{1}{r\\sin \\theta} \\pdv{}{\\theta} (\\sin \\theta v_{\\theta}) + \\frac{1}{r \\sin \\theta} \\pdv{v_{\\phi}}{\\phi} \\tagl{1.71} Curl : \\begin{align*} \\curl \\vec{v} = & \\frac{1}{r \\sin \\theta} \\left[ \\pdv{}{\\theta} (\\sin \\theta V_{\\phi}) - \\pdv{v_{\\theta}}{\\phi} \\right] \\vu{r} \\\\ & \\quad + \\frac{1}{r} \\left[ \\frac{1}{\\sin \\theta} \\pdv{v_r}{\\phi} - \\pdv{}{r} (r v_{\\phi}) \\right] \\vu{\\theta} \\\\ & \\quad + \\frac{1}{r} \\left[ \\pdv{}{r} (r v_{\\theta}) - \\pdv{v_r}{\\theta} \\right] \\vu{\\phi} \\end{align*} \\tagl{1.72} Laplacian : \\laplacian T = \\frac{1}{r^2} \\pdv{}{r} \\left( r^2 \\pdv{T}{r} \\right) + \\frac{1}{r^2 \\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{T}{\\theta} \\right) + \\frac{1}{r^2 \\sin ^2 \\theta} \\frac{\\partial ^2 T}{\\partial \\phi ^2} \\tagl{1.73}","title":"Example 1.13"},{"location":"ch1-4/#142-cylindrical-coordinates","text":"The cylindrical coordinates (s, \\phi, z) of a point P are defined in Fig 1.42. Notice that \\phi has the same meaning as in spherical coordinates, and z is the same as Cartesian. s is the distance to P from the z axis , whereas the spherical coordinate r is the distance from the origin . The relation to Cartesian coordinates is somewhat cleaner than the spherical sort x = s \\cos \\phi, \\qquad y = s \\sin \\phi, \\qquad z = z \\tagl{1.74} The unit vectors (Prob 1.42) are \\begin{align*} \\vu{s} & = \\cos \\phi \\vu{x} + \\sin \\phi \\vu{y} \\\\ \\vu{\\phi} & = - \\sin \\phi \\vu{x} + \\cos \\phi \\vu{y} \\\\ \\vu{z} & = \\vu{z} \\end{align*} \\tagl{1.75} The infinitesimal displacements are dl_s = ds, \\qquad dl_{\\phi} = s \\dd \\phi, \\qquad dl_z = dz \\tagl{1.76} so \\dd \\vec{l} = ds \\vu{s} + s \\dd \\phi \\vu{\\phi} + dz \\vu{z} \\tagl{1.77} and the volume element is \\dd \\tau = s \\, \\dd s \\, \\dd \\phi \\, \\dd z \\tagl{1.78} The range of s is 0 \\rightarrow \\infty , \\phi goes from 0 \\rightarrow 2\\pi , and z from -\\infty \\rightarrow \\infty . The vector derivatives in cylindrical coordinates are: Gradient : \\grad T = \\pdv{T}{s} \\vu{s} + \\frac{1}{s} \\pdv{T}{\\phi} \\vu{\\phi} + \\pdv{T}{z} \\vu{z} \\tagl{1.79} Divergence : \\div \\vec{v} = \\frac{1}{s} \\pdv{}{s} (s v_s) + \\frac{1}{s} \\pdv{v_\\phi}{\\phi} + \\pdv{v_z}{z} \\tagl{1.80} Curl : \\begin{align*} \\curl \\vec{v} = & \\left( \\frac{1}{s} \\pdv{v_z}{\\phi} - \\pdv{v_{\\phi}}{z} \\right) \\vu{s} \\\\ & + \\left( \\pdv{v_s}{z} - \\pdv{v_z}{s} \\right) \\vu{\\phi} \\\\ & + \\frac{1}{s} \\left[ \\pdv{}{s} (s v_{\\phi}) - \\pdv{v_s}{\\phi} \\right] \\vu{z} \\end{align*} \\tagl{1.81} Laplacian : \\laplacian T = \\frac{1}{s} \\pdv{}{s} \\left( s \\pdv{T}{s} \\right) + \\frac{1}{s^2} \\frac{\\partial ^2 T}{\\partial \\phi ^2} + \\frac{\\partial ^2 T}{\\partial z^2} \\tagl{1.82}","title":"1.4.2: Cylindrical Coordinates"},{"location":"ch1-5/","text":"1.5: The Dirac Delta Function 1.5.1: The Divergence of \\vu{r} / r^2 Consider the vector function \\vec{v} = \\frac{1}{r^2} \\vu{r} \\tagl{1.83} At every location, v is directed radially outward (Fig. 1.44); if ever there was a function that ought to have a large positive divergence, this is it. And yet, when you actually calculate the divergence (using Eq. 1.71), you get precisely zero: \\div \\vec{v} = \\frac{1}{r^2} \\pdv{}{r} \\left( r^2 \\frac{1}{r^2} \\right) = \\frac{1}{r^2} \\pdv{}{r} (1) = 0 \\tagl{1.84} (You will have encountered this paradox already, if you worked Prob. 1.16.) The plot thickens when we apply the divergence theorem to this function. Suppose we integrate over a sphere of radius R, centered at the origin (Prob. 1.38b); the surface integral is \\begin{align*} \\oint \\vec{v} \\cdot \\dd \\vec{a} & = \\int \\left( \\frac{1}{R^2} \\vu{r} \\right) \\cdot (R^2 \\sin \\theta \\dd \\theta \\dd \\phi \\vu{r}) \\\\ & = \\left( \\int_0 ^\\pi \\sin \\theta \\dd \\theta \\right) \\left( \\int_0 ^{2 \\pi} \\dd \\phi \\right) = 4 \\pi \\end{align*} \\tagl{1.85} But if we really believe \\eqref{1.84} , then the volume integral \\int \\div \\vec{v} \\dd \\tau must be zero. What the heck is going on here?? The source of the problem is obviously the point r = 0 , where v blows up (and where, in Eq. 1.84, we have unwittingly divided by zero). It is quite true that \\div \\vec{v} = 0 everywhere except the origin, but right at the origin the situation is more complicated. Notice that the surface integral (Eq. 1.85) is independent of R; if the divergence theorem is right (and it is), we should get \\int (\\div \\vec{v}) \\dd \\tau = 4 \\pi for any sphere centered at the origin, no matter how small. Evidently the entire contribution must be coming from the point r = 0 ! Thus, \\div \\vec{v} has the bizarre property that it vanishes everywhere except at one point, and yet its integral (over any volume containing that point) is 4 \\pi . No ordinary function behaves like that. (On the other hand, a physical example does come to mind: the density (mass per unit volume) of a point particle. It's zero except at the exact location of the particle, and yet its integral is finite-namely, the mass of the particle.) What we have stumbled on is a mathematical object known to physicists as the Dirac delta function. It arises in many branches of theoretical physics. Moreover, the specific problem at hand (the divergence of the function \\vu{r} / r^2 ) is not just some arcane curiosity - it is, in fact, central to the whole theory of electrodynamics. So it is worthwhile to pause here and study the Dirac delta function with some care. 1.5.2: The One-Dimensional Dirac Delta Function The one-dimensional Dirac delta function, \\delta(x) , can be pictured as an infinitely high, infinitesimally narrow \"spike,\" with area 1 (Fig 1.45). That is to say: \\delta(x) = \\begin{cases} 0, & \\qquad \\text{ if } x \\neq 0 \\\\ \\infty , & \\qquad \\text{ if } x = 0 \\end{cases} \\tagl{1.86} and \\int_{-\\infty} ^{\\infty} \\delta(x) \\dd x = 1 \\tagl{1.87} Technically, \\delta(x) is not a function at all, since its value is not finite at x = 0; in the mathematical literature it is known as a generalized function , or distribution . It is, if you like, the limit of a sequence of functions , such as rectangles R_n(x) of height n and width 1/n , or isosceles triangles T_n(x) of height n and base 2/n (Fig 1.46) If f(x) is some \"ordinary\" function (let's just say continuous, just to be safe), then the product f(x) \\delta(x) is zero everywhere except at x = 0. It follows that f(x) \\delta(x) = f(0) \\delta(x) \\tagl{1.88} This is probably the most important fact about the delta function! Since the product is zero anyway except at x = 0, we may as well replace f(x) by the value it assumes at the origin. In particular, \\int_{-\\infty} ^{\\infty} f(x) \\delta(x) \\dd x = f(0) \\int_{-\\infty} ^{\\infty} \\delta(x) \\dd x = f(0) \\tagl{1.89} Under an integral, then, the delta function \"picks out\" the value of f(x) at a particular point. Of course, the limits of integration need not be all space, as long as the origin is included. We can also shift the spike from x = 0 to some other point, x = a, as well (Fig 1.47) \\delta(x - a) = \\begin{cases} 0, & \\text{ if } x \\neq a \\\\ \\infty , & \\text{ if } x = a \\end{cases} \\quad \\text{ with } \\quad \\int_{-\\infty} ^{\\infty} \\delta(x - a) \\dd x = 1 \\tagl{1.90} Equation \\eqref{1.88} becomes f(x) \\delta(x - a) = f(a) \\delta(x - a) and \\eqref{1.89} generalizes to \\int_{-\\infty} ^{\\infty} f(x) \\delta(x - a) \\dd x = f(a) \\tagl{1.92} Example 1.14 Evaluate the integral \\int_0 ^3 x^3 \\delta(x-2) \\dd x Solution Easy peasy. The delta function picks out the value of x^3 at the point x = 2, so the integral is 2^3 = 8 . Notice that if the limits of integration had not included x = 2, then the answer would be 0. Although \\delta(x) itself is not a legitimate function, integrals over \\delta are perfectly acceptable. In fact, it's best to think of the delta function as something that is always intended for use under an integral sign . In particular, two expressions involving delta functions are considered equal if \\int_{-\\infty} ^{\\infty} f(x) D_1 (x) \\dd x = \\int_{-\\infty} ^{\\infty} f(x) D_2 (x) \\dd x \\tagl{1.93} for all (\"ordinary\") functions f(x). Example 1.15 Show that \\delta(kx) = \\frac{1}{|k||}\\delta(x) where k is any (nonzero) constant. (In particular, \\delta(-x) = \\delta(x). ) Solution For an arbitrary test function f(x), consider the integral \\int_{-\\infty} ^{\\infty} f(x) \\delta(kx) \\dd x Changing variables, we let y \\equiv kx so that x = y/k and \\dd x = 1 / k \\dd y . If k is positive, the integration still runs from -\\infty to \\infty , but if k is negative, then x = \\infty implies y = -\\infty , and vice versa, so the order of the limits is reversed. Restoring the \"proper\" order costs a minus sign. Thus \\int_{-\\infty} ^{\\infty} f(x) \\delta(kx) \\dd x + \\pm \\int_{-\\infty} ^{\\infty} f(y/k) \\delta(y) \\frac{dy}{k} = \\pm \\frac{1}{k} f(0) = \\frac{1}{|k|} f(0) (where here the lower signs apply when k is negative, and we account for this neatly by putting absolute value bars around the final k.) Under the integral sign, then, \\delta(kx) serves the same purpose as (1/|k|)\\delta(x) : \\int_{-\\infty} ^{\\infty} f(x) \\delta(kx) \\dd x = \\int_{-\\infty} ^{\\infty} f(x) \\left[ \\frac{1}{|k|} \\delta(x) \\right] \\dd x According to our criterion \\eqref{1.93} , therefore, \\delta(kx) and (1/|k|)\\delta(x) are equal. The Three-Dimensional Delta Function It is easy to generalize the delta function to three dimensions: \\delta ^3 (\\vec{r}) = \\delta(x) \\delta(y) \\delta(z) \\tagl{1.96} This three-dimensional delta function is zero everywhere except at (0, 0, 0), where it blows up. Its volume integral is 1. \\int_{\\text{all space}} \\delta ^3 (\\vec{r}) \\dd \\tau = \\int_{-\\infty} ^{\\infty} \\dd x \\int_{-\\infty} ^{\\infty} \\dd y \\int_{-\\infty} ^{\\infty} \\dd z \\delta(x) \\delta(y) \\delta(z) = 1 \\tagl{1.97} And, generalizing \\eqref{1.92} \\int_{\\text{all space}} f(\\vec{r}) \\delta^3(\\vec{r} - \\vec{a}) \\dd \\tau = f(\\vec{a}) \\tagl{1.98} As in the one-dimensional case, integration with \\delta picks out the value of the function f at the location of the spike. We are now in a position to resolve the paradox introduced in Sect. 1.5.1. As you will recall, we found that the divergence of \\vu{r}/r^2 is zero everywhere except at the origin, and yet its integral over any volume containing the origin is a constant (to wit: 4\\pi ). These are precisely the defining conditions for the Dirac delta function; evidently \\div \\left( \\frac{\\vu{r}}{r^2} \\right) = 4 \\pi \\delta^3(\\vec{r}) \\tagl{1.99} More generally, \\div \\left( \\frac{\\vu{\\gr}}{\\gr^2} \\right) = 4 \\pi \\delta^3(\\gr) \\tagl{1.100} where, as always, \\vec{\\gr} is the separation vector \\vec{\\gr} \\equiv \\vec{r} - \\vec{r'} . Note that differentiation here is with respect to \\vec{r} , while \\vec{r'} is held constant. Incidentally, since \\grad \\left( \\frac{1}{\\gr} \\right) = - \\frac{\\vu{\\gr}}{\\gr ^2} (from Problem 1.13), it follows that \\laplacian \\frac{1}{\\gr} = - 4 \\pi \\delta^3 (\\vec{\\gr}) \\tagl{1.102} Example 1.16 Evaluate the integral J = \\int_V (r^2 + 2) \\div \\left( \\frac{\\vu{r}}{r^2} \\right) \\dd \\tau where V is a sphere of radius R centered at the origin. Solution 1 Use \\eqref{1.99} to rewrite the divergence, and \\eqref{1.98} to do the integral: J = \\int_V (r^2 + 2) 4 \\pi \\delta ^3(\\vec{r}) \\dd \\tau = 4 \\pi (0 + 2) = 8 \\pi This one-line solution demonstrates something of the power and beauty of the delta function, but I would like to show you a second method, which is much more cumbersome but serves to illustrate the method of integration by parts (Sect. 1.3.6). Solution 2 Using Eq 1.59, we transfer the derivative from \\vu{r}/r^2 to (r^2 + 2) J = - \\int_V \\frac{\\vu{r}}{r^2} \\cdot [\\grad(r^2 + 2)] \\dd \\tau + \\oint_S (r^2 + 2) \\frac{\\vu{r}}{r^2} \\cdot \\dd \\vec{a} The gradient is \\grad(r^2 + 2) = 2 r \\vu{r} so the volume integral becomes \\int \\frac{2}{r} \\dd \\tau = \\int \\frac{2}{r} r^2 \\sin \\theta \\dd r \\dd \\theta \\dd \\phi = 8 \\pi \\int_0 ^R r \\dd r = 4 \\pi R^2 Meanwhile on the boundary of the sphere (where r = R) \\dd \\vec{a} = R^2 \\sin \\theta \\dd \\theta \\dd \\phi \\vu{r} so the surface integral is \\int(R^2 + 2) \\sin \\theta \\dd \\theta \\dd \\phi = 4 \\pi (R^2 + 2) which, all together makes J = -4 \\pi R^2 + r \\pi (R^2 + 2) = 8 \\pi In proper mathematical jargon, \"sphere\" denotes the surface, and \"ball\" the volume it encloses. But physicists are (as usual) sloppy about this sort of thing, and I use the word \"sphere\" for both the surface and the volume. Where the meaning is not clear from the context, I will write \"spherical surface\" or \"spherical volume.\" The language police tell me that the former is redundant and the latter an oxymoron, but a poll of my physics colleagues reveals that this is (for us) the standard usage.","title":"1.5 - The Dirac Delta Function"},{"location":"ch1-5/#15-the-dirac-delta-function","text":"","title":"1.5: The Dirac Delta Function"},{"location":"ch1-5/#151-the-divergence-of-vur-r2","text":"Consider the vector function \\vec{v} = \\frac{1}{r^2} \\vu{r} \\tagl{1.83} At every location, v is directed radially outward (Fig. 1.44); if ever there was a function that ought to have a large positive divergence, this is it. And yet, when you actually calculate the divergence (using Eq. 1.71), you get precisely zero: \\div \\vec{v} = \\frac{1}{r^2} \\pdv{}{r} \\left( r^2 \\frac{1}{r^2} \\right) = \\frac{1}{r^2} \\pdv{}{r} (1) = 0 \\tagl{1.84} (You will have encountered this paradox already, if you worked Prob. 1.16.) The plot thickens when we apply the divergence theorem to this function. Suppose we integrate over a sphere of radius R, centered at the origin (Prob. 1.38b); the surface integral is \\begin{align*} \\oint \\vec{v} \\cdot \\dd \\vec{a} & = \\int \\left( \\frac{1}{R^2} \\vu{r} \\right) \\cdot (R^2 \\sin \\theta \\dd \\theta \\dd \\phi \\vu{r}) \\\\ & = \\left( \\int_0 ^\\pi \\sin \\theta \\dd \\theta \\right) \\left( \\int_0 ^{2 \\pi} \\dd \\phi \\right) = 4 \\pi \\end{align*} \\tagl{1.85} But if we really believe \\eqref{1.84} , then the volume integral \\int \\div \\vec{v} \\dd \\tau must be zero. What the heck is going on here?? The source of the problem is obviously the point r = 0 , where v blows up (and where, in Eq. 1.84, we have unwittingly divided by zero). It is quite true that \\div \\vec{v} = 0 everywhere except the origin, but right at the origin the situation is more complicated. Notice that the surface integral (Eq. 1.85) is independent of R; if the divergence theorem is right (and it is), we should get \\int (\\div \\vec{v}) \\dd \\tau = 4 \\pi for any sphere centered at the origin, no matter how small. Evidently the entire contribution must be coming from the point r = 0 ! Thus, \\div \\vec{v} has the bizarre property that it vanishes everywhere except at one point, and yet its integral (over any volume containing that point) is 4 \\pi . No ordinary function behaves like that. (On the other hand, a physical example does come to mind: the density (mass per unit volume) of a point particle. It's zero except at the exact location of the particle, and yet its integral is finite-namely, the mass of the particle.) What we have stumbled on is a mathematical object known to physicists as the Dirac delta function. It arises in many branches of theoretical physics. Moreover, the specific problem at hand (the divergence of the function \\vu{r} / r^2 ) is not just some arcane curiosity - it is, in fact, central to the whole theory of electrodynamics. So it is worthwhile to pause here and study the Dirac delta function with some care.","title":"1.5.1: The Divergence of  \\vu{r} / r^2"},{"location":"ch1-5/#152-the-one-dimensional-dirac-delta-function","text":"The one-dimensional Dirac delta function, \\delta(x) , can be pictured as an infinitely high, infinitesimally narrow \"spike,\" with area 1 (Fig 1.45). That is to say: \\delta(x) = \\begin{cases} 0, & \\qquad \\text{ if } x \\neq 0 \\\\ \\infty , & \\qquad \\text{ if } x = 0 \\end{cases} \\tagl{1.86} and \\int_{-\\infty} ^{\\infty} \\delta(x) \\dd x = 1 \\tagl{1.87} Technically, \\delta(x) is not a function at all, since its value is not finite at x = 0; in the mathematical literature it is known as a generalized function , or distribution . It is, if you like, the limit of a sequence of functions , such as rectangles R_n(x) of height n and width 1/n , or isosceles triangles T_n(x) of height n and base 2/n (Fig 1.46) If f(x) is some \"ordinary\" function (let's just say continuous, just to be safe), then the product f(x) \\delta(x) is zero everywhere except at x = 0. It follows that f(x) \\delta(x) = f(0) \\delta(x) \\tagl{1.88} This is probably the most important fact about the delta function! Since the product is zero anyway except at x = 0, we may as well replace f(x) by the value it assumes at the origin. In particular, \\int_{-\\infty} ^{\\infty} f(x) \\delta(x) \\dd x = f(0) \\int_{-\\infty} ^{\\infty} \\delta(x) \\dd x = f(0) \\tagl{1.89} Under an integral, then, the delta function \"picks out\" the value of f(x) at a particular point. Of course, the limits of integration need not be all space, as long as the origin is included. We can also shift the spike from x = 0 to some other point, x = a, as well (Fig 1.47) \\delta(x - a) = \\begin{cases} 0, & \\text{ if } x \\neq a \\\\ \\infty , & \\text{ if } x = a \\end{cases} \\quad \\text{ with } \\quad \\int_{-\\infty} ^{\\infty} \\delta(x - a) \\dd x = 1 \\tagl{1.90} Equation \\eqref{1.88} becomes f(x) \\delta(x - a) = f(a) \\delta(x - a) and \\eqref{1.89} generalizes to \\int_{-\\infty} ^{\\infty} f(x) \\delta(x - a) \\dd x = f(a) \\tagl{1.92}","title":"1.5.2: The One-Dimensional Dirac Delta Function"},{"location":"ch1-5/#example-114","text":"Evaluate the integral \\int_0 ^3 x^3 \\delta(x-2) \\dd x Solution Easy peasy. The delta function picks out the value of x^3 at the point x = 2, so the integral is 2^3 = 8 . Notice that if the limits of integration had not included x = 2, then the answer would be 0. Although \\delta(x) itself is not a legitimate function, integrals over \\delta are perfectly acceptable. In fact, it's best to think of the delta function as something that is always intended for use under an integral sign . In particular, two expressions involving delta functions are considered equal if \\int_{-\\infty} ^{\\infty} f(x) D_1 (x) \\dd x = \\int_{-\\infty} ^{\\infty} f(x) D_2 (x) \\dd x \\tagl{1.93} for all (\"ordinary\") functions f(x).","title":"Example 1.14"},{"location":"ch1-5/#example-115","text":"Show that \\delta(kx) = \\frac{1}{|k||}\\delta(x) where k is any (nonzero) constant. (In particular, \\delta(-x) = \\delta(x). ) Solution For an arbitrary test function f(x), consider the integral \\int_{-\\infty} ^{\\infty} f(x) \\delta(kx) \\dd x Changing variables, we let y \\equiv kx so that x = y/k and \\dd x = 1 / k \\dd y . If k is positive, the integration still runs from -\\infty to \\infty , but if k is negative, then x = \\infty implies y = -\\infty , and vice versa, so the order of the limits is reversed. Restoring the \"proper\" order costs a minus sign. Thus \\int_{-\\infty} ^{\\infty} f(x) \\delta(kx) \\dd x + \\pm \\int_{-\\infty} ^{\\infty} f(y/k) \\delta(y) \\frac{dy}{k} = \\pm \\frac{1}{k} f(0) = \\frac{1}{|k|} f(0) (where here the lower signs apply when k is negative, and we account for this neatly by putting absolute value bars around the final k.) Under the integral sign, then, \\delta(kx) serves the same purpose as (1/|k|)\\delta(x) : \\int_{-\\infty} ^{\\infty} f(x) \\delta(kx) \\dd x = \\int_{-\\infty} ^{\\infty} f(x) \\left[ \\frac{1}{|k|} \\delta(x) \\right] \\dd x According to our criterion \\eqref{1.93} , therefore, \\delta(kx) and (1/|k|)\\delta(x) are equal.","title":"Example 1.15"},{"location":"ch1-5/#the-three-dimensional-delta-function","text":"It is easy to generalize the delta function to three dimensions: \\delta ^3 (\\vec{r}) = \\delta(x) \\delta(y) \\delta(z) \\tagl{1.96} This three-dimensional delta function is zero everywhere except at (0, 0, 0), where it blows up. Its volume integral is 1. \\int_{\\text{all space}} \\delta ^3 (\\vec{r}) \\dd \\tau = \\int_{-\\infty} ^{\\infty} \\dd x \\int_{-\\infty} ^{\\infty} \\dd y \\int_{-\\infty} ^{\\infty} \\dd z \\delta(x) \\delta(y) \\delta(z) = 1 \\tagl{1.97} And, generalizing \\eqref{1.92} \\int_{\\text{all space}} f(\\vec{r}) \\delta^3(\\vec{r} - \\vec{a}) \\dd \\tau = f(\\vec{a}) \\tagl{1.98} As in the one-dimensional case, integration with \\delta picks out the value of the function f at the location of the spike. We are now in a position to resolve the paradox introduced in Sect. 1.5.1. As you will recall, we found that the divergence of \\vu{r}/r^2 is zero everywhere except at the origin, and yet its integral over any volume containing the origin is a constant (to wit: 4\\pi ). These are precisely the defining conditions for the Dirac delta function; evidently \\div \\left( \\frac{\\vu{r}}{r^2} \\right) = 4 \\pi \\delta^3(\\vec{r}) \\tagl{1.99} More generally, \\div \\left( \\frac{\\vu{\\gr}}{\\gr^2} \\right) = 4 \\pi \\delta^3(\\gr) \\tagl{1.100} where, as always, \\vec{\\gr} is the separation vector \\vec{\\gr} \\equiv \\vec{r} - \\vec{r'} . Note that differentiation here is with respect to \\vec{r} , while \\vec{r'} is held constant. Incidentally, since \\grad \\left( \\frac{1}{\\gr} \\right) = - \\frac{\\vu{\\gr}}{\\gr ^2} (from Problem 1.13), it follows that \\laplacian \\frac{1}{\\gr} = - 4 \\pi \\delta^3 (\\vec{\\gr}) \\tagl{1.102}","title":"The Three-Dimensional Delta Function"},{"location":"ch1-5/#example-116","text":"Evaluate the integral J = \\int_V (r^2 + 2) \\div \\left( \\frac{\\vu{r}}{r^2} \\right) \\dd \\tau where V is a sphere of radius R centered at the origin. Solution 1 Use \\eqref{1.99} to rewrite the divergence, and \\eqref{1.98} to do the integral: J = \\int_V (r^2 + 2) 4 \\pi \\delta ^3(\\vec{r}) \\dd \\tau = 4 \\pi (0 + 2) = 8 \\pi This one-line solution demonstrates something of the power and beauty of the delta function, but I would like to show you a second method, which is much more cumbersome but serves to illustrate the method of integration by parts (Sect. 1.3.6). Solution 2 Using Eq 1.59, we transfer the derivative from \\vu{r}/r^2 to (r^2 + 2) J = - \\int_V \\frac{\\vu{r}}{r^2} \\cdot [\\grad(r^2 + 2)] \\dd \\tau + \\oint_S (r^2 + 2) \\frac{\\vu{r}}{r^2} \\cdot \\dd \\vec{a} The gradient is \\grad(r^2 + 2) = 2 r \\vu{r} so the volume integral becomes \\int \\frac{2}{r} \\dd \\tau = \\int \\frac{2}{r} r^2 \\sin \\theta \\dd r \\dd \\theta \\dd \\phi = 8 \\pi \\int_0 ^R r \\dd r = 4 \\pi R^2 Meanwhile on the boundary of the sphere (where r = R) \\dd \\vec{a} = R^2 \\sin \\theta \\dd \\theta \\dd \\phi \\vu{r} so the surface integral is \\int(R^2 + 2) \\sin \\theta \\dd \\theta \\dd \\phi = 4 \\pi (R^2 + 2) which, all together makes J = -4 \\pi R^2 + r \\pi (R^2 + 2) = 8 \\pi In proper mathematical jargon, \"sphere\" denotes the surface, and \"ball\" the volume it encloses. But physicists are (as usual) sloppy about this sort of thing, and I use the word \"sphere\" for both the surface and the volume. Where the meaning is not clear from the context, I will write \"spherical surface\" or \"spherical volume.\" The language police tell me that the former is redundant and the latter an oxymoron, but a poll of my physics colleagues reveals that this is (for us) the standard usage.","title":"Example 1.16"},{"location":"ch1-6/","text":"1.6: The Theory of Vector Fields 1.6.1: The Helmholtz Theorem Ever since Faraday, the laws of electricity and magnetism have been expressed in terms of electric and magnetic fields, E and B . Like many physical laws, these are most compactly expressed as differential equations. Since E and B are vectors, the differential equations naturally involve vector derivatives: divergence and curl. Indeed, Maxwell reduced the entire theory to four equations, specifying respectively the divergence and the curl of E and B . Maxwell's formulation raises an important mathematical question: To what extent is a vector function determined by its divergence and curl? In other words, if I tell you that the divergence of F (which stands for E or B , as the case may be) is a specified (scalar) function D, \\div \\vec{F} = D and the curl of F is a specified (vector) function C , \\curl \\vec{F} = \\vec{C} can you then determine the function F ? Well... not quite. For example, as you may have discovered in Prob. 1.20, there are many functions whose divergence and curl are both zero everywhere - the trivial case \\vec{F} = 0 , of course, but also \\vec{F} = yz \\vu{x} + zx \\vu{y} + xy \\vu{z}, \\vec{F} = \\sin x \\cosh y \\vu{x} - \\cos x \\sinh y \\vu{y} , etc. To solve a differential equation you must also be supplied with appropriate boundary conditions . In electrodynamics we typically require that the fields go to zero \"at infinity\" (far away from all charges). With that extra information, the Helmholtz theorem guarantees that the field is uniquely determined by its divergence and curl. (The Helmholtz theorem is discussed in Appendix B.) 1.6.2: Potentials If the curl of a vector field ( F ) vanishes (everywhere), then F can be written as the gradient of a scalar potential (V): \\curl \\vec{F} = 0 \\Longleftrightarrow \\vec{F} = - \\grad V \\tagl{1.103} (The minus sign is purely conventional.) That's the essential burden of the following theorem: Theorem 1 . Curl-less (or 'irrotational') fields. The following conditions are equivalent (that is, F satisfies one if and only if it satisfies all the others): \\tag{a} \\curl \\vec{F} = 0 \\text{ everywhere } \\tag{b} \\int_a ^b \\vec{F} \\cdot \\dd \\vec{l} \\text{ is independent of path, for any given end points} \\tag{c} \\oint \\vec{F} \\cdot \\dd \\vec{l} = 0 \\text{ for any closed loop} \\tag{d} \\vec{F} \\text{ is the gradient of some scalar function: } \\vec{F} = - \\grad V The potential is not unique, and any constant can be added to V with impunity, since this will not affect its gradient. If the divergence of a vector field ( F ) vanishes (everywhere), then F can be expressed as the curl of a vector potential ( A ): \\div \\vec{F} = 0 \\Longleftrightarrow \\vec{F} = \\curl \\vec{A} \\tagl{1.104} That's the main conclusion of the following theorem: Theorem 2 . Divergence-less (or 'solenoidal') fields. The following conditions are equivalent: \\tag{a} \\div \\vec{F} = 0 \\text{ everywhere} \\tag{b} \\int \\vec{F} \\cdot \\dd \\vec{a} \\text{ is independent of surface, for any given boundary line} \\tag{c} \\oint \\vec{F} \\cdot \\dd \\vec{a} = 0 \\text{ for any closed surface.} \\tag{d} \\vec{F} \\text{ is the curl of some vector function: } \\vec{F} = \\curl \\vec{A} The vector potential is not unique - the gradient of any scalar function can be added to A without affecting the curl, since the curl of a gradient is zero. Incidentally, in all cases (whatever its curl and divergence may be), a vector field F can be written as the gradient of a scalar plus the curl of a vector: \\vec{F} = - \\grad V + \\curl \\vec{A} \\quad \\text{(always)} \\tagl{1.50} In physics, the word field denotes generically any function of position (x, y, z) and time (t). But in electrodynamics two particular fields ( E and B ) are of such paramount importance as to preempt the term. Thus technically the potentials are also \"fields,\" but we never call them that.","title":"1.6 - The Theory of Vector Fields"},{"location":"ch1-6/#16-the-theory-of-vector-fields","text":"","title":"1.6: The Theory of Vector Fields"},{"location":"ch1-6/#161-the-helmholtz-theorem","text":"Ever since Faraday, the laws of electricity and magnetism have been expressed in terms of electric and magnetic fields, E and B . Like many physical laws, these are most compactly expressed as differential equations. Since E and B are vectors, the differential equations naturally involve vector derivatives: divergence and curl. Indeed, Maxwell reduced the entire theory to four equations, specifying respectively the divergence and the curl of E and B . Maxwell's formulation raises an important mathematical question: To what extent is a vector function determined by its divergence and curl? In other words, if I tell you that the divergence of F (which stands for E or B , as the case may be) is a specified (scalar) function D, \\div \\vec{F} = D and the curl of F is a specified (vector) function C , \\curl \\vec{F} = \\vec{C} can you then determine the function F ? Well... not quite. For example, as you may have discovered in Prob. 1.20, there are many functions whose divergence and curl are both zero everywhere - the trivial case \\vec{F} = 0 , of course, but also \\vec{F} = yz \\vu{x} + zx \\vu{y} + xy \\vu{z}, \\vec{F} = \\sin x \\cosh y \\vu{x} - \\cos x \\sinh y \\vu{y} , etc. To solve a differential equation you must also be supplied with appropriate boundary conditions . In electrodynamics we typically require that the fields go to zero \"at infinity\" (far away from all charges). With that extra information, the Helmholtz theorem guarantees that the field is uniquely determined by its divergence and curl. (The Helmholtz theorem is discussed in Appendix B.)","title":"1.6.1: The Helmholtz Theorem"},{"location":"ch1-6/#162-potentials","text":"If the curl of a vector field ( F ) vanishes (everywhere), then F can be written as the gradient of a scalar potential (V): \\curl \\vec{F} = 0 \\Longleftrightarrow \\vec{F} = - \\grad V \\tagl{1.103} (The minus sign is purely conventional.) That's the essential burden of the following theorem: Theorem 1 . Curl-less (or 'irrotational') fields. The following conditions are equivalent (that is, F satisfies one if and only if it satisfies all the others): \\tag{a} \\curl \\vec{F} = 0 \\text{ everywhere } \\tag{b} \\int_a ^b \\vec{F} \\cdot \\dd \\vec{l} \\text{ is independent of path, for any given end points} \\tag{c} \\oint \\vec{F} \\cdot \\dd \\vec{l} = 0 \\text{ for any closed loop} \\tag{d} \\vec{F} \\text{ is the gradient of some scalar function: } \\vec{F} = - \\grad V The potential is not unique, and any constant can be added to V with impunity, since this will not affect its gradient. If the divergence of a vector field ( F ) vanishes (everywhere), then F can be expressed as the curl of a vector potential ( A ): \\div \\vec{F} = 0 \\Longleftrightarrow \\vec{F} = \\curl \\vec{A} \\tagl{1.104} That's the main conclusion of the following theorem: Theorem 2 . Divergence-less (or 'solenoidal') fields. The following conditions are equivalent: \\tag{a} \\div \\vec{F} = 0 \\text{ everywhere} \\tag{b} \\int \\vec{F} \\cdot \\dd \\vec{a} \\text{ is independent of surface, for any given boundary line} \\tag{c} \\oint \\vec{F} \\cdot \\dd \\vec{a} = 0 \\text{ for any closed surface.} \\tag{d} \\vec{F} \\text{ is the curl of some vector function: } \\vec{F} = \\curl \\vec{A} The vector potential is not unique - the gradient of any scalar function can be added to A without affecting the curl, since the curl of a gradient is zero. Incidentally, in all cases (whatever its curl and divergence may be), a vector field F can be written as the gradient of a scalar plus the curl of a vector: \\vec{F} = - \\grad V + \\curl \\vec{A} \\quad \\text{(always)} \\tagl{1.50} In physics, the word field denotes generically any function of position (x, y, z) and time (t). But in electrodynamics two particular fields ( E and B ) are of such paramount importance as to preempt the term. Thus technically the potentials are also \"fields,\" but we never call them that.","title":"1.6.2: Potentials"},{"location":"ch2-1/","text":"2.1: The Electric Field 2.1.1: Introduction The fundamental problem electrodynamics hopes to solve is this (Fig 2.1): We have some electric charges q_1, q_2, q_3, \\ldots (call them source charges ); what force do they exert on another charge, Q (call it the test charge )? The positions of the source charges are given (as functions of time); the trajectory of the test particle is to be calculated. In general, both the source charges and the test charge are in motion. ![Fig 2.1\" src=\"../img/2.1.png\" /> The solution to this problem is facilitated by the principle of superposition, which states that the interaction between any two charges is completely unaffected by the presence of others. This means that to determine the force on Q, we can first compute the force \\vec{F_1} , due to q_1 alone (ignoring all the others); then we compute the force \\vec{F_2} , due to q_2 alone, and so in. Finally, we take the vector sum of all these individual forces: \\vec{F} = \\vec{F_1} + \\vec{F_2} + \\vec{F_3} + \\ldots Thus, if we can find the force on Q due to a single source charge q , we are, in principle, done (the rest is just a question of repeating the same operation over and over, and adding it all up) The principle of superposition may seem \"obvious\" to you, but it did not have to be so simple: if the electromagnetic force were proportional to the square of the total source charge, for instance, the principle of superposition would not hold, since (q_1 + q_2)^2 \\neq q_1 ^2 + q_2 ^2 (there would be \"cross terms\" to consider). Superposition is not a logical necessity, but an experimental fact. Well, at first sight this looks very easy: Why don't I just write down the formula for the force on Q due to q, and be done with it? I could , and in Chapter 10 I shall, but you would be shocked to see it at this stage, for not only does the force on Q depend on the separation distance \\gr between the charges (Fig 2.2), it also depends on both their velocities and on the acceleration of q . Moreover, it is not the position, velocity, and acceleration of q right now that matter: electromagnetic \"news\" travels at the speed of light, so what concerns Q is the position, velocity, and acceleration q had at some earlier time, when the message left. Therefore, in spite of the fact that the basic question (\"What is the force on Q due to q?\") is easy to state, it does not pay to confront it head on; rather, we shall go at it by stages. In the meantime, the theory we develop will allow for the solution of more subtle electromagnetic problems that do not present themselves in quite this simple format. To begin with, we shall consider the special case of electrostatics in which all the source charges are stationary (though the test charge may be moving). 2.1.2: Coulomb's Law What is the force on a test charge Q due to a single point charge q, that is at rest a distance \\gr away? The answer (based on experiments) is given by Coulomb's Law : \\vec{F} = \\frac{1}{4 \\pi \\epsilon_0}\\frac{q Q}{\\gr^2} \\vu{\\vec{\\gr}} \\label{2.1} \\tag{2.1} The constant \\epsilon_0 is called (ludicrously) the permittivity of free space . In SI units, where force is in newtons (N), distance in meters (m), and charge in coulombs (C), \\epsilon_0 = 8.85 \\times 10^{-12} \\frac{C^2}{N \\cdot m ^2} In words, the force is proportional to the product of the charges and inversely proportional to the square of the separation distance. As always (Sect 1.1.4), \\vec{\\gr} is the separation vector from \\vec{r'} (the location of q) to \\vec{r} (the location of Q): \\vec{\\gr} = \\vec{r} - \\vec{r}' \\gr is its magnitude, and \\vu{\\gr} is its direction. The force points along the line from q to Q; it is repulsive if q and Q have the same sign, and attractive if their signs are opposite. Coulomb's law and the principle of superposition constitute the physical input for electrostatics - the rest, except for some special properties of matter, is mathematical elaboration of these fundamental rules. 2.1.3: The Electric Field If we have several point charges q_1, q_2, \\ldots , q_n at distances \\gr_1 \\gr_2 \\ldots, \\gr_n from Q , the total force on Q is evidently \\begin{align} \\vec{F} & = & \\vec{F_1} + \\vec{F_2} + \\ldots \\\\ & = & \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q_1 Q}{\\gr_1 ^2} \\vu{\\gr}_1 + \\frac{q_2 Q}{\\gr_2 ^2} \\vu{\\gr}_2 + \\ldots \\right) \\\\ & = & \\frac{Q}{4 \\pi \\epsilon _0} \\left( \\frac{q_1}{\\gr ^2 _1} \\vu{\\gr_1} + \\frac{q_2}{\\gr _2 ^2}\\vu{\\gr_2} + \\ldots \\right) \\end{align} or \\vec{F} = Q \\vec{E} \\label{2.3} \\tag{2.3} where \\vec{E}(\\vec{r}) \\equiv \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i = 1}^n \\frac{q_i}{\\gr_{i}^2} \\vu{\\gr_i} \\label{2.4} \\tag{2.4} E is called the electric field of the source charges. Notice that it is a function of position ( r ), because the separation vectors \\gr_i depend on the location of the field point P (Fig 2.3). But it makes no reference to the test charge Q. The electric field is a vector quantity that varies from point to point and is determined by the configuration of source charges; physically, \\vec{E}(\\vec{r}) is the force per unit charge that would be exerted on a test charge, if you were to place one at P. What exactly is an electric field? I have deliberately begun with what you might call the \"minimal\" interpretation of E , as an intermediate step in the calculation of electric forces. But I encourage you to think of the field as a \"real\" physical entity, filling the space around electric charges. Maxwell himself came to believe that electric and magnetic fields are stresses and strains in an invisible primordial jellylike \"ether.\" Special relativity has forced us to abandon the notion of either, and with it Maxwell's mechanical interpretation of electromagnetic fields. (It is even possible, although cumbersome, to formulate classical electrodynamics as an \"action-at-a-distance\" theory, and dispense with the field concept altogether.) I can't tell you, then, what a field is -- only how to calculate it and what it can do for you once you've got it. Example 2.1 Find the electric field a distance z above the midpoint between two equal charges (q), a distance d apart (Fig. 2.4a) Solution Let \\vec{E_1} be the field of the left charge alone, and \\vec{E_2} that of the right charge alone (Fig. 2.4b). Adding them (vectorially), the horizontal components cancel and the vertical components conspire E_z = 2 \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{\\gr ^2} \\cos \\theta Here \\gr = \\sqrt{z^2 + (d/2)^2} and \\cos \\theta = z / \\gr , so \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{2qz}{\\left[ z^2 + (d/2)^2 \\right]^{3/2}} \\vu{z} Check : When z \\gg d you're so far away that it just looks like a single charge 2q , so the field should reduce to \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{2q}{z^2} \\vu{z} . And it does, just set d \\rightarrow 0 in the formula). 2.1.4: Continuous Charge Distributions Our definition of the electric field (Eq. \\eqref{2.4} ) assumes that the source of the field is a collection of discrete point charges q_i . If, instead, the charge is distributed continuously over some region, the sum becomes an integral (Fig 2.5a): \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{1}{\\gr ^2} \\vu{\\gr} \\dd{q} If the charge is spread out along a line (Fig. 2.5b), with charge-per-unit-length \\lambda then \\dd{q} = \\lambda \\dd{l}' (where \\dd{l}' ) is an element of length along the line); if the charge is smeared out over a surface (Fig. 2.5c) with charge-per-unit-area \\sigma , then \\dd{q} = \\sigma \\dd{a}' (where \\dd{a'} ) is an element of area on the surface); and if the charge fills a volume (Fig 2.5d), with charge-per-unit-volume \\rho , then \\dd{q} = \\rho\\dd{\\tau'} (where \\dd{\\tau'} is an element of volume): dq \\rightarrow \\lambda \\dd{l'} \\sim \\sigma \\dd{a'} \\sim \\rho \\dd{\\tau'} Thus the electric field of a line charge is \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\lambda(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{l'} for a surface charge, \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{a'} and for a volume charge, \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{\\tau'} \\label{2.8} \\tag{2.8} Equation \\eqref{2.8} itself is often referred to as \"Coulomb's law,\" because it is such a short step from the original, and because a volume charge is in a sense the most general and realistic case. Please note carefully the meaning of \\gr in these formulas. Originally, in \\eqref{2.4} , \\gr_i stood for the vector from the source charge q_i to the field point r . Correspondingly, in Eq.s 9-11, \\gr is the vector from \\dd{q} to the field point \\vec{r} . Warning: the unit vector \\vu{\\gr} is not constant: its direction depends on the source point \\vec{r'} , and hence it cannot be taken outside the integrals (9-11). In practice, you must work with Cartesian components ( \\vu{x}, \\vu{y}, \\vu{z} are constant, and do come out) , even if you use curvilinear coordinates to perform the integration. Example 2.2 Find the electric field a distance z above the midpoint of a straight line segment of length 2L that carries a uniform line charge \\lambda (Fig. 2.6). TODO!","title":"2.1 - The Electric Field"},{"location":"ch2-1/#21-the-electric-field","text":"","title":"2.1: The Electric Field"},{"location":"ch2-1/#211-introduction","text":"The fundamental problem electrodynamics hopes to solve is this (Fig 2.1): We have some electric charges q_1, q_2, q_3, \\ldots (call them source charges ); what force do they exert on another charge, Q (call it the test charge )? The positions of the source charges are given (as functions of time); the trajectory of the test particle is to be calculated. In general, both the source charges and the test charge are in motion. ![Fig 2.1\" src=\"../img/2.1.png\" /> The solution to this problem is facilitated by the principle of superposition, which states that the interaction between any two charges is completely unaffected by the presence of others. This means that to determine the force on Q, we can first compute the force \\vec{F_1} , due to q_1 alone (ignoring all the others); then we compute the force \\vec{F_2} , due to q_2 alone, and so in. Finally, we take the vector sum of all these individual forces: \\vec{F} = \\vec{F_1} + \\vec{F_2} + \\vec{F_3} + \\ldots Thus, if we can find the force on Q due to a single source charge q , we are, in principle, done (the rest is just a question of repeating the same operation over and over, and adding it all up) The principle of superposition may seem \"obvious\" to you, but it did not have to be so simple: if the electromagnetic force were proportional to the square of the total source charge, for instance, the principle of superposition would not hold, since (q_1 + q_2)^2 \\neq q_1 ^2 + q_2 ^2 (there would be \"cross terms\" to consider). Superposition is not a logical necessity, but an experimental fact. Well, at first sight this looks very easy: Why don't I just write down the formula for the force on Q due to q, and be done with it? I could , and in Chapter 10 I shall, but you would be shocked to see it at this stage, for not only does the force on Q depend on the separation distance \\gr between the charges (Fig 2.2), it also depends on both their velocities and on the acceleration of q . Moreover, it is not the position, velocity, and acceleration of q right now that matter: electromagnetic \"news\" travels at the speed of light, so what concerns Q is the position, velocity, and acceleration q had at some earlier time, when the message left. Therefore, in spite of the fact that the basic question (\"What is the force on Q due to q?\") is easy to state, it does not pay to confront it head on; rather, we shall go at it by stages. In the meantime, the theory we develop will allow for the solution of more subtle electromagnetic problems that do not present themselves in quite this simple format. To begin with, we shall consider the special case of electrostatics in which all the source charges are stationary (though the test charge may be moving).","title":"2.1.1: Introduction"},{"location":"ch2-1/#212-coulombs-law","text":"What is the force on a test charge Q due to a single point charge q, that is at rest a distance \\gr away? The answer (based on experiments) is given by Coulomb's Law : \\vec{F} = \\frac{1}{4 \\pi \\epsilon_0}\\frac{q Q}{\\gr^2} \\vu{\\vec{\\gr}} \\label{2.1} \\tag{2.1} The constant \\epsilon_0 is called (ludicrously) the permittivity of free space . In SI units, where force is in newtons (N), distance in meters (m), and charge in coulombs (C), \\epsilon_0 = 8.85 \\times 10^{-12} \\frac{C^2}{N \\cdot m ^2} In words, the force is proportional to the product of the charges and inversely proportional to the square of the separation distance. As always (Sect 1.1.4), \\vec{\\gr} is the separation vector from \\vec{r'} (the location of q) to \\vec{r} (the location of Q): \\vec{\\gr} = \\vec{r} - \\vec{r}' \\gr is its magnitude, and \\vu{\\gr} is its direction. The force points along the line from q to Q; it is repulsive if q and Q have the same sign, and attractive if their signs are opposite. Coulomb's law and the principle of superposition constitute the physical input for electrostatics - the rest, except for some special properties of matter, is mathematical elaboration of these fundamental rules.","title":"2.1.2: Coulomb's Law"},{"location":"ch2-1/#213-the-electric-field","text":"If we have several point charges q_1, q_2, \\ldots , q_n at distances \\gr_1 \\gr_2 \\ldots, \\gr_n from Q , the total force on Q is evidently \\begin{align} \\vec{F} & = & \\vec{F_1} + \\vec{F_2} + \\ldots \\\\ & = & \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q_1 Q}{\\gr_1 ^2} \\vu{\\gr}_1 + \\frac{q_2 Q}{\\gr_2 ^2} \\vu{\\gr}_2 + \\ldots \\right) \\\\ & = & \\frac{Q}{4 \\pi \\epsilon _0} \\left( \\frac{q_1}{\\gr ^2 _1} \\vu{\\gr_1} + \\frac{q_2}{\\gr _2 ^2}\\vu{\\gr_2} + \\ldots \\right) \\end{align} or \\vec{F} = Q \\vec{E} \\label{2.3} \\tag{2.3} where \\vec{E}(\\vec{r}) \\equiv \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i = 1}^n \\frac{q_i}{\\gr_{i}^2} \\vu{\\gr_i} \\label{2.4} \\tag{2.4} E is called the electric field of the source charges. Notice that it is a function of position ( r ), because the separation vectors \\gr_i depend on the location of the field point P (Fig 2.3). But it makes no reference to the test charge Q. The electric field is a vector quantity that varies from point to point and is determined by the configuration of source charges; physically, \\vec{E}(\\vec{r}) is the force per unit charge that would be exerted on a test charge, if you were to place one at P. What exactly is an electric field? I have deliberately begun with what you might call the \"minimal\" interpretation of E , as an intermediate step in the calculation of electric forces. But I encourage you to think of the field as a \"real\" physical entity, filling the space around electric charges. Maxwell himself came to believe that electric and magnetic fields are stresses and strains in an invisible primordial jellylike \"ether.\" Special relativity has forced us to abandon the notion of either, and with it Maxwell's mechanical interpretation of electromagnetic fields. (It is even possible, although cumbersome, to formulate classical electrodynamics as an \"action-at-a-distance\" theory, and dispense with the field concept altogether.) I can't tell you, then, what a field is -- only how to calculate it and what it can do for you once you've got it.","title":"2.1.3: The Electric Field"},{"location":"ch2-1/#example-21","text":"Find the electric field a distance z above the midpoint between two equal charges (q), a distance d apart (Fig. 2.4a) Solution Let \\vec{E_1} be the field of the left charge alone, and \\vec{E_2} that of the right charge alone (Fig. 2.4b). Adding them (vectorially), the horizontal components cancel and the vertical components conspire E_z = 2 \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{\\gr ^2} \\cos \\theta Here \\gr = \\sqrt{z^2 + (d/2)^2} and \\cos \\theta = z / \\gr , so \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{2qz}{\\left[ z^2 + (d/2)^2 \\right]^{3/2}} \\vu{z} Check : When z \\gg d you're so far away that it just looks like a single charge 2q , so the field should reduce to \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{2q}{z^2} \\vu{z} . And it does, just set d \\rightarrow 0 in the formula).","title":"Example 2.1"},{"location":"ch2-1/#214-continuous-charge-distributions","text":"Our definition of the electric field (Eq. \\eqref{2.4} ) assumes that the source of the field is a collection of discrete point charges q_i . If, instead, the charge is distributed continuously over some region, the sum becomes an integral (Fig 2.5a): \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{1}{\\gr ^2} \\vu{\\gr} \\dd{q} If the charge is spread out along a line (Fig. 2.5b), with charge-per-unit-length \\lambda then \\dd{q} = \\lambda \\dd{l}' (where \\dd{l}' ) is an element of length along the line); if the charge is smeared out over a surface (Fig. 2.5c) with charge-per-unit-area \\sigma , then \\dd{q} = \\sigma \\dd{a}' (where \\dd{a'} ) is an element of area on the surface); and if the charge fills a volume (Fig 2.5d), with charge-per-unit-volume \\rho , then \\dd{q} = \\rho\\dd{\\tau'} (where \\dd{\\tau'} is an element of volume): dq \\rightarrow \\lambda \\dd{l'} \\sim \\sigma \\dd{a'} \\sim \\rho \\dd{\\tau'} Thus the electric field of a line charge is \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\lambda(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{l'} for a surface charge, \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{a'} and for a volume charge, \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{\\tau'} \\label{2.8} \\tag{2.8} Equation \\eqref{2.8} itself is often referred to as \"Coulomb's law,\" because it is such a short step from the original, and because a volume charge is in a sense the most general and realistic case. Please note carefully the meaning of \\gr in these formulas. Originally, in \\eqref{2.4} , \\gr_i stood for the vector from the source charge q_i to the field point r . Correspondingly, in Eq.s 9-11, \\gr is the vector from \\dd{q} to the field point \\vec{r} . Warning: the unit vector \\vu{\\gr} is not constant: its direction depends on the source point \\vec{r'} , and hence it cannot be taken outside the integrals (9-11). In practice, you must work with Cartesian components ( \\vu{x}, \\vu{y}, \\vu{z} are constant, and do come out) , even if you use curvilinear coordinates to perform the integration.","title":"2.1.4: Continuous Charge Distributions"},{"location":"ch2-1/#example-22","text":"Find the electric field a distance z above the midpoint of a straight line segment of length 2L that carries a uniform line charge \\lambda (Fig. 2.6). TODO!","title":"Example 2.2"},{"location":"ch2-2/","text":"2.2: Divergence and Curl of Electrostatic Fields 2.2.1 Field Lines, Flux, and Gauss' Law In principle, we are done with the subject of electrostatics. Eq. 2.8 tells us how to compute the field of a charge distribution, and Eq. 2.3 tells us what the force on a charge Q placed in this field will be. Unfortunately, as you may have discovered, the integrals involved in computing E can be formidable, even for reasonably simple charge distributions. Much of the rest of electrostatics is devoted to assembling a bag of tools and tricks for avoiding these integrals. It all begins with the divergence and curl of E . I shall calculate the divergence of E directly from Eq. 2.8 in section 2.2.2, but first I want to show you a more qualitative, and perhaps more illuminating, intuitive approach. Let's begin with the simplest possible case: a single point charge q , situated at the origin: \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\vu{\\vec{r}} \\tag{2.10} \\label{2.10} To get a \"feel\" for this field, I might sketch a few representative vectors, as in Fig. 2.12a. Because the field falls off like 1/r^2 , the vectors get shorter as you go farther away from the origin; they always point radially outward. But there is a nicer way to represent this field, and that's to connect up the arrows, to form field lines (Fig. 2.12b). You might think that I have thereby thrown away information about the strength of the field, which was contained in the length of the arrows. But actually I have not. The magnitude of the field is indicated by the density of the field lines: it's strong near the center where the field lines are close together, and weak farther out, where they are relatively far apart. In truth, the field-line diagram is deceptive, when I draw it on a two-dimensional surface, for the density of lines passing through a circle of radius r is the total number divided by the circumference ( n / 2 \\pi r ), which goes like (1/r) , not (1/r^2) . But if you imagine the model in three dimensions (a pincushion with needles sticking out in all directions), then the density of lines is the total number divided by the area of the sphere (n/4 \\pi r^2) , which does go like (1/r^2) . Such diagrams are also convenient for representing more complicated fields. Of course, the number of lines you draw depends on how lazy you are (and how sharp your pencil is), though you ought to include enough to get an accurate sense of the field, and you must be consistent: if q gets 8 lines, then 2q deserves 16. And you must space them fairly - they emanate from a point charge symmetrically in all directions. Field lines begin on positive charges and end on negative ones; they cannot simply terminate in midair, though they may extend out to infinity. Moreover, field lines can never cross - at the intersection the field would have two different directions at once! With all this in mind, it is easy to sketch the field of any simple configuration of point charges: Begin by drawing the lines in the neighborhood of each charge, and then connect them up or extend them to infinity (Figs. 2.13 and 2.14) In this model, the flux of E through a surface S, \\Phi_E \\equiv \\int _S \\vec{E} \\cdot \\dd{\\vec{a}} \\label{2.11} \\tag{2.11} is a measure of the \"number of lines\" passing through S. I put this in quotes because of course we can only draw a representative sample of field lines - the total number would be infinite. But for a given sampling rate the flux is proportional to the number of lines drawn, because the field strength, remember, is proportional to the density of field lines (the number per unit area), and hence \\vec{E} \\cdot \\dd{\\vec{a}} is proportional to the number of lines passing through the infinitesimal area \\dd{\\vec{a}} . (The dot product picks out the component of \\dd{\\vec{a}} along the direction of E , as indicated in Fig 2.15. It is the area in the plane perpendicular to E that we have in mind when we say that the density of field lines is the number per unit area). This suggests that the flux through any closed surface is a measure of the total charge inside. For the field lines that originate on a positive charge must either pass out through the surface or else terminate on a negative charge inside (Fig 2.16a). On the other hand, a charge outside the surface will contribute nothing to the total flux, since its field lines pass in one side and out the other (Fig 2.16b). This is the essence of Gauss's law. Now let's make it quantitative. In the case of a point charge q at the origin, the flux of E through a spherical surface or radius r is \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\int \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{r^2} \\vu{r} \\right) \\cdot \\left( r^2 \\sin \\theta \\dd{\\theta} \\dd{\\phi} \\vu{r} \\right) = \\frac{1}{\\epsilon_0} q \\label{2.12} \\tag{2.12} Notice that the radius of the sphere cancels out, for while the surface area goes up as r^2 , the field goes down as 1/r^2 , so the product is constant. In terms of the field-line picture, this makes good sense, since the same number of field lines pass through any sphere centered at the origin, regardless of its size. In fact, it didn't have to be a sphere - any closed surface, whatever its shape, would be pierced by the same number of field lines. Evidently, the flux through any surface enclosing the charge is q / \\epsilon_0 . Now suppose that instead of a single charge at the origin, we have a bunch of charges scattered about. According to the principle of superposition, the total field is the (vector) sum of all the individual fields: \\vec{E} = \\sum _{i = 1} ^\\nu \\vec{E}_i The flux through a surface that encloses them all is \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = \\sum _{i = 1}^n \\left( \\oint \\vec{E_i} \\cdot \\dd{\\vec{a}} \\right) = \\sum_{i = 1}^n \\left( \\frac{1}{\\epsilon_0} q_i \\right) For any closed surface, then \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} \\label{2.13} \\tag{2.13} where Q_{enc} is the total charge enclosed within the surface. This is the quantitative statement of Gauss's law. Although it contains no information that was not already present in Coulomb's law plus the principle of superposition, it is of almost magical power, as you will see in Sect. 2.2.3. Notice that it all hinges on the 1/r^2 character of Coulomb's law; without that, the crucial cancellation of the r 's in \\eqref{2.12} would not take place, and the total flux of E would depend on the surface chosen, not merely on the total charge enclosed. Other 1/r^2 forces (I am thinking particularly of Newton's law of universal gravitation) will obey \"Gauss's laws\" of their own, and the applications we develop here carry over directly. As it stands, Gauss's law is an integral equation, but we can easily turn it into a differential one by applying the divergence theorem: \\oint_{S} \\vec{E} \\cdot \\dd{\\vec{a}} = \\int_{\\mathscr{V}} (\\div{\\vec{E}}) \\dd{\\tau} Rewriting Q_{enc} in terms of the charge density \\rho we have Q_{enc} = \\int_{\\mathscr{V}} \\rho \\dd{\\tau} So Gauss's law becomes \\int_{\\mathscr{V}} (\\div{\\vec{E}}) \\dd{\\tau} = \\int_{\\mathscr{V}} \\left( \\frac{\\rho}{\\epsilon_0} \\dd{\\tau} \\right) And since this holds for any volume, the integrands must be equal: \\nabla \\cdot \\vec{E} = \\frac{1}{\\epsilon_0} \\rho \\label{2.14} \\tag{2.14} Equation \\eqref{2.14} carries the same message as \\eqref{2.13} ; it is Gauss's law in differential form . The differential version is tidier, but the integral form has the advantage that it accommodates point, line, and surface charges more naturally. 2.2.2: The Divergence of E Let's go back now, and calculate the divergence of \\vec{E} directly from Eq. 2.8: \\vec{E}(\\vec{r}) = \\frac{1}{4\\pi\\epsilon_0} \\int_{\\text{all space}} \\frac{\\vu{\\gr}}{\\gr ^2} \\rho(\\vec{r}') \\dd{\\tau'} \\label{2.15} (Originally the integration was over the volume occupied by the charge, but I may as well extend it to all space, since \\rho = 0 in the exterior region anyway.) Noting that the r-dependence is contained in \\gr = r - r' , we have \\div{\\vec{E}} = \\frac{1}{4\\pi\\epsilon_0} \\int \\vec{\\nabla} \\cdot \\left( \\frac{\\vu{\\gr}}{\\gr^2} \\right) \\rho(\\vec{r'}) \\dd{\\tau'} We calculated this divergence in Section 1.5: \\div{\\left( \\frac{\\vu{\\gr}}{\\gr^2} \\right)} = 4 \\pi \\delta ^3(\\gr) Thus \\div{\\vec{E}} = \\frac{1}{4\\pi\\epsilon_0} \\int 4 \\pi \\delta^3(\\vec{r} - \\vec{r'}) \\rho(\\vec{r'}) \\dd{\\tau'} = \\frac{1}{\\epsilon_0} \\rho(\\vec{r}) \\label{2.16} \\tag{2.16} which is Gauss's law in differential form \\eqref{2.14} . To recover the integral form \\eqref{2.13} we run the previous argument in reverse - integrate over a volume and apply the divergence theorem: \\int_{\\mathscr{V}} \\div{\\vec{E}} \\dd{\\tau} = \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} \\int_{\\mathscr{V}} \\rho \\dd{\\tau} = \\frac{1}{\\epsilon_0} Q_{enc} 2.2.3: Applications of Gauss's Law I must interrupt the theoretical development at this point to show you the extraordinary power of Gauss's law, in integral form. When symmetry permits, it affords by far the quickest and easiest way of computing electric fields. I'll illustrate the method with a series of examples. Example 2.3 Find the field outside a uniformly charged solid sphere of radius R and total charge q Solution Imagine a spherical surface at radius r > R (Fig. 2.18). This is called a Gaussian surface in the trade. Gauss's law says that \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} and in this case Q_{enc} = q . At first glance this doesn't seem to get us very far, because the quantity we want (E) is buried inside the surface integral. Luckily, symmetry allows us to extract E from under the integral sign: E certainly points radially outward, as does \\dd{\\vec{a}} , so we can drop the dot product \\int_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\int_{\\mathscr{S}} | \\vec{E} | da and the magnitude of E is constant over the Gaussian surface, so it comes outside the integral: \\int_{S} | E | da = |E| \\int_{S} da = E 4 \\pi r^2 Thus |\\vec{E}|4\\pi r^2 = \\frac{1}{\\epsilon_0} q or \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r} Notice a remarkable feature of this result: the field outside the sphere is exactly the same as it would have been if all the charge had been concentrated at the center. Gauss's law is always true , but not always useful . If \\rho had not been uniform (or at any rate, not spherically symmetrical), or if I had chosen some other shape for my Gaussian surface, it would have still been true that the flux of \\vec{E} is q / \\epsilon_0 , but \\vec{E} would not have pointed in the same direction as \\dd{\\vec{a}} , and its magnitude would not have been constant over the surface, and without that I cannot get |\\vec{E}| outside the integral. Symmetry is crucial to this application of Gauss's law. As far as I know, there are only three kinds of symmetry that work: Spherical symmetry. Make your Gaussian survace a concentric sphere. Cylindrical symmetry. Make your Gaussian surface a coaxial cylinder. Plane symmetry. Use a Gaussian \"pillbox\" that straddles the surface. Although 2 and 3 technically require infinitely long cylinders, and planes extending to infinity, we shall often use them to get approximate answers for \"long\" cylinders or \"large\" planes, at points far from the edges. Example 2.4 A long cylinder (Fig 2.21) carries a charge density that is proportional to the distance from the axis: \\rho = ks for some constant k . Find the electric field inside this cylinder. Solution : Draw a Gaussian cylinder of length l and radius s. For this surface, Gauss's law states \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} The enclosed charge is \\begin{align} Q_{enc} & = & \\int \\rho \\dd{\\tau} \\\\ & = & \\int(ks')(s' \\dd{s'} \\dd{\\phi} \\dd{z}) \\\\ & = & 2 \\pi k l \\int_{0}^{s} s'^2 \\dd{s'} \\\\ & = & \\frac{2}{3} \\pi k l s^3 \\end{align} (I used the volume element appropriate to cylindrical coordinates, and integrated \\phi from 0 to 2\\pi , \\dd{z} from 0 to l . I put a prime on the integration variable s' to distinguish it from the radius s of the Gaussian surface.) Now, symmetry dictates that \\vec{E} must point radially outward, so for the curved portion of the Gaussian cylinder we have: \\int \\vec{E} \\cdot \\dd{\\vec{a}} = \\int | \\vec{E}| da = | \\vec{E}| \\int da = |\\vec{E} 2 \\pi s l while the two ends contribute nothing (here \\vec{E} is perpendicular to \\dd{\\vec{a}} ). Thus, |\\vec{E} | 2 \\pi s l = \\frac{1}{\\epsilon_0} \\frac{2}{3} \\pi k l s^3 or, finally, \\vec{E} = \\frac{1}{3\\epsilon_0} k s^2 \\vu{s} Example 2.5 An infinite plane carries a uniform surface charge \\sigma . Find its electric field. Solution Draw a Gaussian pillbox, extending equal distances above and below the plane (Fig. 2.22). Apply Gauss's law to this surface: \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} In this case, Q = \\sigma A , where A is the area of the lid of the pillbox. By symmetry, \\vec{E} points away from the plane (upward for points above, downward for points below). So the top and bottom surfaces yield \\int \\vec{E} \\cdot \\dd{\\vec{a}} = 2 A |\\vec{E}|, whereas the sides contribute nothing. Thus 2 A | \\vec{E} | = \\frac{1}{\\epsilon_0} \\sigma A or \\vec{E} = \\frac{\\sigma}{2 \\epsilon_0} \\vu{n} where \\vu{n} is a unit vector pointing away from the surface. In Prob 2.6, you obtained this same result by a much more laborious method. It seems surprising, at first, that the field of an infinite plane is independent of how fara away you are . What about the 1/r^2 in Coulomb's law? The point is that as you move farther and farther away from the plane, more and more charge comes into your \"field of view,\" and this compensates for the diminishing influence of any particular piece. The electric field of a sphere falls off like 1/r^2 ; the electric field of an infinite line falls off like 1/r ; and the electric field of an infinite plane does not fall off at all (you cannot escape from an infinite plane). Although the direct use of Gauss's law to compute fields is limited to cases of spherical, cylindrical, and planar symmetry, we can put together combinations of objects posessing such symmetry, even though the arrangement as a whole is not symmetrical. For example, invoking the principle of superposition, we could find the field in the vicinity of two uniformly charged parallel cylinders, or a sphere near an infinite charged plane. Example 2.6 Two infinite parallel planes carry equal but opposite uniform charge densities \\pm \\sigma (Fig 2.23). Find the field in each of the three regions: (i) to the left of both, (ii) between them, (iii) to the right of both. Solution The left plate produces a field (1/2 \\epsilon_0)\\sigma , which points away from it (Fig. 2.24) to the left in region in (i) and to the right in regions (ii) and (iii). The right plate, being negatively charged, produces a field (1/2 \\epsilon_0)\\sigma which points toward it - to the right in regions (i) and (ii) and to the left in region (iii). The two fields cancel in regions (i) and (iii); they conspire in region (ii). Conclusion: The field between the plates is \\sigma / \\epsilon_0 , and points to the right; elsewhere it is zero. 2.2.4: The Curl of E I'll calculate the curl of \\vec{E} as I did the divergence in Sect 2.2.1, by studying first the simplest possible configuration: a point charge at the origin. In this case \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r} Now, a glance at Fig 2.12 should convince you that the curl of this field has to be zero, but I suppose we ought to come up with something a little more rigorous than that. What if we calculate the line integral of this field from some point \\vec{a} to some other point \\vec{b} (Fig 2.29): \\int_{\\vec{a}}^{\\vec{b}} \\vec{E} \\cdot \\dd{\\vec{l}} In spherical coordinates, \\dd{\\vec{l}} = \\dd{r} \\vu{r} + r \\dd{\\theta} \\vu{\\theta} + r \\sin \\theta \\dd{\\phi} \\vu{\\phi} , so \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\dd{r} Therefore, \\int_{\\vec{a}}^{\\vec{b}} \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{a}^{b} \\frac{q}{r^2} \\dd{r} \\\\ = \\left.\\frac{-1}{4 \\pi \\epsilon_0} \\frac{q}{r} \\right|_{r_a} ^{r_b} \\\\ = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{r_a} - \\frac{q}{r_b} \\right) The integral around a closed path is evidently zero (for then r_a = r_b ): \\oint \\vec{E} \\cdot \\dd\\vec{l} = 0 \\label{2.19} \\tag{2.19} and hence, applying Stokes' theorem \\curl{\\vec{E}} = 0 \\label{2.20} \\tag{2.20} Now, I proved eqs. \\eqref{2.19} and \\eqref{2.20} only for the field of a single point charge at the origin, but these results make no reference to what is, after all, a perfectly arbitrary choice of coordinates; they hold no matter where the charge is located. Moreover, if we have many charges, the principle of superposition states that the total field is a vector sum of their individual fields: \\vec{E} = \\vec{E_1} + \\vec{E_2} + \\ldots so \\curl{\\vec{E}} = \\curl{(\\vec{E_1} + \\vec{E_2} + \\ldots)} = (\\curl{\\vec{E_1}}) + (\\curl{\\vec{E_2}}) + \\ldots = 0 Thus, Eqs. \\eqref{2.19} and \\eqref{2.20} hold for any static charge distribution whatever.","title":"2.2 - Divergence and Curl of Electrostatic Fields"},{"location":"ch2-2/#22-divergence-and-curl-of-electrostatic-fields","text":"","title":"2.2: Divergence and Curl of Electrostatic Fields"},{"location":"ch2-2/#221-field-lines-flux-and-gauss-law","text":"In principle, we are done with the subject of electrostatics. Eq. 2.8 tells us how to compute the field of a charge distribution, and Eq. 2.3 tells us what the force on a charge Q placed in this field will be. Unfortunately, as you may have discovered, the integrals involved in computing E can be formidable, even for reasonably simple charge distributions. Much of the rest of electrostatics is devoted to assembling a bag of tools and tricks for avoiding these integrals. It all begins with the divergence and curl of E . I shall calculate the divergence of E directly from Eq. 2.8 in section 2.2.2, but first I want to show you a more qualitative, and perhaps more illuminating, intuitive approach. Let's begin with the simplest possible case: a single point charge q , situated at the origin: \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\vu{\\vec{r}} \\tag{2.10} \\label{2.10} To get a \"feel\" for this field, I might sketch a few representative vectors, as in Fig. 2.12a. Because the field falls off like 1/r^2 , the vectors get shorter as you go farther away from the origin; they always point radially outward. But there is a nicer way to represent this field, and that's to connect up the arrows, to form field lines (Fig. 2.12b). You might think that I have thereby thrown away information about the strength of the field, which was contained in the length of the arrows. But actually I have not. The magnitude of the field is indicated by the density of the field lines: it's strong near the center where the field lines are close together, and weak farther out, where they are relatively far apart. In truth, the field-line diagram is deceptive, when I draw it on a two-dimensional surface, for the density of lines passing through a circle of radius r is the total number divided by the circumference ( n / 2 \\pi r ), which goes like (1/r) , not (1/r^2) . But if you imagine the model in three dimensions (a pincushion with needles sticking out in all directions), then the density of lines is the total number divided by the area of the sphere (n/4 \\pi r^2) , which does go like (1/r^2) . Such diagrams are also convenient for representing more complicated fields. Of course, the number of lines you draw depends on how lazy you are (and how sharp your pencil is), though you ought to include enough to get an accurate sense of the field, and you must be consistent: if q gets 8 lines, then 2q deserves 16. And you must space them fairly - they emanate from a point charge symmetrically in all directions. Field lines begin on positive charges and end on negative ones; they cannot simply terminate in midair, though they may extend out to infinity. Moreover, field lines can never cross - at the intersection the field would have two different directions at once! With all this in mind, it is easy to sketch the field of any simple configuration of point charges: Begin by drawing the lines in the neighborhood of each charge, and then connect them up or extend them to infinity (Figs. 2.13 and 2.14) In this model, the flux of E through a surface S, \\Phi_E \\equiv \\int _S \\vec{E} \\cdot \\dd{\\vec{a}} \\label{2.11} \\tag{2.11} is a measure of the \"number of lines\" passing through S. I put this in quotes because of course we can only draw a representative sample of field lines - the total number would be infinite. But for a given sampling rate the flux is proportional to the number of lines drawn, because the field strength, remember, is proportional to the density of field lines (the number per unit area), and hence \\vec{E} \\cdot \\dd{\\vec{a}} is proportional to the number of lines passing through the infinitesimal area \\dd{\\vec{a}} . (The dot product picks out the component of \\dd{\\vec{a}} along the direction of E , as indicated in Fig 2.15. It is the area in the plane perpendicular to E that we have in mind when we say that the density of field lines is the number per unit area). This suggests that the flux through any closed surface is a measure of the total charge inside. For the field lines that originate on a positive charge must either pass out through the surface or else terminate on a negative charge inside (Fig 2.16a). On the other hand, a charge outside the surface will contribute nothing to the total flux, since its field lines pass in one side and out the other (Fig 2.16b). This is the essence of Gauss's law. Now let's make it quantitative. In the case of a point charge q at the origin, the flux of E through a spherical surface or radius r is \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\int \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{r^2} \\vu{r} \\right) \\cdot \\left( r^2 \\sin \\theta \\dd{\\theta} \\dd{\\phi} \\vu{r} \\right) = \\frac{1}{\\epsilon_0} q \\label{2.12} \\tag{2.12} Notice that the radius of the sphere cancels out, for while the surface area goes up as r^2 , the field goes down as 1/r^2 , so the product is constant. In terms of the field-line picture, this makes good sense, since the same number of field lines pass through any sphere centered at the origin, regardless of its size. In fact, it didn't have to be a sphere - any closed surface, whatever its shape, would be pierced by the same number of field lines. Evidently, the flux through any surface enclosing the charge is q / \\epsilon_0 . Now suppose that instead of a single charge at the origin, we have a bunch of charges scattered about. According to the principle of superposition, the total field is the (vector) sum of all the individual fields: \\vec{E} = \\sum _{i = 1} ^\\nu \\vec{E}_i The flux through a surface that encloses them all is \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = \\sum _{i = 1}^n \\left( \\oint \\vec{E_i} \\cdot \\dd{\\vec{a}} \\right) = \\sum_{i = 1}^n \\left( \\frac{1}{\\epsilon_0} q_i \\right) For any closed surface, then \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} \\label{2.13} \\tag{2.13} where Q_{enc} is the total charge enclosed within the surface. This is the quantitative statement of Gauss's law. Although it contains no information that was not already present in Coulomb's law plus the principle of superposition, it is of almost magical power, as you will see in Sect. 2.2.3. Notice that it all hinges on the 1/r^2 character of Coulomb's law; without that, the crucial cancellation of the r 's in \\eqref{2.12} would not take place, and the total flux of E would depend on the surface chosen, not merely on the total charge enclosed. Other 1/r^2 forces (I am thinking particularly of Newton's law of universal gravitation) will obey \"Gauss's laws\" of their own, and the applications we develop here carry over directly. As it stands, Gauss's law is an integral equation, but we can easily turn it into a differential one by applying the divergence theorem: \\oint_{S} \\vec{E} \\cdot \\dd{\\vec{a}} = \\int_{\\mathscr{V}} (\\div{\\vec{E}}) \\dd{\\tau} Rewriting Q_{enc} in terms of the charge density \\rho we have Q_{enc} = \\int_{\\mathscr{V}} \\rho \\dd{\\tau} So Gauss's law becomes \\int_{\\mathscr{V}} (\\div{\\vec{E}}) \\dd{\\tau} = \\int_{\\mathscr{V}} \\left( \\frac{\\rho}{\\epsilon_0} \\dd{\\tau} \\right) And since this holds for any volume, the integrands must be equal: \\nabla \\cdot \\vec{E} = \\frac{1}{\\epsilon_0} \\rho \\label{2.14} \\tag{2.14} Equation \\eqref{2.14} carries the same message as \\eqref{2.13} ; it is Gauss's law in differential form . The differential version is tidier, but the integral form has the advantage that it accommodates point, line, and surface charges more naturally.","title":"2.2.1 Field Lines, Flux, and Gauss' Law"},{"location":"ch2-2/#222-the-divergence-of-e","text":"Let's go back now, and calculate the divergence of \\vec{E} directly from Eq. 2.8: \\vec{E}(\\vec{r}) = \\frac{1}{4\\pi\\epsilon_0} \\int_{\\text{all space}} \\frac{\\vu{\\gr}}{\\gr ^2} \\rho(\\vec{r}') \\dd{\\tau'} \\label{2.15} (Originally the integration was over the volume occupied by the charge, but I may as well extend it to all space, since \\rho = 0 in the exterior region anyway.) Noting that the r-dependence is contained in \\gr = r - r' , we have \\div{\\vec{E}} = \\frac{1}{4\\pi\\epsilon_0} \\int \\vec{\\nabla} \\cdot \\left( \\frac{\\vu{\\gr}}{\\gr^2} \\right) \\rho(\\vec{r'}) \\dd{\\tau'} We calculated this divergence in Section 1.5: \\div{\\left( \\frac{\\vu{\\gr}}{\\gr^2} \\right)} = 4 \\pi \\delta ^3(\\gr) Thus \\div{\\vec{E}} = \\frac{1}{4\\pi\\epsilon_0} \\int 4 \\pi \\delta^3(\\vec{r} - \\vec{r'}) \\rho(\\vec{r'}) \\dd{\\tau'} = \\frac{1}{\\epsilon_0} \\rho(\\vec{r}) \\label{2.16} \\tag{2.16} which is Gauss's law in differential form \\eqref{2.14} . To recover the integral form \\eqref{2.13} we run the previous argument in reverse - integrate over a volume and apply the divergence theorem: \\int_{\\mathscr{V}} \\div{\\vec{E}} \\dd{\\tau} = \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} \\int_{\\mathscr{V}} \\rho \\dd{\\tau} = \\frac{1}{\\epsilon_0} Q_{enc}","title":"2.2.2: The Divergence of E"},{"location":"ch2-2/#223-applications-of-gausss-law","text":"I must interrupt the theoretical development at this point to show you the extraordinary power of Gauss's law, in integral form. When symmetry permits, it affords by far the quickest and easiest way of computing electric fields. I'll illustrate the method with a series of examples.","title":"2.2.3: Applications of Gauss's Law"},{"location":"ch2-2/#example-23","text":"Find the field outside a uniformly charged solid sphere of radius R and total charge q Solution Imagine a spherical surface at radius r > R (Fig. 2.18). This is called a Gaussian surface in the trade. Gauss's law says that \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} and in this case Q_{enc} = q . At first glance this doesn't seem to get us very far, because the quantity we want (E) is buried inside the surface integral. Luckily, symmetry allows us to extract E from under the integral sign: E certainly points radially outward, as does \\dd{\\vec{a}} , so we can drop the dot product \\int_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\int_{\\mathscr{S}} | \\vec{E} | da and the magnitude of E is constant over the Gaussian surface, so it comes outside the integral: \\int_{S} | E | da = |E| \\int_{S} da = E 4 \\pi r^2 Thus |\\vec{E}|4\\pi r^2 = \\frac{1}{\\epsilon_0} q or \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r} Notice a remarkable feature of this result: the field outside the sphere is exactly the same as it would have been if all the charge had been concentrated at the center. Gauss's law is always true , but not always useful . If \\rho had not been uniform (or at any rate, not spherically symmetrical), or if I had chosen some other shape for my Gaussian surface, it would have still been true that the flux of \\vec{E} is q / \\epsilon_0 , but \\vec{E} would not have pointed in the same direction as \\dd{\\vec{a}} , and its magnitude would not have been constant over the surface, and without that I cannot get |\\vec{E}| outside the integral. Symmetry is crucial to this application of Gauss's law. As far as I know, there are only three kinds of symmetry that work: Spherical symmetry. Make your Gaussian survace a concentric sphere. Cylindrical symmetry. Make your Gaussian surface a coaxial cylinder. Plane symmetry. Use a Gaussian \"pillbox\" that straddles the surface. Although 2 and 3 technically require infinitely long cylinders, and planes extending to infinity, we shall often use them to get approximate answers for \"long\" cylinders or \"large\" planes, at points far from the edges.","title":"Example 2.3"},{"location":"ch2-2/#example-24","text":"A long cylinder (Fig 2.21) carries a charge density that is proportional to the distance from the axis: \\rho = ks for some constant k . Find the electric field inside this cylinder. Solution : Draw a Gaussian cylinder of length l and radius s. For this surface, Gauss's law states \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} The enclosed charge is \\begin{align} Q_{enc} & = & \\int \\rho \\dd{\\tau} \\\\ & = & \\int(ks')(s' \\dd{s'} \\dd{\\phi} \\dd{z}) \\\\ & = & 2 \\pi k l \\int_{0}^{s} s'^2 \\dd{s'} \\\\ & = & \\frac{2}{3} \\pi k l s^3 \\end{align} (I used the volume element appropriate to cylindrical coordinates, and integrated \\phi from 0 to 2\\pi , \\dd{z} from 0 to l . I put a prime on the integration variable s' to distinguish it from the radius s of the Gaussian surface.) Now, symmetry dictates that \\vec{E} must point radially outward, so for the curved portion of the Gaussian cylinder we have: \\int \\vec{E} \\cdot \\dd{\\vec{a}} = \\int | \\vec{E}| da = | \\vec{E}| \\int da = |\\vec{E} 2 \\pi s l while the two ends contribute nothing (here \\vec{E} is perpendicular to \\dd{\\vec{a}} ). Thus, |\\vec{E} | 2 \\pi s l = \\frac{1}{\\epsilon_0} \\frac{2}{3} \\pi k l s^3 or, finally, \\vec{E} = \\frac{1}{3\\epsilon_0} k s^2 \\vu{s}","title":"Example 2.4"},{"location":"ch2-2/#example-25","text":"An infinite plane carries a uniform surface charge \\sigma . Find its electric field. Solution Draw a Gaussian pillbox, extending equal distances above and below the plane (Fig. 2.22). Apply Gauss's law to this surface: \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} In this case, Q = \\sigma A , where A is the area of the lid of the pillbox. By symmetry, \\vec{E} points away from the plane (upward for points above, downward for points below). So the top and bottom surfaces yield \\int \\vec{E} \\cdot \\dd{\\vec{a}} = 2 A |\\vec{E}|, whereas the sides contribute nothing. Thus 2 A | \\vec{E} | = \\frac{1}{\\epsilon_0} \\sigma A or \\vec{E} = \\frac{\\sigma}{2 \\epsilon_0} \\vu{n} where \\vu{n} is a unit vector pointing away from the surface. In Prob 2.6, you obtained this same result by a much more laborious method. It seems surprising, at first, that the field of an infinite plane is independent of how fara away you are . What about the 1/r^2 in Coulomb's law? The point is that as you move farther and farther away from the plane, more and more charge comes into your \"field of view,\" and this compensates for the diminishing influence of any particular piece. The electric field of a sphere falls off like 1/r^2 ; the electric field of an infinite line falls off like 1/r ; and the electric field of an infinite plane does not fall off at all (you cannot escape from an infinite plane). Although the direct use of Gauss's law to compute fields is limited to cases of spherical, cylindrical, and planar symmetry, we can put together combinations of objects posessing such symmetry, even though the arrangement as a whole is not symmetrical. For example, invoking the principle of superposition, we could find the field in the vicinity of two uniformly charged parallel cylinders, or a sphere near an infinite charged plane.","title":"Example 2.5"},{"location":"ch2-2/#example-26","text":"Two infinite parallel planes carry equal but opposite uniform charge densities \\pm \\sigma (Fig 2.23). Find the field in each of the three regions: (i) to the left of both, (ii) between them, (iii) to the right of both. Solution The left plate produces a field (1/2 \\epsilon_0)\\sigma , which points away from it (Fig. 2.24) to the left in region in (i) and to the right in regions (ii) and (iii). The right plate, being negatively charged, produces a field (1/2 \\epsilon_0)\\sigma which points toward it - to the right in regions (i) and (ii) and to the left in region (iii). The two fields cancel in regions (i) and (iii); they conspire in region (ii). Conclusion: The field between the plates is \\sigma / \\epsilon_0 , and points to the right; elsewhere it is zero.","title":"Example 2.6"},{"location":"ch2-2/#224-the-curl-of-e","text":"I'll calculate the curl of \\vec{E} as I did the divergence in Sect 2.2.1, by studying first the simplest possible configuration: a point charge at the origin. In this case \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r} Now, a glance at Fig 2.12 should convince you that the curl of this field has to be zero, but I suppose we ought to come up with something a little more rigorous than that. What if we calculate the line integral of this field from some point \\vec{a} to some other point \\vec{b} (Fig 2.29): \\int_{\\vec{a}}^{\\vec{b}} \\vec{E} \\cdot \\dd{\\vec{l}} In spherical coordinates, \\dd{\\vec{l}} = \\dd{r} \\vu{r} + r \\dd{\\theta} \\vu{\\theta} + r \\sin \\theta \\dd{\\phi} \\vu{\\phi} , so \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\dd{r} Therefore, \\int_{\\vec{a}}^{\\vec{b}} \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{a}^{b} \\frac{q}{r^2} \\dd{r} \\\\ = \\left.\\frac{-1}{4 \\pi \\epsilon_0} \\frac{q}{r} \\right|_{r_a} ^{r_b} \\\\ = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{r_a} - \\frac{q}{r_b} \\right) The integral around a closed path is evidently zero (for then r_a = r_b ): \\oint \\vec{E} \\cdot \\dd\\vec{l} = 0 \\label{2.19} \\tag{2.19} and hence, applying Stokes' theorem \\curl{\\vec{E}} = 0 \\label{2.20} \\tag{2.20} Now, I proved eqs. \\eqref{2.19} and \\eqref{2.20} only for the field of a single point charge at the origin, but these results make no reference to what is, after all, a perfectly arbitrary choice of coordinates; they hold no matter where the charge is located. Moreover, if we have many charges, the principle of superposition states that the total field is a vector sum of their individual fields: \\vec{E} = \\vec{E_1} + \\vec{E_2} + \\ldots so \\curl{\\vec{E}} = \\curl{(\\vec{E_1} + \\vec{E_2} + \\ldots)} = (\\curl{\\vec{E_1}}) + (\\curl{\\vec{E_2}}) + \\ldots = 0 Thus, Eqs. \\eqref{2.19} and \\eqref{2.20} hold for any static charge distribution whatever.","title":"2.2.4: The Curl of E"},{"location":"ch2-3/","text":"2.3: Electric Potential 2.3.1: Introduction to Potential The electric field E is not just any old vector function. It is a very special kind of vector function: one whose curl id zero. \\vec{E} = y \\vu{x} , for example, could not possibly be an electrostatic field; no set of charges, regardless of their sizes and positions, could ever produce such a field. We're going to exploit this special property of electric fields to reduce a vector problem (finding E ) to a much simpler scalar problem. The first theorem in Sect 1.6.2 asserts that any vector whose curl is zero is equal to the gradient of some scalar. What I'm going to do now amounts to a proof of that claim, in the context of electrostatics. Because \\nabla \\cross \\vec{E} = 0 , the line integral of E around any closed loop is zero (that follows from Stokes' theorem). Because \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = 0 , the line integral of E from point a to point b is the same for all paths (otherwise you could go out along path (i) and return along path (ii) - Fig 2.30 - and obtain \\oint \\vec{E} \\cdot \\dd{\\vec{l}} \\neq 0 ). Because the line integral is independent of path, we can define a function V(\\vec{r}) \\equiv - \\int _{O} ^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\label{2.21} \\tag{2.21} Here O is some standard reference point on which we have agreed beforehand; V then depends only on the point \\vec{r} . It is called the electric potential . The potential difference between two points a and b is \\begin{align} V(\\vec{b}) - V(\\vec{a}) & = & -\\int_{O}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} + \\int_{O}^{\\vec{a}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ & = & -\\int_{O}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} - \\int_{\\vec{a}}^{O} \\vec{E}\\cdot \\dd{\\vec{l}} \\\\ & = & - \\int_{\\vec{a}} ^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} \\end{align} \\label{2.22} \\tag{2.22} Now, the fundamental theorem for gradients states that V(\\vec{b}) - V(\\vec{a}) = \\int_{\\vec{a}} ^{\\vec{b}} (\\grad{V}) \\cdot \\dd{\\vec{l}} so \\int_{\\vec{a}}^{\\vec{b}} (\\grad{V})\\cdot \\dd{\\vec{l}} = - \\int_{\\vec{a}}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} Since, finally, this is true for any points a and b , the integrands must be equal: \\vec{E} = - \\grad{V} \\label{2.23} \\tag{2.23} Equation \\eqref{2.23} is the differential version of \\eqref{2.21} ; it says that the electric field is the gradient of a scalar potential, which is what we set out to prove. Notice the subtle but crucial role played by path independence (or, equivalently, the fact that \\nabla \\times \\vec{E} = 0 ) in this argument. If the line integral of E depended on the path taken, then the \"definition\" of V \\eqref{2.21} would be nonsense. It simply would not define a function, since changing the path would alter the value of V(\\vec{r}) . By the way, don't let the minus sign in \\eqref{2.23} distract you; it carries over from \\eqref{2.21} and is largely a matter of convention. 2.3.2: Comments on Potential The name . The word \"potential\" is a hideous misnomer because it inevitably reminds you of potential energy . This is particularly insidious, because there is a connection between \"potential\" and \"potential energy,\" as you will see in Sect 2.4. I'm sorry that it is impossible to escape this word. The best I can do is to insist once and for all that \"potential\" and \"potential energy\" are completely different terms and should, by all rights, have different names. Incidentially, a surface over which the potential is constant is called an equipotential . Advantage of the potential formulation . If you know V, you can easily get E - just take the gradient: \\vec{E} =- \\grad{V} . This is quite extraordinary when you stop to think about it, for E is a vector quantity (three components), but V is a scalar (one component). How can one function possibly contain all the information that three independent functions carry? The answer is that the three components of E are not really as independent as they look; in fact, they are explicitly interrelated by the very condition we started with, \\nabla \\times \\vec{E} = 0 . In terms of components, \\pdv{E_x}{y} = \\pdv{E_y}{x}, \\qquad \\pdv{E_z}{y} = \\pdv{E_y}{z}, \\qquad \\pdv{E_x}{z} = \\pdv{E_z}{x} This brings us back to my observation at the beginning of Sect 2.3.1: E is a very special kind of vector.What the potential formulation does is to exploit this feature to maximum advantage, reducing a vector problem to a scalar one, in which there is no need to fuss with components. The reference point \\mathscr{O} . There is an essential ambiguity in the definition of potential, since the choice of reference point \\mathscr{O} was arbitrary. Changing reference points amounts to adding a constant K to the potential: V'(r) = -\\int_{\\mathscr{O}'}^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = - \\int_{\\mathscr{O}'} ^{\\mathscr{O}} \\vec{E} \\cdot \\dd{\\vec{l}} - \\int_{\\mathscr{O}}^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = K + V(\\vec{r}) where K is the line integral of E from the old reference point \\mathscr{O} to the new one \\mathscr{O}' . Of course, adding a constant to V will not affect the potential difference between two points, since the K's cancel out. Nor does the ambiguity affect the gradient of V: \\grad{V'} = \\grad{V} since the derivative of a constant is zero. That's why all such V's, differing only in their choice of reference point, correspond to the same field E Potential as such carries no real physical significance, for at any given point we can adjust its value at will by suitable relocation of \\mathscr{O} . In this sense, it is rather like altitude: if I ask you how high Denver is, you will probably tell me its height above sea level, because that is a convenient and traditional reference point. But we could as well agree to measure altitude above Washington, DC, or Greenwich, or wherever. That would add (or rather, subtract) a fixed amount from all our sea-level readings, but it wouldn't change anything about the real world. The only quantity of interest is the difference in altitude between two points, and that is the same whatever your reference level. Having said this, however, there is a \"natural\" spot to use for \\mathscr{O} in electrostatics - analogous to sea level for altitude - and that is a point infinitely far from the charge. Ordinarily, then, we s\"set the zero of potential at infinity.\" (Since V(\\mathscr{O}) = 0 , choosing a reference point is equivalent to selecting a place where V is to be zero.) But I must warn you that there is one special circumstance in which this convention fails: when the charge distribution itself extends to infinity. The symptom of trouble, in such cases, is that the potential blows up. For instance, the field of a uniformly charged plane is (\\sigma / 2 \\epsilon_0) \\vu{n} , as we found in Ex 2.5; if we naively put \\mathscr{O} = \\infty , then the potential at height z above the plane becomes V(z) = - \\int_{\\infty}^{z}\\frac{1}{2\\epsilon_0} \\sigma \\dd{z} = - \\frac{1}{2\\epsilon_0} \\sigma(z - \\infty) The remedy is simply to choose some other reference point (in this example you might use a point on the plane). Notice that the difficulty occurs only in textbook problems; in \"real life\" there is no such thing as a charge distribution that goes on forever, and we can always use infinity as our reference point. Potential obeys the superposition principle . The original superposition principle pertains to the force on a test charge Q. It says that the total force on Q is the vector sum of the forces attributable to the source charges individually: \\vec{F} = \\vec{F_1} + \\vec{F_2} + \\ldots Dividing through by Q, we see that the electric field, too, obeys the superposition principle: \\vec{E} = \\vec{E_1} + \\vec{E_2} + \\ldots \\label{2.38} Integrating from the common reference point to \\vec{r} , it follows that the potential also satisfies such a principle: V = V_1 + V_2 + \\ldots That is, the potential at any given point is the sum of the potentials due to all the source charges separately. Only this time it is an ordinary sum, not a vector sum, which makes it a lot easier to work with. Units of Potential . In our units, force is measured in newtons and charge in coulombs, so electric fields are in newtons per coulomb. Accordingly, potential is newton-meters per coulomb, or joules per coulomb. A joule per coulomb is a volt . 2.3.3: Poisson's Equation and Laplace's Equation We found in Sect 2.3.1 that the electric field can be written as the gradient of a scalar potential \\vec{E} = - \\grad{V} The question arises, what do the divergence and curl of E , \\div{\\vec{E}} = \\frac{\\rho}{\\epsilon_0} \\qquad \\text{ and } \\qquad \\curl{\\vec{E}} = 0 look like, in terms of V? Well, \\div{\\vec{E}} = \\div(-\\grad{V}) = -\\laplacian{V} , so, apart from that persistent minus sign, the divergence of E is the Laplacian of V. Gauss's law, then, says \\laplacian{V} = -\\frac{\\rho}{\\epsilon_0} \\label{2.24} This is known as Poisson's equation . In regions where there is no charge, so \\rho = 0 , Poisson's equation reduces to Laplace's equation, \\laplacian{V} = 0 \\label{2.25} We'll explore this equation more fully in Chapter 3. So much for Gauss's law. What about the curl law? This says that \\curl{\\vec{E}} = \\curl(-\\grad{V}) = 0 But that's no condition on V - curl of gradient is always zero. Of course, we used the curl law to show that E could be expressed as the gradient of a scalar, so it's not really surprising that this works out: \\curl{\\vec{E}} = 0 permits our definition of V; in return, \\vec{E} = - \\grad{V} guarantees \\curl{\\vec{E}} = 0 . It only takes one differential equation (Poisson's) to determine V, because V is a scalar. For \\vec{E} we needed two, the divergence and the curl. 2.3.4: The potential of a Localized Charge Distribution I defined V in terms of \\vec{E} \\eqref{2.21} . Ordinarily, though, it's E that we're looking for (if we already knew E , there wouldn't be much point in calculating V). The idea is that it might be easier to get V first, and then calculate E by taking the gradient. Typically, then, we know where the charge is (that is, we know \\rho ), and we want to find V. Now, Poisson's equation relates V and \\rho , but unfortunately it's \"the wrong way round\": it would give us \\rho if we knew V, whereas we want V, knowing \\rho . What we must do, then, is \"invert\" Poisson's equation. That's the program for this section, although I shall do it by roundabout means, beginning, as always, with a point charge at the origin. The electric field is \\vec{E} = (1 / 4 \\pi \\epsilon_0)(1 / r^2) \\vu{r} , and \\dd{\\vec{l}} = \\dd{r} \\vu{r} , and \\dd{\\vec{l}} = \\dd{r} \\vu{r} + r \\dd{\\theta} \\vu{\\theta} + r \\sin \\theta \\dd{\\theta} \\vu{\\phi} , so \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\dd{r} Setting the reference point at infinity, the potential of a point charge q at the origin is V(r) = - \\int_{\\mathscr{O}} ^r \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = \\frac{-1}{4 \\pi \\epsilon_0} \\int_{\\infty}^r \\frac{q}{r' ^2} \\dd{r'} \\\\ = \\left.\\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r'} \\right| ^r _{\\infty} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r} (You see here the advantage of using infinity for the reference point: it kills the lower limit on the integral.) Notice the sign of V; presumably the conventional minus sign in the definition was chosen in order to make the potential of a positive charge come out positive. It is useful to remember that regions of positive charge are potential \"hills,\" and electric field points \"downhill\" from plus toward minus. In general, the potential of a point charge q is V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{\\gr} where \\gr , as always, is the distance from q to \\vec{r} (Fig 2.32). Invoking the superposition principle, then, the potential of a collection of charges is V(r) = \\frac{1}{4\\pi \\epsilon_0} \\sum_{i=1} ^n \\frac{q_i}{\\gr _i} or, for a continuous distribution, V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r}')}{\\gr} \\dd{\\tau'} \\label{2.29} \\tag{2.29} This is the equation we were looking for, telling us how to compute V when we know \\rho ; it is, if you like, the \"solution\" to Poisson's equation, for a localized charge distribution. Compare \\eqref{2.29} with the corresponding formula for the electric field in terms of \\rho : \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{\\tau'} The main point is that the pesky unit vector \\vu{\\gr} is gone, so there is no need to fuss with components. The potentials of line and surface charges are V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\lambda(\\vec{r'})}{\\gr} \\dd{l'} \\qquad \\text{ and } \\qquad V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma(\\vec{r'})}{\\gr} \\dd{a'} I should warn you that everything in this section is predicated on the assumption that the reference point is at infinity. This is hardly apparent in \\eqref{2.29} , but remember that we got the equation from the potential of a point charge at the origin, (1/4 \\pi \\epsilon_0) (q / r) , which is valid only when \\mathscr{O} = \\infty . If you try to apply these formulas to one of those artificial problems in which the charge itself extends to infinity, the integral will diverge. 2.3.5: Boundary Conditions In the typical electrostatic problem you are given a source charge distribution \\rho , and you want to find the electric field \\vec{E} it produces. Unless the symmetry of the problem allows a solution by Gauss's law, it is generally to your advantage to calculate the potential first, as an intermediate step. These are the three fundamental quantities of electrostatics: \\rho , \\vec{E} , and V . We have, in the course of our discussion, derived all six formulas interrelating them. These equations are neatly summarized in Fig. 2.35. We began with just two experimental observations: (1) the principle of superposition - a broad general rule applying to all electromagnetic forces, and (2) Coulomb's law - the fundamental law of electrostatics. From these, all else followed. You may have noticed, in studying the exercises in this chapter, that the electric field always undergoes a discontinuity when you cross a surface charge \\sigma . In fact, it is a simple matter to find the amount by which E changes at such a boundary. Suppose we draw a wafer-thin Gaussian pillbox, extending just barely over the edge in each direction (Fig. 2.36). Gauss's law says that \\oint _{S} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} = \\frac{1}{\\epsilon_0} \\sigma A where A is the area of the pillbox lid. If \\sigma varies from point to point or the surface is curved, we can simply pick A to be extremely small. Now, the sides of the pillbox contribute nothing to the flux, in the limit as the thickness \\epsilon goes to zero, so we are left with E_{above}^{\\perp} - E_{below} ^{\\perp} = \\frac{1}{\\epsilon_0} \\sigma \\label{2.31} \\tag{2.31} where E_{above}^{\\perp} denotes the component of \\vec{E} that is perpendicular to the surface immediately above, and E_{below} ^{\\perp} is the same, only just below the surface. For consistency, let \"upward\" be the positive direction for both. Conclusion: the normal component of \\vec{E} is discontinuous by an amount \\sigma / \\epsilon_0 at any boundary. In particular, where there is no surface charge, \\vec{E}^{\\perp} is continuous, as for instance at the surface of a uniformly charged solid sphere. The tangential component of \\vec{E} , by contrast, is always continuous. For if we apply Eq. 2.19, \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = 0 to the thin rectangular loop of Fig 2.37, the ends give nothing (as \\epsilon \\rightarrow 0 ), and the sides give (E_{above} ^{\\parallel} l - E_{below} ^{\\parallel} l) , so \\vec{E}_{above} ^{\\parallel} = \\vec{E}_{below} ^{\\parallel} \\label{2.32} \\tag{2.32} where \\vec{E}^{\\parallel} stands for the components of \\vec{E} parallel to the surface. The boundary conditions on \\vec{E} (Eqs. \\eqref{2.31} and \\eqref{2.32} ) can be combined into a single formula: \\vec{E}_{above} - \\vec{E}_{below} = \\frac{\\sigma}{\\epsilon_0} \\vu{n} \\label{2.33} where \\vu{n} is a unit vector perpendicular to the surface, pointing from \"below\" to \"above.\" The potential, meanwhile, is continuous across any boundary (Fig 2.38), since V_{above} - V_{below} = -\\int_{a}^{b} \\vec{E} \\cdot \\dd{\\vec{l}} as the path length shrinks to zero, so too does the integral V_{above} = V_{below} \\label{2.34} \\tag{2.34} However, the gradient of V inherits the discontinuity in \\vec{E} , since \\vec{E} - \\grad{V} , so \\grad{V}_{above} - \\grad{V}_{below} = - \\frac{\\sigma}{\\epsilon_0} \\vu{n} or more conveniently \\pdv{V_{above}}{n} - \\pdv{V_{below}}{n} = - \\frac{1}{\\epsilon_0} \\sigma \\label{2.36} \\tag{2.36} where \\pdv{V}{n} = \\grad{V} \\cdot \\vu{n} denotes the normal derivative of V (that is, the rate of change in the direction perpendicular to the surface). Please note that these boundary conditions relate the fields and potentials just above and just below the surface. For example, the derivatives in \\eqref{2.36} are the limiting values as we approach the surface from either side.","title":"2.3 - Electric Potential"},{"location":"ch2-3/#23-electric-potential","text":"","title":"2.3: Electric Potential"},{"location":"ch2-3/#231-introduction-to-potential","text":"The electric field E is not just any old vector function. It is a very special kind of vector function: one whose curl id zero. \\vec{E} = y \\vu{x} , for example, could not possibly be an electrostatic field; no set of charges, regardless of their sizes and positions, could ever produce such a field. We're going to exploit this special property of electric fields to reduce a vector problem (finding E ) to a much simpler scalar problem. The first theorem in Sect 1.6.2 asserts that any vector whose curl is zero is equal to the gradient of some scalar. What I'm going to do now amounts to a proof of that claim, in the context of electrostatics. Because \\nabla \\cross \\vec{E} = 0 , the line integral of E around any closed loop is zero (that follows from Stokes' theorem). Because \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = 0 , the line integral of E from point a to point b is the same for all paths (otherwise you could go out along path (i) and return along path (ii) - Fig 2.30 - and obtain \\oint \\vec{E} \\cdot \\dd{\\vec{l}} \\neq 0 ). Because the line integral is independent of path, we can define a function V(\\vec{r}) \\equiv - \\int _{O} ^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\label{2.21} \\tag{2.21} Here O is some standard reference point on which we have agreed beforehand; V then depends only on the point \\vec{r} . It is called the electric potential . The potential difference between two points a and b is \\begin{align} V(\\vec{b}) - V(\\vec{a}) & = & -\\int_{O}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} + \\int_{O}^{\\vec{a}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ & = & -\\int_{O}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} - \\int_{\\vec{a}}^{O} \\vec{E}\\cdot \\dd{\\vec{l}} \\\\ & = & - \\int_{\\vec{a}} ^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} \\end{align} \\label{2.22} \\tag{2.22} Now, the fundamental theorem for gradients states that V(\\vec{b}) - V(\\vec{a}) = \\int_{\\vec{a}} ^{\\vec{b}} (\\grad{V}) \\cdot \\dd{\\vec{l}} so \\int_{\\vec{a}}^{\\vec{b}} (\\grad{V})\\cdot \\dd{\\vec{l}} = - \\int_{\\vec{a}}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} Since, finally, this is true for any points a and b , the integrands must be equal: \\vec{E} = - \\grad{V} \\label{2.23} \\tag{2.23} Equation \\eqref{2.23} is the differential version of \\eqref{2.21} ; it says that the electric field is the gradient of a scalar potential, which is what we set out to prove. Notice the subtle but crucial role played by path independence (or, equivalently, the fact that \\nabla \\times \\vec{E} = 0 ) in this argument. If the line integral of E depended on the path taken, then the \"definition\" of V \\eqref{2.21} would be nonsense. It simply would not define a function, since changing the path would alter the value of V(\\vec{r}) . By the way, don't let the minus sign in \\eqref{2.23} distract you; it carries over from \\eqref{2.21} and is largely a matter of convention.","title":"2.3.1: Introduction to Potential"},{"location":"ch2-3/#232-comments-on-potential","text":"The name . The word \"potential\" is a hideous misnomer because it inevitably reminds you of potential energy . This is particularly insidious, because there is a connection between \"potential\" and \"potential energy,\" as you will see in Sect 2.4. I'm sorry that it is impossible to escape this word. The best I can do is to insist once and for all that \"potential\" and \"potential energy\" are completely different terms and should, by all rights, have different names. Incidentially, a surface over which the potential is constant is called an equipotential . Advantage of the potential formulation . If you know V, you can easily get E - just take the gradient: \\vec{E} =- \\grad{V} . This is quite extraordinary when you stop to think about it, for E is a vector quantity (three components), but V is a scalar (one component). How can one function possibly contain all the information that three independent functions carry? The answer is that the three components of E are not really as independent as they look; in fact, they are explicitly interrelated by the very condition we started with, \\nabla \\times \\vec{E} = 0 . In terms of components, \\pdv{E_x}{y} = \\pdv{E_y}{x}, \\qquad \\pdv{E_z}{y} = \\pdv{E_y}{z}, \\qquad \\pdv{E_x}{z} = \\pdv{E_z}{x} This brings us back to my observation at the beginning of Sect 2.3.1: E is a very special kind of vector.What the potential formulation does is to exploit this feature to maximum advantage, reducing a vector problem to a scalar one, in which there is no need to fuss with components. The reference point \\mathscr{O} . There is an essential ambiguity in the definition of potential, since the choice of reference point \\mathscr{O} was arbitrary. Changing reference points amounts to adding a constant K to the potential: V'(r) = -\\int_{\\mathscr{O}'}^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = - \\int_{\\mathscr{O}'} ^{\\mathscr{O}} \\vec{E} \\cdot \\dd{\\vec{l}} - \\int_{\\mathscr{O}}^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = K + V(\\vec{r}) where K is the line integral of E from the old reference point \\mathscr{O} to the new one \\mathscr{O}' . Of course, adding a constant to V will not affect the potential difference between two points, since the K's cancel out. Nor does the ambiguity affect the gradient of V: \\grad{V'} = \\grad{V} since the derivative of a constant is zero. That's why all such V's, differing only in their choice of reference point, correspond to the same field E Potential as such carries no real physical significance, for at any given point we can adjust its value at will by suitable relocation of \\mathscr{O} . In this sense, it is rather like altitude: if I ask you how high Denver is, you will probably tell me its height above sea level, because that is a convenient and traditional reference point. But we could as well agree to measure altitude above Washington, DC, or Greenwich, or wherever. That would add (or rather, subtract) a fixed amount from all our sea-level readings, but it wouldn't change anything about the real world. The only quantity of interest is the difference in altitude between two points, and that is the same whatever your reference level. Having said this, however, there is a \"natural\" spot to use for \\mathscr{O} in electrostatics - analogous to sea level for altitude - and that is a point infinitely far from the charge. Ordinarily, then, we s\"set the zero of potential at infinity.\" (Since V(\\mathscr{O}) = 0 , choosing a reference point is equivalent to selecting a place where V is to be zero.) But I must warn you that there is one special circumstance in which this convention fails: when the charge distribution itself extends to infinity. The symptom of trouble, in such cases, is that the potential blows up. For instance, the field of a uniformly charged plane is (\\sigma / 2 \\epsilon_0) \\vu{n} , as we found in Ex 2.5; if we naively put \\mathscr{O} = \\infty , then the potential at height z above the plane becomes V(z) = - \\int_{\\infty}^{z}\\frac{1}{2\\epsilon_0} \\sigma \\dd{z} = - \\frac{1}{2\\epsilon_0} \\sigma(z - \\infty) The remedy is simply to choose some other reference point (in this example you might use a point on the plane). Notice that the difficulty occurs only in textbook problems; in \"real life\" there is no such thing as a charge distribution that goes on forever, and we can always use infinity as our reference point. Potential obeys the superposition principle . The original superposition principle pertains to the force on a test charge Q. It says that the total force on Q is the vector sum of the forces attributable to the source charges individually: \\vec{F} = \\vec{F_1} + \\vec{F_2} + \\ldots Dividing through by Q, we see that the electric field, too, obeys the superposition principle: \\vec{E} = \\vec{E_1} + \\vec{E_2} + \\ldots \\label{2.38} Integrating from the common reference point to \\vec{r} , it follows that the potential also satisfies such a principle: V = V_1 + V_2 + \\ldots That is, the potential at any given point is the sum of the potentials due to all the source charges separately. Only this time it is an ordinary sum, not a vector sum, which makes it a lot easier to work with. Units of Potential . In our units, force is measured in newtons and charge in coulombs, so electric fields are in newtons per coulomb. Accordingly, potential is newton-meters per coulomb, or joules per coulomb. A joule per coulomb is a volt .","title":"2.3.2: Comments on Potential"},{"location":"ch2-3/#233-poissons-equation-and-laplaces-equation","text":"We found in Sect 2.3.1 that the electric field can be written as the gradient of a scalar potential \\vec{E} = - \\grad{V} The question arises, what do the divergence and curl of E , \\div{\\vec{E}} = \\frac{\\rho}{\\epsilon_0} \\qquad \\text{ and } \\qquad \\curl{\\vec{E}} = 0 look like, in terms of V? Well, \\div{\\vec{E}} = \\div(-\\grad{V}) = -\\laplacian{V} , so, apart from that persistent minus sign, the divergence of E is the Laplacian of V. Gauss's law, then, says \\laplacian{V} = -\\frac{\\rho}{\\epsilon_0} \\label{2.24} This is known as Poisson's equation . In regions where there is no charge, so \\rho = 0 , Poisson's equation reduces to Laplace's equation, \\laplacian{V} = 0 \\label{2.25} We'll explore this equation more fully in Chapter 3. So much for Gauss's law. What about the curl law? This says that \\curl{\\vec{E}} = \\curl(-\\grad{V}) = 0 But that's no condition on V - curl of gradient is always zero. Of course, we used the curl law to show that E could be expressed as the gradient of a scalar, so it's not really surprising that this works out: \\curl{\\vec{E}} = 0 permits our definition of V; in return, \\vec{E} = - \\grad{V} guarantees \\curl{\\vec{E}} = 0 . It only takes one differential equation (Poisson's) to determine V, because V is a scalar. For \\vec{E} we needed two, the divergence and the curl.","title":"2.3.3: Poisson's Equation and Laplace's Equation"},{"location":"ch2-3/#234-the-potential-of-a-localized-charge-distribution","text":"I defined V in terms of \\vec{E} \\eqref{2.21} . Ordinarily, though, it's E that we're looking for (if we already knew E , there wouldn't be much point in calculating V). The idea is that it might be easier to get V first, and then calculate E by taking the gradient. Typically, then, we know where the charge is (that is, we know \\rho ), and we want to find V. Now, Poisson's equation relates V and \\rho , but unfortunately it's \"the wrong way round\": it would give us \\rho if we knew V, whereas we want V, knowing \\rho . What we must do, then, is \"invert\" Poisson's equation. That's the program for this section, although I shall do it by roundabout means, beginning, as always, with a point charge at the origin. The electric field is \\vec{E} = (1 / 4 \\pi \\epsilon_0)(1 / r^2) \\vu{r} , and \\dd{\\vec{l}} = \\dd{r} \\vu{r} , and \\dd{\\vec{l}} = \\dd{r} \\vu{r} + r \\dd{\\theta} \\vu{\\theta} + r \\sin \\theta \\dd{\\theta} \\vu{\\phi} , so \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\dd{r} Setting the reference point at infinity, the potential of a point charge q at the origin is V(r) = - \\int_{\\mathscr{O}} ^r \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = \\frac{-1}{4 \\pi \\epsilon_0} \\int_{\\infty}^r \\frac{q}{r' ^2} \\dd{r'} \\\\ = \\left.\\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r'} \\right| ^r _{\\infty} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r} (You see here the advantage of using infinity for the reference point: it kills the lower limit on the integral.) Notice the sign of V; presumably the conventional minus sign in the definition was chosen in order to make the potential of a positive charge come out positive. It is useful to remember that regions of positive charge are potential \"hills,\" and electric field points \"downhill\" from plus toward minus. In general, the potential of a point charge q is V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{\\gr} where \\gr , as always, is the distance from q to \\vec{r} (Fig 2.32). Invoking the superposition principle, then, the potential of a collection of charges is V(r) = \\frac{1}{4\\pi \\epsilon_0} \\sum_{i=1} ^n \\frac{q_i}{\\gr _i} or, for a continuous distribution, V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r}')}{\\gr} \\dd{\\tau'} \\label{2.29} \\tag{2.29} This is the equation we were looking for, telling us how to compute V when we know \\rho ; it is, if you like, the \"solution\" to Poisson's equation, for a localized charge distribution. Compare \\eqref{2.29} with the corresponding formula for the electric field in terms of \\rho : \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{\\tau'} The main point is that the pesky unit vector \\vu{\\gr} is gone, so there is no need to fuss with components. The potentials of line and surface charges are V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\lambda(\\vec{r'})}{\\gr} \\dd{l'} \\qquad \\text{ and } \\qquad V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma(\\vec{r'})}{\\gr} \\dd{a'} I should warn you that everything in this section is predicated on the assumption that the reference point is at infinity. This is hardly apparent in \\eqref{2.29} , but remember that we got the equation from the potential of a point charge at the origin, (1/4 \\pi \\epsilon_0) (q / r) , which is valid only when \\mathscr{O} = \\infty . If you try to apply these formulas to one of those artificial problems in which the charge itself extends to infinity, the integral will diverge.","title":"2.3.4: The potential of a Localized Charge Distribution"},{"location":"ch2-3/#235-boundary-conditions","text":"In the typical electrostatic problem you are given a source charge distribution \\rho , and you want to find the electric field \\vec{E} it produces. Unless the symmetry of the problem allows a solution by Gauss's law, it is generally to your advantage to calculate the potential first, as an intermediate step. These are the three fundamental quantities of electrostatics: \\rho , \\vec{E} , and V . We have, in the course of our discussion, derived all six formulas interrelating them. These equations are neatly summarized in Fig. 2.35. We began with just two experimental observations: (1) the principle of superposition - a broad general rule applying to all electromagnetic forces, and (2) Coulomb's law - the fundamental law of electrostatics. From these, all else followed. You may have noticed, in studying the exercises in this chapter, that the electric field always undergoes a discontinuity when you cross a surface charge \\sigma . In fact, it is a simple matter to find the amount by which E changes at such a boundary. Suppose we draw a wafer-thin Gaussian pillbox, extending just barely over the edge in each direction (Fig. 2.36). Gauss's law says that \\oint _{S} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} = \\frac{1}{\\epsilon_0} \\sigma A where A is the area of the pillbox lid. If \\sigma varies from point to point or the surface is curved, we can simply pick A to be extremely small. Now, the sides of the pillbox contribute nothing to the flux, in the limit as the thickness \\epsilon goes to zero, so we are left with E_{above}^{\\perp} - E_{below} ^{\\perp} = \\frac{1}{\\epsilon_0} \\sigma \\label{2.31} \\tag{2.31} where E_{above}^{\\perp} denotes the component of \\vec{E} that is perpendicular to the surface immediately above, and E_{below} ^{\\perp} is the same, only just below the surface. For consistency, let \"upward\" be the positive direction for both. Conclusion: the normal component of \\vec{E} is discontinuous by an amount \\sigma / \\epsilon_0 at any boundary. In particular, where there is no surface charge, \\vec{E}^{\\perp} is continuous, as for instance at the surface of a uniformly charged solid sphere. The tangential component of \\vec{E} , by contrast, is always continuous. For if we apply Eq. 2.19, \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = 0 to the thin rectangular loop of Fig 2.37, the ends give nothing (as \\epsilon \\rightarrow 0 ), and the sides give (E_{above} ^{\\parallel} l - E_{below} ^{\\parallel} l) , so \\vec{E}_{above} ^{\\parallel} = \\vec{E}_{below} ^{\\parallel} \\label{2.32} \\tag{2.32} where \\vec{E}^{\\parallel} stands for the components of \\vec{E} parallel to the surface. The boundary conditions on \\vec{E} (Eqs. \\eqref{2.31} and \\eqref{2.32} ) can be combined into a single formula: \\vec{E}_{above} - \\vec{E}_{below} = \\frac{\\sigma}{\\epsilon_0} \\vu{n} \\label{2.33} where \\vu{n} is a unit vector perpendicular to the surface, pointing from \"below\" to \"above.\" The potential, meanwhile, is continuous across any boundary (Fig 2.38), since V_{above} - V_{below} = -\\int_{a}^{b} \\vec{E} \\cdot \\dd{\\vec{l}} as the path length shrinks to zero, so too does the integral V_{above} = V_{below} \\label{2.34} \\tag{2.34} However, the gradient of V inherits the discontinuity in \\vec{E} , since \\vec{E} - \\grad{V} , so \\grad{V}_{above} - \\grad{V}_{below} = - \\frac{\\sigma}{\\epsilon_0} \\vu{n} or more conveniently \\pdv{V_{above}}{n} - \\pdv{V_{below}}{n} = - \\frac{1}{\\epsilon_0} \\sigma \\label{2.36} \\tag{2.36} where \\pdv{V}{n} = \\grad{V} \\cdot \\vu{n} denotes the normal derivative of V (that is, the rate of change in the direction perpendicular to the surface). Please note that these boundary conditions relate the fields and potentials just above and just below the surface. For example, the derivatives in \\eqref{2.36} are the limiting values as we approach the surface from either side.","title":"2.3.5: Boundary Conditions"},{"location":"ch2-4/","text":"2.4: Work and Energy in Electrostatics 2.4.1: The Work it Takes to Move a Charge Suppose you have a stationary configuration of source charges, and you want to move a test charge Q from point a to point b (Fig. 2.39). Question : how much work will you have to do? At any point along the path, the electric force on Q is \\vec{F} = Q \\vec{E} ; the force you must exert, in opposition to the electric force, is -Q\\vec{E} . The work you do is therefore W = \\int_{a}^{b} \\vec{F} \\cdot \\dd{\\vec{l}} \\\\ = - Q \\int_{a}^{b} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = - Q[V(b) - V(a)] Notice that the answer is independent of the path you take from a to b; in mechanics, then, we would call the electrostatic force \"conservative.\" Dividing through by Q, we have V(b) - V(a) = \\frac{W}{Q} In words, the potential difference between points a and b is equal to the work per unit charge required to carry a particle from a to b. In particular, if you want to bring Q in from far away and stick it at point r, the work you must do is W = Q[V(\\vec{r}) - V(\\infty)], so if you have set the reference point at infinity, W = Q V(\\vec{r}) \\label{2.39} \\tag{2.39} In this sense, potential is potential energy (the work it takes to create a system) per unit charge (just as the field is force per unit charge). 2.4.2: The Energy of a Point Charge Distribution How much work would it take to assemble an entire collection of point charges? Imagine bringing in the charges, one by one, from far away (Fig 2.40). The first charge q_1 takes no work, since there is no field to fight against. Now bring in q_2 . According to \\eqref{2.39} this will cost you q_2 V_1(\\vec{r}_2) , where V_1 is the potential due to q_1 , and \\vec{r}_2 is the place we're putting q_2 : W_2 = \\frac{1}{4 \\pi \\epsilon_0} q_2 \\left( \\frac{q_1}{\\gr_{12}} \\right) ( \\gr_{12} is the distance between q_1 and q_2 , once they are in position). As you bring in each charge, nail it down in its final location, so it doesn't move when you bring in the next charge. Now bring in q_3 . This requires work q_3 V_{1,2}(\\vec{r}_3) , where V_{1,2} is the potential due to charges q_1 and q_2 , namely (1 / 4 \\pi \\epsilon_0) (q_1 / \\gr_{13} + q_2 / \\gr_{23} ) . Thus W_3 = \\frac{1}{4 \\pi \\epsilon_0} q_3 \\left( \\frac{q_1}{\\gr_{13}} + \\frac{q_2}{\\gr_{23}} \\right) Similarly, the extra work to bring in q_4 will be W_4 = \\frac{1}{4 \\pi \\epsilon_0} q_4 \\left( \\frac{q_1}{\\gr_{14}} + \\frac{q_2}{\\gr_{24}} + \\frac{q_3}{\\gr_{34}} \\right) The total work necessary to assemble the first four charges, then, is W = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q_1 q_2}{\\gr_{12}} + \\frac{q_1 q_3}{\\gr_{13}} + \\frac{q_1 q_4}{\\gr_{14}} + \\frac{q_2 q_3}{\\gr_{23}} + \\frac{q_2 q_4}{\\gr_{24}} + \\frac{q_3 q_4}{\\gr_{34}} \\right) You see the general rule: Take the product of each pair of charges, divide by their separation distance, and add it all up: W = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i = 1} ^{n} \\sum_{j > i} ^n \\frac{q_i q_j}{\\gr_{ij}} The stipulation j > i is to remind you not to count the same pair twice. A nicer way to accomplish this is intentionally to count each pair twice, and then divide by 2: W = \\frac{1}{8 \\pi \\epsilon_0} \\sum_{i = 1} ^n \\sum_{j \\neq i} \\frac{q_i q_j}{\\gr_{ij}} (we must still avoid i = j , of course). Notice that in this form the answer plainly does not depend on the order in which you assemble the charges, since every pair occurs in the sum. Finally, let's pull out the factor q_i : W = \\frac{1}{2} \\sum_{i = 1}^n q_i \\left( \\sum_{j \\neq i} ^n \\frac{1}{4 \\pi \\epsilon_0} \\frac{q_j}{\\gr_{ij}} \\right) The term in parentheses is the potential at point \\vec{r_i} (the position of q_i ) due to all the other charges - all of them, now, not just the ones that were present at some stage during the assembly. Thus, W = \\frac{1}{2} \\sum_{i = 1} ^n q_i V(\\vec{r_i}) \\label{2.42} \\tag{2.42} That's how much work it takes to assemble a configuration of point charges; it's also the amount of work you'd get back if you dismantled the system. In the meantime, it represents energy stored in the configuration (\"potential\" energy, if you insist, though for obvious reasons I prefer to avoid that word in this context). 2.4.3: The Energy of a Continuous Charge Distribution For a volume charge density \\rho , \\eqref{2.42} becomes W = \\frac{1}{2} \\int \\rho V \\dd{\\tau} \\label{2.43} \\tag{2.43} There is a lovely way to write this result, in which \\rho and V are eliminated in favor of \\vec{E} . First, use Gauss's law to express \\rho in terms of \\vec{E} \\rho = \\epsilon_0 \\div{\\vec{E}} \\qquad \\text{so,} \\qquad W = \\frac{\\epsilon_0}{2} \\int (\\div{\\vec{E}}) V \\dd{\\tau} Now, use integration by parts to transfer the derivative from \\vec{E} to V : W = \\frac{\\epsilon_0}{2} \\left[ - \\int \\vec{E} \\cdot (\\grad{V}) \\dd{\\tau} + \\oint V \\vec{E} \\cdot \\dd{\\vec{a}} \\right] But \\grad{V} = - \\vec{E} , so W = \\frac{\\epsilon_0}{2} \\left( \\int_{\\mathscr{V}} E^2 \\dd{\\tau} + \\oint V \\vec{E} \\cdot \\dd{\\vec{a}} \\right) \\label{2.44} \\tag{2.44} But what volume is this we're integrating over? Let's go back to the formula we started with, \\eqref{2.43} . From its derivation, it is clear that we should integrate over the region where the charge is located. But actually, any larger volume would do just as well: The \"extra\" territory we throw in will contribute nothing to the integral, since \\rho = 0 out there. With this in mind, we return to \\eqref{2.44} . What happens here, as we enlarge the volume beyond the minimum necessary to trap all the charge? Well, the integral of E^2 can only increase (the integrand being positive); evidently the surface integral must decrease accordingly to leave the sum intact. (In fact, at large distances from the charge, E goes like 1 / r^2 and V like 1/r , while the surface area grows like r^2 ; roughly speaking, then, the surface integral goes down like 1/r . Please understand: \\eqref{2.44} gives you the correct energy W, whatever volume you use (as long as it encloses all the charge), but the contribution of the volume integral goes up, and that of the surface integral goes down, as you take larger and larger volumes. In particular, why not integrate over all space? Then the surface integral goes to zero, and we are left with W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} \\quad \\text{(all space)} \\label{2.45} \\tag{2.45} Example 2.9 Find the energy of a uniformly charged spherical shell of total charge q and radius R Solution Use \\eqref{2.43} in the version appropriate to surface charges W = \\frac{1}{2} \\sigma V \\dd{a} Now, the potential at the surface of this sphere is (1/4 \\pi \\epsilon_0)q/R (a constant), so W = \\frac{1}{8\\pi \\epsilon_0} \\frac{q}{R} \\int \\sigma \\dd{a} = \\frac{1}{8 \\pi \\epsilon_0} \\frac{q^2}{R} Solution 2 Use \\eqref{2.45} . Inside the sphere, \\vec{E} = 0 ; outside: \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r} \\quad \\text{so} \\quad E^2 = \\frac{q^2}{(4 \\pi \\epsilon_0)^2 r^4} Therefore, W_{tot} = \\frac{\\epsilon_0}{2 (4 \\pi \\epsilon_0)^2}\\int \\left( \\frac{q^2}{r^4} \\right) (r^2 \\sin \\theta \\dd{r} \\dd{\\theta} \\dd{\\phi}) \\\\ = \\frac{1}{32 \\pi ^2 \\epsilon_0} q^2 4 \\pi \\int_{R} ^{\\infty} \\frac{1}{r^2} \\dd{r} = \\frac{1}{8 \\pi \\epsilon_0} \\frac{q^2}{R} 2.4.4: Comments on Electrostatic Energy A perplexing \"inconsistency\" Equation \\eqref{2.45} clearly implies that the energy of a stationary charge distribution is always positive. On the other hand, \\eqref{2.42} (from which \\eqref{2.45} was in fact derived), can be positive or negative. For instance, according to \\eqref{2.42} the energy of two equal but opposite charges a distance \\gr apart is -(1/4 \\pi \\epsilon_0) (q^2/\\gr) . What's gone wrong? Which equation is correct? The answer is that both are correct, but they speak to slightly different questions. Equation \\eqref{2.42} does not take into account the work necessary to make the point charges in the first place; we started with point charges and simply found the work required to bring them together. This is wise strategy, since \\eqref{2.45} indicates that the energy of a point charge is in fact infinite W = \\frac{\\epsilon_0}{2(4 \\pi \\epsilon_0)^2} \\int \\left( \\frac{q^2}{r^4} \\right) (r^2 \\sin \\theta \\dd{r} \\dd{\\theta} \\dd{\\phi}) = \\frac{q^2}{8 \\pi \\epsilon_0} \\int_{0} ^{\\infty} \\frac{1}{r^2} \\dd{r} = \\infty Equation \\eqref{2.45} is more complete , in the sense that it tells you the total energy stored in a charge configuration, but \\eqref{2.42} is more appropriate when you're dealing with point charges, because we prefer (for good reason!) to leave out that portion of the total energy that is attributable to the fabrication of the point charges themselves. In practice, after all, the point charges (electrons, say) are given to us ready-made; all we do is move them around. Since we did not put them together, and we cannot take them apart, it is immaterial how much work the process would involve. (Still, the infinite energy of a point charge is a recurring source of embarrassment for electromagnetic theory, afflicting the quantum version as well as the classical. We shall return to the problem in Chapter 11). Now, you may wonder where the inconsistency crept into an apparently water-tight derivation. The \"flaw\" lies between \\eqref{2.42} and \\eqref{2.43} : in the former, V(\\vec{r_i}) represents the potential due to all the other charges, but not q_i , whereas in the latter, V(\\vec{r}) is the full potential. For a continuous distribution, there is no distinction, since the amount of charge right at the point \\vec{r} is vanishingly small, and its contribution to the potential is zero. But in the presence of point charges you'd better stick with \\eqref{2.42} . Where is the energy stored? Equations \\eqref{2.43} and \\eqref{2.45} offer two different ways of calculating the same thing. The first is an integral over the charge distribution, the second is an integral over the field. These can involve completely different regions. For instance, in the case of a spherical shell, the charge is confined to the surface, whereas the electric field is everywhere outside this surface. Where is the energy, then? Is it stored in the field, as \\eqref{2.45} seems to suggest, or is it stored in the charge, as \\eqref{2.43} implies? At the present stage this is simply an unanswerable question: I can tell you what the total energy is, and I can provide you with several different ways to compute it, but it is impertinent to worry about where the energy is located. In the context of radiation theory (Chapter 11) it is useful (and in general relativity it is essential) to regard the energy as stored in the field, with a density \\frac{\\epsilon_0}{2} E^2 = \\text{ energy per unit volume} \\label{2.46} \\tag{2.46} But in electrostatics one could just as well say it is stored in the charge, with a density \\frac{1}{2} \\rho V . The difference is purely a matter of bookkeeping. The superposition principle . Because electrostatic energy is quadratic in the fields, it does not obey a superposition principle. The energy of a compound system is not the sum of the energies of its parts considered separately - there are also \"cross terms\": \\begin{align} W_{tot} & = & \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} = \\frac{\\epsilon_0}{2} \\int (\\vec{E_1} + \\vec{E_2})^2 \\dd{\\tau} \\\\ & = & \\frac{\\epsilon_0}{2} \\int (E_1 ^2 + E_2 ^2 + 2 \\vec{E_1} \\cdot \\vec{E_2}) \\dd{\\tau} \\\\ & = & W_1 + W_2 + \\epsilon_0 \\int \\vec{E_1} \\cdot \\vec{E_2} \\dd{\\tau} \\end{align} For example, if you double the charge everywhere, you quadruple the total energy.","title":"2.4 - Work and Energy in Electrostatics"},{"location":"ch2-4/#24-work-and-energy-in-electrostatics","text":"","title":"2.4: Work and Energy in Electrostatics"},{"location":"ch2-4/#241-the-work-it-takes-to-move-a-charge","text":"Suppose you have a stationary configuration of source charges, and you want to move a test charge Q from point a to point b (Fig. 2.39). Question : how much work will you have to do? At any point along the path, the electric force on Q is \\vec{F} = Q \\vec{E} ; the force you must exert, in opposition to the electric force, is -Q\\vec{E} . The work you do is therefore W = \\int_{a}^{b} \\vec{F} \\cdot \\dd{\\vec{l}} \\\\ = - Q \\int_{a}^{b} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = - Q[V(b) - V(a)] Notice that the answer is independent of the path you take from a to b; in mechanics, then, we would call the electrostatic force \"conservative.\" Dividing through by Q, we have V(b) - V(a) = \\frac{W}{Q} In words, the potential difference between points a and b is equal to the work per unit charge required to carry a particle from a to b. In particular, if you want to bring Q in from far away and stick it at point r, the work you must do is W = Q[V(\\vec{r}) - V(\\infty)], so if you have set the reference point at infinity, W = Q V(\\vec{r}) \\label{2.39} \\tag{2.39} In this sense, potential is potential energy (the work it takes to create a system) per unit charge (just as the field is force per unit charge).","title":"2.4.1: The Work it Takes to Move a Charge"},{"location":"ch2-4/#242-the-energy-of-a-point-charge-distribution","text":"How much work would it take to assemble an entire collection of point charges? Imagine bringing in the charges, one by one, from far away (Fig 2.40). The first charge q_1 takes no work, since there is no field to fight against. Now bring in q_2 . According to \\eqref{2.39} this will cost you q_2 V_1(\\vec{r}_2) , where V_1 is the potential due to q_1 , and \\vec{r}_2 is the place we're putting q_2 : W_2 = \\frac{1}{4 \\pi \\epsilon_0} q_2 \\left( \\frac{q_1}{\\gr_{12}} \\right) ( \\gr_{12} is the distance between q_1 and q_2 , once they are in position). As you bring in each charge, nail it down in its final location, so it doesn't move when you bring in the next charge. Now bring in q_3 . This requires work q_3 V_{1,2}(\\vec{r}_3) , where V_{1,2} is the potential due to charges q_1 and q_2 , namely (1 / 4 \\pi \\epsilon_0) (q_1 / \\gr_{13} + q_2 / \\gr_{23} ) . Thus W_3 = \\frac{1}{4 \\pi \\epsilon_0} q_3 \\left( \\frac{q_1}{\\gr_{13}} + \\frac{q_2}{\\gr_{23}} \\right) Similarly, the extra work to bring in q_4 will be W_4 = \\frac{1}{4 \\pi \\epsilon_0} q_4 \\left( \\frac{q_1}{\\gr_{14}} + \\frac{q_2}{\\gr_{24}} + \\frac{q_3}{\\gr_{34}} \\right) The total work necessary to assemble the first four charges, then, is W = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q_1 q_2}{\\gr_{12}} + \\frac{q_1 q_3}{\\gr_{13}} + \\frac{q_1 q_4}{\\gr_{14}} + \\frac{q_2 q_3}{\\gr_{23}} + \\frac{q_2 q_4}{\\gr_{24}} + \\frac{q_3 q_4}{\\gr_{34}} \\right) You see the general rule: Take the product of each pair of charges, divide by their separation distance, and add it all up: W = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i = 1} ^{n} \\sum_{j > i} ^n \\frac{q_i q_j}{\\gr_{ij}} The stipulation j > i is to remind you not to count the same pair twice. A nicer way to accomplish this is intentionally to count each pair twice, and then divide by 2: W = \\frac{1}{8 \\pi \\epsilon_0} \\sum_{i = 1} ^n \\sum_{j \\neq i} \\frac{q_i q_j}{\\gr_{ij}} (we must still avoid i = j , of course). Notice that in this form the answer plainly does not depend on the order in which you assemble the charges, since every pair occurs in the sum. Finally, let's pull out the factor q_i : W = \\frac{1}{2} \\sum_{i = 1}^n q_i \\left( \\sum_{j \\neq i} ^n \\frac{1}{4 \\pi \\epsilon_0} \\frac{q_j}{\\gr_{ij}} \\right) The term in parentheses is the potential at point \\vec{r_i} (the position of q_i ) due to all the other charges - all of them, now, not just the ones that were present at some stage during the assembly. Thus, W = \\frac{1}{2} \\sum_{i = 1} ^n q_i V(\\vec{r_i}) \\label{2.42} \\tag{2.42} That's how much work it takes to assemble a configuration of point charges; it's also the amount of work you'd get back if you dismantled the system. In the meantime, it represents energy stored in the configuration (\"potential\" energy, if you insist, though for obvious reasons I prefer to avoid that word in this context).","title":"2.4.2: The Energy of a Point Charge Distribution"},{"location":"ch2-4/#243-the-energy-of-a-continuous-charge-distribution","text":"For a volume charge density \\rho , \\eqref{2.42} becomes W = \\frac{1}{2} \\int \\rho V \\dd{\\tau} \\label{2.43} \\tag{2.43} There is a lovely way to write this result, in which \\rho and V are eliminated in favor of \\vec{E} . First, use Gauss's law to express \\rho in terms of \\vec{E} \\rho = \\epsilon_0 \\div{\\vec{E}} \\qquad \\text{so,} \\qquad W = \\frac{\\epsilon_0}{2} \\int (\\div{\\vec{E}}) V \\dd{\\tau} Now, use integration by parts to transfer the derivative from \\vec{E} to V : W = \\frac{\\epsilon_0}{2} \\left[ - \\int \\vec{E} \\cdot (\\grad{V}) \\dd{\\tau} + \\oint V \\vec{E} \\cdot \\dd{\\vec{a}} \\right] But \\grad{V} = - \\vec{E} , so W = \\frac{\\epsilon_0}{2} \\left( \\int_{\\mathscr{V}} E^2 \\dd{\\tau} + \\oint V \\vec{E} \\cdot \\dd{\\vec{a}} \\right) \\label{2.44} \\tag{2.44} But what volume is this we're integrating over? Let's go back to the formula we started with, \\eqref{2.43} . From its derivation, it is clear that we should integrate over the region where the charge is located. But actually, any larger volume would do just as well: The \"extra\" territory we throw in will contribute nothing to the integral, since \\rho = 0 out there. With this in mind, we return to \\eqref{2.44} . What happens here, as we enlarge the volume beyond the minimum necessary to trap all the charge? Well, the integral of E^2 can only increase (the integrand being positive); evidently the surface integral must decrease accordingly to leave the sum intact. (In fact, at large distances from the charge, E goes like 1 / r^2 and V like 1/r , while the surface area grows like r^2 ; roughly speaking, then, the surface integral goes down like 1/r . Please understand: \\eqref{2.44} gives you the correct energy W, whatever volume you use (as long as it encloses all the charge), but the contribution of the volume integral goes up, and that of the surface integral goes down, as you take larger and larger volumes. In particular, why not integrate over all space? Then the surface integral goes to zero, and we are left with W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} \\quad \\text{(all space)} \\label{2.45} \\tag{2.45}","title":"2.4.3: The Energy of a Continuous Charge Distribution"},{"location":"ch2-4/#example-29","text":"Find the energy of a uniformly charged spherical shell of total charge q and radius R Solution Use \\eqref{2.43} in the version appropriate to surface charges W = \\frac{1}{2} \\sigma V \\dd{a} Now, the potential at the surface of this sphere is (1/4 \\pi \\epsilon_0)q/R (a constant), so W = \\frac{1}{8\\pi \\epsilon_0} \\frac{q}{R} \\int \\sigma \\dd{a} = \\frac{1}{8 \\pi \\epsilon_0} \\frac{q^2}{R} Solution 2 Use \\eqref{2.45} . Inside the sphere, \\vec{E} = 0 ; outside: \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r} \\quad \\text{so} \\quad E^2 = \\frac{q^2}{(4 \\pi \\epsilon_0)^2 r^4} Therefore, W_{tot} = \\frac{\\epsilon_0}{2 (4 \\pi \\epsilon_0)^2}\\int \\left( \\frac{q^2}{r^4} \\right) (r^2 \\sin \\theta \\dd{r} \\dd{\\theta} \\dd{\\phi}) \\\\ = \\frac{1}{32 \\pi ^2 \\epsilon_0} q^2 4 \\pi \\int_{R} ^{\\infty} \\frac{1}{r^2} \\dd{r} = \\frac{1}{8 \\pi \\epsilon_0} \\frac{q^2}{R}","title":"Example 2.9"},{"location":"ch2-4/#244-comments-on-electrostatic-energy","text":"A perplexing \"inconsistency\" Equation \\eqref{2.45} clearly implies that the energy of a stationary charge distribution is always positive. On the other hand, \\eqref{2.42} (from which \\eqref{2.45} was in fact derived), can be positive or negative. For instance, according to \\eqref{2.42} the energy of two equal but opposite charges a distance \\gr apart is -(1/4 \\pi \\epsilon_0) (q^2/\\gr) . What's gone wrong? Which equation is correct? The answer is that both are correct, but they speak to slightly different questions. Equation \\eqref{2.42} does not take into account the work necessary to make the point charges in the first place; we started with point charges and simply found the work required to bring them together. This is wise strategy, since \\eqref{2.45} indicates that the energy of a point charge is in fact infinite W = \\frac{\\epsilon_0}{2(4 \\pi \\epsilon_0)^2} \\int \\left( \\frac{q^2}{r^4} \\right) (r^2 \\sin \\theta \\dd{r} \\dd{\\theta} \\dd{\\phi}) = \\frac{q^2}{8 \\pi \\epsilon_0} \\int_{0} ^{\\infty} \\frac{1}{r^2} \\dd{r} = \\infty Equation \\eqref{2.45} is more complete , in the sense that it tells you the total energy stored in a charge configuration, but \\eqref{2.42} is more appropriate when you're dealing with point charges, because we prefer (for good reason!) to leave out that portion of the total energy that is attributable to the fabrication of the point charges themselves. In practice, after all, the point charges (electrons, say) are given to us ready-made; all we do is move them around. Since we did not put them together, and we cannot take them apart, it is immaterial how much work the process would involve. (Still, the infinite energy of a point charge is a recurring source of embarrassment for electromagnetic theory, afflicting the quantum version as well as the classical. We shall return to the problem in Chapter 11). Now, you may wonder where the inconsistency crept into an apparently water-tight derivation. The \"flaw\" lies between \\eqref{2.42} and \\eqref{2.43} : in the former, V(\\vec{r_i}) represents the potential due to all the other charges, but not q_i , whereas in the latter, V(\\vec{r}) is the full potential. For a continuous distribution, there is no distinction, since the amount of charge right at the point \\vec{r} is vanishingly small, and its contribution to the potential is zero. But in the presence of point charges you'd better stick with \\eqref{2.42} . Where is the energy stored? Equations \\eqref{2.43} and \\eqref{2.45} offer two different ways of calculating the same thing. The first is an integral over the charge distribution, the second is an integral over the field. These can involve completely different regions. For instance, in the case of a spherical shell, the charge is confined to the surface, whereas the electric field is everywhere outside this surface. Where is the energy, then? Is it stored in the field, as \\eqref{2.45} seems to suggest, or is it stored in the charge, as \\eqref{2.43} implies? At the present stage this is simply an unanswerable question: I can tell you what the total energy is, and I can provide you with several different ways to compute it, but it is impertinent to worry about where the energy is located. In the context of radiation theory (Chapter 11) it is useful (and in general relativity it is essential) to regard the energy as stored in the field, with a density \\frac{\\epsilon_0}{2} E^2 = \\text{ energy per unit volume} \\label{2.46} \\tag{2.46} But in electrostatics one could just as well say it is stored in the charge, with a density \\frac{1}{2} \\rho V . The difference is purely a matter of bookkeeping. The superposition principle . Because electrostatic energy is quadratic in the fields, it does not obey a superposition principle. The energy of a compound system is not the sum of the energies of its parts considered separately - there are also \"cross terms\": \\begin{align} W_{tot} & = & \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} = \\frac{\\epsilon_0}{2} \\int (\\vec{E_1} + \\vec{E_2})^2 \\dd{\\tau} \\\\ & = & \\frac{\\epsilon_0}{2} \\int (E_1 ^2 + E_2 ^2 + 2 \\vec{E_1} \\cdot \\vec{E_2}) \\dd{\\tau} \\\\ & = & W_1 + W_2 + \\epsilon_0 \\int \\vec{E_1} \\cdot \\vec{E_2} \\dd{\\tau} \\end{align} For example, if you double the charge everywhere, you quadruple the total energy.","title":"2.4.4: Comments on Electrostatic Energy"},{"location":"ch2-5/","text":"2.5: Conductors 2.5.1: Basic Properties In an insulator , such as glass or rubber, each electron is on a short leash, attached to a particular atom. In a metallic conductor , by contrast, one or more electrons per atom are free to roam. (In liquid conductors such as salt water, it is ions that do the moving). A perfect conductor would contain an unlimited supply of free charges. In real life there are no perfect conductors, but metals come pretty close, for most purposes. From this definition, the basic electrostatic properties of ideal conductors immediately follow: (i) E = 0 inside a conductor . Why? Because if there were any field, those free charges would move, and it wouldn't be electrostatics any more. Hmm... that's hardly a satisfactory explanation; maybe all it proves is that you can't have electrostatics when conductors are present. We had better examine what happens when you put a conductor into an external electric field \\vec{E_0} (Fig. 2.42). Initially, the field will drive any free positive charges to the right, and negative ones to the left. (In practice, it's the negative charges - electrons - that do the moving, but when they depart, the right side is left with a net positive charge - the stationary nuclei - so it doesn't really matter which charges move; the effect is the same). When they come to the edge of the material, the charges pile up: plus on the right side, minus on the left. Now, these induced charges produce a field of their own, \\vec{E_1} , which, as you can see from the figure, is in the opposite direction to \\vec{E_0} . That's the crucial point, for it means that the field of the induced charges tends to cancel the original field. Charge will continue to flow until this cancellation is complete, and the resultant field inside the conductor is precisely zero. The whole process is practically instantaneous. (ii) \\rho = 0 inside a conductor. This follows from Gauss's law: if E is zero, so also is \\rho . There is still charge around, but exactly as much plus as minus, so the net charge density in the interior is zero. (iii) Any net charge resides on the surface . That's the only place left. (iv) A conductor is an equipotential . For if a and b are any two points within (or at the surface of) a given conductor, V(b) - V(a) = - \\int _{a} ^{b} \\vec{E} \\cdot \\dd{\\vec{l}} = 0 , and hence V(a) = V(b) . (v) E is perpendicular to the surface, just outside a conductor. Otherwise, as in (i), charge will immediately flow around the surface until it kills off the tangential component (Fig. 2.43). (Perpendicular to the surface, charge cannot flow, of course, since it is confined to the conducting object.) I think it is astonishing that the charge on a conductor flows to the surface. Because of their mutual repulsion, the charges naturally spread out as much as possible, but for all of them to go to the surface seems like a waste of the interior space. Surely we could do better, from the point of view of making each charge as possible from its neighbors, to sprinkle some of them throughout the volume. Well, it simply is not so. You do best to put all the charge on the surface, and this is true regardless of the size or shape of the conductor. The problem can also be phrased in terms of energy. Like any other free dynamical system, the charge on a conductor will seek the configuration that minimizes its potential energy. What property (iii) asserts is that the electrostatic energy of a solid object (with specified shape and total charge) is a minimum when that charge is spread over the surface. For instance, the energy of a sphere is (1 / 8 \\pi \\epsilon_0)(q^2 / R) if the charge is uniformly distributed over the surface, as we found in Ex 2.9, but it is greater (3/20 \\pi \\epsilon_0)(q^2 / R) if the charge is uniformly distributed throughout the volume (Prob. 2.34). 2.5.2: Induced Charges If you hold a charge +q near an uncharged conductor (Fig 2.44), the two will attract one another. The reason for this is that q will pull minus charges over to the near side and repel plus charges to the far side (Another way to think of it is that the charge moves around in such a way as to kill off the field of q for points inside the conductor, where the total field must be zero.) Since the negative induced charge is closer to q, there is a net force of attraction. (In chapter 3 we will calculate this force explicitly, for the case of a spherical conductor.) When I speak of the field, charge, or potential \"inside\" a conductor, I mean in the \"meat\" of the conductor. If there is some hollow cavity in the conductor, and within that cavity you put some charge, then the field in the cavity will not be zero. But in a remarkable way the cavity and its contents are electrically isolated from the outside world by the surrounding conductor (Fig. 2.45). No external fields penetrate the conductor; they are canceled at the outer surface by the induced charge there. Similarly, the field due to charges within the cavity is canceled, for all exterior points, by the induced charge on the inner surface. However, the compensating charge left over on the outer surface of the conductor effectively \"communicates\" the presence of q to the outside world. The total charge induced on the cavity wall is equal and opposite to the charge inside, for if we surround the cavity with a Gaussian surface, all points of which are in the conductor (Fig 2.45), \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = 0 , and hence (by Gauss's law) the net enclosed charge must be zero. But Q_{enc} = q + q_{induced} , so q_{induced} = - q . Then if the conductor as a whole is electrically neutral, there must be a charge +q on its outer surface. Example 2.10 An uncharged spherical conductor centered at the origin has a cavity of some weird shape carved out of it (Fig. 2.46). Somewhere within the cavity is a charge q. Question: What is the field outside the sphere? Solution At first glance, it would appear that the answer depends on the shape of the cavity and the location of the charge. But that's wrong: the answer is \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r} regardless. The conductor conceals from us all information concerning the nature of the cavity, revealing only the total charge it contains. How can this be? Well, the charge +q induces an opposite charge -q on the wall of the cavity, which distributes itself in such a way that its field cancels that of q, for all points exterior to the cavity. Since the conductor carries no net charge, this leaves +q to distribute itself uniformly over the surface of the sphere. (It's uniform because the asymmetrical influence of the point charge +q is negated by that of the induced charge -q on the inner surface.) For points outside the sphere, then, the only thing that survives is the field of the leftover +q, uniformly distributed over the outer surface. It may occur to you that in one respect this argument is open to challenge: There are actually three fields at work here: \\vec{E_q}, \\vec{E_{induced}} , and \\vec{E_{leftover}} . All we know for certain is that the sum of the three is zero inside the conductor, yet I claimed that the first two alone cancel, while the third is separately zero there. Moreover, even if the first two cancel within the conductor, who is to say they still cancel for points outside? They do not, after all, cancel for points inside the cavity. I cannot give you a completely satisfactory answer at the moment, but this much at least is true: there exists a way of distributing -q over the inner surface so as to cancel the field of q at all exterior points. For that same cavity could have been carved out of a huge spherical conductor with a radius of 27 miles or light years or whatever. In that case, the leftover +q on the outer surface is simply too far away to produce a significant field, and the other two fields would have to accomplish the cancellation by themselves. So we know they can do it... but are we sure they choose to? Perhaps for small spheres nature prefers some complicated three-way cancellation? Nope: As we'll see in the uniqueness theorems of Chapter 3, electrostatics is very stingy with its options; there is always precisely one way - no more - of distributing the charge on a conductor so as to make the field inside zero. Having found a possible way, we are guaranteed that no alternative exists, even in principle. If a cavity surrounded by conducting material is itself empty of charge, then the field within the cavity is zero. For any field line would have to begin and end on the cavity wall, going from a plus charge to a minus charge (Fig 2.47). Letting that field line be part of a closed loop, the rest of which is entirely inside the conductor (where E = 0), the integral \\oint \\vec{E} \\cdot \\dd{\\vec{l}} is distinctly positive , in violation of Eq. 2.19. It follows that E = 0 within an empty cavity, and there is in vact no charge on the surface of the cavity. (This is why you are relatively safe inside a metal car during a thunderstorm - you may get cooked, if lightning strikes, but you will not be electrocuted. The same principle applies to the placement of sensitive apparatus inside a grounded Faraday cage , to shield out stray electric fields. In practice, the enclosure doesn't even have to be solid conductor - chicken wire will often suffice.) 2.5.3: Surface Charge and the Force on a Conductor Because the field inside a conductor is zero, boundary condition Eq. 2.33 requires that the field immediately outside is \\vec{E} = \\frac{\\sigma}{\\epsilon_0} \\vu{n} \\label{2.48} \\tag{2.48} consistent with our earlier conclusion that the field is normal to the surface. In terms of potential, Eq. 2.36 yields \\sigma = - \\epsilon_0 \\pdv{V}{n} \\label{2.49} \\tag{2.49} These equations enable you to calculate the surface charge on a conductor, if you can determine \\vec{E} or V ; we shall use them frequently in the next chapter. In the presence of an electric field, a surface charge will experience a force; the force per unit area, \\vec{f} , is \\sigma \\vec{E} . But there's a problem here, for the electric field is discontinuous at a surface charge, so what are we supposed to use: \\vec{E}_{above}, \\vec{E}_{below} , or something in between? The answer is that we should use the average of the two \\vec{f} = \\sigma \\vec{E}_{average} = \\frac{1}{2} \\sigma (\\vec{E}_{above} + \\vec{E}_{below}) \\label{2.50} \\tag{2.50} Why the average? The reason is very simple, thought the telling makes it sound complicated: Let's focus our attention on a tiny patch of surface surrounding the point in question (Fig. 2.50). Make it small enough so it is essentially flat and the surface in question is essentially constant. The total field consists of two parts - that attributable to the patch itself, and that due to everything else (other regions of the surface, as well as any external sources that may be present) \\vec{E} = \\vec{E}_{patch} + \\vec{E}_{other} Now, the patch cannot exert a force on itself, any more than you can lift yourself by standing in a basket and pulling up on the handles. The force on the patch, then, is exclusively due to \\vec{E}_{other} , and this suffers no discontinuity (if we removed the patch, the field in the \"hole\" would be perfectly smooth). The discontinuity is due entirely to the charge on the patch, which puts out a field (\\sigma / 2 \\epsilon_0) on either side, pointing away from the surface. Thus, \\vec{E}_{above} = \\vec{E}_{other} + \\frac{\\sigma}{2 \\epsilon_0} \\vu{n} \\\\ \\vec{E}_{below} = \\vec{E}_{other} - \\frac{\\sigma}{2 \\epsilon_0} \\vu{n} \\\\ and hence \\vec{E}_{other} = \\frac{1}{2} (\\vec{E}_{above} + \\vec{E}_{below}) = \\vec{E}_{average} Averaging is really just a device for removing the contribution of the patch itself. That argument applies to any surface charge; in the particular case of a conductor, the field is zero inside and (\\sigma / \\epsilon_0)\\vu{n} outside ( \\eqref{2.48} , so the average is (\\sigma / 2 \\epsilon_0) \\vu{n} , and the force per unit area is f = \\frac{1}{2 \\epsilon_0} \\sigma ^2 \\vu{n} \\label{2.51} \\tag{2.51} This amounts to an outward electrostatic pressure on the surface, tending to draw the conductor into the field, regardless of the sign of \\sigma . Expressing the pressure in terms of the field just outside the surface P = \\frac{\\epsilon_0}{2} E^2 2.5.4: Capacitors Suppose we have two conductors, and we put charge +Q on one and -Q on the other (Fig 2.51). Since V is constant over a conductor, we can speak unambiguously of the potential difference between them: V = V_{+} - V_{-} = - \\int_{(-)}^{(+)} \\vec{E} \\cdot \\dd{\\vec{l}} We don't know how the charge distributes itself over the two conductors, and calculating the field would be a nightmare, if their shapes are complicated, but this much we do know: \\vec{E} is proportional to Q . For \\vec{E} is given by Coulomb's law: \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho}{\\gr^2} \\vu{\\gr} \\dd{\\tau} so if you double \\rho , you double \\vec{E} . Wait a minute! How do we know that doubling Q (and also -Q) simply doubles \\rho ? Maybe the charge moves around into a completely different configuration, quadrupling \\rho in some places and halving it in others, just so the total charge on the conductor is doubled. The fact is that this concern is unwarranted - doubling Q does double \\rho everywhere; it doesn't shift charge around. The proof will come in Chapter 3; for now you'll have to trust me. Since \\vec{E} is proportional to Q, so also is V. The constant of proportionality is called the capacitance of the arrangement C \\equiv \\frac{Q}{V} \\label{2.53} \\tag{2.53} Capacitance is a purely geometrical quantity, determined by the sizes, shapes, and separation of the two conductors. In SI units, C is measured in farads (F); a farad is a coulomb-per-volt. Actually this turns out to be inconveniently large; more practical units are the microfarad ( 10^{-6} F ) and the picofarad ( 10^{-12} F ) Notice that V is, by definition, the potential of the positive conductor less that of the negative one; likewise, Q is the charge of the positive conductor. Accordingly, capacitance is an intrinsically positive quantity. By the way, you will occasionally hear someone speak of the capacitance of a single conductor. In this case the \"second conductor\" is an imaginary spherical shell of infinite radius surrounding the one conductor. It contributes nothing to the field, so the capacitance is given by \\eqref{2.53} , where V is the potential with infinity as the reference point. Example 2.11 Find the capacitance of a parallel-plate capacitor consisting of two metal surfaces of area A held a distance d apart (Fig. 2.52) Solution If we put +Q on the top and -Q on the bottom, they will spread out uniformly over the two surfaces, provided the area is reasonably large and the separation small. The surface charge density, then, is \\sigma = Q / A on the top plate, and so the field, according to Ex. 2.6, is (1 / \\epsilon_0) Q / A . The potential difference between the plates is therefore V = \\frac{Q}{A \\epsilon_0} d and hence C = \\frac{A \\epsilon_0}{d} \\label{2.54} \\tag{2.54} If, for instance, the plates are square with sides 1 cm long, and they are held 1 mm apart, then the capacitance is 9 \\times 10^{-13} F Example 2.12 Find the capacitance of two concentric spherical metal shells, with radii a and b. Solution Place charge +Q on the inner sphere, and -Q on the outer one. The field between the spheres is \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q}{r^2} \\vec{r} so the potential difference between them is V = - \\int_{b}^{a} \\vec{E} \\cdot \\dd{\\vec{l}} = - \\frac{Q}{4 \\pi \\epsilon_0} \\int_{b}^a \\frac{1}{r^2} \\dd{r} = \\frac{Q}{4 \\pi \\epsilon_0} \\left( \\frac{1}{a} - \\frac{1}{b} \\right) As promised, V is proportional to Q; the capacitance is C = \\frac{Q}{V} = 4 \\pi \\epsilon_0 \\frac{ab}{(b - a)} To \"charge up\" a capacitor, you have to remove electrons from the positive plate and carry them to the negative plate. In doing so, you fight against the electric field, which is pulling them back toward the positive conductor and pushing them away from the negative one. How much work does it take, then, to charge the capacitor up to a final amount Q ? Suppose that at some intermediate stage in the process the charge on the positive plate is q , so that the potential difference is q / C . According to Eq. 2.38, the work you must do to transport the next piece of charge dq is \\dd{W} = \\left( \\frac{q}{C} \\right) \\dd{q} The total work necessary, then, to go from q = 0 to q = Q , is W = \\int_{0} ^Q \\left( \\frac{q}{C} \\dd{q} \\right) = \\frac{1}{2} \\frac{Q^2}{C} or, since Q = CV , W = \\frac{1}{2} C V^2 \\label{2.55} \\tag{2.55} where V is the final potential of the capacitor.","title":"2.5 - Conductors"},{"location":"ch2-5/#25-conductors","text":"","title":"2.5: Conductors"},{"location":"ch2-5/#251-basic-properties","text":"In an insulator , such as glass or rubber, each electron is on a short leash, attached to a particular atom. In a metallic conductor , by contrast, one or more electrons per atom are free to roam. (In liquid conductors such as salt water, it is ions that do the moving). A perfect conductor would contain an unlimited supply of free charges. In real life there are no perfect conductors, but metals come pretty close, for most purposes. From this definition, the basic electrostatic properties of ideal conductors immediately follow: (i) E = 0 inside a conductor . Why? Because if there were any field, those free charges would move, and it wouldn't be electrostatics any more. Hmm... that's hardly a satisfactory explanation; maybe all it proves is that you can't have electrostatics when conductors are present. We had better examine what happens when you put a conductor into an external electric field \\vec{E_0} (Fig. 2.42). Initially, the field will drive any free positive charges to the right, and negative ones to the left. (In practice, it's the negative charges - electrons - that do the moving, but when they depart, the right side is left with a net positive charge - the stationary nuclei - so it doesn't really matter which charges move; the effect is the same). When they come to the edge of the material, the charges pile up: plus on the right side, minus on the left. Now, these induced charges produce a field of their own, \\vec{E_1} , which, as you can see from the figure, is in the opposite direction to \\vec{E_0} . That's the crucial point, for it means that the field of the induced charges tends to cancel the original field. Charge will continue to flow until this cancellation is complete, and the resultant field inside the conductor is precisely zero. The whole process is practically instantaneous. (ii) \\rho = 0 inside a conductor. This follows from Gauss's law: if E is zero, so also is \\rho . There is still charge around, but exactly as much plus as minus, so the net charge density in the interior is zero. (iii) Any net charge resides on the surface . That's the only place left. (iv) A conductor is an equipotential . For if a and b are any two points within (or at the surface of) a given conductor, V(b) - V(a) = - \\int _{a} ^{b} \\vec{E} \\cdot \\dd{\\vec{l}} = 0 , and hence V(a) = V(b) . (v) E is perpendicular to the surface, just outside a conductor. Otherwise, as in (i), charge will immediately flow around the surface until it kills off the tangential component (Fig. 2.43). (Perpendicular to the surface, charge cannot flow, of course, since it is confined to the conducting object.) I think it is astonishing that the charge on a conductor flows to the surface. Because of their mutual repulsion, the charges naturally spread out as much as possible, but for all of them to go to the surface seems like a waste of the interior space. Surely we could do better, from the point of view of making each charge as possible from its neighbors, to sprinkle some of them throughout the volume. Well, it simply is not so. You do best to put all the charge on the surface, and this is true regardless of the size or shape of the conductor. The problem can also be phrased in terms of energy. Like any other free dynamical system, the charge on a conductor will seek the configuration that minimizes its potential energy. What property (iii) asserts is that the electrostatic energy of a solid object (with specified shape and total charge) is a minimum when that charge is spread over the surface. For instance, the energy of a sphere is (1 / 8 \\pi \\epsilon_0)(q^2 / R) if the charge is uniformly distributed over the surface, as we found in Ex 2.9, but it is greater (3/20 \\pi \\epsilon_0)(q^2 / R) if the charge is uniformly distributed throughout the volume (Prob. 2.34).","title":"2.5.1: Basic Properties"},{"location":"ch2-5/#252-induced-charges","text":"If you hold a charge +q near an uncharged conductor (Fig 2.44), the two will attract one another. The reason for this is that q will pull minus charges over to the near side and repel plus charges to the far side (Another way to think of it is that the charge moves around in such a way as to kill off the field of q for points inside the conductor, where the total field must be zero.) Since the negative induced charge is closer to q, there is a net force of attraction. (In chapter 3 we will calculate this force explicitly, for the case of a spherical conductor.) When I speak of the field, charge, or potential \"inside\" a conductor, I mean in the \"meat\" of the conductor. If there is some hollow cavity in the conductor, and within that cavity you put some charge, then the field in the cavity will not be zero. But in a remarkable way the cavity and its contents are electrically isolated from the outside world by the surrounding conductor (Fig. 2.45). No external fields penetrate the conductor; they are canceled at the outer surface by the induced charge there. Similarly, the field due to charges within the cavity is canceled, for all exterior points, by the induced charge on the inner surface. However, the compensating charge left over on the outer surface of the conductor effectively \"communicates\" the presence of q to the outside world. The total charge induced on the cavity wall is equal and opposite to the charge inside, for if we surround the cavity with a Gaussian surface, all points of which are in the conductor (Fig 2.45), \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = 0 , and hence (by Gauss's law) the net enclosed charge must be zero. But Q_{enc} = q + q_{induced} , so q_{induced} = - q . Then if the conductor as a whole is electrically neutral, there must be a charge +q on its outer surface.","title":"2.5.2: Induced Charges"},{"location":"ch2-5/#example-210","text":"An uncharged spherical conductor centered at the origin has a cavity of some weird shape carved out of it (Fig. 2.46). Somewhere within the cavity is a charge q. Question: What is the field outside the sphere? Solution At first glance, it would appear that the answer depends on the shape of the cavity and the location of the charge. But that's wrong: the answer is \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\vu{r} regardless. The conductor conceals from us all information concerning the nature of the cavity, revealing only the total charge it contains. How can this be? Well, the charge +q induces an opposite charge -q on the wall of the cavity, which distributes itself in such a way that its field cancels that of q, for all points exterior to the cavity. Since the conductor carries no net charge, this leaves +q to distribute itself uniformly over the surface of the sphere. (It's uniform because the asymmetrical influence of the point charge +q is negated by that of the induced charge -q on the inner surface.) For points outside the sphere, then, the only thing that survives is the field of the leftover +q, uniformly distributed over the outer surface. It may occur to you that in one respect this argument is open to challenge: There are actually three fields at work here: \\vec{E_q}, \\vec{E_{induced}} , and \\vec{E_{leftover}} . All we know for certain is that the sum of the three is zero inside the conductor, yet I claimed that the first two alone cancel, while the third is separately zero there. Moreover, even if the first two cancel within the conductor, who is to say they still cancel for points outside? They do not, after all, cancel for points inside the cavity. I cannot give you a completely satisfactory answer at the moment, but this much at least is true: there exists a way of distributing -q over the inner surface so as to cancel the field of q at all exterior points. For that same cavity could have been carved out of a huge spherical conductor with a radius of 27 miles or light years or whatever. In that case, the leftover +q on the outer surface is simply too far away to produce a significant field, and the other two fields would have to accomplish the cancellation by themselves. So we know they can do it... but are we sure they choose to? Perhaps for small spheres nature prefers some complicated three-way cancellation? Nope: As we'll see in the uniqueness theorems of Chapter 3, electrostatics is very stingy with its options; there is always precisely one way - no more - of distributing the charge on a conductor so as to make the field inside zero. Having found a possible way, we are guaranteed that no alternative exists, even in principle. If a cavity surrounded by conducting material is itself empty of charge, then the field within the cavity is zero. For any field line would have to begin and end on the cavity wall, going from a plus charge to a minus charge (Fig 2.47). Letting that field line be part of a closed loop, the rest of which is entirely inside the conductor (where E = 0), the integral \\oint \\vec{E} \\cdot \\dd{\\vec{l}} is distinctly positive , in violation of Eq. 2.19. It follows that E = 0 within an empty cavity, and there is in vact no charge on the surface of the cavity. (This is why you are relatively safe inside a metal car during a thunderstorm - you may get cooked, if lightning strikes, but you will not be electrocuted. The same principle applies to the placement of sensitive apparatus inside a grounded Faraday cage , to shield out stray electric fields. In practice, the enclosure doesn't even have to be solid conductor - chicken wire will often suffice.)","title":"Example 2.10"},{"location":"ch2-5/#253-surface-charge-and-the-force-on-a-conductor","text":"Because the field inside a conductor is zero, boundary condition Eq. 2.33 requires that the field immediately outside is \\vec{E} = \\frac{\\sigma}{\\epsilon_0} \\vu{n} \\label{2.48} \\tag{2.48} consistent with our earlier conclusion that the field is normal to the surface. In terms of potential, Eq. 2.36 yields \\sigma = - \\epsilon_0 \\pdv{V}{n} \\label{2.49} \\tag{2.49} These equations enable you to calculate the surface charge on a conductor, if you can determine \\vec{E} or V ; we shall use them frequently in the next chapter. In the presence of an electric field, a surface charge will experience a force; the force per unit area, \\vec{f} , is \\sigma \\vec{E} . But there's a problem here, for the electric field is discontinuous at a surface charge, so what are we supposed to use: \\vec{E}_{above}, \\vec{E}_{below} , or something in between? The answer is that we should use the average of the two \\vec{f} = \\sigma \\vec{E}_{average} = \\frac{1}{2} \\sigma (\\vec{E}_{above} + \\vec{E}_{below}) \\label{2.50} \\tag{2.50} Why the average? The reason is very simple, thought the telling makes it sound complicated: Let's focus our attention on a tiny patch of surface surrounding the point in question (Fig. 2.50). Make it small enough so it is essentially flat and the surface in question is essentially constant. The total field consists of two parts - that attributable to the patch itself, and that due to everything else (other regions of the surface, as well as any external sources that may be present) \\vec{E} = \\vec{E}_{patch} + \\vec{E}_{other} Now, the patch cannot exert a force on itself, any more than you can lift yourself by standing in a basket and pulling up on the handles. The force on the patch, then, is exclusively due to \\vec{E}_{other} , and this suffers no discontinuity (if we removed the patch, the field in the \"hole\" would be perfectly smooth). The discontinuity is due entirely to the charge on the patch, which puts out a field (\\sigma / 2 \\epsilon_0) on either side, pointing away from the surface. Thus, \\vec{E}_{above} = \\vec{E}_{other} + \\frac{\\sigma}{2 \\epsilon_0} \\vu{n} \\\\ \\vec{E}_{below} = \\vec{E}_{other} - \\frac{\\sigma}{2 \\epsilon_0} \\vu{n} \\\\ and hence \\vec{E}_{other} = \\frac{1}{2} (\\vec{E}_{above} + \\vec{E}_{below}) = \\vec{E}_{average} Averaging is really just a device for removing the contribution of the patch itself. That argument applies to any surface charge; in the particular case of a conductor, the field is zero inside and (\\sigma / \\epsilon_0)\\vu{n} outside ( \\eqref{2.48} , so the average is (\\sigma / 2 \\epsilon_0) \\vu{n} , and the force per unit area is f = \\frac{1}{2 \\epsilon_0} \\sigma ^2 \\vu{n} \\label{2.51} \\tag{2.51} This amounts to an outward electrostatic pressure on the surface, tending to draw the conductor into the field, regardless of the sign of \\sigma . Expressing the pressure in terms of the field just outside the surface P = \\frac{\\epsilon_0}{2} E^2","title":"2.5.3: Surface Charge and the Force on a Conductor"},{"location":"ch2-5/#254-capacitors","text":"Suppose we have two conductors, and we put charge +Q on one and -Q on the other (Fig 2.51). Since V is constant over a conductor, we can speak unambiguously of the potential difference between them: V = V_{+} - V_{-} = - \\int_{(-)}^{(+)} \\vec{E} \\cdot \\dd{\\vec{l}} We don't know how the charge distributes itself over the two conductors, and calculating the field would be a nightmare, if their shapes are complicated, but this much we do know: \\vec{E} is proportional to Q . For \\vec{E} is given by Coulomb's law: \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho}{\\gr^2} \\vu{\\gr} \\dd{\\tau} so if you double \\rho , you double \\vec{E} . Wait a minute! How do we know that doubling Q (and also -Q) simply doubles \\rho ? Maybe the charge moves around into a completely different configuration, quadrupling \\rho in some places and halving it in others, just so the total charge on the conductor is doubled. The fact is that this concern is unwarranted - doubling Q does double \\rho everywhere; it doesn't shift charge around. The proof will come in Chapter 3; for now you'll have to trust me. Since \\vec{E} is proportional to Q, so also is V. The constant of proportionality is called the capacitance of the arrangement C \\equiv \\frac{Q}{V} \\label{2.53} \\tag{2.53} Capacitance is a purely geometrical quantity, determined by the sizes, shapes, and separation of the two conductors. In SI units, C is measured in farads (F); a farad is a coulomb-per-volt. Actually this turns out to be inconveniently large; more practical units are the microfarad ( 10^{-6} F ) and the picofarad ( 10^{-12} F ) Notice that V is, by definition, the potential of the positive conductor less that of the negative one; likewise, Q is the charge of the positive conductor. Accordingly, capacitance is an intrinsically positive quantity. By the way, you will occasionally hear someone speak of the capacitance of a single conductor. In this case the \"second conductor\" is an imaginary spherical shell of infinite radius surrounding the one conductor. It contributes nothing to the field, so the capacitance is given by \\eqref{2.53} , where V is the potential with infinity as the reference point.","title":"2.5.4: Capacitors"},{"location":"ch2-5/#example-211","text":"Find the capacitance of a parallel-plate capacitor consisting of two metal surfaces of area A held a distance d apart (Fig. 2.52) Solution If we put +Q on the top and -Q on the bottom, they will spread out uniformly over the two surfaces, provided the area is reasonably large and the separation small. The surface charge density, then, is \\sigma = Q / A on the top plate, and so the field, according to Ex. 2.6, is (1 / \\epsilon_0) Q / A . The potential difference between the plates is therefore V = \\frac{Q}{A \\epsilon_0} d and hence C = \\frac{A \\epsilon_0}{d} \\label{2.54} \\tag{2.54} If, for instance, the plates are square with sides 1 cm long, and they are held 1 mm apart, then the capacitance is 9 \\times 10^{-13} F","title":"Example 2.11"},{"location":"ch2-5/#example-212","text":"Find the capacitance of two concentric spherical metal shells, with radii a and b. Solution Place charge +Q on the inner sphere, and -Q on the outer one. The field between the spheres is \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q}{r^2} \\vec{r} so the potential difference between them is V = - \\int_{b}^{a} \\vec{E} \\cdot \\dd{\\vec{l}} = - \\frac{Q}{4 \\pi \\epsilon_0} \\int_{b}^a \\frac{1}{r^2} \\dd{r} = \\frac{Q}{4 \\pi \\epsilon_0} \\left( \\frac{1}{a} - \\frac{1}{b} \\right) As promised, V is proportional to Q; the capacitance is C = \\frac{Q}{V} = 4 \\pi \\epsilon_0 \\frac{ab}{(b - a)} To \"charge up\" a capacitor, you have to remove electrons from the positive plate and carry them to the negative plate. In doing so, you fight against the electric field, which is pulling them back toward the positive conductor and pushing them away from the negative one. How much work does it take, then, to charge the capacitor up to a final amount Q ? Suppose that at some intermediate stage in the process the charge on the positive plate is q , so that the potential difference is q / C . According to Eq. 2.38, the work you must do to transport the next piece of charge dq is \\dd{W} = \\left( \\frac{q}{C} \\right) \\dd{q} The total work necessary, then, to go from q = 0 to q = Q , is W = \\int_{0} ^Q \\left( \\frac{q}{C} \\dd{q} \\right) = \\frac{1}{2} \\frac{Q^2}{C} or, since Q = CV , W = \\frac{1}{2} C V^2 \\label{2.55} \\tag{2.55} where V is the final potential of the capacitor.","title":"Example 2.12"},{"location":"ch3-1/","text":"3.1: Laplace's Equation 3.1.1: Introduction The primary task of electrostatics is to find the electric field of a given stationary charge distribution. In principle, this purpose is accomplished by Coulomb's law, in the form of \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{\\tau'} \\label{3.1} Unfortunately, integrals of this type can be difficult to calculate for any but the simplest charge configurations. Occasionally we can get around this by exploiting symmetry and using Gauss's law, but ordinarily the best strategy is first to calculate the potential , V, which is given by the somewhat more tractable V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r}')}{\\gr} \\dd{\\tau'} \\tagl{3.2} Still, even this integral is often too tough to handle analytically. Moreover, in problems involving conductors \\rho itself may not be known in advance; since charge is free to move around, the only thing we control directly is the total charge (or perhaps the potential) of each conductor. In such cases, it is fruitful to recast the problem in differential form, using Poisson's equation \\laplacian{V} = - \\frac{1}{\\epsilon_0} \\rho \\label{3.3} which, together with appropriate boundary conditions, is equivalent to \\eqref{3.2} . Very often, in fact, we are interested in finding the potential in a region where \\rho = 0 . (If \\rho = 0 everywhere, of course, then V = 0 , and there is nothing further to say - that's not what I mean. There may be plenty of charge elsewhere, but we're confining our attention to places where there is no charge.) In this case, Poisson's equation reduces to Laplace's equation \\laplacian{V} = 0 \\label{3.4} or, written out in Cartesian coordinates, \\frac{\\partial^2{V}}{\\partial{x^2}} + \\frac{\\partial^2 V}{\\partial{y^2}} + \\frac{\\partial^2{V}}{\\partial{z^2}} = 0 \\label{3.5} This formula is so fundamental to the subject that one might almost say electrostatics is the study of Laplace's equation. At the same time, it is a ubiquitous equation, appearing in such diverse branches of physics as gravitation and magnetism, the theory of heat, and the study of soap bubbles. In mathematics, it plays a major role in analytic function theory. To get a feel for Laplace's equation and its solutions (which are called harmonic functions ), we shall begin with the one- and two-dimensional versions, which are easier to picture, and illustrate all the essential properties of the three-dimensional case. 3.1.2: Laplace's Equation in One Dimension Suppose V depends on only one variable, x. Then Laplace's equation becomes \\frac{d^2 V}{dx^2} = 0 The general solution is V(x) = mx + b \\label{3.6} \\tag{3.6} the equation for a straight line. It contains two undetermined constants (m and b), as is appropriate for a second-order (ordinary) differential equation. They are fixed, in any particular case, by the boundary conditions of that problem. For instance, it might be specified that V = 4 at x = 1 and V = 0 at x = 5 . In that case, m = -1 and b = 5 , so V = - x + 5 (See Fig. 3.1) I want to call your attention to two features of this result; they may seem silly and obvious in one dimension, where I can write down the general solution explicitly, but the analogs in two and three dimensions are powerful and by no means obvious: V(x) is the average of V(x + a) and V(x - a) for any a: V(x) = \\frac{1}{2} [V(x + a) + V(x-a)] Laplace's equation is a kind of averaging instruction; it tells you to assign to the point x the average of the values to the left and to the right of x. Solutions to Laplace's equation are, in this sense, as boring as they could possibly be, and yet fit the end points properly. Laplace's equation tolerates no local maxima or minima ; extreme values of V must occur at the end points. Actually, this is a consequence of (1), for if there were a local maximum, V would be greater at that point than on either side, and therefore could not be the average. (Ordinarily, you expect the second derivative to be negative at a maximum and positive at a minimum. Since Laplace's equation requires, on the contrary, that the second derivative is zero, it seems reasonable that solutions should exhibit no extrema. However, this is not a proof, since there exist functions that have maxima and minima at points where the second derivative vanishes: x^4 for example, has such a minimum point at x=0 ). 3.1.3: Laplace's Equation in Two Dimensions If V depends on two variables, Laplace's equation becomes \\frac{\\partial^2{V}}{\\partial{x^2}} + \\frac{\\partial^2{V}}{\\partial{y^2}} = 0 This is no longer an ordinary differential equation (that is, one involving ordinary derivatives only); it is a partial differential equation. As a consequence, some of the simple rules you may be familiar with do not apply. For instance, the general solution to this equation doesn't contain just two arbitrary constants - or, for that matter, any finite number - despite the fact that it's a second order equation. Indeed, one cannot write down a \"general solution\" (at least, not in a closed form like \\eqref{3.6} ). Nevertheless, it is possible to deduce certain properties common to all solutions. It may help to have a physical example in mind. Picture a thin rubber sheet (or a soap film) stretched over some support. For definiteness, suppose you take a cardboard box, cut a wavy line all the way around, and remove the top part (Fig. 3.2). Now glue a tightly stretched rubber membrane over the box, so that it fits like a drum head (it won't be a flat drumhead, of course, unless you choose to cut the edges off straight). Now, if you lay out the coordinates (x, y) on the bottom of the box, the height V(x, y) of the sheet above the point (x, y) will satisfy Laplace's equation. (The one-dimensional analog would be a rubber band stretched between two points. Of course, it would form a straight line.) Actually, the equation satisfied by a rubber sheet is \\frac{\\partial}{\\partial{x}} \\left( g \\pdv{V}{y} \\right) + \\frac{\\partial}{\\partial{y}} \\left( g \\pdv{V}{y} \\right) = 0, \\\\ \\qquad \\text{ where } g = \\left[ 1 + \\left( \\pdv{V}{x} \\right)^2 + \\left( \\pdv{V}{y} \\right)^2 \\right]^{-1/2} Harmonic functions in two dimensions have the same properties we noted in one dimension: The value of V at a point (x, y) is the average of those around the point. More precisely, if you draw a circle of any radius R about the point (x, y), the average value of V on the circle is equal to the value at the center: V(x, y) = \\frac{1}{2 \\pi R} \\oint_{\\text{circle}} = V \\dd{l} (This, incidentally, suggests the method of relaxation, on which computer solutions to Laplace's equation are based: Starting with specified values for V at the boundary, and reasonable guesses for V on a grid of interior points, the first pass reassigns to each point the average of its nearest neighbors. The second pass repeats this process, using the corrected values, and so on. After a few iterations, the numbers begin to settle down, so that subsequent passes produce negligible changes, and a numerical solution to Laplace's equation, with the given boundary values, has been achieved.) V has no local maxima or minima; all extrema occur at the boundaries. (As before, this follows from (1)). Again, Laplace's equation picks the most featureless function possible, consistent with the boundary conditions: no hills, no valleys, just the smoothest conceivable surface. For instance, if you put a ping-pong ball on the stretched rubber sheet of Fig 3.2, it will roll over to one side and fall off - it will not find a \"pocket\" somewhere to settle into, for Laplace's equation allows no such dents in the surface. From a geometrical point of view, just as a straight line is the shortest distance between two points, so a harmonic function in two dimensions minimizes the surface area spanning the given boundary line. 3.1.4: Laplace's Equation in Three Dimensions In three dimensions I can neither provide you with an explicit solution (as in one dimension) nor offer a suggestive physical example to guide your intuition (as I did in two dimensions). Nevertheless, the same two properties remain true, and this time I will sketch a proof. For a proof that does not rely on Coulomb's law (only on Laplace's equation), see Prob. 3.37 The value of V at point r is the average value of V over a spherical surface of radius R centered at r : V(\\vec{r}) = \\frac{1}{4 \\pi R^2} \\oint_{\\text{sphere}} V \\dd{a} As a consequence, V can have no local maxima or minima; the extreme values of V must occur at the boundaries (For if V had a local maximum at r , then by the very nature of the maximum I could draw a sphere around r over which all the values of V - and a fortiori the average - would be less than at r .) Proof: V is a solution to the three-dimensional Laplace's equation. Then the value of V at point r is the average value of V over a spherical surface of radius R centered at r Let's begin by calculating the average potential over a spherical surface of radius R due to a single point charge q located outside the sphere. We may as well center the sphere at the origin and choose coordinates so that q lies on the z-axis (Fig 3.3). The potential at a point on the surface is V = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{\\gr} where \\gr ^2 = z^2 + R^2 - 2z R \\cos \\theta so V_{\\text{ave}} = \\frac{1}{4\\pi R^2} \\frac{1}{4 \\pi \\epsilon_0} \\int [z^2 + R^2 - 2 z R \\cos \\theta]^{-1/2} R^2 \\sin \\theta \\dd{\\theta} \\dd{\\phi} \\\\ = \\left. \\frac{q}{4 \\pi \\epsilon_0} \\frac{1}{2 z R} \\sqrt{z^2 + R^2 - 2 z R \\cos\\theta} \\right|_{0} ^{\\pi} \\\\ = \\frac{q}{4 \\pi \\epsilon_0} \\frac{1}{2zR} [(z + R) - (z-R)] = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{z} But this is precisely the potential due to q at the center of the sphere! By the superposition principle, the same goes for any collection of charges outside the sphere: their average potential over the sphere is equal to the net potential they produce at the center 3.1.5: Boundary Conditions and Uniqueness Theorems Laplace's equation does not by itself determine V; in addition, suitable boundary conditions must be supplied. This raises a delicate question: What are appropriate boundary conditions, sufficient to determine the answer and yet not so strong as to generate inconsistencies? The one-dimensional case is easy, for here the general solution V = mx + b contains two arbitrary constants, and we therefore require two boundary conditions. We might, for instance, specify the value of the function at each end, or we might give the value of the function and its derivative at one end, or the value at one end and the derivative at the other, and so on. But we cannot get away with just the value or just the derivative at one end - this is insufficient information. Nor would it do to specify the derivatives at both ends - this would be either redundant (if the two are equal) or inconsistent (if they are not). In two or three dimensions we are confronted by a partial differential equation, and it is not so obvious what would constitute acceptable boundary conditions. Is the shape of a taut rubber membrane, for instance, uniquely determined by the frame over which it is stretched, or, like a canning jar lid, can it snap from one stable configuration to another? The answer, as I think your intuition would suggest, that V is uniquely determined by its value at the boundary (canning jars evidently do not obey Laplace's equation). However, other boundary conditions can also be used (see Prob. 3.5). The proof that a proposed set of boundary conditions will suffice is usually presented in the form of a uniqueness theorem. There are many such theorems for electrostatics, all sharing the same basic format - I'll show you the two most useful ones: First Uniqueness Theorem: The solution to Laplace's equation in some volume \\mathscr{V} is uniquely determined if V is specified on the boundary surface \\mathscr{S} . In Fig. 3.5 I have drawn such a region and its boundary. (There could also be \"islands\" inside, so long as V is given on all their surfaces; also, the outer boundary could be at infinity, where V is ordinarily taken to be zero.) Proof : Suppose there were two solutions to Laplace's equation: \\laplacian{V_1} = 0 \\quad \\text{and} \\quad \\laplacian{V_2} = 0 both of which assume the specified value on the surface. I want to prove that they must be equal. The trick is to look at their difference : V_3 \\equiv V_1 - V_2 This obeys Laplace's equation (obviously) \\laplacian{V_3} = \\laplacian{V_1} - \\laplacian{V_2} = 0 and it takes the value zero on all boundaries (since V_1 and V_2 are equal there). But Laplace's equation allows no local maxima or minima - all extrema occur on the boundaries. So the maximum and minimum of V_3 are both zero. Therefore V_3 must be zero everywhere, and hence V_1 = V_2 Example 3.1 Show that the potential is constant inside an enclosure completely surrounded by conducting material, provided there is no charge within the enclosure. Solution The potential on the cavity wall is some constant V_0 (that's item (iv) in Sect. 2.5.1), so the potential inside is a function that satisfies Laplace's equation and has the constant value V_0 at the boundary. It doesn't take a genius to think of one solution to this problem: V = V_0 everywhere. The uniqueness theorem guarantees that this is the only solution. (It follows that the field inside an empty cavity is zero - the same result we found in Sect. 2.5.2 on rather different grounds.) The uniqueness theorem is a license to your imagination. It doesn't matter how you come by your solution; if (a) it satisfies Laplace's equation and (b) it has the correct value on the boundaries, then it's right . You'll see the power of this argument when we come to the method of images. Incidentally, it is easy to improve on the first uniqueness theorem: I assumed there was no charge inside the region in question, so the potential obeyed Laplace's equation, but we may as well throw in some charge (in which case V obeys Poisson's equation). Corollary: The potential in a volume \\mathscr{V} is uniquely determined if (a) the charge density throughout the region, and (b) the value of V on all boundaries, are specified The argument is the same, only this time \\laplacian{V_1} = -\\frac{1}{\\epsilon_0} \\rho, \\qquad \\laplacian{V_2} = - \\frac{1}{\\epsilon_0} \\rho so \\laplacian{V_3} = \\laplacian{V_1} - \\laplacian{V_2} = - \\frac{1}{\\epsilon_0} \\rho + \\frac{1}{\\epsilon_0} \\rho = 0 Once again the difference (V_3 \\equiv V_1 - V_2) satisfies Laplace's equation and has the value zero on all boundaries, so V_3 = 0 and hence V_1 = V_2 3.1.6: Conductors and the Second Uniqueness Theorem The simplest way to set the boundary conditions for an electrostatic problem is to specify the value of V on all surfaces surrounding the region of interest. And this situation often occurs in practice: In the laboratory, we have conductors connected to batteries, which maintain a given potential, or to ground , which is the experimentalist's word for V = 0 . However, there are other circumstances in which we do not know the potential at the boundary, but rather the charges on various conducting surfaces. Suppose I put Q_a on the first conductor, Q_b on the second conductor, and so on - I'm not telling you how the charge distributes itself over each conducting surface, because as soon as I put it on, it moves around in a way I do not control. And for good measure, let's say there is some specified charge density \\rho in the region between the conductors. Is the electric field now uniquely determined? Or are there perhaps a number of different ways the charges could arrange themselves on their respective conductors, each leading to a different field? Second uniqueness theorem : In a volume \\mathscr{V} surrounded by conductors and containing a specified charge density \\rho , the electric field is uniquely determined if the total charge on each conductor is given (Fig. 3.6). (The region as a whole can be bounded by another conductor, or else unbounded.) Proof : Suppose there are two fields satisfying the conditions of the problem. Both obey Gauss's law in differential form in the space betwen the conductors: \\div{\\vec{E_1}} = \\frac{1}{\\epsilon_0} \\rho, \\qquad \\div{\\vec{E_2}} = \\frac{1}{\\epsilon_0} \\rho And both obey Gauss's law in integral form for a Gaussian surface enclosing each conductor \\oint_{i_{th} \\text{ conducting surface}} \\vec{E_1} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_i \\quad \\text{ and } \\\\ \\oint_{i_{th} \\text{ conducting surface}} \\vec{E_2} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_i \\quad \\text{ and } \\\\ Likewise, for the outer boundary (whether this is just inside an enclosing conductor at infinity), \\oint_{\\text{outer boundary}} \\vec{E_1} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{tot} \\\\ \\oint_{\\text{outer boundary}} \\vec{E_2} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{tot} As before, we examine the difference \\vec{E_3} \\equiv \\vec{E_1} - \\vec{E_2} which obeys \\div{\\vec{E_3}} = 0 \\label{3.7} \\tag{3.7} in the region between the conductors, and \\oint \\vec{E_3} \\cdot \\dd{\\vec{a}} = 0 \\label{3.8} \\tag{3.8} over each boundary surface. Now there is one final piece of information we must exploit: Although we do not know how the charge Q_i distributes itself over the _i_th conductor, we do know that each conductor is an equipotential, and hence V_3 is a constant (not necessarily the same constant) over each conducting surface. (It need not be zero, for the potentials V_1 and V_2 may not be equal - all we know for sure is that both are constant over any given conductor.) Next comes a trick. Invoking the product rule \\div{(V_3 \\vec{E_3})} = V_3 (\\div{\\vec{E_3}}) + \\vec{E_3} \\cdot (\\grad{V_3}) = - (E_3)^2 Here I have used \\eqref{3.7} and \\vec{E_3} = - \\grad{V_3} . Integrating this over \\mathscr{V} and applying the divergence theorem to the left side: \\int_{\\mathscr{V}} \\div{(V_3 \\vec{E_3})} \\dd{\\tau} = \\oint_{S} V_3 \\vec{E_3} \\cdot \\dd{\\vec{a}} = - \\int_{\\mathscr{V}} (E_3)^2 \\dd{\\tau} The surface integral covers all boundaries of the region in question - the conductors and outer boundary. Now V_3 is a constant over each surface (if the outer boundary is invinity, V_3 = 0 there), so it comes outside each integral, and what remains is zero, according to \\eqref{3.8} . Therefore \\int_{\\mathscr{V}}(E_3)^2 \\dd{\\tau} = 0 The integrand is never negative, so the only way the integral can vanish is if E_3 = 0 everywhere. Consequently, \\vec{E_1} = \\vec{E_2} and the theorem is proved. This proof was not easy, and there is a real danger that the theorem itself will seem more plausible to you than the proof. In case you think the second uniqueness theorem is \"obvious,\" consider this example of Purcell's: Figure 3.7 shows a simple electrostatic configuration, consisting of four conductors with charges \\pm Q , situated so that the plusses are near the minuses. It all looks very comfortable. Now, what happens if we join them in pairs, by tiny wires, as indicated in Fig. 3.8? Since the positive charges are very near negative charges (which is where they like to be) you might well guess that nothing will happen - the configuration looks stable. Well, that sounds reasonable, but it's wrong. The configuration in Fig. 3.8 is impossible. For there are now effectively two conductors, and the total charge on each is zero. One possible way to distribute zero charge over these conductors is to have no accumulation of charge anywhere, and hence zero field everywhere (Fig. 3.9). By the second uniqueness theorem, this must be the solution: The charge will flow down the tiny wires, canceling itself off.","title":"3.1 - Laplace's Equation"},{"location":"ch3-1/#31-laplaces-equation","text":"","title":"3.1: Laplace's Equation"},{"location":"ch3-1/#311-introduction","text":"The primary task of electrostatics is to find the electric field of a given stationary charge distribution. In principle, this purpose is accomplished by Coulomb's law, in the form of \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r'})}{\\gr ^2} \\vu{\\gr} \\dd{\\tau'} \\label{3.1} Unfortunately, integrals of this type can be difficult to calculate for any but the simplest charge configurations. Occasionally we can get around this by exploiting symmetry and using Gauss's law, but ordinarily the best strategy is first to calculate the potential , V, which is given by the somewhat more tractable V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r}')}{\\gr} \\dd{\\tau'} \\tagl{3.2} Still, even this integral is often too tough to handle analytically. Moreover, in problems involving conductors \\rho itself may not be known in advance; since charge is free to move around, the only thing we control directly is the total charge (or perhaps the potential) of each conductor. In such cases, it is fruitful to recast the problem in differential form, using Poisson's equation \\laplacian{V} = - \\frac{1}{\\epsilon_0} \\rho \\label{3.3} which, together with appropriate boundary conditions, is equivalent to \\eqref{3.2} . Very often, in fact, we are interested in finding the potential in a region where \\rho = 0 . (If \\rho = 0 everywhere, of course, then V = 0 , and there is nothing further to say - that's not what I mean. There may be plenty of charge elsewhere, but we're confining our attention to places where there is no charge.) In this case, Poisson's equation reduces to Laplace's equation \\laplacian{V} = 0 \\label{3.4} or, written out in Cartesian coordinates, \\frac{\\partial^2{V}}{\\partial{x^2}} + \\frac{\\partial^2 V}{\\partial{y^2}} + \\frac{\\partial^2{V}}{\\partial{z^2}} = 0 \\label{3.5} This formula is so fundamental to the subject that one might almost say electrostatics is the study of Laplace's equation. At the same time, it is a ubiquitous equation, appearing in such diverse branches of physics as gravitation and magnetism, the theory of heat, and the study of soap bubbles. In mathematics, it plays a major role in analytic function theory. To get a feel for Laplace's equation and its solutions (which are called harmonic functions ), we shall begin with the one- and two-dimensional versions, which are easier to picture, and illustrate all the essential properties of the three-dimensional case.","title":"3.1.1: Introduction"},{"location":"ch3-1/#312-laplaces-equation-in-one-dimension","text":"Suppose V depends on only one variable, x. Then Laplace's equation becomes \\frac{d^2 V}{dx^2} = 0 The general solution is V(x) = mx + b \\label{3.6} \\tag{3.6} the equation for a straight line. It contains two undetermined constants (m and b), as is appropriate for a second-order (ordinary) differential equation. They are fixed, in any particular case, by the boundary conditions of that problem. For instance, it might be specified that V = 4 at x = 1 and V = 0 at x = 5 . In that case, m = -1 and b = 5 , so V = - x + 5 (See Fig. 3.1) I want to call your attention to two features of this result; they may seem silly and obvious in one dimension, where I can write down the general solution explicitly, but the analogs in two and three dimensions are powerful and by no means obvious: V(x) is the average of V(x + a) and V(x - a) for any a: V(x) = \\frac{1}{2} [V(x + a) + V(x-a)] Laplace's equation is a kind of averaging instruction; it tells you to assign to the point x the average of the values to the left and to the right of x. Solutions to Laplace's equation are, in this sense, as boring as they could possibly be, and yet fit the end points properly. Laplace's equation tolerates no local maxima or minima ; extreme values of V must occur at the end points. Actually, this is a consequence of (1), for if there were a local maximum, V would be greater at that point than on either side, and therefore could not be the average. (Ordinarily, you expect the second derivative to be negative at a maximum and positive at a minimum. Since Laplace's equation requires, on the contrary, that the second derivative is zero, it seems reasonable that solutions should exhibit no extrema. However, this is not a proof, since there exist functions that have maxima and minima at points where the second derivative vanishes: x^4 for example, has such a minimum point at x=0 ).","title":"3.1.2: Laplace's Equation in One Dimension"},{"location":"ch3-1/#313-laplaces-equation-in-two-dimensions","text":"If V depends on two variables, Laplace's equation becomes \\frac{\\partial^2{V}}{\\partial{x^2}} + \\frac{\\partial^2{V}}{\\partial{y^2}} = 0 This is no longer an ordinary differential equation (that is, one involving ordinary derivatives only); it is a partial differential equation. As a consequence, some of the simple rules you may be familiar with do not apply. For instance, the general solution to this equation doesn't contain just two arbitrary constants - or, for that matter, any finite number - despite the fact that it's a second order equation. Indeed, one cannot write down a \"general solution\" (at least, not in a closed form like \\eqref{3.6} ). Nevertheless, it is possible to deduce certain properties common to all solutions. It may help to have a physical example in mind. Picture a thin rubber sheet (or a soap film) stretched over some support. For definiteness, suppose you take a cardboard box, cut a wavy line all the way around, and remove the top part (Fig. 3.2). Now glue a tightly stretched rubber membrane over the box, so that it fits like a drum head (it won't be a flat drumhead, of course, unless you choose to cut the edges off straight). Now, if you lay out the coordinates (x, y) on the bottom of the box, the height V(x, y) of the sheet above the point (x, y) will satisfy Laplace's equation. (The one-dimensional analog would be a rubber band stretched between two points. Of course, it would form a straight line.) Actually, the equation satisfied by a rubber sheet is \\frac{\\partial}{\\partial{x}} \\left( g \\pdv{V}{y} \\right) + \\frac{\\partial}{\\partial{y}} \\left( g \\pdv{V}{y} \\right) = 0, \\\\ \\qquad \\text{ where } g = \\left[ 1 + \\left( \\pdv{V}{x} \\right)^2 + \\left( \\pdv{V}{y} \\right)^2 \\right]^{-1/2} Harmonic functions in two dimensions have the same properties we noted in one dimension: The value of V at a point (x, y) is the average of those around the point. More precisely, if you draw a circle of any radius R about the point (x, y), the average value of V on the circle is equal to the value at the center: V(x, y) = \\frac{1}{2 \\pi R} \\oint_{\\text{circle}} = V \\dd{l} (This, incidentally, suggests the method of relaxation, on which computer solutions to Laplace's equation are based: Starting with specified values for V at the boundary, and reasonable guesses for V on a grid of interior points, the first pass reassigns to each point the average of its nearest neighbors. The second pass repeats this process, using the corrected values, and so on. After a few iterations, the numbers begin to settle down, so that subsequent passes produce negligible changes, and a numerical solution to Laplace's equation, with the given boundary values, has been achieved.) V has no local maxima or minima; all extrema occur at the boundaries. (As before, this follows from (1)). Again, Laplace's equation picks the most featureless function possible, consistent with the boundary conditions: no hills, no valleys, just the smoothest conceivable surface. For instance, if you put a ping-pong ball on the stretched rubber sheet of Fig 3.2, it will roll over to one side and fall off - it will not find a \"pocket\" somewhere to settle into, for Laplace's equation allows no such dents in the surface. From a geometrical point of view, just as a straight line is the shortest distance between two points, so a harmonic function in two dimensions minimizes the surface area spanning the given boundary line.","title":"3.1.3: Laplace's Equation in Two Dimensions"},{"location":"ch3-1/#314-laplaces-equation-in-three-dimensions","text":"In three dimensions I can neither provide you with an explicit solution (as in one dimension) nor offer a suggestive physical example to guide your intuition (as I did in two dimensions). Nevertheless, the same two properties remain true, and this time I will sketch a proof. For a proof that does not rely on Coulomb's law (only on Laplace's equation), see Prob. 3.37 The value of V at point r is the average value of V over a spherical surface of radius R centered at r : V(\\vec{r}) = \\frac{1}{4 \\pi R^2} \\oint_{\\text{sphere}} V \\dd{a} As a consequence, V can have no local maxima or minima; the extreme values of V must occur at the boundaries (For if V had a local maximum at r , then by the very nature of the maximum I could draw a sphere around r over which all the values of V - and a fortiori the average - would be less than at r .) Proof: V is a solution to the three-dimensional Laplace's equation. Then the value of V at point r is the average value of V over a spherical surface of radius R centered at r Let's begin by calculating the average potential over a spherical surface of radius R due to a single point charge q located outside the sphere. We may as well center the sphere at the origin and choose coordinates so that q lies on the z-axis (Fig 3.3). The potential at a point on the surface is V = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{\\gr} where \\gr ^2 = z^2 + R^2 - 2z R \\cos \\theta so V_{\\text{ave}} = \\frac{1}{4\\pi R^2} \\frac{1}{4 \\pi \\epsilon_0} \\int [z^2 + R^2 - 2 z R \\cos \\theta]^{-1/2} R^2 \\sin \\theta \\dd{\\theta} \\dd{\\phi} \\\\ = \\left. \\frac{q}{4 \\pi \\epsilon_0} \\frac{1}{2 z R} \\sqrt{z^2 + R^2 - 2 z R \\cos\\theta} \\right|_{0} ^{\\pi} \\\\ = \\frac{q}{4 \\pi \\epsilon_0} \\frac{1}{2zR} [(z + R) - (z-R)] = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{z} But this is precisely the potential due to q at the center of the sphere! By the superposition principle, the same goes for any collection of charges outside the sphere: their average potential over the sphere is equal to the net potential they produce at the center","title":"3.1.4: Laplace's Equation in Three Dimensions"},{"location":"ch3-1/#315-boundary-conditions-and-uniqueness-theorems","text":"Laplace's equation does not by itself determine V; in addition, suitable boundary conditions must be supplied. This raises a delicate question: What are appropriate boundary conditions, sufficient to determine the answer and yet not so strong as to generate inconsistencies? The one-dimensional case is easy, for here the general solution V = mx + b contains two arbitrary constants, and we therefore require two boundary conditions. We might, for instance, specify the value of the function at each end, or we might give the value of the function and its derivative at one end, or the value at one end and the derivative at the other, and so on. But we cannot get away with just the value or just the derivative at one end - this is insufficient information. Nor would it do to specify the derivatives at both ends - this would be either redundant (if the two are equal) or inconsistent (if they are not). In two or three dimensions we are confronted by a partial differential equation, and it is not so obvious what would constitute acceptable boundary conditions. Is the shape of a taut rubber membrane, for instance, uniquely determined by the frame over which it is stretched, or, like a canning jar lid, can it snap from one stable configuration to another? The answer, as I think your intuition would suggest, that V is uniquely determined by its value at the boundary (canning jars evidently do not obey Laplace's equation). However, other boundary conditions can also be used (see Prob. 3.5). The proof that a proposed set of boundary conditions will suffice is usually presented in the form of a uniqueness theorem. There are many such theorems for electrostatics, all sharing the same basic format - I'll show you the two most useful ones: First Uniqueness Theorem: The solution to Laplace's equation in some volume \\mathscr{V} is uniquely determined if V is specified on the boundary surface \\mathscr{S} . In Fig. 3.5 I have drawn such a region and its boundary. (There could also be \"islands\" inside, so long as V is given on all their surfaces; also, the outer boundary could be at infinity, where V is ordinarily taken to be zero.) Proof : Suppose there were two solutions to Laplace's equation: \\laplacian{V_1} = 0 \\quad \\text{and} \\quad \\laplacian{V_2} = 0 both of which assume the specified value on the surface. I want to prove that they must be equal. The trick is to look at their difference : V_3 \\equiv V_1 - V_2 This obeys Laplace's equation (obviously) \\laplacian{V_3} = \\laplacian{V_1} - \\laplacian{V_2} = 0 and it takes the value zero on all boundaries (since V_1 and V_2 are equal there). But Laplace's equation allows no local maxima or minima - all extrema occur on the boundaries. So the maximum and minimum of V_3 are both zero. Therefore V_3 must be zero everywhere, and hence V_1 = V_2","title":"3.1.5: Boundary Conditions and Uniqueness Theorems"},{"location":"ch3-1/#example-31","text":"Show that the potential is constant inside an enclosure completely surrounded by conducting material, provided there is no charge within the enclosure. Solution The potential on the cavity wall is some constant V_0 (that's item (iv) in Sect. 2.5.1), so the potential inside is a function that satisfies Laplace's equation and has the constant value V_0 at the boundary. It doesn't take a genius to think of one solution to this problem: V = V_0 everywhere. The uniqueness theorem guarantees that this is the only solution. (It follows that the field inside an empty cavity is zero - the same result we found in Sect. 2.5.2 on rather different grounds.) The uniqueness theorem is a license to your imagination. It doesn't matter how you come by your solution; if (a) it satisfies Laplace's equation and (b) it has the correct value on the boundaries, then it's right . You'll see the power of this argument when we come to the method of images. Incidentally, it is easy to improve on the first uniqueness theorem: I assumed there was no charge inside the region in question, so the potential obeyed Laplace's equation, but we may as well throw in some charge (in which case V obeys Poisson's equation). Corollary: The potential in a volume \\mathscr{V} is uniquely determined if (a) the charge density throughout the region, and (b) the value of V on all boundaries, are specified The argument is the same, only this time \\laplacian{V_1} = -\\frac{1}{\\epsilon_0} \\rho, \\qquad \\laplacian{V_2} = - \\frac{1}{\\epsilon_0} \\rho so \\laplacian{V_3} = \\laplacian{V_1} - \\laplacian{V_2} = - \\frac{1}{\\epsilon_0} \\rho + \\frac{1}{\\epsilon_0} \\rho = 0 Once again the difference (V_3 \\equiv V_1 - V_2) satisfies Laplace's equation and has the value zero on all boundaries, so V_3 = 0 and hence V_1 = V_2","title":"Example 3.1"},{"location":"ch3-1/#316-conductors-and-the-second-uniqueness-theorem","text":"The simplest way to set the boundary conditions for an electrostatic problem is to specify the value of V on all surfaces surrounding the region of interest. And this situation often occurs in practice: In the laboratory, we have conductors connected to batteries, which maintain a given potential, or to ground , which is the experimentalist's word for V = 0 . However, there are other circumstances in which we do not know the potential at the boundary, but rather the charges on various conducting surfaces. Suppose I put Q_a on the first conductor, Q_b on the second conductor, and so on - I'm not telling you how the charge distributes itself over each conducting surface, because as soon as I put it on, it moves around in a way I do not control. And for good measure, let's say there is some specified charge density \\rho in the region between the conductors. Is the electric field now uniquely determined? Or are there perhaps a number of different ways the charges could arrange themselves on their respective conductors, each leading to a different field? Second uniqueness theorem : In a volume \\mathscr{V} surrounded by conductors and containing a specified charge density \\rho , the electric field is uniquely determined if the total charge on each conductor is given (Fig. 3.6). (The region as a whole can be bounded by another conductor, or else unbounded.) Proof : Suppose there are two fields satisfying the conditions of the problem. Both obey Gauss's law in differential form in the space betwen the conductors: \\div{\\vec{E_1}} = \\frac{1}{\\epsilon_0} \\rho, \\qquad \\div{\\vec{E_2}} = \\frac{1}{\\epsilon_0} \\rho And both obey Gauss's law in integral form for a Gaussian surface enclosing each conductor \\oint_{i_{th} \\text{ conducting surface}} \\vec{E_1} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_i \\quad \\text{ and } \\\\ \\oint_{i_{th} \\text{ conducting surface}} \\vec{E_2} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_i \\quad \\text{ and } \\\\ Likewise, for the outer boundary (whether this is just inside an enclosing conductor at infinity), \\oint_{\\text{outer boundary}} \\vec{E_1} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{tot} \\\\ \\oint_{\\text{outer boundary}} \\vec{E_2} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{tot} As before, we examine the difference \\vec{E_3} \\equiv \\vec{E_1} - \\vec{E_2} which obeys \\div{\\vec{E_3}} = 0 \\label{3.7} \\tag{3.7} in the region between the conductors, and \\oint \\vec{E_3} \\cdot \\dd{\\vec{a}} = 0 \\label{3.8} \\tag{3.8} over each boundary surface. Now there is one final piece of information we must exploit: Although we do not know how the charge Q_i distributes itself over the _i_th conductor, we do know that each conductor is an equipotential, and hence V_3 is a constant (not necessarily the same constant) over each conducting surface. (It need not be zero, for the potentials V_1 and V_2 may not be equal - all we know for sure is that both are constant over any given conductor.) Next comes a trick. Invoking the product rule \\div{(V_3 \\vec{E_3})} = V_3 (\\div{\\vec{E_3}}) + \\vec{E_3} \\cdot (\\grad{V_3}) = - (E_3)^2 Here I have used \\eqref{3.7} and \\vec{E_3} = - \\grad{V_3} . Integrating this over \\mathscr{V} and applying the divergence theorem to the left side: \\int_{\\mathscr{V}} \\div{(V_3 \\vec{E_3})} \\dd{\\tau} = \\oint_{S} V_3 \\vec{E_3} \\cdot \\dd{\\vec{a}} = - \\int_{\\mathscr{V}} (E_3)^2 \\dd{\\tau} The surface integral covers all boundaries of the region in question - the conductors and outer boundary. Now V_3 is a constant over each surface (if the outer boundary is invinity, V_3 = 0 there), so it comes outside each integral, and what remains is zero, according to \\eqref{3.8} . Therefore \\int_{\\mathscr{V}}(E_3)^2 \\dd{\\tau} = 0 The integrand is never negative, so the only way the integral can vanish is if E_3 = 0 everywhere. Consequently, \\vec{E_1} = \\vec{E_2} and the theorem is proved. This proof was not easy, and there is a real danger that the theorem itself will seem more plausible to you than the proof. In case you think the second uniqueness theorem is \"obvious,\" consider this example of Purcell's: Figure 3.7 shows a simple electrostatic configuration, consisting of four conductors with charges \\pm Q , situated so that the plusses are near the minuses. It all looks very comfortable. Now, what happens if we join them in pairs, by tiny wires, as indicated in Fig. 3.8? Since the positive charges are very near negative charges (which is where they like to be) you might well guess that nothing will happen - the configuration looks stable. Well, that sounds reasonable, but it's wrong. The configuration in Fig. 3.8 is impossible. For there are now effectively two conductors, and the total charge on each is zero. One possible way to distribute zero charge over these conductors is to have no accumulation of charge anywhere, and hence zero field everywhere (Fig. 3.9). By the second uniqueness theorem, this must be the solution: The charge will flow down the tiny wires, canceling itself off.","title":"3.1.6: Conductors and the Second Uniqueness Theorem"},{"location":"ch3-2/","text":"3.2: The Method of Images 3.1.1: The Classic Image Problem Suppose a point charge q is held a distance d above an infinite grounded conducting plane (Fig. 3.10). Question : what is the potential in the region above the plane? It's not just (1/4 \\pi \\epsilon_0) q / \\gr , for q will induce a certain amount of negative charge on the nearby surface of the conductor; the total potential is due in part to q directly, and in part to this induced charge. But how can we possibly calculate the potential, when we don't know how much charge is induced or how it is distributed? From a mathematical point of view, our problem is to solve Poisson's equation in the region z > 0 , with a single point charge q at (0, 0, d) , subject to the boundary conditions V = 0 when z = 0 (since the conducting plane is grounded) V \\rightarrow 0 far from the charge (that is, for x^2 + y^2 + z^2 \\gg d^2 The first uniqueness theorem (actually, its corollary) guarantees that there is only one function that meets these requirements. If by trick or clever guess we can discover such a function, it's got to be the answer. Trick: Forget about the actual problem; we're going to study a completely different situation. This new configuration consists of two point charges, +q at (0, 0, d) and -q at (0, 0, -d) , and no conducting plane (Fig. 3.11). For this configuration I can easily write down the potential: V(x, y, z) = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{q}{\\sqrt{x^2 + y^2 + (z - d)^2 }} - \\frac{q}{\\sqrt{x^2 + y^2 + (z + d)^2}} \\right] \\label{3.9} \\tag{3.9} It follows that V = 0 when z = 0 V \\rightarrow 0 for x^2 + y^2 + z^2 \\gg d^2 and the only charge in the region z > 0 is the point charge +q at (0, 0, d) . But these are precisely the conditions of the original problem! Evidently the second configuration happens to produce exactly the same potential as the first configuration, in the \"upper\" region z \\geq 0 . (The \"lower\" region, z < 0 , is completely different, but who cares? The upper part is all we need.) Conclusion : The potential of a point charge above an infinite grounded conductor is given by \\eqref{3.9} , for z > 0 . Notice the crucial role played by the uniqueness theorem in this argument: without it, no one would believe this solution, since it was obtained for a completely different charge distribution. But the uniqueness theorem certifies it: If it satisfies Poisson's equation in the region of interest, and assumes the correct value at the boundaries, then it must be right. 3.2.2: Induced Surface Charge Now that we know the potential, it is a straightforward matter to compute the surface charge \\sigma induced on the conductor. According to Eq. 2.49, \\sigma = - \\epsilon_0 \\pdv{V}{n} where \\partial V / \\partial n is the normal derivative of V at the surface. In this case the normal direction is the z direction, so \\sigma = \\left. - \\epsilon_0 \\pdv{V}{z} \\right|_{z = 0} From Eq. 3.9 \\pdv{V}{z} = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{-q (z - d)}{[x^2 + y^2 + (z - d)^2]^{3/2}} + \\frac{q(z + d)}{[x^2 + y^2 + (z + d)^2 ]^{3/2}} \\right] so \\sigma(x, y) = \\frac{-qd}{2 \\pi (x^2 + y^2 + d^2)^{3/2}} \\label{3.10} \\tag{3.10} As expected, the induced charge is negative (assuming q is positive) and greatest at x = y = 0 . While we're at it, let's compute the total induced charge Q = \\int \\sigma \\dd{a} This integral, over the xy plane, could be done in Cartesian coordinates, with \\dd{a} = \\dd{x} \\dd{y} , but it's easier to use polar coordinates (r, \\phi) , with r^2 = x^2 + y^2 and \\dd{a} = r \\dd{r} \\dd{\\phi} . Then \\sigma(r) = \\frac{-qd}{2 \\pi (r^2 + d^2)^{3/2}} and Q = \\int_{0} ^{2\\pi} \\int_{0} ^{\\infty} \\frac{-qd}{2 \\pi (r^2 + d^2)^{3/2}} r \\dd{r} \\dd{\\phi} = \\left. \\frac{qd}{\\sqrt{r^2 + d^2}} \\right|_{0} ^{\\infty} = -q \\label{3.11} \\tag{3.11} The total charge induced on the plane is -q , as (with benefit of hindsight) you can perhaps convince yourself it had to be. 3.2.3: Force and Energy The charge q is attracted toward the plane, because of the negative induced charge. Let's calculate the force of attraction. Since the potential in the vicinity of q is the same as in the analog problem (the one with +q and -q but no conductor), so also is the field and, therefore, the force \\vec{F} = -\\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{(2d)^2} \\vu{z} \\label{3.12} \\tag{3.12} Beware : It is easy to get carried away, and assume that everything is the same in the two problems. Energy, however, is not the same. With the two point charges and no conductor, Eq. 2.42 gives W = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{2d} But for a single charge and conducting plane, the energy is half this W = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{4d} \\label{3.14} \\tag{3.14} Why half? Think of the energy stored in the fields (Eq. 2.45): W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} In the first case, both the upper region (z > 0) and the lower region (z < 0) contribute, and by symmetry they contribute equally. But in the second case, only the upper region contains a nonzero field, and hence the energy is half as great. Of course, one could also determine the energy by calculating the work required to bring q in from infinity. The force required (to oppose the electrical force in \\eqref{3.12} is (1 / 4 \\pi \\epsilon_0)(q^2/4z^2) \\vu{z} , so \\begin{align} W & = & \\int _{\\infty} ^{d} \\vec{F} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{\\infty} ^d \\frac{q^2}{4z^2} \\dd{z} \\\\ & = & \\frac{1}{4 \\pi \\epsilon_0} \\left. \\left( - \\frac{q^2}{4z} \\right) \\right|_{\\infty} ^d = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{4d} \\end{align} As I move q toward the conductor, I do work only on q. It is true that induced charge is moving in over the conductor, but this costs me nothing, since the whole conductor is at potential zero. By contrast, if I simultaneously bring in two point charges (with no conductor), I do work on both of them, and the total is (again) twice as great. 3.2.4: Other Image Problems The method just described is not limited to a single point charge; any stationary charge distribution near a grounded conducting plane can be treated in the same way, by introducing its mirror image - hence the name method of images. (Remember that the image charges have the opposite sign; this is what guarantees that the xy plane will be at potential zero.) There are also some exotic problems that can be handled in similar fashion; the nicest of these is the following. Example 3.2 A point charge q is situated a distance a from the center of a grounded conducting sphere of radius R (Fig. 3.12). Find the potential outside the sphere Solution Examine the completely different configuration, consisting of the point charge q together with another point charge q' = - \\frac{R}{a} q \\label{3.15} \\tag{3.15} placed a distance b = \\frac{R^2}{a} \\label{3.16} \\tag{3.16} to the right of the center of the sphere (Fig 3.13). No conductor, now - just the two point charges. The potential of this configuration is V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{\\gr} + \\frac{q'}{\\gr '} \\right) \\label{3.17} \\tag{3.17} where \\gr and \\gr' are the distances from q and q' , respectively. Now, it happens (see Prob. 3.8) that this potential vanishes at all points on the sphere, and therefore fits the boundary conditions for our original problem, in the exterior region. Conclusion : \\eqref{3.17} is the potential of a point charge near a grounded conducting sphere. (Notice that b is less than R , so the \"image\" charge q' is safely inside the sphere - you cannot put image charges in the region where you are calculating V ; that would change \\rho , and you'd be solving Poisson's equation with the wrong source.) In particular, the force of attraction between the charge and the sphere is F = \\frac{1}{4\\pi \\epsilon_0} \\frac{q q'}{(a - b)^2} = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2 R a}{(a^2 - R^2)^2} \\label{3.18} \\tag{3.18} The method of images is delightfully simple... when it works. But it is as much an art as a science, for you must somehow think up just the right \"auxiliary\" configuration, and for most shapes this is forbiddingly complicated, if not impossible.","title":"3.2 - The Method of Images"},{"location":"ch3-2/#32-the-method-of-images","text":"","title":"3.2: The Method of Images"},{"location":"ch3-2/#311-the-classic-image-problem","text":"Suppose a point charge q is held a distance d above an infinite grounded conducting plane (Fig. 3.10). Question : what is the potential in the region above the plane? It's not just (1/4 \\pi \\epsilon_0) q / \\gr , for q will induce a certain amount of negative charge on the nearby surface of the conductor; the total potential is due in part to q directly, and in part to this induced charge. But how can we possibly calculate the potential, when we don't know how much charge is induced or how it is distributed? From a mathematical point of view, our problem is to solve Poisson's equation in the region z > 0 , with a single point charge q at (0, 0, d) , subject to the boundary conditions V = 0 when z = 0 (since the conducting plane is grounded) V \\rightarrow 0 far from the charge (that is, for x^2 + y^2 + z^2 \\gg d^2 The first uniqueness theorem (actually, its corollary) guarantees that there is only one function that meets these requirements. If by trick or clever guess we can discover such a function, it's got to be the answer. Trick: Forget about the actual problem; we're going to study a completely different situation. This new configuration consists of two point charges, +q at (0, 0, d) and -q at (0, 0, -d) , and no conducting plane (Fig. 3.11). For this configuration I can easily write down the potential: V(x, y, z) = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{q}{\\sqrt{x^2 + y^2 + (z - d)^2 }} - \\frac{q}{\\sqrt{x^2 + y^2 + (z + d)^2}} \\right] \\label{3.9} \\tag{3.9} It follows that V = 0 when z = 0 V \\rightarrow 0 for x^2 + y^2 + z^2 \\gg d^2 and the only charge in the region z > 0 is the point charge +q at (0, 0, d) . But these are precisely the conditions of the original problem! Evidently the second configuration happens to produce exactly the same potential as the first configuration, in the \"upper\" region z \\geq 0 . (The \"lower\" region, z < 0 , is completely different, but who cares? The upper part is all we need.) Conclusion : The potential of a point charge above an infinite grounded conductor is given by \\eqref{3.9} , for z > 0 . Notice the crucial role played by the uniqueness theorem in this argument: without it, no one would believe this solution, since it was obtained for a completely different charge distribution. But the uniqueness theorem certifies it: If it satisfies Poisson's equation in the region of interest, and assumes the correct value at the boundaries, then it must be right.","title":"3.1.1: The Classic Image Problem"},{"location":"ch3-2/#322-induced-surface-charge","text":"Now that we know the potential, it is a straightforward matter to compute the surface charge \\sigma induced on the conductor. According to Eq. 2.49, \\sigma = - \\epsilon_0 \\pdv{V}{n} where \\partial V / \\partial n is the normal derivative of V at the surface. In this case the normal direction is the z direction, so \\sigma = \\left. - \\epsilon_0 \\pdv{V}{z} \\right|_{z = 0} From Eq. 3.9 \\pdv{V}{z} = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{-q (z - d)}{[x^2 + y^2 + (z - d)^2]^{3/2}} + \\frac{q(z + d)}{[x^2 + y^2 + (z + d)^2 ]^{3/2}} \\right] so \\sigma(x, y) = \\frac{-qd}{2 \\pi (x^2 + y^2 + d^2)^{3/2}} \\label{3.10} \\tag{3.10} As expected, the induced charge is negative (assuming q is positive) and greatest at x = y = 0 . While we're at it, let's compute the total induced charge Q = \\int \\sigma \\dd{a} This integral, over the xy plane, could be done in Cartesian coordinates, with \\dd{a} = \\dd{x} \\dd{y} , but it's easier to use polar coordinates (r, \\phi) , with r^2 = x^2 + y^2 and \\dd{a} = r \\dd{r} \\dd{\\phi} . Then \\sigma(r) = \\frac{-qd}{2 \\pi (r^2 + d^2)^{3/2}} and Q = \\int_{0} ^{2\\pi} \\int_{0} ^{\\infty} \\frac{-qd}{2 \\pi (r^2 + d^2)^{3/2}} r \\dd{r} \\dd{\\phi} = \\left. \\frac{qd}{\\sqrt{r^2 + d^2}} \\right|_{0} ^{\\infty} = -q \\label{3.11} \\tag{3.11} The total charge induced on the plane is -q , as (with benefit of hindsight) you can perhaps convince yourself it had to be.","title":"3.2.2: Induced Surface Charge"},{"location":"ch3-2/#323-force-and-energy","text":"The charge q is attracted toward the plane, because of the negative induced charge. Let's calculate the force of attraction. Since the potential in the vicinity of q is the same as in the analog problem (the one with +q and -q but no conductor), so also is the field and, therefore, the force \\vec{F} = -\\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{(2d)^2} \\vu{z} \\label{3.12} \\tag{3.12} Beware : It is easy to get carried away, and assume that everything is the same in the two problems. Energy, however, is not the same. With the two point charges and no conductor, Eq. 2.42 gives W = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{2d} But for a single charge and conducting plane, the energy is half this W = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{4d} \\label{3.14} \\tag{3.14} Why half? Think of the energy stored in the fields (Eq. 2.45): W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} In the first case, both the upper region (z > 0) and the lower region (z < 0) contribute, and by symmetry they contribute equally. But in the second case, only the upper region contains a nonzero field, and hence the energy is half as great. Of course, one could also determine the energy by calculating the work required to bring q in from infinity. The force required (to oppose the electrical force in \\eqref{3.12} is (1 / 4 \\pi \\epsilon_0)(q^2/4z^2) \\vu{z} , so \\begin{align} W & = & \\int _{\\infty} ^{d} \\vec{F} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{\\infty} ^d \\frac{q^2}{4z^2} \\dd{z} \\\\ & = & \\frac{1}{4 \\pi \\epsilon_0} \\left. \\left( - \\frac{q^2}{4z} \\right) \\right|_{\\infty} ^d = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{4d} \\end{align} As I move q toward the conductor, I do work only on q. It is true that induced charge is moving in over the conductor, but this costs me nothing, since the whole conductor is at potential zero. By contrast, if I simultaneously bring in two point charges (with no conductor), I do work on both of them, and the total is (again) twice as great.","title":"3.2.3: Force and Energy"},{"location":"ch3-2/#324-other-image-problems","text":"The method just described is not limited to a single point charge; any stationary charge distribution near a grounded conducting plane can be treated in the same way, by introducing its mirror image - hence the name method of images. (Remember that the image charges have the opposite sign; this is what guarantees that the xy plane will be at potential zero.) There are also some exotic problems that can be handled in similar fashion; the nicest of these is the following.","title":"3.2.4: Other Image Problems"},{"location":"ch3-2/#example-32","text":"A point charge q is situated a distance a from the center of a grounded conducting sphere of radius R (Fig. 3.12). Find the potential outside the sphere Solution Examine the completely different configuration, consisting of the point charge q together with another point charge q' = - \\frac{R}{a} q \\label{3.15} \\tag{3.15} placed a distance b = \\frac{R^2}{a} \\label{3.16} \\tag{3.16} to the right of the center of the sphere (Fig 3.13). No conductor, now - just the two point charges. The potential of this configuration is V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{\\gr} + \\frac{q'}{\\gr '} \\right) \\label{3.17} \\tag{3.17} where \\gr and \\gr' are the distances from q and q' , respectively. Now, it happens (see Prob. 3.8) that this potential vanishes at all points on the sphere, and therefore fits the boundary conditions for our original problem, in the exterior region. Conclusion : \\eqref{3.17} is the potential of a point charge near a grounded conducting sphere. (Notice that b is less than R , so the \"image\" charge q' is safely inside the sphere - you cannot put image charges in the region where you are calculating V ; that would change \\rho , and you'd be solving Poisson's equation with the wrong source.) In particular, the force of attraction between the charge and the sphere is F = \\frac{1}{4\\pi \\epsilon_0} \\frac{q q'}{(a - b)^2} = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2 R a}{(a^2 - R^2)^2} \\label{3.18} \\tag{3.18} The method of images is delightfully simple... when it works. But it is as much an art as a science, for you must somehow think up just the right \"auxiliary\" configuration, and for most shapes this is forbiddingly complicated, if not impossible.","title":"Example 3.2"},{"location":"ch3-3/","text":"3.3: Separation of Variables In this section we shall attack Laplace's equation directly, using the method of separation of variables , which is the physicist's favorite tool for solving partial differential equations. The method is applicable in circumstances where the potential (V) or the charge density (\\sigma) is specified on the boundaries of some region, and we are asked to find the potential in the interior. The basic strategy is very simple: We look for solutions that are products of functions, each of which depends on only one of the coordinates. The algebraic details, however, can be formidable, so I'm going to develop the method through a sequence of examples. We'll start with Cartesian coordinates and then do spherical coordinates (I'll leave the cylindrical case for you to tackle on your own, in Prob 3.24). 3.3.1: Cartesian Coordinates Example 3.3 Two infinite grounded metal plates lie parallel to the xz plane, one at y = 0 , the other at y = a (Fig. 3.17). The left end, at x = 0 , is closed off with an infinite strip insulated from the two plates, and maintained at a specific potential V_0(y) . Find the potential inside this 'slot.' Solution The configuration is independent of z, so this is really a two-dimensional problem. In mathematical terms, we must solve Laplace's equation, \\frac{\\partial ^2 V}{\\partial{x^2}} + \\frac{\\partial ^2 V}{\\partial{y^2}} = 0 \\label{3.20} \\tag{3.20} subject to the boundary conditions (i) V = 0 when y = 0 (ii) V = 0 when y = a (iii) V = V_0(y) when x = 0 (iv) V \\rightarrow 0 as x \\rightarrow \\infty (The latter, although not explicitly stated in the problem, is necessary on physical grounds: as you get farther and farther away from the \"hot\" strip at x = 0 , the potential should drop to zero.) Since the potential is specified on all boundaries, the answer is uniquely determined. The first step is to look for solutions in the form of products: V(x, y) = X(x)Y(y) \\tagl{3.22} On the face of it, this is an absurd restriction - the overwhelming majority of solutions to Laplace's equation do not have such a form. For example, V(x, y) = (5x + 6y) satisfies the equation, but you can't express it as the product of a function of x times a function of y. Obviously, we're only going to get a tiny subset of all possible solutions by this means, and it would be a miracle if one of them happened to fit the boundary conditions of our problem... But hang on, because the solutions we do get are very special, and it turns out that by pasting them together we can construct the general solution. Anyway, putting \\eqref{3.22} into \\eqref{3.20} we obtain Y \\frac{d^2X}{dx^2} + X \\frac{d^2 Y}{dy^2} = 0 The next step is to \"separate the variables\" (that is, collect all the x-dependence into one term and all the y-dependence into another). Typically, this is accomplished by dividing through by V: \\frac{1}{X} \\frac{d^2 X}{dx^2} + \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = 0 \\tagl{3.23} Here the first term depends only on x and the second term only on y; in other words, we have an equation of the form f(x) + g(y) = 0 \\label{3.24} \\tag{3.24} Now, there's only one way this could possibly be true: f and g must both be constant . For what if f(x) changed, as you vary x - then if we held y fixed and fiddled with x, the sum f(x) + g(y) would change, in violation of \\eqref{3.24} , which says it's always zero. (That's a simple but somehow rather elusive argument; don't accept it without due thought, because the whole method rides on it.) It follows from \\eqref{3.23} , then, that \\frac{1}{X} \\frac{d^2 X}{dx^2} = C_1 \\quad \\text{ and } \\quad \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = C_2, \\quad \\text{ with } C_1 + C_2 = 0 \\tagl{3.25} One of these constants is positive, the other negative (or perhaps both are zero). In general, one must investigate all possibilities; however in our particular problem we need C_1 positive and C_2 negative, for reasons that will appear in a moment. Thus \\frac{d^2X}{dx^2} = k^2 X, \\qquad \\frac{d^2 Y}{dy^2} = - k^2 Y \\tagl{3.26} Notice what has happened: A partial differential equation has been converted into two ordinary differential equations. The advantage of this is obvious - ordinary differential equations are a lot easier to solve. Indeed: X(x) = A e^{kt} + B e^{-kt}, \\qquad Y(y) = C \\sin ky + D \\cos ky so V(x, y) = (A e^{kt} + B e^{-kt})(C \\sin ky + D \\cos ky) \\tagl{3.27} This is the appropriate separable solution to Laplace's equation; it remains to impose the boundary conditions, and see what they tell us about the constants. To begin at the end, condition (iv) requires tha A equal zero. Absorbing B into C and D, we are left with V(x, y) = e^{-kx} (C\\sin ky + D \\cos ky) Condition (i) now demands that D equal zero V(x, y) = C ^{-kx} \\sin ky \\tagl{3.28} Meanwhile (ii) yields \\sin ka = 0 , from which it follows that k = \\frac{n \\pi}{a} \\quad (n = 1, 2, 3, \\ldots) \\tagl{3.29} (At this point you can see why I chose C_1 positive and C_2 negative: If X were sinusoidal, we could never manage for it to go to zero at infinity, and if Y were exponential we could not make it vanish at both 0 and a . Incidentally, n = 0 is no good, for in that case the potential vanishes everywhere. And we have already excluded negative n's) That's as far as we can go, using separable solutions, and unless V_0(y) just happens to have the form \\sin(n \\pi / a) for some integer n , we simply can't fit the final boundary condition at x = 0 . But now comes the crucial step that redeems the method: Separation of variables has given us an infinite family of solutions (one for each n), and whereas none of them by itself satisfies the final boundary condition, it is possible to combine them in a way that does . Laplace's equation is linear, in the sense that if V_1, V_2, V_3, \\ldots satisfy it, so does any linear combination, for \\laplacian{V} = \\alpha_1 \\laplacian{V_1} + \\alpha_2 \\laplacian{V_2} + \\ldots = 0 \\alpha_1 + 0 \\alpha_2 + \\ldots = 0 Exploiting this fact, we can patch together the separable solutions \\eqref{3.28} to construct a much more general solution: V(x, y) = \\sum_{n=1} ^{\\infty} C_n e^{-n \\pi x / a} \\sin (n \\pi y / a) \\tagl{3.30} This still satisfies three of the boundary conditions; the question is, can we (by astute choice of the coefficients C_n ) fit the final boundary condition (iii)? V(0, y) = \\sum_{n=1} ^{\\infty} C_n \\sin (n \\pi y / a) = V_0(y) \\tagl{3.31} Well, you may recognize this sum - it's a Fourier sine series. And Dirichlet's theorem guarantees that virtually any function V_0(y) - it can even have a finite number of discontinuities - can be expanded in such a series. But how do we actually determine the coefficients C_n , buried as they are in that infinite sum? The device for accomplishing this is so lovely it deserves a name - I call it Fourier's trick , though it seems Euler had used essentially the same idea somewhat earlier. Here's how it goes: Multiply \\eqref{3.31} by \\sin(n' \\pi y /a) (where n' is a positive integer), and integrate from 0 to a: \\sum_{n=1} ^{\\infty} C_n \\int_{0} ^{a} \\sin(n \\pi y / a) \\sin(n' \\pi y/a) \\dd{y} = \\int_{0} ^a V_0(y) \\sin (n' \\pi /a) \\dd{y} \\tagl{3.32} You can work out the integral on the left yourself; the answer is \\int_{0} ^a \\sin (n \\pi y /a) \\sin (n' \\pi y / a) \\dd{y} = \\begin{cases} 0 & \\quad \\text{if } n' \\neq n \\\\ \\frac{a}{2} & \\quad \\text{if } n' = n \\end{cases} \\tagl{3.33} Thus all the terms in the series drop out, save only the one where n = n' , and the left side of \\eqref{3.32} reduces to (a/2)C_{n'} . Conclusion : C_n = \\frac{2}{a} \\int_{0}^a V_0(y) \\sin (n \\pi y /a) \\dd{y} \\tagl{3.34} That does it: \\eqref{3.30} is the solution, with coefficients given by eqref{3.34} . As a concrete example, suppose the strip at x = 0 is a metal plate with constant potential V_0 (remember, it's insulated from the grounded plates at y = 0 and y = a . Then C_n = \\frac{2V_0}{a} \\int_0 ^a \\sin (n \\pi y / a) \\dd y \\\\ = \\frac{2 V_0}{n \\pi} (1 - \\cos n \\pi) = \\begin{cases} 0 & \\quad \\text{if n is even } \\\\ \\frac{4 V_0}{n \\pi} & \\quad \\text{if n is odd} \\end{cases} \\tagl{3.35} Thus V(x, y) = \\frac{4 V_0}{\\pi} \\sum_{n = 1, 3, 5, \\ldots} \\frac{1}{n} e^{- n \\pi x / a} \\sin (n \\pi y / a) \\tagl{3.36} Figure 3.18 is a plot of this potential; Fig. 3.10 shows how the first few terms in the Fourier series combine to make a better and better approximation to the constant V_0 : (a) is the n=1 term only, (b) includes n up to 5, (c) is the sum of the first 10 terms, and (d) is the sum of the first 100 terms. Incidentally, the infinite series in Eq. 3.36 can be summed explicitly (try your hand at it if you like); the result is V(x, y) = \\frac{2V_0}{\\pi} \\tan^{-1} \\left( \\frac{\\sin(\\pi y / a)}{\\sinh(\\pi x /a )} \\right) \\tagl{3.37} In this form, it is easy to check that Laplace's equation is obeyed and the four boundary conditions are satisfied The success of this method hinged on two extraordinary properties of the separable solutions \\eqref{3.28} and \\eqref{3.29} : completeness and orthogonality . A set of functions f_n(y) is said to be complete if any other function f(y) can be expressed as a linear combination of them: f(y) = \\sum_{n=1} ^{\\infty} C_n f_n(y) \\tagl{3.38} The functions \\sin (n \\pi y/a) are complete on the interval 0 \\leq y \\leq a . It was this fact, guaranteed by Dirichlet's theorem, that assured us \\eqref{3.31} could be satisfied, given the proper choice of the coefficients C_n . (The proof of completeness, for a particular set of functions, is an extremely difficult business, and I'm afraid physicists tend to assume it's true and leave the checking to others.) A set of functions is orthogonal if the integral of the product of any two different members of the set is zero: \\int_0 ^a f_n(y) f_{n'} (y) \\dd{y} = 0 \\quad \\text{for } n' \\neq n The sine functions are orthogonal \\eqref{3.33} ; that is the property on which Fourier's trick is based, allowing us to kill off all terms but one in the infinite series and thereby solve for the coefficients C_n (Proof of orthogonality is generally quite simple, either by direct integration or by analysis of the differential equation from which the functions came.) Example 3.4 Two infinitely-long grounded metal plates, again at y=0 and y=a are connected at x= \\pm b by metal strips maintained at a constant potential V_0 , as shown in Fig. 3.20 (a thin layer of insulation at each corner prevents them from shorting out). Find the potential inside the resulting rectangular pipe. Solution Once again, the configuration is independent of z. Our problem is to solve Laplace's equation \\frac{\\partial ^2 V}{\\partial{x^2}} + \\frac{\\partial ^2 V}{\\partial{y^2}} = 0 subject to the boundary conditions (i) V = 0 when y = 0 (ii) V = 0 when y = a (iii) V = V_0 when x = b (iv) V = V_0 when x = -b The argument runs as before, up to \\eqref{3.27} : V(x, y) = (A e^{kt} + B e^{-kt})(C \\sin ky + D \\cos ky) This time, however, we cannot set A = 0 ; the region in question does not extend to x = \\infty , so e^{kx} is perfectly acceptable. On the other hand, the situation is symmetric with respect to x, so V(-x, y) = V(x, y) , and it follows that A = B . Using e^{kx} + e^{-kx} = 2 \\cosh kx and absorbing 2A into C and D , we have V(x, y) = \\cosh kx (C \\sin ky + D\\cos ky) Boundary conditions (i) and (ii) require, as before, that D = 0 and k = n\\pi /a , so V(x, y) = C \\cosh (n \\pi x /a )\\sin(n \\pi y/a) \\tagl{3.41} Because V(x, y) is even in x, it will automatically meet conditions (iv) if it fits (iii). It remains, therefore, to construct the general linear combination V(x, y) = \\sum _{n=1}^{\\infty} C_n \\cosh (n \\pi x / a) \\sin(n \\pi y /a) and pick the coefficients C_n in such a way as to satisfy condition (iii): V(b, y) = \\sum_{n=1}^{\\infty} C_n \\cosh (n \\pi b /a) \\sin(n \\pi y/a) = V_0 This is the same problem in Fourier analysis that we faced before; I quote the result from \\eqref{3.35} ; C_n \\cosh (n \\pi b/a) = \\begin{cases} 0 & \\quad \\text {if n is even} \\\\ \\frac{4 V_0}{n \\pi} & \\quad \\text{if n is odd} \\end{cases} Conclusion : The potential in this case is given by V(x, y) = \\frac{4 V_0}{\\pi} \\sum_{n=1, 3, 5\\ldots} \\frac{1}{n} \\frac{\\cosh(n \\pi x/a)}{\\cosh(n \\pi b/a)} \\sin(n \\pi y/a) \\tagl{3.42} This function is shown in Fig. 3.21 Example 3.5 An infinitely long rectangular metal pipe (sides a and b) is grounded, but one end, at x = 0 , a 'hot' plate is maintained at a specified potential V_0(y, z) , as indicated in Fig. 3.22. Find the potential inside the pipe. Solution This is genuinely a three-dimensional problem, \\frac{\\partial ^2 V}{\\partial{x^2}} + \\frac{\\partial ^2 V}{\\partial{y^2}} + \\frac{\\partial ^2 V}{\\partial{z^2}} = 0 \\tagl{3.43} subject to the boundary conditions - (i) V = 0 when y = 0 - (ii) V = 0 when y = a - (iii) V = 0 when z = 0 - (iv) V = 0 when z = b - (v) V \\rightarrow 0 as x \\rightarrow \\infty - (vi) V = V_0(y, z) whem x = 0 As always, we look for solutions that are products: V(x, y, z) = X(x)Y(y)Z(z) \\tagl{3.45} Putting this into \\eqref{3.43} and dividing by V, we find \\frac{1}{X} \\frac{d^2 X}{dx^2} + \\frac{1}{Y} \\frac{d^2 Y}{dy^2} + \\frac{1}{Z} \\frac{d^2 Z}{dz^2} = 0 It follows that \\frac{1}{X} \\frac{d^2 X}{dx^2} = C_1 , \\quad \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = C_2 , \\quad \\frac{1}{Z} \\frac{d^2 Z}{dz^2} = C_3 , \\text{ with } C_1 + C_2 + C_3 = 0 Our previous experience in Ex. 3.3 suggests that C_1 must be positive, C_2 and C_3 negative. Setting C_2 = -k^2 and C_3 = -l^2 , we have C_1 = k^2 + l^2 , and hence \\frac{d^2 X}{dx^2} = (k^2 + l^2)X, \\quad \\frac{d^2 Y}{dy^2} = -k^2 Y, \\quad \\frac{d^2 Z}{dz^2} = -l^2 Z \\tagl{3.46} Once again, separation of variables has turned a partial differential equation into ordinary differential equations. The solutions are \\begin{align*} X(x) & = A e^{\\sqrt{k^2 + l^2} x} + B e^{- \\sqrt{k^2 + l^2} x} \\\\ Y(y) & = C \\sin ky + D \\cos ky \\\\ Z(z) & = E \\sin lz + F \\cos lz \\end{align*} Boundary condition (v) implies A =0 , (i) gives D = 0 , and (iii) yields F = 0 whereas (ii) and (iv) require that k = n\\pi /a and l =m \\pi /b , where n and m are positive integers. Combining the remaining constants, we are left with V(x, y, z) = C e^{-\\pi \\sqrt{(n/a)^2 + (m/b)^2}x} \\sin (n \\pi y / a) \\sin(m \\pi z /b) \\tagl{3.47} This solution meets all the boundary conditions except (vi). It contains two unspecified integers (n and m), and the most general linear combination is a double sum V(x, y, z) = \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C e^{-\\pi \\sqrt{(n/a)^2 + (m/b)^2}x} \\sin (n \\pi y / a) \\sin(m \\pi z /b) = V_0(y, z) \\tagl{3.48} We hope to fit the remaining boundary condition, V(0, y, z) = \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C \\sin (n \\pi y / a) \\sin(m \\pi z /b) = V_0(y, z) \\tagl{3.49} by appropriate choice of the coefficients C_{n, m} . To determine these constants, we multiply by \\sin(n' n \\pi y/a) \\sin(m' \\pi z / b) , where n' and m' are arbitrary positive integers, and integrate \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C_{n, m} \\int_0 ^a \\sin (n \\pi y/a) \\sin(n' \\pi y/a) \\dd{y} \\int_0 ^b \\sin(m \\pi z/b) \\sin (m' \\pi z/b) \\dd{z} \\\\ = \\int_0 ^a \\int_0 ^b V_0(y, z) \\sin(n' \\pi y/a) \\sin(m' \\pi z/b) \\dd{y} \\dd{z} Quoting \\eqref{3.33} , the left side is (ab/4) C_{n', m'} , so C_{n, m} = \\frac{4}{ab} \\int_0 ^a \\int_0 ^b V_0(y, z) \\sin (n \\pi y/a) \\sin(m\\pi z/b) \\dd{y} \\dd{z} \\tagl{3.50} Equation \\eqref{3.48} , with the coefficients given by \\eqref{3.50} , is the solution to our problem. For instance, if the end of the tube is a conductor at constant potential V_0 , C_{n, m} = \\frac{4}{ab} \\int_0 ^a \\sin(n \\pi y/a) \\dd{y} \\int_0 ^b \\sin(m \\pi z/b) \\dd{z} \\\\ = \\begin{cases} 0 & \\qquad \\text{if n or m is even} \\\\ \\frac{16 V_0}{\\pi^2 nm} & \\qquad \\text{if n and m are odd} \\end{cases} \\tagl{3.51} In this case, V(x, y, z) = \\frac{16V_0}{\\pi^2} \\sum_{n,m=1,3,5,\\ldots} ^{\\infty} \\frac{1}{nm} e^{-\\pi \\sqrt{(n/a)^2 + (m/b)^2}x} \\sin(n \\pi y/a) \\sin(m \\pi z/b) \\tagl{3.52} Notice that successive terms decrease rapidly; a reasonable approximation would be obtained by keeping only the first few. 3.3.2: Spherical Coordinates In the examples considered so far, Cartesian coordinates were clearly appropriate, since the boundaries were planes. For round objects, spherical coordinates are more natural. In the spherical system, Laplace's equation reads: \\frac{1}{r^2} \\pdv{}{r} \\left( r^2 \\pdv{V}{r} \\right) + \\frac{1}{r^2 \\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{V}{\\theta} \\right) + \\frac{1}{r^2\\sin ^2 \\theta} \\frac{\\partial ^2 V}{\\partial \\phi ^2} = 0 \\tagl{3.53} I shall assume the problem has azimuthal symmetry , so that V is independent of \\phi ; In that case, \\eqref{3.53} reduces to \\pdv{}{r} \\left( r^2 \\pdv{V}{r} \\right) + \\frac{1}{\\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{V}{\\theta} \\right) = 0 \\tagl{3.54} As before, we look for solutions that are products: V(r, \\theta) = R(r) \\Theta (\\theta) \\tagl{3.55} Putting this into \\eqref{3.54} , and dividing by V , \\frac{1}{R} \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) + \\frac{1}{\\Theta \\sin \\theta} \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = 0 \\tagl{3.56} Since the first term depends only on r , and the second only on \\theta , it follows that each must be a constant: \\frac{1}{R} \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) = l(l+1), \\quad \\frac{1}{\\Theta \\sin \\theta} \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = -l(l+1) \\tagl{3.57} Here l(l+1) is just a fancy way of writing the separation constant, whose convenience will appear shortly. As always, separation of variables has converted a partial differential equation into ordinary differential equations. The radial equation, \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) = l(l+1)R \\tagl{3.58} has the general solution R(r) = A r^l + \\frac{B}{r^{l+1}} \\tagl{3.59} as you can easily check; A and B are the two arbitrary constants to be expected in the solution of a second-order differential equation. But the angular equation, \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = -l(l+1) \\sin \\theta \\Theta \\tagl{3.60} is not so simple. The solutions are Legendre polynomials in the variable \\cos \\theta : \\Theta (\\theta ) = P_l (\\cos \\theta ) \\tagl{3.61} P_l (x) is most conveniently defined by the Rodrigues formula : P_l(x) \\equiv \\frac{1}{2^l l!}\\left( \\dv{}{x} \\right)^l (x^2 - 1)^l \\tagl{3.62} The first few Legendre polynomials are listed: Legendre Polynomials P_0 - P_5 \\begin{align*} P_0(x) & = 1 \\\\ P_1(x) & = x \\\\ P_2(x) & = (3x^2 - 1)/2 \\\\ P_3(x) & = (5x^3 - 3x)/2 \\\\ P_4(x) & = (35x^4 - 30x^2 + 3)/8 \\\\ P_5(x) & = (63x^5 - 70x^3 + 15x)/8 \\end{align*} Notice that P_l(x) is (as the name suggests) an _l_th-order polynomial in x; it contains only even powers if l is even, and only odd powers if l is odd. The factor in front (1/2^l l! was chosen in order that P_l(1) = 1 \\tagl{3.63} The Rodrigues formula obviously only works for nonnegative integer values of l. Moreover, it provides us with only one solution. But \\eqref{3.60} is second-order, and it should possess two independent solutions for every value of l . It turns out that these \"other solutions\" blow up at \\theta = 0 and/or \\theta = \\pi , and are therefore unacceptable on physical grounds. For instance, the second solution for l=0 is \\Theta(\\theta) = \\ln \\left( \\tan \\frac{\\theta}{2} \\right) \\tagl{3.64} You might want to check for yourself that this satisfies \\eqref{3.60} . In the case of azimuthal symmetry, then, the most general separable solution to Laplace's equation, consistent with minimal physical requirements, is V(r, \\theta) = \\left( A r^l + \\frac{B}{r^{l+1}} \\right) P_l(\\cos \\theta) (There was no need to include an overall constant in \\eqref{3.61} because it can be absorbed into A and B at this stage.) As before, separation of variables yields an infinite set of solutions, one for each l . The general solution is the linear combination of separable solutions: V(r, \\theta) = \\sum_{l=0} ^{\\infty} \\left( A r^l + \\frac{B}{r^{l+1}} \\right) P_l(\\cos \\theta) \\tagl{3.65} The following examples illustrate the power of this important result. Example 3.6 The potential V_0(\\theta) is specified on the surface of a hollow sphere, of radius R . Find the potential inside the sphere. Solution In this case, B_l = 0 for all l , otherwise the potential would blow up at the origin. Thus, V(r, \\theta) = \\sum_{l = 0} ^{\\infty} A_l r^l P_l(\\cos \\theta) \\tagl{3.66} At r = R this must match the specified function V_0(\\theta) : V(R, \\theta) = \\sum_{l=0}^\\infty A_l R^l P_l(\\cos \\theta) = V_0(\\theta) \\tagl{3.67} Can this equation be satisfied, for an appropriate choice of coefficients A_l ? Yes: The Legendre polynomials (like the sines) constitute a complete set of functions, on the interval -1 \\leq x \\leq 1 (0 \\leq \\theta \\leq \\pi) . How do we determine the constants? Again, by Fourier's trick, for the Legendre polynomials (like the sines) are orthogonal functions: \\begin{align*} \\int_{-1}^1 P_l(x) P_{l'}(x) \\dd{x} & = \\int_0 ^\\pi P_l(\\cos \\theta) P_{l'}(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\\\ & = \\begin{cases} 0, & \\quad \\text{if } l' \\neq l \\\\ \\frac{2}{2l +1} , & \\quad \\text{if } l' = l \\end{cases} \\end{align*} \\tagl{3.68} Thus, multiplying \\eqref{3.67} by P_{l'}(\\cos \\theta) \\sin \\theta and integrating, we have A_{l'} R^{l'} \\frac{2}{2l' + 1} = \\int_{0} ^\\pi V_0(\\theta) P_{l'}(\\cos \\theta) \\sin \\theta \\dd{\\theta} or A_l = \\frac{2l+1}{2R^l} \\int_0 ^\\pi V_0(\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.69} \\eqref{3.66} is the solution to our problem, with the coefficients given by \\eqref{3.69} . It can be difficult to evaluate integrals of the form \\eqref{3.69} analytically, and in practice it is often easier to solve \\eqref{3.67} \"by eyeball.\" For instance, suppose we are told that the potential on the sphere is V_0(\\theta) = k \\sin^2 (\\theta/2) \\tagl{3.70} where k is constant. Using the half-angle formula, we rewrite this as V_0(\\theta) = \\frac{k}{2}(1 - \\cos \\theta) = \\frac{k}{2} [P_0(\\cos \\theta) - P_1 (\\cos \\theta)] Putting this into \\eqref{3.67} , we read off immediately that A_0 = k/2 , A_1 = -k/(2R) , and all other A_l 's vanish. Therefore V(r, \\theta) = \\frac{k}{2} \\left[ r^0 P_{0}(\\cos \\theta) - \\frac{r^1}{R} P_1 (\\cos \\theta) \\right] = \\frac{k}{2} \\left( 1 - \\frac{r}{R} \\cos \\theta \\right) \\tagl{3.71} Example 3.7 The potential V_0(\\theta) is again specified on the surface of a sphere of radius R , but this time we are asked to find the potential outside , assuming there is no charge there. Solution In this case it's the A_l 's that must be zero (or else V would not go to zero at \\infty ), so V(r, \\theta) = \\sum_{l=0} ^\\infty \\frac{B_l}{r^{l+1}} P_l(\\cos \\theta) = V_0(\\theta) \\tagl{3.72} Multiplying by P_{l'}(\\cos \\theta) \\sin \\theta and integrating - exploiting, again, the orthogonality relation 3.68 - we have \\frac{B_{l'}}{R^{l'+1}} \\frac{2}{2l' + 1} = \\int_0 ^\\pi V_0(\\theta) P_{l'}(\\cos \\theta) \\sin \\theta \\dd{\\theta} or B_l = \\frac{2l + 1}{2} R^{l+1} \\int_0 ^\\pi V_0(\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.73} \\eqref{3.72} , with the coefficients given by \\eqref{3.73} , is the solution to our problem. Example 3.8 An uncharged metal sphere of radius R is placed in an otherwise uniform electric field \\vec{E} = E_0 \\vu{z} . The field will push positive charge to the 'northern' surface of the sphere, and - symmetrically - negative charge to the 'southern' surface (Fig. 3.24). This induced charge, in turn, distorts the field in the neighborhood of the sphere. Find the potential in the region outside the sphere. Solution The sphere is an equipotential - we may as well set it to zero. Then by symmetry the entire xy plane is at potential zero. This time, however, V does not go to zero at large z . In fact, far from the sphere the field is E_0 \\vu{z} and hence V \\rightarrow - E_0 z + C Since V = 0 in the equatorial plane, the constant C must be zero. Accordingly, the boundary conditions for this problem are - (i) V = 0 when r = R - (ii) V \\rightarrow - E_0 r \\cos \\theta for r \\gg R We must fit these boundary conditions with a function of the form \\eqref{3.65} . The first condition yields A_l R^l + \\frac{B_l}{R^{l+1}} = 0 or B_l = -A_l R^{2l+1} \\tagl{3.75} so V(r, \\theta) = \\sum_{l = 0} ^{\\infty} A_l \\left( r^l - \\frac{R^{2l+1}}{r^{l+1}} \\right) P_l(\\cos \\theta) For r \\gg R , the second term in parentheses is negligible, and therefore the condition (ii) requires that \\sum_{l=0}^\\infty A_l R^{l} P_l (\\cos \\theta) = - E_0 r \\cos \\theta Evidently only one term is present: l = 1 . In fact, since P_1(\\cos \\theta) = \\cos \\theta we can read off immediately A_1 = - E_0, \\qquad \\text{ all other }A_l's \\text{ zero} Conclusion : V(r, \\theta) = - E_0 \\left( r - \\frac{R^3}{r^2} \\right) \\cos \\theta \\tagl{3.76} The first term (-E_0 r \\cos \\theta) is due to the external field; the contribution attributable to the induced charge is E_0 \\frac{R^3}{r^2} \\cos \\theta If you want to know the induced charge density, it can be calculated in the usual way: \\sigma(\\theta) = - \\epsilon_0 \\left. \\pdv{V}{r} \\right|_{r = R} = \\epsilon_0 E_0 \\left. \\left( 1 + 2 \\frac{R^3}{r^3} \\right) \\cos \\theta \\right|_{r = R} = 3 \\epsilon_0 E_0 \\cos \\theta \\tagl{3.77} As expected, it is positive in the 'northern' hemisphere 0 \\leq \\theta \\leq \\pi /2 and negative in the 'southern' \\pi/2 \\leq \\theta \\leq \\pi . Example 3.9 A specified charge density \\sigma_0(\\theta) is glued over the surface of a spherical shell of radius R . Find the resulting potential inside and outside the sphere. Solution You could, of course, do this by direct integration: V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma_0}{\\gr} \\dd{a} but separation of variables is often easier. For the interior region, we have V(r, \\theta) = \\sum_{l = 0}^\\infty A_l r^l P_l (\\cos \\theta) \\quad (r \\leq R) \\tagl{3.78} (no B_l terms - they blow up at the origin); in the exterior region V(r, \\theta) = \\sum_{l=0}^\\infty \\frac{B_l}{r^{l+1}} P_l(\\cos \\theta) \\quad (r \\geq R) \\tagl{3.79} (no A_l terms - they don't go to zero at infinity). These two functions must be joined together by the appropriate boundary conditions at the surface itself. First, the potential is continuous at r = R (Eq. 2.34): \\sum_{l=0}^\\infty A_l R^l P_l(\\cos \\theta) = \\sum_{l=0} ^\\infty \\frac{B_l}{R^{l+1}} P_l(\\cos \\theta) \\tagl{3.80} It follows that the coefficients of like Legendre polynomial are equal: B_l = A_l R^{2l+1} \\tagl{3.81} (To prove that formally, multiply both sides of \\eqref{3.80} by P_{l'} (\\cos \\theta)\\sin \\theta and integrate from 0 to \\pi , using the orthogonality relation \\eqref{3.68} .) Second, the radial derivative of V suffers a discontinuity at the surface (Eq. 2.36): \\left. \\left( \\pdv{V_{out}}{r} - \\pdv{V_{in}}{r} \\right) \\right|_{r = R} = - \\frac{1}{\\epsilon_0} \\sigma_0(\\theta) \\tagl{3.82} Thus, - \\sum_{l=0}^\\infty (l+1) \\frac{B_l}{R^{l+2}} P_l(\\cos \\theta) - \\sum_{l=0}^\\infty l A_l R^{l-1} P_l(\\cos \\theta) = - \\frac{1}{\\epsilon_0} \\sigma_0 (\\theta) or, using \\eqref{3.81} , \\sum_{l=0}^\\infty (2l+1) A_l R^{l-1} P_l(\\cos \\theta) = \\frac{1}{\\epsilon_0} \\sigma_0(\\theta) \\tagl{3.83} From here, the coefficients can be determined using Fourier's trick A_l = \\frac{1}{2 \\epsilon_0 R^{l-1}} \\int_0 ^\\pi \\sigma_0 (\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.84} Equations 3.78 and 3.79 constitute the solution to our problem, with the coefficients given by \\eqref{3.81} and \\eqref{3.84} . For instance, if \\sigma_0(\\theta) = k \\cos \\theta = k P_1 (\\cos \\theta) \\tagl{3.85} for some constant k , then all the A_l 's are zero except for l = 1 , and A_1 = \\frac{k}{2 \\epsilon_0} \\int_0 ^\\pi [P_1(\\cos \\theta)]^2 \\sin \\theta \\dd{\\theta} = \\frac{k}{3\\epsilon_0} The potential inside the sphere is therefore V(r, \\theta) = \\frac{k}{3 \\epsilon_0} r \\cos \\theta \\quad (r \\leq R) \\tagl{3.86} whereas outside the sphere V(r, \\theta) = \\frac{kR^3}{3 \\epsilon_0} \\frac{1}{r^2} \\cos \\theta \\quad (r \\geq R) \\tagl{3.87} In particular, if \\sigma_0(\\theta) is the induced charge on a metal sphere in an external field E_0(\\vu{z}) , so that k = 3 \\epsilon_0 E_0 \\eqref{3.77} , then the potential inside is E_0 r \\cos \\theta = E_0 z , and the field is -E_0 \\vu{z} - exactly right to cancel off the external field, as of course it should be. Outside the sphere the potential due to this surface charge is E_0 \\frac{R^3}{r^2} \\cos \\theta consistent with our conclusion in Example 3.8.","title":"3.3 - Separation of Variables"},{"location":"ch3-3/#33-separation-of-variables","text":"In this section we shall attack Laplace's equation directly, using the method of separation of variables , which is the physicist's favorite tool for solving partial differential equations. The method is applicable in circumstances where the potential (V) or the charge density (\\sigma) is specified on the boundaries of some region, and we are asked to find the potential in the interior. The basic strategy is very simple: We look for solutions that are products of functions, each of which depends on only one of the coordinates. The algebraic details, however, can be formidable, so I'm going to develop the method through a sequence of examples. We'll start with Cartesian coordinates and then do spherical coordinates (I'll leave the cylindrical case for you to tackle on your own, in Prob 3.24).","title":"3.3: Separation of Variables"},{"location":"ch3-3/#331-cartesian-coordinates","text":"","title":"3.3.1: Cartesian Coordinates"},{"location":"ch3-3/#example-33","text":"Two infinite grounded metal plates lie parallel to the xz plane, one at y = 0 , the other at y = a (Fig. 3.17). The left end, at x = 0 , is closed off with an infinite strip insulated from the two plates, and maintained at a specific potential V_0(y) . Find the potential inside this 'slot.' Solution The configuration is independent of z, so this is really a two-dimensional problem. In mathematical terms, we must solve Laplace's equation, \\frac{\\partial ^2 V}{\\partial{x^2}} + \\frac{\\partial ^2 V}{\\partial{y^2}} = 0 \\label{3.20} \\tag{3.20} subject to the boundary conditions (i) V = 0 when y = 0 (ii) V = 0 when y = a (iii) V = V_0(y) when x = 0 (iv) V \\rightarrow 0 as x \\rightarrow \\infty (The latter, although not explicitly stated in the problem, is necessary on physical grounds: as you get farther and farther away from the \"hot\" strip at x = 0 , the potential should drop to zero.) Since the potential is specified on all boundaries, the answer is uniquely determined. The first step is to look for solutions in the form of products: V(x, y) = X(x)Y(y) \\tagl{3.22} On the face of it, this is an absurd restriction - the overwhelming majority of solutions to Laplace's equation do not have such a form. For example, V(x, y) = (5x + 6y) satisfies the equation, but you can't express it as the product of a function of x times a function of y. Obviously, we're only going to get a tiny subset of all possible solutions by this means, and it would be a miracle if one of them happened to fit the boundary conditions of our problem... But hang on, because the solutions we do get are very special, and it turns out that by pasting them together we can construct the general solution. Anyway, putting \\eqref{3.22} into \\eqref{3.20} we obtain Y \\frac{d^2X}{dx^2} + X \\frac{d^2 Y}{dy^2} = 0 The next step is to \"separate the variables\" (that is, collect all the x-dependence into one term and all the y-dependence into another). Typically, this is accomplished by dividing through by V: \\frac{1}{X} \\frac{d^2 X}{dx^2} + \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = 0 \\tagl{3.23} Here the first term depends only on x and the second term only on y; in other words, we have an equation of the form f(x) + g(y) = 0 \\label{3.24} \\tag{3.24} Now, there's only one way this could possibly be true: f and g must both be constant . For what if f(x) changed, as you vary x - then if we held y fixed and fiddled with x, the sum f(x) + g(y) would change, in violation of \\eqref{3.24} , which says it's always zero. (That's a simple but somehow rather elusive argument; don't accept it without due thought, because the whole method rides on it.) It follows from \\eqref{3.23} , then, that \\frac{1}{X} \\frac{d^2 X}{dx^2} = C_1 \\quad \\text{ and } \\quad \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = C_2, \\quad \\text{ with } C_1 + C_2 = 0 \\tagl{3.25} One of these constants is positive, the other negative (or perhaps both are zero). In general, one must investigate all possibilities; however in our particular problem we need C_1 positive and C_2 negative, for reasons that will appear in a moment. Thus \\frac{d^2X}{dx^2} = k^2 X, \\qquad \\frac{d^2 Y}{dy^2} = - k^2 Y \\tagl{3.26} Notice what has happened: A partial differential equation has been converted into two ordinary differential equations. The advantage of this is obvious - ordinary differential equations are a lot easier to solve. Indeed: X(x) = A e^{kt} + B e^{-kt}, \\qquad Y(y) = C \\sin ky + D \\cos ky so V(x, y) = (A e^{kt} + B e^{-kt})(C \\sin ky + D \\cos ky) \\tagl{3.27} This is the appropriate separable solution to Laplace's equation; it remains to impose the boundary conditions, and see what they tell us about the constants. To begin at the end, condition (iv) requires tha A equal zero. Absorbing B into C and D, we are left with V(x, y) = e^{-kx} (C\\sin ky + D \\cos ky) Condition (i) now demands that D equal zero V(x, y) = C ^{-kx} \\sin ky \\tagl{3.28} Meanwhile (ii) yields \\sin ka = 0 , from which it follows that k = \\frac{n \\pi}{a} \\quad (n = 1, 2, 3, \\ldots) \\tagl{3.29} (At this point you can see why I chose C_1 positive and C_2 negative: If X were sinusoidal, we could never manage for it to go to zero at infinity, and if Y were exponential we could not make it vanish at both 0 and a . Incidentally, n = 0 is no good, for in that case the potential vanishes everywhere. And we have already excluded negative n's) That's as far as we can go, using separable solutions, and unless V_0(y) just happens to have the form \\sin(n \\pi / a) for some integer n , we simply can't fit the final boundary condition at x = 0 . But now comes the crucial step that redeems the method: Separation of variables has given us an infinite family of solutions (one for each n), and whereas none of them by itself satisfies the final boundary condition, it is possible to combine them in a way that does . Laplace's equation is linear, in the sense that if V_1, V_2, V_3, \\ldots satisfy it, so does any linear combination, for \\laplacian{V} = \\alpha_1 \\laplacian{V_1} + \\alpha_2 \\laplacian{V_2} + \\ldots = 0 \\alpha_1 + 0 \\alpha_2 + \\ldots = 0 Exploiting this fact, we can patch together the separable solutions \\eqref{3.28} to construct a much more general solution: V(x, y) = \\sum_{n=1} ^{\\infty} C_n e^{-n \\pi x / a} \\sin (n \\pi y / a) \\tagl{3.30} This still satisfies three of the boundary conditions; the question is, can we (by astute choice of the coefficients C_n ) fit the final boundary condition (iii)? V(0, y) = \\sum_{n=1} ^{\\infty} C_n \\sin (n \\pi y / a) = V_0(y) \\tagl{3.31} Well, you may recognize this sum - it's a Fourier sine series. And Dirichlet's theorem guarantees that virtually any function V_0(y) - it can even have a finite number of discontinuities - can be expanded in such a series. But how do we actually determine the coefficients C_n , buried as they are in that infinite sum? The device for accomplishing this is so lovely it deserves a name - I call it Fourier's trick , though it seems Euler had used essentially the same idea somewhat earlier. Here's how it goes: Multiply \\eqref{3.31} by \\sin(n' \\pi y /a) (where n' is a positive integer), and integrate from 0 to a: \\sum_{n=1} ^{\\infty} C_n \\int_{0} ^{a} \\sin(n \\pi y / a) \\sin(n' \\pi y/a) \\dd{y} = \\int_{0} ^a V_0(y) \\sin (n' \\pi /a) \\dd{y} \\tagl{3.32} You can work out the integral on the left yourself; the answer is \\int_{0} ^a \\sin (n \\pi y /a) \\sin (n' \\pi y / a) \\dd{y} = \\begin{cases} 0 & \\quad \\text{if } n' \\neq n \\\\ \\frac{a}{2} & \\quad \\text{if } n' = n \\end{cases} \\tagl{3.33} Thus all the terms in the series drop out, save only the one where n = n' , and the left side of \\eqref{3.32} reduces to (a/2)C_{n'} . Conclusion : C_n = \\frac{2}{a} \\int_{0}^a V_0(y) \\sin (n \\pi y /a) \\dd{y} \\tagl{3.34} That does it: \\eqref{3.30} is the solution, with coefficients given by eqref{3.34} . As a concrete example, suppose the strip at x = 0 is a metal plate with constant potential V_0 (remember, it's insulated from the grounded plates at y = 0 and y = a . Then C_n = \\frac{2V_0}{a} \\int_0 ^a \\sin (n \\pi y / a) \\dd y \\\\ = \\frac{2 V_0}{n \\pi} (1 - \\cos n \\pi) = \\begin{cases} 0 & \\quad \\text{if n is even } \\\\ \\frac{4 V_0}{n \\pi} & \\quad \\text{if n is odd} \\end{cases} \\tagl{3.35} Thus V(x, y) = \\frac{4 V_0}{\\pi} \\sum_{n = 1, 3, 5, \\ldots} \\frac{1}{n} e^{- n \\pi x / a} \\sin (n \\pi y / a) \\tagl{3.36} Figure 3.18 is a plot of this potential; Fig. 3.10 shows how the first few terms in the Fourier series combine to make a better and better approximation to the constant V_0 : (a) is the n=1 term only, (b) includes n up to 5, (c) is the sum of the first 10 terms, and (d) is the sum of the first 100 terms. Incidentally, the infinite series in Eq. 3.36 can be summed explicitly (try your hand at it if you like); the result is V(x, y) = \\frac{2V_0}{\\pi} \\tan^{-1} \\left( \\frac{\\sin(\\pi y / a)}{\\sinh(\\pi x /a )} \\right) \\tagl{3.37} In this form, it is easy to check that Laplace's equation is obeyed and the four boundary conditions are satisfied The success of this method hinged on two extraordinary properties of the separable solutions \\eqref{3.28} and \\eqref{3.29} : completeness and orthogonality . A set of functions f_n(y) is said to be complete if any other function f(y) can be expressed as a linear combination of them: f(y) = \\sum_{n=1} ^{\\infty} C_n f_n(y) \\tagl{3.38} The functions \\sin (n \\pi y/a) are complete on the interval 0 \\leq y \\leq a . It was this fact, guaranteed by Dirichlet's theorem, that assured us \\eqref{3.31} could be satisfied, given the proper choice of the coefficients C_n . (The proof of completeness, for a particular set of functions, is an extremely difficult business, and I'm afraid physicists tend to assume it's true and leave the checking to others.) A set of functions is orthogonal if the integral of the product of any two different members of the set is zero: \\int_0 ^a f_n(y) f_{n'} (y) \\dd{y} = 0 \\quad \\text{for } n' \\neq n The sine functions are orthogonal \\eqref{3.33} ; that is the property on which Fourier's trick is based, allowing us to kill off all terms but one in the infinite series and thereby solve for the coefficients C_n (Proof of orthogonality is generally quite simple, either by direct integration or by analysis of the differential equation from which the functions came.)","title":"Example 3.3"},{"location":"ch3-3/#example-34","text":"Two infinitely-long grounded metal plates, again at y=0 and y=a are connected at x= \\pm b by metal strips maintained at a constant potential V_0 , as shown in Fig. 3.20 (a thin layer of insulation at each corner prevents them from shorting out). Find the potential inside the resulting rectangular pipe. Solution Once again, the configuration is independent of z. Our problem is to solve Laplace's equation \\frac{\\partial ^2 V}{\\partial{x^2}} + \\frac{\\partial ^2 V}{\\partial{y^2}} = 0 subject to the boundary conditions (i) V = 0 when y = 0 (ii) V = 0 when y = a (iii) V = V_0 when x = b (iv) V = V_0 when x = -b The argument runs as before, up to \\eqref{3.27} : V(x, y) = (A e^{kt} + B e^{-kt})(C \\sin ky + D \\cos ky) This time, however, we cannot set A = 0 ; the region in question does not extend to x = \\infty , so e^{kx} is perfectly acceptable. On the other hand, the situation is symmetric with respect to x, so V(-x, y) = V(x, y) , and it follows that A = B . Using e^{kx} + e^{-kx} = 2 \\cosh kx and absorbing 2A into C and D , we have V(x, y) = \\cosh kx (C \\sin ky + D\\cos ky) Boundary conditions (i) and (ii) require, as before, that D = 0 and k = n\\pi /a , so V(x, y) = C \\cosh (n \\pi x /a )\\sin(n \\pi y/a) \\tagl{3.41} Because V(x, y) is even in x, it will automatically meet conditions (iv) if it fits (iii). It remains, therefore, to construct the general linear combination V(x, y) = \\sum _{n=1}^{\\infty} C_n \\cosh (n \\pi x / a) \\sin(n \\pi y /a) and pick the coefficients C_n in such a way as to satisfy condition (iii): V(b, y) = \\sum_{n=1}^{\\infty} C_n \\cosh (n \\pi b /a) \\sin(n \\pi y/a) = V_0 This is the same problem in Fourier analysis that we faced before; I quote the result from \\eqref{3.35} ; C_n \\cosh (n \\pi b/a) = \\begin{cases} 0 & \\quad \\text {if n is even} \\\\ \\frac{4 V_0}{n \\pi} & \\quad \\text{if n is odd} \\end{cases} Conclusion : The potential in this case is given by V(x, y) = \\frac{4 V_0}{\\pi} \\sum_{n=1, 3, 5\\ldots} \\frac{1}{n} \\frac{\\cosh(n \\pi x/a)}{\\cosh(n \\pi b/a)} \\sin(n \\pi y/a) \\tagl{3.42} This function is shown in Fig. 3.21","title":"Example 3.4"},{"location":"ch3-3/#example-35","text":"An infinitely long rectangular metal pipe (sides a and b) is grounded, but one end, at x = 0 , a 'hot' plate is maintained at a specified potential V_0(y, z) , as indicated in Fig. 3.22. Find the potential inside the pipe. Solution This is genuinely a three-dimensional problem, \\frac{\\partial ^2 V}{\\partial{x^2}} + \\frac{\\partial ^2 V}{\\partial{y^2}} + \\frac{\\partial ^2 V}{\\partial{z^2}} = 0 \\tagl{3.43} subject to the boundary conditions - (i) V = 0 when y = 0 - (ii) V = 0 when y = a - (iii) V = 0 when z = 0 - (iv) V = 0 when z = b - (v) V \\rightarrow 0 as x \\rightarrow \\infty - (vi) V = V_0(y, z) whem x = 0 As always, we look for solutions that are products: V(x, y, z) = X(x)Y(y)Z(z) \\tagl{3.45} Putting this into \\eqref{3.43} and dividing by V, we find \\frac{1}{X} \\frac{d^2 X}{dx^2} + \\frac{1}{Y} \\frac{d^2 Y}{dy^2} + \\frac{1}{Z} \\frac{d^2 Z}{dz^2} = 0 It follows that \\frac{1}{X} \\frac{d^2 X}{dx^2} = C_1 , \\quad \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = C_2 , \\quad \\frac{1}{Z} \\frac{d^2 Z}{dz^2} = C_3 , \\text{ with } C_1 + C_2 + C_3 = 0 Our previous experience in Ex. 3.3 suggests that C_1 must be positive, C_2 and C_3 negative. Setting C_2 = -k^2 and C_3 = -l^2 , we have C_1 = k^2 + l^2 , and hence \\frac{d^2 X}{dx^2} = (k^2 + l^2)X, \\quad \\frac{d^2 Y}{dy^2} = -k^2 Y, \\quad \\frac{d^2 Z}{dz^2} = -l^2 Z \\tagl{3.46} Once again, separation of variables has turned a partial differential equation into ordinary differential equations. The solutions are \\begin{align*} X(x) & = A e^{\\sqrt{k^2 + l^2} x} + B e^{- \\sqrt{k^2 + l^2} x} \\\\ Y(y) & = C \\sin ky + D \\cos ky \\\\ Z(z) & = E \\sin lz + F \\cos lz \\end{align*} Boundary condition (v) implies A =0 , (i) gives D = 0 , and (iii) yields F = 0 whereas (ii) and (iv) require that k = n\\pi /a and l =m \\pi /b , where n and m are positive integers. Combining the remaining constants, we are left with V(x, y, z) = C e^{-\\pi \\sqrt{(n/a)^2 + (m/b)^2}x} \\sin (n \\pi y / a) \\sin(m \\pi z /b) \\tagl{3.47} This solution meets all the boundary conditions except (vi). It contains two unspecified integers (n and m), and the most general linear combination is a double sum V(x, y, z) = \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C e^{-\\pi \\sqrt{(n/a)^2 + (m/b)^2}x} \\sin (n \\pi y / a) \\sin(m \\pi z /b) = V_0(y, z) \\tagl{3.48} We hope to fit the remaining boundary condition, V(0, y, z) = \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C \\sin (n \\pi y / a) \\sin(m \\pi z /b) = V_0(y, z) \\tagl{3.49} by appropriate choice of the coefficients C_{n, m} . To determine these constants, we multiply by \\sin(n' n \\pi y/a) \\sin(m' \\pi z / b) , where n' and m' are arbitrary positive integers, and integrate \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C_{n, m} \\int_0 ^a \\sin (n \\pi y/a) \\sin(n' \\pi y/a) \\dd{y} \\int_0 ^b \\sin(m \\pi z/b) \\sin (m' \\pi z/b) \\dd{z} \\\\ = \\int_0 ^a \\int_0 ^b V_0(y, z) \\sin(n' \\pi y/a) \\sin(m' \\pi z/b) \\dd{y} \\dd{z} Quoting \\eqref{3.33} , the left side is (ab/4) C_{n', m'} , so C_{n, m} = \\frac{4}{ab} \\int_0 ^a \\int_0 ^b V_0(y, z) \\sin (n \\pi y/a) \\sin(m\\pi z/b) \\dd{y} \\dd{z} \\tagl{3.50} Equation \\eqref{3.48} , with the coefficients given by \\eqref{3.50} , is the solution to our problem. For instance, if the end of the tube is a conductor at constant potential V_0 , C_{n, m} = \\frac{4}{ab} \\int_0 ^a \\sin(n \\pi y/a) \\dd{y} \\int_0 ^b \\sin(m \\pi z/b) \\dd{z} \\\\ = \\begin{cases} 0 & \\qquad \\text{if n or m is even} \\\\ \\frac{16 V_0}{\\pi^2 nm} & \\qquad \\text{if n and m are odd} \\end{cases} \\tagl{3.51} In this case, V(x, y, z) = \\frac{16V_0}{\\pi^2} \\sum_{n,m=1,3,5,\\ldots} ^{\\infty} \\frac{1}{nm} e^{-\\pi \\sqrt{(n/a)^2 + (m/b)^2}x} \\sin(n \\pi y/a) \\sin(m \\pi z/b) \\tagl{3.52} Notice that successive terms decrease rapidly; a reasonable approximation would be obtained by keeping only the first few.","title":"Example 3.5"},{"location":"ch3-3/#332-spherical-coordinates","text":"In the examples considered so far, Cartesian coordinates were clearly appropriate, since the boundaries were planes. For round objects, spherical coordinates are more natural. In the spherical system, Laplace's equation reads: \\frac{1}{r^2} \\pdv{}{r} \\left( r^2 \\pdv{V}{r} \\right) + \\frac{1}{r^2 \\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{V}{\\theta} \\right) + \\frac{1}{r^2\\sin ^2 \\theta} \\frac{\\partial ^2 V}{\\partial \\phi ^2} = 0 \\tagl{3.53} I shall assume the problem has azimuthal symmetry , so that V is independent of \\phi ; In that case, \\eqref{3.53} reduces to \\pdv{}{r} \\left( r^2 \\pdv{V}{r} \\right) + \\frac{1}{\\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{V}{\\theta} \\right) = 0 \\tagl{3.54} As before, we look for solutions that are products: V(r, \\theta) = R(r) \\Theta (\\theta) \\tagl{3.55} Putting this into \\eqref{3.54} , and dividing by V , \\frac{1}{R} \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) + \\frac{1}{\\Theta \\sin \\theta} \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = 0 \\tagl{3.56} Since the first term depends only on r , and the second only on \\theta , it follows that each must be a constant: \\frac{1}{R} \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) = l(l+1), \\quad \\frac{1}{\\Theta \\sin \\theta} \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = -l(l+1) \\tagl{3.57} Here l(l+1) is just a fancy way of writing the separation constant, whose convenience will appear shortly. As always, separation of variables has converted a partial differential equation into ordinary differential equations. The radial equation, \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) = l(l+1)R \\tagl{3.58} has the general solution R(r) = A r^l + \\frac{B}{r^{l+1}} \\tagl{3.59} as you can easily check; A and B are the two arbitrary constants to be expected in the solution of a second-order differential equation. But the angular equation, \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = -l(l+1) \\sin \\theta \\Theta \\tagl{3.60} is not so simple. The solutions are Legendre polynomials in the variable \\cos \\theta : \\Theta (\\theta ) = P_l (\\cos \\theta ) \\tagl{3.61} P_l (x) is most conveniently defined by the Rodrigues formula : P_l(x) \\equiv \\frac{1}{2^l l!}\\left( \\dv{}{x} \\right)^l (x^2 - 1)^l \\tagl{3.62} The first few Legendre polynomials are listed: Legendre Polynomials P_0 - P_5 \\begin{align*} P_0(x) & = 1 \\\\ P_1(x) & = x \\\\ P_2(x) & = (3x^2 - 1)/2 \\\\ P_3(x) & = (5x^3 - 3x)/2 \\\\ P_4(x) & = (35x^4 - 30x^2 + 3)/8 \\\\ P_5(x) & = (63x^5 - 70x^3 + 15x)/8 \\end{align*} Notice that P_l(x) is (as the name suggests) an _l_th-order polynomial in x; it contains only even powers if l is even, and only odd powers if l is odd. The factor in front (1/2^l l! was chosen in order that P_l(1) = 1 \\tagl{3.63} The Rodrigues formula obviously only works for nonnegative integer values of l. Moreover, it provides us with only one solution. But \\eqref{3.60} is second-order, and it should possess two independent solutions for every value of l . It turns out that these \"other solutions\" blow up at \\theta = 0 and/or \\theta = \\pi , and are therefore unacceptable on physical grounds. For instance, the second solution for l=0 is \\Theta(\\theta) = \\ln \\left( \\tan \\frac{\\theta}{2} \\right) \\tagl{3.64} You might want to check for yourself that this satisfies \\eqref{3.60} . In the case of azimuthal symmetry, then, the most general separable solution to Laplace's equation, consistent with minimal physical requirements, is V(r, \\theta) = \\left( A r^l + \\frac{B}{r^{l+1}} \\right) P_l(\\cos \\theta) (There was no need to include an overall constant in \\eqref{3.61} because it can be absorbed into A and B at this stage.) As before, separation of variables yields an infinite set of solutions, one for each l . The general solution is the linear combination of separable solutions: V(r, \\theta) = \\sum_{l=0} ^{\\infty} \\left( A r^l + \\frac{B}{r^{l+1}} \\right) P_l(\\cos \\theta) \\tagl{3.65} The following examples illustrate the power of this important result.","title":"3.3.2: Spherical Coordinates"},{"location":"ch3-3/#example-36","text":"The potential V_0(\\theta) is specified on the surface of a hollow sphere, of radius R . Find the potential inside the sphere. Solution In this case, B_l = 0 for all l , otherwise the potential would blow up at the origin. Thus, V(r, \\theta) = \\sum_{l = 0} ^{\\infty} A_l r^l P_l(\\cos \\theta) \\tagl{3.66} At r = R this must match the specified function V_0(\\theta) : V(R, \\theta) = \\sum_{l=0}^\\infty A_l R^l P_l(\\cos \\theta) = V_0(\\theta) \\tagl{3.67} Can this equation be satisfied, for an appropriate choice of coefficients A_l ? Yes: The Legendre polynomials (like the sines) constitute a complete set of functions, on the interval -1 \\leq x \\leq 1 (0 \\leq \\theta \\leq \\pi) . How do we determine the constants? Again, by Fourier's trick, for the Legendre polynomials (like the sines) are orthogonal functions: \\begin{align*} \\int_{-1}^1 P_l(x) P_{l'}(x) \\dd{x} & = \\int_0 ^\\pi P_l(\\cos \\theta) P_{l'}(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\\\ & = \\begin{cases} 0, & \\quad \\text{if } l' \\neq l \\\\ \\frac{2}{2l +1} , & \\quad \\text{if } l' = l \\end{cases} \\end{align*} \\tagl{3.68} Thus, multiplying \\eqref{3.67} by P_{l'}(\\cos \\theta) \\sin \\theta and integrating, we have A_{l'} R^{l'} \\frac{2}{2l' + 1} = \\int_{0} ^\\pi V_0(\\theta) P_{l'}(\\cos \\theta) \\sin \\theta \\dd{\\theta} or A_l = \\frac{2l+1}{2R^l} \\int_0 ^\\pi V_0(\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.69} \\eqref{3.66} is the solution to our problem, with the coefficients given by \\eqref{3.69} . It can be difficult to evaluate integrals of the form \\eqref{3.69} analytically, and in practice it is often easier to solve \\eqref{3.67} \"by eyeball.\" For instance, suppose we are told that the potential on the sphere is V_0(\\theta) = k \\sin^2 (\\theta/2) \\tagl{3.70} where k is constant. Using the half-angle formula, we rewrite this as V_0(\\theta) = \\frac{k}{2}(1 - \\cos \\theta) = \\frac{k}{2} [P_0(\\cos \\theta) - P_1 (\\cos \\theta)] Putting this into \\eqref{3.67} , we read off immediately that A_0 = k/2 , A_1 = -k/(2R) , and all other A_l 's vanish. Therefore V(r, \\theta) = \\frac{k}{2} \\left[ r^0 P_{0}(\\cos \\theta) - \\frac{r^1}{R} P_1 (\\cos \\theta) \\right] = \\frac{k}{2} \\left( 1 - \\frac{r}{R} \\cos \\theta \\right) \\tagl{3.71}","title":"Example 3.6"},{"location":"ch3-3/#example-37","text":"The potential V_0(\\theta) is again specified on the surface of a sphere of radius R , but this time we are asked to find the potential outside , assuming there is no charge there. Solution In this case it's the A_l 's that must be zero (or else V would not go to zero at \\infty ), so V(r, \\theta) = \\sum_{l=0} ^\\infty \\frac{B_l}{r^{l+1}} P_l(\\cos \\theta) = V_0(\\theta) \\tagl{3.72} Multiplying by P_{l'}(\\cos \\theta) \\sin \\theta and integrating - exploiting, again, the orthogonality relation 3.68 - we have \\frac{B_{l'}}{R^{l'+1}} \\frac{2}{2l' + 1} = \\int_0 ^\\pi V_0(\\theta) P_{l'}(\\cos \\theta) \\sin \\theta \\dd{\\theta} or B_l = \\frac{2l + 1}{2} R^{l+1} \\int_0 ^\\pi V_0(\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.73} \\eqref{3.72} , with the coefficients given by \\eqref{3.73} , is the solution to our problem.","title":"Example 3.7"},{"location":"ch3-3/#example-38","text":"An uncharged metal sphere of radius R is placed in an otherwise uniform electric field \\vec{E} = E_0 \\vu{z} . The field will push positive charge to the 'northern' surface of the sphere, and - symmetrically - negative charge to the 'southern' surface (Fig. 3.24). This induced charge, in turn, distorts the field in the neighborhood of the sphere. Find the potential in the region outside the sphere. Solution The sphere is an equipotential - we may as well set it to zero. Then by symmetry the entire xy plane is at potential zero. This time, however, V does not go to zero at large z . In fact, far from the sphere the field is E_0 \\vu{z} and hence V \\rightarrow - E_0 z + C Since V = 0 in the equatorial plane, the constant C must be zero. Accordingly, the boundary conditions for this problem are - (i) V = 0 when r = R - (ii) V \\rightarrow - E_0 r \\cos \\theta for r \\gg R We must fit these boundary conditions with a function of the form \\eqref{3.65} . The first condition yields A_l R^l + \\frac{B_l}{R^{l+1}} = 0 or B_l = -A_l R^{2l+1} \\tagl{3.75} so V(r, \\theta) = \\sum_{l = 0} ^{\\infty} A_l \\left( r^l - \\frac{R^{2l+1}}{r^{l+1}} \\right) P_l(\\cos \\theta) For r \\gg R , the second term in parentheses is negligible, and therefore the condition (ii) requires that \\sum_{l=0}^\\infty A_l R^{l} P_l (\\cos \\theta) = - E_0 r \\cos \\theta Evidently only one term is present: l = 1 . In fact, since P_1(\\cos \\theta) = \\cos \\theta we can read off immediately A_1 = - E_0, \\qquad \\text{ all other }A_l's \\text{ zero} Conclusion : V(r, \\theta) = - E_0 \\left( r - \\frac{R^3}{r^2} \\right) \\cos \\theta \\tagl{3.76} The first term (-E_0 r \\cos \\theta) is due to the external field; the contribution attributable to the induced charge is E_0 \\frac{R^3}{r^2} \\cos \\theta If you want to know the induced charge density, it can be calculated in the usual way: \\sigma(\\theta) = - \\epsilon_0 \\left. \\pdv{V}{r} \\right|_{r = R} = \\epsilon_0 E_0 \\left. \\left( 1 + 2 \\frac{R^3}{r^3} \\right) \\cos \\theta \\right|_{r = R} = 3 \\epsilon_0 E_0 \\cos \\theta \\tagl{3.77} As expected, it is positive in the 'northern' hemisphere 0 \\leq \\theta \\leq \\pi /2 and negative in the 'southern' \\pi/2 \\leq \\theta \\leq \\pi .","title":"Example 3.8"},{"location":"ch3-3/#example-39","text":"A specified charge density \\sigma_0(\\theta) is glued over the surface of a spherical shell of radius R . Find the resulting potential inside and outside the sphere. Solution You could, of course, do this by direct integration: V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma_0}{\\gr} \\dd{a} but separation of variables is often easier. For the interior region, we have V(r, \\theta) = \\sum_{l = 0}^\\infty A_l r^l P_l (\\cos \\theta) \\quad (r \\leq R) \\tagl{3.78} (no B_l terms - they blow up at the origin); in the exterior region V(r, \\theta) = \\sum_{l=0}^\\infty \\frac{B_l}{r^{l+1}} P_l(\\cos \\theta) \\quad (r \\geq R) \\tagl{3.79} (no A_l terms - they don't go to zero at infinity). These two functions must be joined together by the appropriate boundary conditions at the surface itself. First, the potential is continuous at r = R (Eq. 2.34): \\sum_{l=0}^\\infty A_l R^l P_l(\\cos \\theta) = \\sum_{l=0} ^\\infty \\frac{B_l}{R^{l+1}} P_l(\\cos \\theta) \\tagl{3.80} It follows that the coefficients of like Legendre polynomial are equal: B_l = A_l R^{2l+1} \\tagl{3.81} (To prove that formally, multiply both sides of \\eqref{3.80} by P_{l'} (\\cos \\theta)\\sin \\theta and integrate from 0 to \\pi , using the orthogonality relation \\eqref{3.68} .) Second, the radial derivative of V suffers a discontinuity at the surface (Eq. 2.36): \\left. \\left( \\pdv{V_{out}}{r} - \\pdv{V_{in}}{r} \\right) \\right|_{r = R} = - \\frac{1}{\\epsilon_0} \\sigma_0(\\theta) \\tagl{3.82} Thus, - \\sum_{l=0}^\\infty (l+1) \\frac{B_l}{R^{l+2}} P_l(\\cos \\theta) - \\sum_{l=0}^\\infty l A_l R^{l-1} P_l(\\cos \\theta) = - \\frac{1}{\\epsilon_0} \\sigma_0 (\\theta) or, using \\eqref{3.81} , \\sum_{l=0}^\\infty (2l+1) A_l R^{l-1} P_l(\\cos \\theta) = \\frac{1}{\\epsilon_0} \\sigma_0(\\theta) \\tagl{3.83} From here, the coefficients can be determined using Fourier's trick A_l = \\frac{1}{2 \\epsilon_0 R^{l-1}} \\int_0 ^\\pi \\sigma_0 (\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.84} Equations 3.78 and 3.79 constitute the solution to our problem, with the coefficients given by \\eqref{3.81} and \\eqref{3.84} . For instance, if \\sigma_0(\\theta) = k \\cos \\theta = k P_1 (\\cos \\theta) \\tagl{3.85} for some constant k , then all the A_l 's are zero except for l = 1 , and A_1 = \\frac{k}{2 \\epsilon_0} \\int_0 ^\\pi [P_1(\\cos \\theta)]^2 \\sin \\theta \\dd{\\theta} = \\frac{k}{3\\epsilon_0} The potential inside the sphere is therefore V(r, \\theta) = \\frac{k}{3 \\epsilon_0} r \\cos \\theta \\quad (r \\leq R) \\tagl{3.86} whereas outside the sphere V(r, \\theta) = \\frac{kR^3}{3 \\epsilon_0} \\frac{1}{r^2} \\cos \\theta \\quad (r \\geq R) \\tagl{3.87} In particular, if \\sigma_0(\\theta) is the induced charge on a metal sphere in an external field E_0(\\vu{z}) , so that k = 3 \\epsilon_0 E_0 \\eqref{3.77} , then the potential inside is E_0 r \\cos \\theta = E_0 z , and the field is -E_0 \\vu{z} - exactly right to cancel off the external field, as of course it should be. Outside the sphere the potential due to this surface charge is E_0 \\frac{R^3}{r^2} \\cos \\theta consistent with our conclusion in Example 3.8.","title":"Example 3.9"},{"location":"ch3-4/","text":"3.4: Multipole Expansion 3.4.1: Approximate Potentials at Large Distances If you are very far away from a localized charge distribution, it \"looks\" like a point charge, and the potential is - to good approximation - (1/4 \\pi \\epsilon_0) Q/r , where Q is the total charge. We have often used this as a check on formulas for V . But what if Q is zero? You might reply that the potential is then approximately zero, and of course, you're right in a sense (indeed, the potential at large r is pretty small even if Q is not zero). But we're looking for something a bit more informative than that. Example 3.10 A (physical) electric dipole consists of two equal and opposite charges (\\pm q) separated by a distance d . Find the approximate potential at points far from the dipole Solution Let \\gr_- be the distance from -q and \\gr_{+} be the distance from +q (Fig. 3.26). Then V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{\\gr_{+}} - \\frac{q}{\\gr_{-}} \\right) and (from the law of cosines), \\gr_{\\pm} ^2 = r^2 + (d/2)^2 \\mp r d \\cos \\theta = r^2 \\left( 1 \\mp \\frac{d}{r} \\cos \\theta + \\frac{d^2}{4r^2} \\right) We're interested in the regime r \\gg d , so the third term is negligible, and the binomial expansion yields \\frac{1}{\\gr_{\\pm}} \\approx \\frac{1}{r} \\left( 1 \\mp \\frac{d}{r} \\cos \\theta \\right) ^{-1/2} \\approx \\frac{1}{r} \\left( 1 \\pm \\frac{d}{2r} \\cos \\theta \\right) Thus \\frac{1}{\\gr_+} - \\frac{1}{\\gr_{-}} \\approx \\frac{d}{r^2} \\cos \\theta and hence V(r) \\approx \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd \\cos \\theta}{r^2} \\tagl{3.90} The potential of a dipole goes like 1/r^2 at large r ; as we might have anticipated, it falls off more rapidly than the potential of a point charge. If we put together a pair of equal and opposite dipoles to make a quadrupole , the potential goes like 1/r^3 ; for back-to-back quadrupoles (an octopole ), it goes like 1/r^4 , and so on. Figure 3.27 summarizes the hierarchy; for completeness I have included the electric monopole (point charge), whose potential, of course, goes like 1/r Example 3.10 pertains to a very special charge configuration. I propose now to develop a systematic expansion for the potential of any localized charge distribution, in powers of 1/r . Figure 3.28 defines the relevant variables; the potential at r is given by V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{1}{\\gr} \\rho(\\vec{r'}) \\dd{\\tau'} \\tagl{3.91} Using the law of cosines, \\gr ^2 = r^2 + (r')^2 - 2r r' \\cos \\alpha = r^2 \\left[ 1 + \\left( \\frac{r'}{r} \\right)^2 - 2 \\left( \\frac{r'}{r} \\right)\\cos \\alpha \\right] where \\alpha is the angle between \\vec{r} and \\vec{r'} . Thus, \\gr = r \\sqrt{1 + \\epsilon} \\tagl{3.92} with \\epsilon \\equiv \\left( \\frac{r'}{r} \\right)\\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right) For points well outside the charge distribution, \\epsilon is much less than 1, and this invites a binomial expansion: \\frac{1}{\\gr} = \\frac{1}{r} (1 + \\epsilon)^{-1/2} = \\frac{1}{r} \\left( 1 - \\frac{1}{2} \\epsilon + \\frac{3}{8} \\epsilon^2 - \\frac{5}{16} \\epsilon^3 + \\ldots \\right) \\tagl{3.93} or, in terms of r, r' , and \\alpha , \\begin{align*} \\frac{1}{\\gr} & = \\frac{1}{r} \\left[ 1 - \\frac{1}{2} \\left( \\frac{r'}{r} \\right) \\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right) + \\frac{3}{8} \\left( \\frac{r'}{r} \\right)^2 \\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right)^2 \\right. \\\\ & \\qquad \\left. - \\frac{5}{16} \\left( \\frac{r'}{r} \\right)^3 \\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right)^3 + \\ldots \\right] \\\\ & = \\frac{1}{r} \\left[ 1 + \\left( \\frac{r'}{r} \\right)(\\cos \\alpha) + \\left( \\frac{r'}{r} \\right) \\left( \\frac{3 \\cos ^2 \\alpha - 1}{2} \\right) \\right. \\\\ & \\qquad \\left. \\left( \\frac{r'}{r} \\right)^3 \\left( \\frac{5\\cos ^3 \\alpha - 3 \\cos \\alpha}{2} \\right) + \\ldots \\right] \\end{align*} In the last step, I have collected together like powers of (r'/r) ; surprisingly, their coefficients (the terms in parentheses) are Legendre polynomials! The remarkable result is that \\frac{1}{\\gr} = \\frac{1}{r} \\sum_{n=0}^\\infty \\left( \\frac{r'}{r} \\right)^n P_n (\\cos \\alpha) \\tagl{3.94} Substituting this back into \\eqref{3.91} , and noting that r is constant, as far as the integration is concerned, I conclude that V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{n=0} ^\\infty \\frac{1}{r^{(n+1)}} \\int (r') P_n(\\cos \\alpha) \\rho(\\vec{r'}) \\dd{\\tau'} \\tagl{3.95} or, more explicitly, V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{1}{r} \\int \\rho(\\vec{r'}) \\dd{\\tau'} + \\frac{1}{r^2} \\int r' \\cos \\alpha \\rho(\\vec{r'})\\dd{\\tau'} \\right. \\\\ + \\left. \\frac{1}{r^3} \\int (r')^2 \\left( \\frac{3}{2} \\cos^2 \\alpha - \\frac{1}{2} \\right) \\rho(\\vec{r'}) \\dd{\\tau'} + \\ldots \\right] \\tagl{3.96} This is the desired result - the multipole expansion of V in powers of 1/r . The first term (n=0) is the monopole contribution (it goes like 1/r ); the second (n=1) is the dipole (it goes like 1/r^2 ); the third is quadrupole; the fourth octopole, and so on. Remember that \\alpha is the angle between \\vec{r} and \\vec{r'} , so the integrals depend on the direction to the field point. If you are interested in the potential along the z' axis (or - putting it the other way round - if you orient your \\vec{r'} coordinates so the z' axis lies along \\vec{r} ), then \\alpha is the usual polar angle \\theta' . As it stands, \\eqref{3.95} is exact , but it is useful primarily as an approximation scheme: the lowest nonzero term in the expansion provides the approximate potential at large r , and the successive terms tell us how to improve the approximation if greater precision is required. 3.4.2: The Monopole and Dipole Terms Ordinarily, the multipole expansion is dominated (at large r) by the monopole term: V_{mon} (\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q}{r} where Q =\\int \\rho \\dd{\\tau} is the total charge of the configuration. This is just what we expect for the approximate potential at large distances from the charge. For a point charge at the origin, V_{mon} is the exact potential, not merely a first approximation at large r; in this case, all the higher multipoles vanish. If the total charge is zero, the dominant term in the potential will be the dipole (unless, of course, it also vanishes): V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\int r' \\cos \\alpha \\rho(\\vec{r'}) \\dd{\\tau'} Since \\alpha is the angle between r' and r (Fig 2.38), r' \\cos \\alpha = \\vu{r} \\cdot \\vec{r'} and the dipole potential can be written more succinctly: V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\vu{r} \\cdot \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} This integral (which does not depend on \\vec{r} ) is called the dipole moment of the distribution: \\vec{p} \\equiv \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} \\tagl{3.98} and the dipole contribution to the potential simplifies to V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} \\tagl{3.99} The dipole moment is determined by the geometry (size, shape, and density) of the charge distribution. \\eqref{3.98} translates in the usual way (Sect 2.1.4) for point, line, and surface charges. Thus, the dipole moment for a collection of point charges is \\vec{p} = \\sum_{i=1} ^n q_i \\vec{r'}_i \\tagl{3.100} For a physical dipole (equal and opposite charges \\pm q ), \\vec{p} = q\\vec{r'_+} - q \\vec{r_- ' } = q(\\vec{r' _+} - \\vec{r'_-}) = q \\vec{d} \\tagl{3.101} where \\vec{d} is the vector from the negative charge to the positive one (Fig. 3.29). Is this consistent with what we got in Example 3.10? Yes: If you put \\eqref{3.101} into \\eqref{3.99} , you recover \\eqref{3.90} . Notice, however, that this is only the approximate potential of the physical dipole - evidently there are higher multipole contributions. Of course, as you go farther and farther away, V_{dip} becomes a better and better approximation, since the higher terms die off more rapidly with increasing r . By the same token, at a fixed r the dipole approximation improves as you shrink the separation distance d . To construct a perfect dipole whose potential is given exactly by \\eqref{3.99} , you'd have to let d approach zero. Unfortunately you then lose the dipole term too, unless you simultaneously arrange for q to go to infinity! A physical dipole becomes a pure dipole, then, in the rather artificial limit d \\rightarrow 0. q \\rightarrow \\infty , with the product qd = p held fixed. When someone uses the word \"dipole,\" you can't always tell whether they mean a physical dipole (with finite separation between the charges) or an ideal dipole. If in doubt, assume that d is small enough that you can safely apply \\eqref{3.99} . Dipole moments are vectors , and they add accordingly: if you have two dipoles \\vec{p_1} and \\vec{p_2} , the total dipole moment is \\vec{p_1} + \\vec{p_2} . For instance, with four charges at the corners of a square, as shown in Fig. 3.30, the net dipole moment is zero. You can see this by combining the charges in pairs or by adding up the four contributions individually using \\eqref{3.100} . This is a quadrupole , as I indicated earlier, and its potential is dominated by the quadrupole term in the multipole expansion. 3.4.3: Origin of Coordinates in Multipole Expansions I mentioned earlier that a point charge at the origin constitutes a \"pure\" monopole. If it is not at the origin, it's no longer a pure monopole. For instance, the charge in Fig. 3.32 has a dipole moment \\vec{p} = q d \\vu{y} , and a corresponding dipole term in its potential. The monopole potential (1/4 \\pi \\epsilon_0) q/r is not quite correct for this configuration; rather, the exact potential is (1/4 \\pi \\epsilon_0) q/\\gr . The multipole expansion is, remember, a series in inverse powers of r (the distance to the origin), and when we expand 1/\\gr , we get all powers, not just the first. So moving the origin (or, what amounts to the same thing, moving the charge) can radically alter a multipole expansion. The monopole moment Q does not change, since the total charge is obviously independent of the coordinate system. (In Fig. 3.32, the monopole term was unaffected when we moved q away from the origin - it's just that it was no longer the whole story: a dipole term - and for that matter all higher poles - appeared as well.) Ordinarily, the dipole moment does change when you shift the origin, but there is an important exception: If the total charge is zero, then the dipole moment is independent of the choice of origin. For suppose we displace the origin by an amount \\vec{a} (Fig. 3.33). The new dipole moment is then \\begin{align*} \\vec{p_2} & = \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} = \\int (\\vec{r'} - \\vec{a} ) \\rho (\\vec{r'}) \\dd{\\tau'} \\\\ & = \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} - \\vec{a} \\int \\rho(\\vec{r'}) \\dd{\\tau'} = \\vec{p} - Q \\vec{a} \\end{align*} In particular, if Q = 0 , the \\vec{p_2} = \\vec{p} . So if someone asks for the dipole moment in Fig 3.34(a), you can answer with confidence \" q \\vec{d} ,\" but if you're asked for the dipole moment in Fig 3.34(b), the appropriate response would be \"With respect to what origin?\" 3.4.4: The Electric Field of a Dipole So far we have only worked with potentials . Now I would like to calculate the electric field of a (perfect) dipole. If we choose coordinates so that \\vec{p} is at the origin and points in the z direction (Fig. 3.36), then the potential at r, \\theta is \\eqref{3.99} : V_{dip} (r, \\theta) = \\frac{\\vu{r} \\cdot \\vec{p}}{4 \\pi \\epsilon_0 r^2} = \\frac{p \\cos \\theta}{4 \\pi \\epsilon_0 r^2} \\tagl{3.102} To get the field, we take the negative gradient of V : \\begin{align*} E_r & = - \\pdv{V}{r} = \\frac{2 p \\cos \\theta}{4 \\pi \\epsilon_0 r^3} \\\\ E_\\theta & = - \\frac{1}{r} \\pdv{V}{\\theta} = \\frac{p \\sin \\theta}{4 \\pi \\epsilon_0 r^3} \\\\ E_\\phi & = - \\frac{1}{r \\sin \\theta} \\pdv{V}{\\phi} = 0 \\end{align*} Thus, \\vec{E_{dip}} (r, \\theta) = \\frac{p}{4 \\pi \\epsilon_0 r^3}(2 \\cos \\theta \\vu{r} + \\sin \\theta \\vu{\\theta}) \\tagl{3.103} This formula makes explicit reference to a particular coordinate system (spherical) and assumes a particular orientation for \\vec{p} (along z). It can be recast in a coordinate-free form, analogous to the potential in \\eqref{3.99} - See problem 3.36. Notice that the dipole falls off as the inverse cube of r; the monopole field (Q / 4 \\pi \\epsilon_0 r^2) \\vu{r} goes as the inverse square, of course. Quadrupole fields go like 1/r^4 , octopole like 1/r^5 , and so on. (This merely reflects how the respective potentials fall off - the gradient introduces another factor of 1/r ). Figure 3.37(a) shows the field lines of a \"pure\" dipole \\eqref{3.103} . For comparison, I have also sketched the field lines for a \"physical\" dipole, in Fig 3.37(b). Notice how similar the two pictures become if you blot out the central region; up close, however, they are entirely different. Only for points r \\gg d does \\eqref{3.103} represent a valid approximation to the field of a physical dipole. As I mentioned earlier, this regime can be reached either by going to large r or by squeezing the charges very close together.","title":"3.4 - Multipole Expansion"},{"location":"ch3-4/#34-multipole-expansion","text":"","title":"3.4: Multipole Expansion"},{"location":"ch3-4/#341-approximate-potentials-at-large-distances","text":"If you are very far away from a localized charge distribution, it \"looks\" like a point charge, and the potential is - to good approximation - (1/4 \\pi \\epsilon_0) Q/r , where Q is the total charge. We have often used this as a check on formulas for V . But what if Q is zero? You might reply that the potential is then approximately zero, and of course, you're right in a sense (indeed, the potential at large r is pretty small even if Q is not zero). But we're looking for something a bit more informative than that.","title":"3.4.1: Approximate Potentials at Large Distances"},{"location":"ch3-4/#example-310","text":"A (physical) electric dipole consists of two equal and opposite charges (\\pm q) separated by a distance d . Find the approximate potential at points far from the dipole Solution Let \\gr_- be the distance from -q and \\gr_{+} be the distance from +q (Fig. 3.26). Then V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{\\gr_{+}} - \\frac{q}{\\gr_{-}} \\right) and (from the law of cosines), \\gr_{\\pm} ^2 = r^2 + (d/2)^2 \\mp r d \\cos \\theta = r^2 \\left( 1 \\mp \\frac{d}{r} \\cos \\theta + \\frac{d^2}{4r^2} \\right) We're interested in the regime r \\gg d , so the third term is negligible, and the binomial expansion yields \\frac{1}{\\gr_{\\pm}} \\approx \\frac{1}{r} \\left( 1 \\mp \\frac{d}{r} \\cos \\theta \\right) ^{-1/2} \\approx \\frac{1}{r} \\left( 1 \\pm \\frac{d}{2r} \\cos \\theta \\right) Thus \\frac{1}{\\gr_+} - \\frac{1}{\\gr_{-}} \\approx \\frac{d}{r^2} \\cos \\theta and hence V(r) \\approx \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd \\cos \\theta}{r^2} \\tagl{3.90} The potential of a dipole goes like 1/r^2 at large r ; as we might have anticipated, it falls off more rapidly than the potential of a point charge. If we put together a pair of equal and opposite dipoles to make a quadrupole , the potential goes like 1/r^3 ; for back-to-back quadrupoles (an octopole ), it goes like 1/r^4 , and so on. Figure 3.27 summarizes the hierarchy; for completeness I have included the electric monopole (point charge), whose potential, of course, goes like 1/r Example 3.10 pertains to a very special charge configuration. I propose now to develop a systematic expansion for the potential of any localized charge distribution, in powers of 1/r . Figure 3.28 defines the relevant variables; the potential at r is given by V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{1}{\\gr} \\rho(\\vec{r'}) \\dd{\\tau'} \\tagl{3.91} Using the law of cosines, \\gr ^2 = r^2 + (r')^2 - 2r r' \\cos \\alpha = r^2 \\left[ 1 + \\left( \\frac{r'}{r} \\right)^2 - 2 \\left( \\frac{r'}{r} \\right)\\cos \\alpha \\right] where \\alpha is the angle between \\vec{r} and \\vec{r'} . Thus, \\gr = r \\sqrt{1 + \\epsilon} \\tagl{3.92} with \\epsilon \\equiv \\left( \\frac{r'}{r} \\right)\\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right) For points well outside the charge distribution, \\epsilon is much less than 1, and this invites a binomial expansion: \\frac{1}{\\gr} = \\frac{1}{r} (1 + \\epsilon)^{-1/2} = \\frac{1}{r} \\left( 1 - \\frac{1}{2} \\epsilon + \\frac{3}{8} \\epsilon^2 - \\frac{5}{16} \\epsilon^3 + \\ldots \\right) \\tagl{3.93} or, in terms of r, r' , and \\alpha , \\begin{align*} \\frac{1}{\\gr} & = \\frac{1}{r} \\left[ 1 - \\frac{1}{2} \\left( \\frac{r'}{r} \\right) \\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right) + \\frac{3}{8} \\left( \\frac{r'}{r} \\right)^2 \\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right)^2 \\right. \\\\ & \\qquad \\left. - \\frac{5}{16} \\left( \\frac{r'}{r} \\right)^3 \\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right)^3 + \\ldots \\right] \\\\ & = \\frac{1}{r} \\left[ 1 + \\left( \\frac{r'}{r} \\right)(\\cos \\alpha) + \\left( \\frac{r'}{r} \\right) \\left( \\frac{3 \\cos ^2 \\alpha - 1}{2} \\right) \\right. \\\\ & \\qquad \\left. \\left( \\frac{r'}{r} \\right)^3 \\left( \\frac{5\\cos ^3 \\alpha - 3 \\cos \\alpha}{2} \\right) + \\ldots \\right] \\end{align*} In the last step, I have collected together like powers of (r'/r) ; surprisingly, their coefficients (the terms in parentheses) are Legendre polynomials! The remarkable result is that \\frac{1}{\\gr} = \\frac{1}{r} \\sum_{n=0}^\\infty \\left( \\frac{r'}{r} \\right)^n P_n (\\cos \\alpha) \\tagl{3.94} Substituting this back into \\eqref{3.91} , and noting that r is constant, as far as the integration is concerned, I conclude that V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{n=0} ^\\infty \\frac{1}{r^{(n+1)}} \\int (r') P_n(\\cos \\alpha) \\rho(\\vec{r'}) \\dd{\\tau'} \\tagl{3.95} or, more explicitly, V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{1}{r} \\int \\rho(\\vec{r'}) \\dd{\\tau'} + \\frac{1}{r^2} \\int r' \\cos \\alpha \\rho(\\vec{r'})\\dd{\\tau'} \\right. \\\\ + \\left. \\frac{1}{r^3} \\int (r')^2 \\left( \\frac{3}{2} \\cos^2 \\alpha - \\frac{1}{2} \\right) \\rho(\\vec{r'}) \\dd{\\tau'} + \\ldots \\right] \\tagl{3.96} This is the desired result - the multipole expansion of V in powers of 1/r . The first term (n=0) is the monopole contribution (it goes like 1/r ); the second (n=1) is the dipole (it goes like 1/r^2 ); the third is quadrupole; the fourth octopole, and so on. Remember that \\alpha is the angle between \\vec{r} and \\vec{r'} , so the integrals depend on the direction to the field point. If you are interested in the potential along the z' axis (or - putting it the other way round - if you orient your \\vec{r'} coordinates so the z' axis lies along \\vec{r} ), then \\alpha is the usual polar angle \\theta' . As it stands, \\eqref{3.95} is exact , but it is useful primarily as an approximation scheme: the lowest nonzero term in the expansion provides the approximate potential at large r , and the successive terms tell us how to improve the approximation if greater precision is required.","title":"Example 3.10"},{"location":"ch3-4/#342-the-monopole-and-dipole-terms","text":"Ordinarily, the multipole expansion is dominated (at large r) by the monopole term: V_{mon} (\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q}{r} where Q =\\int \\rho \\dd{\\tau} is the total charge of the configuration. This is just what we expect for the approximate potential at large distances from the charge. For a point charge at the origin, V_{mon} is the exact potential, not merely a first approximation at large r; in this case, all the higher multipoles vanish. If the total charge is zero, the dominant term in the potential will be the dipole (unless, of course, it also vanishes): V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\int r' \\cos \\alpha \\rho(\\vec{r'}) \\dd{\\tau'} Since \\alpha is the angle between r' and r (Fig 2.38), r' \\cos \\alpha = \\vu{r} \\cdot \\vec{r'} and the dipole potential can be written more succinctly: V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\vu{r} \\cdot \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} This integral (which does not depend on \\vec{r} ) is called the dipole moment of the distribution: \\vec{p} \\equiv \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} \\tagl{3.98} and the dipole contribution to the potential simplifies to V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} \\tagl{3.99} The dipole moment is determined by the geometry (size, shape, and density) of the charge distribution. \\eqref{3.98} translates in the usual way (Sect 2.1.4) for point, line, and surface charges. Thus, the dipole moment for a collection of point charges is \\vec{p} = \\sum_{i=1} ^n q_i \\vec{r'}_i \\tagl{3.100} For a physical dipole (equal and opposite charges \\pm q ), \\vec{p} = q\\vec{r'_+} - q \\vec{r_- ' } = q(\\vec{r' _+} - \\vec{r'_-}) = q \\vec{d} \\tagl{3.101} where \\vec{d} is the vector from the negative charge to the positive one (Fig. 3.29). Is this consistent with what we got in Example 3.10? Yes: If you put \\eqref{3.101} into \\eqref{3.99} , you recover \\eqref{3.90} . Notice, however, that this is only the approximate potential of the physical dipole - evidently there are higher multipole contributions. Of course, as you go farther and farther away, V_{dip} becomes a better and better approximation, since the higher terms die off more rapidly with increasing r . By the same token, at a fixed r the dipole approximation improves as you shrink the separation distance d . To construct a perfect dipole whose potential is given exactly by \\eqref{3.99} , you'd have to let d approach zero. Unfortunately you then lose the dipole term too, unless you simultaneously arrange for q to go to infinity! A physical dipole becomes a pure dipole, then, in the rather artificial limit d \\rightarrow 0. q \\rightarrow \\infty , with the product qd = p held fixed. When someone uses the word \"dipole,\" you can't always tell whether they mean a physical dipole (with finite separation between the charges) or an ideal dipole. If in doubt, assume that d is small enough that you can safely apply \\eqref{3.99} . Dipole moments are vectors , and they add accordingly: if you have two dipoles \\vec{p_1} and \\vec{p_2} , the total dipole moment is \\vec{p_1} + \\vec{p_2} . For instance, with four charges at the corners of a square, as shown in Fig. 3.30, the net dipole moment is zero. You can see this by combining the charges in pairs or by adding up the four contributions individually using \\eqref{3.100} . This is a quadrupole , as I indicated earlier, and its potential is dominated by the quadrupole term in the multipole expansion.","title":"3.4.2: The Monopole and Dipole Terms"},{"location":"ch3-4/#343-origin-of-coordinates-in-multipole-expansions","text":"I mentioned earlier that a point charge at the origin constitutes a \"pure\" monopole. If it is not at the origin, it's no longer a pure monopole. For instance, the charge in Fig. 3.32 has a dipole moment \\vec{p} = q d \\vu{y} , and a corresponding dipole term in its potential. The monopole potential (1/4 \\pi \\epsilon_0) q/r is not quite correct for this configuration; rather, the exact potential is (1/4 \\pi \\epsilon_0) q/\\gr . The multipole expansion is, remember, a series in inverse powers of r (the distance to the origin), and when we expand 1/\\gr , we get all powers, not just the first. So moving the origin (or, what amounts to the same thing, moving the charge) can radically alter a multipole expansion. The monopole moment Q does not change, since the total charge is obviously independent of the coordinate system. (In Fig. 3.32, the monopole term was unaffected when we moved q away from the origin - it's just that it was no longer the whole story: a dipole term - and for that matter all higher poles - appeared as well.) Ordinarily, the dipole moment does change when you shift the origin, but there is an important exception: If the total charge is zero, then the dipole moment is independent of the choice of origin. For suppose we displace the origin by an amount \\vec{a} (Fig. 3.33). The new dipole moment is then \\begin{align*} \\vec{p_2} & = \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} = \\int (\\vec{r'} - \\vec{a} ) \\rho (\\vec{r'}) \\dd{\\tau'} \\\\ & = \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} - \\vec{a} \\int \\rho(\\vec{r'}) \\dd{\\tau'} = \\vec{p} - Q \\vec{a} \\end{align*} In particular, if Q = 0 , the \\vec{p_2} = \\vec{p} . So if someone asks for the dipole moment in Fig 3.34(a), you can answer with confidence \" q \\vec{d} ,\" but if you're asked for the dipole moment in Fig 3.34(b), the appropriate response would be \"With respect to what origin?\"","title":"3.4.3: Origin of Coordinates in Multipole Expansions"},{"location":"ch3-4/#344-the-electric-field-of-a-dipole","text":"So far we have only worked with potentials . Now I would like to calculate the electric field of a (perfect) dipole. If we choose coordinates so that \\vec{p} is at the origin and points in the z direction (Fig. 3.36), then the potential at r, \\theta is \\eqref{3.99} : V_{dip} (r, \\theta) = \\frac{\\vu{r} \\cdot \\vec{p}}{4 \\pi \\epsilon_0 r^2} = \\frac{p \\cos \\theta}{4 \\pi \\epsilon_0 r^2} \\tagl{3.102} To get the field, we take the negative gradient of V : \\begin{align*} E_r & = - \\pdv{V}{r} = \\frac{2 p \\cos \\theta}{4 \\pi \\epsilon_0 r^3} \\\\ E_\\theta & = - \\frac{1}{r} \\pdv{V}{\\theta} = \\frac{p \\sin \\theta}{4 \\pi \\epsilon_0 r^3} \\\\ E_\\phi & = - \\frac{1}{r \\sin \\theta} \\pdv{V}{\\phi} = 0 \\end{align*} Thus, \\vec{E_{dip}} (r, \\theta) = \\frac{p}{4 \\pi \\epsilon_0 r^3}(2 \\cos \\theta \\vu{r} + \\sin \\theta \\vu{\\theta}) \\tagl{3.103} This formula makes explicit reference to a particular coordinate system (spherical) and assumes a particular orientation for \\vec{p} (along z). It can be recast in a coordinate-free form, analogous to the potential in \\eqref{3.99} - See problem 3.36. Notice that the dipole falls off as the inverse cube of r; the monopole field (Q / 4 \\pi \\epsilon_0 r^2) \\vu{r} goes as the inverse square, of course. Quadrupole fields go like 1/r^4 , octopole like 1/r^5 , and so on. (This merely reflects how the respective potentials fall off - the gradient introduces another factor of 1/r ). Figure 3.37(a) shows the field lines of a \"pure\" dipole \\eqref{3.103} . For comparison, I have also sketched the field lines for a \"physical\" dipole, in Fig 3.37(b). Notice how similar the two pictures become if you blot out the central region; up close, however, they are entirely different. Only for points r \\gg d does \\eqref{3.103} represent a valid approximation to the field of a physical dipole. As I mentioned earlier, this regime can be reached either by going to large r or by squeezing the charges very close together.","title":"3.4.4: The Electric Field of a Dipole"},{"location":"ch4-1/","text":"4.1: Polarization 4.1.1: Dielectrics This chapter is all about what happens to an electric field when you take matter into account. Matter, of course, comes in many varieties - phase, composition, state, etc. - and depending upon which type of matter we're dealing with, the electrostatic field response can be very different. Nevertheless, most everyday objects belong (at least, to good approximation) to one of two large classes: conductors and insulators (or dielectrics ). We have already gone over what happens to an electrostatic field in a conductor; the \"unlimited\" free charges within a conductor distribute themselves through the material so as to form an equipotential. In practice, this usually means that many electrons (one or two per atom, in a typical metal) are not associated with a particular nucleus, but roam around at will. In dielectrics, by contrast, all charges are attached to specific atoms or molecules - they cannot escape their leash, and can only move a bit within the atom or molecule. Such microscopic displacements are not as dramatic as the wholesale rearrangement of charge in a conductor, but their cumulative effects account for the characteristic behavior of dielectric materials. There are actually two principal mechanisms by which electric fields can distort the charge distribution of a dielectric atom or molecule: stretching and rotating. In the next two sections I'll discuss these processes. 4.1.2: Induced Dipoles Say we have a totally neutral atom and place it in an electric field E ? What happens? At first guess, you might think \"Nothing at all! The atom is not charged, so the field has no effect on it.\" That's incorrect. Although the atom as a whole is electrically neutral (just like the dipoles we looked at in the last chapter), there is a positively charged core (the nucleus) and negatively charged electron(s) surrounding it. These two regions of charge within the atom are influenced by the field: the nucleus is pushed in the direction of the field, and the electrons the opposite way. In principle, if the field is large enough, it can pull the atom apart completely, \"ionizing\" it (the substance then becomes a conductor). With less extreme fields, however, an equilibrium is soon established, for if the center of the electron cloud does not coincide with the nucleus, these positive and negative charges attract one another, and that holds the atom together. The two opposing forces - E pulling the electrons and nucleus apart, and their mutual attraction drawing them back together - reach a balance, leaving the atom polarized , with plus charge shifted slightly one way, and minus the other. The atom now has a tiny dipole moment p , which points in the same direction as E . Typically, this induced dipole moment is approximately proportional to the field (as long as the latter is not too strong): \\vec{p} = \\alpha \\vec{E} \\tagl{4.1} The constant of proportionality \\alpha is called atomic polarizability . Its value depends on the detailed structure of the atom in question. Table 4.1 lists some experimentally determined atomic polarizabilities. Example 4.1 A primitive model for an atom consists of a point nucleus (+q) surrounded by a uniformly charged spherical cloud (-q) of radius a (Fig 4.1). Calculate the atomic polarizability of such an atom. Solution In the presence of an external field E , the nucleus will be shifted slightly to the right and the electron cloud to the left, as shown in Fig 4.2. (Because the actual displacements involved are extremely small, as you'll see in Prob 4.1, it is reasonable to assume that the electron cloud retains its spherical shape.) Say that equilibrium occurs when the nucleus is displaced a distance d from the center of the sphere. At that point, the external field pushing the nucleus to the right exactly balances the internal field pulling it to the left: E = E_e , where E_e is the field produced by the electron cloud. Now the field at a distance d from the center of a uniformly charged sphere is E_e = \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd}{a^3} At equilibrium, then E = \\frac{1}{4 \\pi \\epsilon_0 } \\frac{qd}{a^3}, \\quad \\text{ or } p = qd = (4 \\pi \\epsilon_0 a^3 ) E The atomic polarizability is therefore \\alpha = 4 \\pi \\epsilon_0 a^3 = 3 \\epsilon_0 v \\tagl{4.2} where v is the volume of the atom. Although this atomic model is extremely crude, the result \\eqref{4.2} is not too bad - it's accurate to within a factor of four or so for many simple atoms. For molecules the situation is not quite so simple, because frequently they polarize more readily in some directions than in others. Carbon dioxide (Fig 4.3), for instance, has a polarizability of 4.5 \\times 10^{-40} when you apply the field along the axis of the molecule, but only 2 \\times 10^{-40} for fields perpendicular to this direction. When the field is at some angle to the axis, you must first resolve it into parallel and perpendicular components, and multiply each component by the pertinent polarizability \\vec{p} = a_{\\perp} E_{\\perp} + \\alpha_{\\parallel} E_{\\parallel} In this case, the induced dipole moment may not even be in the same direction as E . And CO_2 is relatively simple, as molecules go, since at least the atoms arrange themselves in a straight line; for a completely asymmetrical molecule, \\eqref{4.1} is replaced by the most general linear relation between E and p : \\begin{align*} p_x = \\alpha_{xx} E_x + \\alpha_{xy} E_y + \\alpha_{xz} E_z\\\\ p_y = \\alpha_{yx} E_x + \\alpha_{yy} E_y + \\alpha_{yz} E_z\\\\ p_z = \\alpha_{zx} E_x + \\alpha_{zy} E_y + \\alpha_{zz} E_z \\end{align*} \\tagl{4.3} The set of nine constants \\alpha_{ij} constitute the polarizability tensor for the molecule. Their values depend on the orientation of the axes you use, though it is always possible to choose \"principal\" axes such that all off-diagonal terms vanish, leaving just three nonzero polarizabilities. 4.1.3: Alignment of Polar Molecules The neutral atom discussed in section 4.1.2 had no dipole moment to start with - p was entirely induced by the applied field. Some molecules have built-in, permanent dipole moments. In the water molecule, for example, the electrons tend to cluster around the oxygen atom (Fig 4.4), and since the molecule is bent at 105^{\\circ} , this leaves a negative charge at the vertex and a positive charge on the opposite side. (The dipole moment of water is unusually large: 6.1 \\times 10^{-30} C \\cdot m ; in fact, this is what accounts for its effectiveness as a solvent.) What happens when such molecules (called polar molecules ) are placed in an electric field? If the field is uniform, the force on the positive end, \\vec{F_+} = q \\vec{E} , exactly cancels the force on the negative end, \\vec{F_-} = - q \\vec{E} (Fig 4.5). However, there will be a torque: \\vec{N } = (\\vec{r_{+}} \\cross \\vec{F_+}) + (\\vec{r_{-}} \\cross \\vec{F_{-}}) \\\\ = \\left[ ( \\vec{d}/2) \\cross (q \\vec{E}) \\right] + \\left[ ( -\\vec{d}/2) \\cross (- q \\vec{E}) \\right] = q \\vec{d} \\cross \\vec{E} Thus a dipole \\vec{p} = q \\vec{d} in a uniform field \\vec{E} experiences a torque \\vec{N} = \\vec{p} \\cross \\vec{E} \\tagl{4.4} Notice that N is in such a direction as to line p up parallel to E ; a polar molecule that is free to rotate will swing around until it points in the direction of the applied field. If the field is nonuniform, so that \\vec{F_{+}} does not exactly balance \\vec{F_-} , there will be a net force on the dipole, in addition to the torque. Of course, E must change rather abruptly for there to be significant variation in the space of one molecule, so this is not ordinarily a major consideration in discussing the behavior of dielectrics. Nevertheless, the formula for the force on a dipole in a nonuniform field is of some interest: \\vec{F} = \\vec{F_{+}} + \\vec{F_-} = q(\\vec{E_+} - \\vec{E_-}) = q(\\Delta \\vec{E}) where \\Delta \\vec{E} represents the difference between the field at the plus end and the field at the minus end. Assuming the dipole is very short, we may use Eq 1.35 to approximate the small change in E \\Delta \\vec{E} = (\\vec{d} \\cdot \\grad ) \\vec{E} and therefore \\vec{F} = ( \\vec{p} \\cdot \\grad) \\vec{E} \\tagl{4.5} For a \"perfect\" dipole of infinitesimal length, \\eqref{4.4} gives the torque about the center of the dipole even in a nonuniform field; about any other point, \\vec{N} = ( \\vec{p} \\cross \\vec{E}) + (\\vec{r} \\cross \\vec{F}) . 4.1.4: Polarization In the previous two sections, we have considered the effect of an external electric field on an individual atom or molecule. We are now in a position to answer (quantitatively) the original question: What happens to a piece of dielectric material when it is placed in an electric field? If the substance consists of neutral atoms (or nonpolar molecules), the field will induce in each a tiny dipole moment, pointing in the same direction as the field. If the material is made up of polar molecules, each permanent dipole will experience a torque, tending to line it up along the field direction. (Random thermal motions compete with this process, so the alignment is never complete, especially at higher temperatures, and disappears almost at once when the field is removed.) Notice that these two mechanisms produce the same basic result: a lot of little dipoles pointing along the direction of the field - the material becomes polarized . A convenient measure of this effect is \\vec{P} = \\text{ dipole moment per unit volume } which is called the polarization . From now on we shall not worry much about how the polarization got there. Actually, the two mechanisms I described are not as clear-cut as I tried to pretend. Even in polar molecules there will be some polarization by displacement (though generally it is a lot easier to rotate a molecule than to stretch it, so the second mechanism dominates). It's even possible in some materials to \"freeze in\" polarization, so that it persists after the field is removed. But let's forget for a moment about the cause of the polarization, and let's study the field that a chunk of polarized material itself produces. Then in section 4.3 we'll put it all together: the original field, which was responsible for P , plus the new field, which is due to P .","title":"4.1 - Polarization"},{"location":"ch4-1/#41-polarization","text":"","title":"4.1: Polarization"},{"location":"ch4-1/#411-dielectrics","text":"This chapter is all about what happens to an electric field when you take matter into account. Matter, of course, comes in many varieties - phase, composition, state, etc. - and depending upon which type of matter we're dealing with, the electrostatic field response can be very different. Nevertheless, most everyday objects belong (at least, to good approximation) to one of two large classes: conductors and insulators (or dielectrics ). We have already gone over what happens to an electrostatic field in a conductor; the \"unlimited\" free charges within a conductor distribute themselves through the material so as to form an equipotential. In practice, this usually means that many electrons (one or two per atom, in a typical metal) are not associated with a particular nucleus, but roam around at will. In dielectrics, by contrast, all charges are attached to specific atoms or molecules - they cannot escape their leash, and can only move a bit within the atom or molecule. Such microscopic displacements are not as dramatic as the wholesale rearrangement of charge in a conductor, but their cumulative effects account for the characteristic behavior of dielectric materials. There are actually two principal mechanisms by which electric fields can distort the charge distribution of a dielectric atom or molecule: stretching and rotating. In the next two sections I'll discuss these processes.","title":"4.1.1: Dielectrics"},{"location":"ch4-1/#412-induced-dipoles","text":"Say we have a totally neutral atom and place it in an electric field E ? What happens? At first guess, you might think \"Nothing at all! The atom is not charged, so the field has no effect on it.\" That's incorrect. Although the atom as a whole is electrically neutral (just like the dipoles we looked at in the last chapter), there is a positively charged core (the nucleus) and negatively charged electron(s) surrounding it. These two regions of charge within the atom are influenced by the field: the nucleus is pushed in the direction of the field, and the electrons the opposite way. In principle, if the field is large enough, it can pull the atom apart completely, \"ionizing\" it (the substance then becomes a conductor). With less extreme fields, however, an equilibrium is soon established, for if the center of the electron cloud does not coincide with the nucleus, these positive and negative charges attract one another, and that holds the atom together. The two opposing forces - E pulling the electrons and nucleus apart, and their mutual attraction drawing them back together - reach a balance, leaving the atom polarized , with plus charge shifted slightly one way, and minus the other. The atom now has a tiny dipole moment p , which points in the same direction as E . Typically, this induced dipole moment is approximately proportional to the field (as long as the latter is not too strong): \\vec{p} = \\alpha \\vec{E} \\tagl{4.1} The constant of proportionality \\alpha is called atomic polarizability . Its value depends on the detailed structure of the atom in question. Table 4.1 lists some experimentally determined atomic polarizabilities.","title":"4.1.2: Induced Dipoles"},{"location":"ch4-1/#example-41","text":"A primitive model for an atom consists of a point nucleus (+q) surrounded by a uniformly charged spherical cloud (-q) of radius a (Fig 4.1). Calculate the atomic polarizability of such an atom. Solution In the presence of an external field E , the nucleus will be shifted slightly to the right and the electron cloud to the left, as shown in Fig 4.2. (Because the actual displacements involved are extremely small, as you'll see in Prob 4.1, it is reasonable to assume that the electron cloud retains its spherical shape.) Say that equilibrium occurs when the nucleus is displaced a distance d from the center of the sphere. At that point, the external field pushing the nucleus to the right exactly balances the internal field pulling it to the left: E = E_e , where E_e is the field produced by the electron cloud. Now the field at a distance d from the center of a uniformly charged sphere is E_e = \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd}{a^3} At equilibrium, then E = \\frac{1}{4 \\pi \\epsilon_0 } \\frac{qd}{a^3}, \\quad \\text{ or } p = qd = (4 \\pi \\epsilon_0 a^3 ) E The atomic polarizability is therefore \\alpha = 4 \\pi \\epsilon_0 a^3 = 3 \\epsilon_0 v \\tagl{4.2} where v is the volume of the atom. Although this atomic model is extremely crude, the result \\eqref{4.2} is not too bad - it's accurate to within a factor of four or so for many simple atoms. For molecules the situation is not quite so simple, because frequently they polarize more readily in some directions than in others. Carbon dioxide (Fig 4.3), for instance, has a polarizability of 4.5 \\times 10^{-40} when you apply the field along the axis of the molecule, but only 2 \\times 10^{-40} for fields perpendicular to this direction. When the field is at some angle to the axis, you must first resolve it into parallel and perpendicular components, and multiply each component by the pertinent polarizability \\vec{p} = a_{\\perp} E_{\\perp} + \\alpha_{\\parallel} E_{\\parallel} In this case, the induced dipole moment may not even be in the same direction as E . And CO_2 is relatively simple, as molecules go, since at least the atoms arrange themselves in a straight line; for a completely asymmetrical molecule, \\eqref{4.1} is replaced by the most general linear relation between E and p : \\begin{align*} p_x = \\alpha_{xx} E_x + \\alpha_{xy} E_y + \\alpha_{xz} E_z\\\\ p_y = \\alpha_{yx} E_x + \\alpha_{yy} E_y + \\alpha_{yz} E_z\\\\ p_z = \\alpha_{zx} E_x + \\alpha_{zy} E_y + \\alpha_{zz} E_z \\end{align*} \\tagl{4.3} The set of nine constants \\alpha_{ij} constitute the polarizability tensor for the molecule. Their values depend on the orientation of the axes you use, though it is always possible to choose \"principal\" axes such that all off-diagonal terms vanish, leaving just three nonzero polarizabilities.","title":"Example 4.1"},{"location":"ch4-1/#413-alignment-of-polar-molecules","text":"The neutral atom discussed in section 4.1.2 had no dipole moment to start with - p was entirely induced by the applied field. Some molecules have built-in, permanent dipole moments. In the water molecule, for example, the electrons tend to cluster around the oxygen atom (Fig 4.4), and since the molecule is bent at 105^{\\circ} , this leaves a negative charge at the vertex and a positive charge on the opposite side. (The dipole moment of water is unusually large: 6.1 \\times 10^{-30} C \\cdot m ; in fact, this is what accounts for its effectiveness as a solvent.) What happens when such molecules (called polar molecules ) are placed in an electric field? If the field is uniform, the force on the positive end, \\vec{F_+} = q \\vec{E} , exactly cancels the force on the negative end, \\vec{F_-} = - q \\vec{E} (Fig 4.5). However, there will be a torque: \\vec{N } = (\\vec{r_{+}} \\cross \\vec{F_+}) + (\\vec{r_{-}} \\cross \\vec{F_{-}}) \\\\ = \\left[ ( \\vec{d}/2) \\cross (q \\vec{E}) \\right] + \\left[ ( -\\vec{d}/2) \\cross (- q \\vec{E}) \\right] = q \\vec{d} \\cross \\vec{E} Thus a dipole \\vec{p} = q \\vec{d} in a uniform field \\vec{E} experiences a torque \\vec{N} = \\vec{p} \\cross \\vec{E} \\tagl{4.4} Notice that N is in such a direction as to line p up parallel to E ; a polar molecule that is free to rotate will swing around until it points in the direction of the applied field. If the field is nonuniform, so that \\vec{F_{+}} does not exactly balance \\vec{F_-} , there will be a net force on the dipole, in addition to the torque. Of course, E must change rather abruptly for there to be significant variation in the space of one molecule, so this is not ordinarily a major consideration in discussing the behavior of dielectrics. Nevertheless, the formula for the force on a dipole in a nonuniform field is of some interest: \\vec{F} = \\vec{F_{+}} + \\vec{F_-} = q(\\vec{E_+} - \\vec{E_-}) = q(\\Delta \\vec{E}) where \\Delta \\vec{E} represents the difference between the field at the plus end and the field at the minus end. Assuming the dipole is very short, we may use Eq 1.35 to approximate the small change in E \\Delta \\vec{E} = (\\vec{d} \\cdot \\grad ) \\vec{E} and therefore \\vec{F} = ( \\vec{p} \\cdot \\grad) \\vec{E} \\tagl{4.5} For a \"perfect\" dipole of infinitesimal length, \\eqref{4.4} gives the torque about the center of the dipole even in a nonuniform field; about any other point, \\vec{N} = ( \\vec{p} \\cross \\vec{E}) + (\\vec{r} \\cross \\vec{F}) .","title":"4.1.3: Alignment of Polar Molecules"},{"location":"ch4-1/#414-polarization","text":"In the previous two sections, we have considered the effect of an external electric field on an individual atom or molecule. We are now in a position to answer (quantitatively) the original question: What happens to a piece of dielectric material when it is placed in an electric field? If the substance consists of neutral atoms (or nonpolar molecules), the field will induce in each a tiny dipole moment, pointing in the same direction as the field. If the material is made up of polar molecules, each permanent dipole will experience a torque, tending to line it up along the field direction. (Random thermal motions compete with this process, so the alignment is never complete, especially at higher temperatures, and disappears almost at once when the field is removed.) Notice that these two mechanisms produce the same basic result: a lot of little dipoles pointing along the direction of the field - the material becomes polarized . A convenient measure of this effect is \\vec{P} = \\text{ dipole moment per unit volume } which is called the polarization . From now on we shall not worry much about how the polarization got there. Actually, the two mechanisms I described are not as clear-cut as I tried to pretend. Even in polar molecules there will be some polarization by displacement (though generally it is a lot easier to rotate a molecule than to stretch it, so the second mechanism dominates). It's even possible in some materials to \"freeze in\" polarization, so that it persists after the field is removed. But let's forget for a moment about the cause of the polarization, and let's study the field that a chunk of polarized material itself produces. Then in section 4.3 we'll put it all together: the original field, which was responsible for P , plus the new field, which is due to P .","title":"4.1.4: Polarization"},{"location":"ch4-2/","text":"4.2: The Field of a Polarized Object 4.2.1: Bound Charges Suppose we have a piece of polarized material - that is, an object containing a lot of microscopic dipoles lined up. The dipole moment per unit volume P is given. Question : What is the field produced by this object (not the field that may have caused the polarization, but the field the polarization itself causes)? Well, we know what the field of an individual dipole looks like, so why not chop the material up into infinitesimal dipoles and integrate to get the total? As usual, it's easier to work with the potential. For a single dipole p (Eq. 3.99) V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{\\gr}}{\\gr ^2} \\tagl{4.8} where \\vec{\\gr} is the vector from the dipole to the point at which we are evaluating the potential (Fig 4.8). In the present context, we have a dipole moment \\vec{p} = \\vec{P} \\dd \\tau' in each volume element \\dd \\tau' , so the total potential is V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int_{V} \\frac{\\vec{P}(\\vec{r'}) \\cdot \\vu{\\gr}}{\\gr ^2} \\dd \\tau' That does it, in principle. But a little sleight of hand casts this integral into a much more illuminating form. Observing that \\grad ' \\left( \\frac{1}{\\gr} \\right) = \\frac{\\vu{\\gr}}{\\gr ^2} where the differentiation is with respect to the source coordinates (r'), we have V = \\frac{1}{4 \\pi \\epsilon_0} \\int _V \\vec{P} \\cdot \\grad ' \\left( \\frac{1}{\\gr} \\right) \\dd \\tau' Peeling the \\grad leftwards with integration by parts, we have V = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\int _V \\grad' \\cdot \\left( \\frac{\\vec{P}}{\\gr} \\right) \\dd \\tau' - \\int _V \\frac{1}{\\gr} ( \\grad' \\cdot \\vec{P} ) \\dd \\tau' \\right] The left-hand integral is a volume integral of a divergence, so with Gauss's law V = \\frac{1}{4 \\pi \\epsilon_0 } \\oint _S \\frac{1}{\\gr } \\vec{P} \\cdot \\dd \\vec{a'} - \\frac{1}{4 \\pi \\epsilon_0} \\int _V \\frac{1}{\\gr} ( \\grad' \\cdot \\vec{P} ) \\dd \\tau' \\tagl{4.10} The first term looks like the potential of a surface charge \\sigma_b \\equiv \\vec{P} \\cdot \\vu{n} \\tagl{4.11} while the second term looks like the potential of a volume charge \\rho_b \\equiv - \\div \\vec{P} \\tagl{4.12} With these definitions, \\eqref{4.10} becomes V(\\vec{r} = \\frac{1}{4 \\pi \\epsilon_0} \\oint_S \\frac{\\sigma_b}{\\gr} \\dd a' + \\frac{1}{4 \\pi \\epsilon_0} \\int_V \\frac{\\rho_b}{\\gr} \\dd \\tau' \\tagl{4.13} What this means is that the potential (and hence also the field) of a polarized object is the same as that produced by a volume charge density \\rho_b = - \\div \\vec{P} plus a surface charge density \\sigma_b = \\vec{P} \\cdot \\vu{n} . Instead of integrating the contributions of all the infinitesimal dipoles, we could just find those bound charges , and then calculate the fields they produce, in the same way we calculate the field of any other volume and surface charges. Example 4.2 Find the electric field produced by a uniformly polarized sphere of radius R Solution We may as well choose the z axis to coincide with the direction of polarization (Fig 4.9). The volume bound charge density \\rho_b is zero, since \\vec{P} is uniform. The surface bound charge density is then \\sigma_b = \\vec{P} \\cdot \\vu{n} = P \\cos \\theta where \\theta is the usual spherical coordinate. What we want, then is the field produced by a charge density P \\cos \\theta plastered over the surface of a sphere. We happen to have already computed that potential in Exercise 3.9: V(r, \\theta) = \\begin{cases} \\frac{P}{3 \\epsilon_0} r \\cos \\theta, & \\qquad \\text{ for } r \\leq R \\\\ \\frac{P}{3 \\epsilon_0} \\frac{R^3}{r^2} \\cos \\theta, & \\qquad \\text{ for } r \\geq R \\end{cases} Since r \\cos \\theta = z , the field inside the sphere is uniform : \\vec{E} = - \\grad V = - \\frac{P}{3 \\epsilon_0} \\vu{z} = - \\frac{1}{3 \\epsilon_0} \\vec{P} , \\quad \\text{ for } r < R \\tagl{4.14} This is a pretty remarkable result, and will be very useful in what follows. Outside the sphere the potential is the same as that of a perfect dipole at the origin V = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} , \\quad \\text{ for } r \\geq R \\tagl{4.15} whose dipole moment is, not surprisingly, equal to the total dipole moment of the sphere: \\vec{p} = \\frac{4}{3} \\pi R^3 \\vec{P} \\tagl{4.16} The field of the uniformly polarized sphere is shown in Fig 4.10. 4.2.2: Physical Interpretation of Bound Charges In the last section we found that the field of a polarized object is identical to the field that would be produced by a certain distribution of \"bound charges,\" \\sigma_b and \\rho_b . But this conclusion emerged in the course of abstract manipulations on the integral in Eq. 4.9, and left us with no clue as to the physical meaning of these bound charges. Indeed, some authors give you the impression that bound charges are in some sense \"fictitious\" - mere bookkeeping devices used to facilitate the calculation of fields. Nothing could be further from the truth: \\rho_b and \\sigma_b represent perfectly genuine accumulations of charge. In this section I'll explain how polarization leads to these charge distributions. The basic idea is very simple: Suppose we have a long string of dipoles, as shown in Fig. 4.11. Along the line, the head of one effectively cancels the tail of its neighbor, but at the ends there are two charges left over: plus at the right end and minus at the left. It is as if we had peeled off an electron at one end and carried it all the way down to the other end, though in fact no single electron made the whole trip - a lot of tiny displacements add up to one large one. We call the net charge at the ends a bound charge to remind ourselves that it cannot be removed; in a dielectric every electron is attached to a specific atom or molecule. But apart from that, bound charge is no different from any other kind. To calculate the actual amount of bound charge resulting from a given polarization, examine a \"tube\" of dielectric parallel to P. The dipole moment of the tiny chunk shown in Fig. 4.12 is P(Ad) , where A is the cross-sectional area of the tube and dis the length of the chunk. In terms of the charge (q) at the end, this same dipole moment can be written qd . The bound charge that piles up at the right end of the tube is therefore q = PA If the ends have been sliced off perpendicularly, the surface charge density is \\sigma_b = \\frac{q}{A} = P For an oblique cut (Fig 4.13), the charge is still the same, but A = A_{end} \\cos \\theta so \\sigma_b = \\frac{1}{A_{end}} = P \\cos \\theta = \\vec{P} \\cdot \\vu{n} The effect of the polarization, then, is to paint a bound charge \\sigma_b = \\vec{P} \\cdot \\vu{n} over the surface of the material. This is exactly what we found by more rigorous means in Sect. 4.2.1. But now we know where the bound charge comes from. If the polarization is nonuniform, we get accumulations of bound charge within the material, as well as on the surface. A glance at Fig. 4.14 suggests that a diverging P results in a pileup of negative charge. Indeed, the net bound charge \\int \\rho_b \\dd \\tau in a given volume is equal and opposite to the amount that has been pushed out through the surface. The latter (by the same reasoning we used before) is \\vec{P} \\cdot \\vu{n} per unit area, so \\int_v \\rho_b \\dd \\tau = - \\oint _S \\vec{P} \\cdot \\dd \\vec{a} = - \\int_V (\\div \\vec{P}) \\dd \\tau Since this is true for any volume, we have \\rho_b = - \\div \\vec{P} confirming, again, the more rigorous conclusion of Sect. 4.2.1. Example 4.3 There is another way of analyzing the uniformly polarized sphere, which nicely illustrates the idea of bound charge. What we have, really, is two spheres of charge: a positive sphere and a negative sphere. Without polarization the two are superimposed and cancel completely. But when the material is uniformly polarized, all the plus charges move slightly upward (the z direction) and all the minus charges move slightly downward (Fig 4.15). The two spheres no longer overlap perfectly: at the top there's a 'cap' of leftover positive charge and at the bottom a cap of negative charge. This 'leftover' charge is precisely the bound surface charge \\sigma_b In Prob 2.18, you calculated the field in the region of overlap between two uniformly charged spheres; the answer was \\vec{E} = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{1 \\vec{d}}{R^3} where q is the total charge of the positive sphere, d is the vector from the negative center to the positive center, and R is the radius of the sphere. We can express this in terms of the polarization of the sphere, \\vec{p} = q \\vec{d} = (\\frac{4}{3} \\pi R^3 )\\vec{P} , as \\vec{E} = - \\frac{1}{3 \\epsilon_0} \\vec{P} Meanwhile, for points outside, it is as though all charge on each sphere were concentrated at the respective center. We have, then, a dipole, with potential V = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} (Remember that d is some small fraction of an atomic radius; Fig 4.15 is grossly exaggerated). These answers agree, of course, wtih the results from Ex 4.2. 4.2.3: The Field Inside a Dielectric I have been sloppy about the distinction between \"pure\" dipoles and \"physical\" dipoles. In developing the theory of bound charges, I assumed we were working with the pure kind - indeed, I stated in Eq 4.8 the formula for the potential of a perfect dipole. And yet an actual polarized dielectric consists of physical dipoles, albeit extremely tiny ones. What is more, I presumed to represent discrete molecular dipoles by a continuous density function P . How should I justify this method? Outside the dielectric there is no real problem: here we are far away from the molecules ( \\gr is many times greater than the separation distance between plus and minus charges), so the dipole potential dominates overwhelmingly and the detailed \"graininess\" of the source is blurred by distance. Inside the dielectric, however, we can hardly pretend to be far from all the dipoles, and the procedure I used in Sect. 4.2.1 is open to serious challenge. In fact, when you stop to think about it, the electric field inside matter must be fantastically complicated, on the microscopic level. If you happen to be very near an electron, the field is gigantic, whereas a short distance away it may be small or may point in a totally different direction. Moreover, an instant later, as the atoms move about, the field will have altered entirely. This true microscopic field would be utterly impossible to calculate, nor would it be of much interest if you could. Just as, for macroscopic purposes, we regard water as a continuous fluid, ignoring its molecular structure, so also we can ignore the microscopic bumps and wrinkles in the electric field inside matter, and concentrate on the macroscopic field. This is defined as the average field over regions large enough to contain many thousands of atoms (so that the uninteresting microscopic fluctuations are smoothed over), and yet small enough to ensure that we do not wash out any significant large-scale variations in the field. (In practice, this means we must average over regions much smaller than the dimensions of the object itself.) Ordinarily, the macroscopic field is what people mean when they speak of \"the\" field inside matter. It remains to show that the macroscopic field is what we actually obtain when we use the methods of Sect. 4.2.1. The argument is subtle, so hang on. Suppose I want to calculate the macroscopic field at some point r within a dielectric. I know I must average the true (microscopic) field over an appropriate volume, so let me draw a small sphere about r , of radius, say, a thousand times the size of a molecule. The macroscopic field at r, then, consists of two parts: the average field over the sphere due to all charges outside, plus the average due to all charges inside: \\vec{E} = \\vec{E}_{out} + \\vec{E}_{in} You proved in problem 3.47 that the average field over a sphere produced by charges outside is equal to the field they produce at the center, so \\vec{E_{out}} is the field at r due to the dipoles exterior to the sphere. These are far enough away that we can safely use Eq 4.9 V_{out} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{outside} \\frac{\\vec{P}(\\vec{r'}) \\cdot \\vu{\\gr}}{\\gr ^2} \\dd \\tau' \\tagl{4.17} The dipoles inside the sphere are too close to treat in this fashion. But fortunately all we need is their average field, which we already know (Eq 3.105) \\vec{E}_{in} = -\\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p}}{R^3} regardless of the details of the charge distribution within the sphere. The only relevant quantity is the total dipole moment, \\vec{p} = (\\frac{4}{3} \\pi R^3 )\\vec{P} : \\vec{E_{in}} = - \\frac{1}{3 \\epsilon_0} \\vec{P} \\tagl{4.18} Now, by assumption, the sphere is small enough that P does not vary significantly over its volume, so the term left out of the integral in \\eqref{4.17} corresponds to the field at the center of a uniformly polarized sphere, to wit: -(1/3\\epsilon_0 )\\vec{P} \\eqref{4.14} . But this is precisely what \\vec{E_{in}} puts back in! The macroscopic field, then, is given by the potential V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\vec{P}(\\vec{r'}) \\cdot \\vu{\\gr}}{\\gr ^2} \\dd \\tau' \\tagl{4.19} where the integral runs over the entire volume of the dielectric. This is, of course, what we used under the assumption of perfect dipoles in Sect 4.2.1; without realizing it, we were correctly calculating the averaged, macroscopic field, for points inside the dielectric. Notice that this argument all revolves around the curious fact that the average field over any sphere (due to the charge inside) is the same as the field at the center of a uniformly polarized sphere with the same total dipole moment. This means that no matter how crazy the actual microscopic charge configuration, we can replace it with a nice smooth distribution of perfect dipoles, if all we care about is the macroscopic (average) field. Incidentally, while the argument ostensibly relies on the spherical shape I chose to average over, the macroscopic field is certainly independent of the geometry of the averaging region, and this is reflected in the final answer \\eqref{4.19} . Presumably one could reproduce the same argument for a cube or ellipsoid or whatever by performing some more grueling calculations.","title":"4.2 - The Field of a Polarized Object"},{"location":"ch4-2/#42-the-field-of-a-polarized-object","text":"","title":"4.2: The Field of a Polarized Object"},{"location":"ch4-2/#421-bound-charges","text":"Suppose we have a piece of polarized material - that is, an object containing a lot of microscopic dipoles lined up. The dipole moment per unit volume P is given. Question : What is the field produced by this object (not the field that may have caused the polarization, but the field the polarization itself causes)? Well, we know what the field of an individual dipole looks like, so why not chop the material up into infinitesimal dipoles and integrate to get the total? As usual, it's easier to work with the potential. For a single dipole p (Eq. 3.99) V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{\\gr}}{\\gr ^2} \\tagl{4.8} where \\vec{\\gr} is the vector from the dipole to the point at which we are evaluating the potential (Fig 4.8). In the present context, we have a dipole moment \\vec{p} = \\vec{P} \\dd \\tau' in each volume element \\dd \\tau' , so the total potential is V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int_{V} \\frac{\\vec{P}(\\vec{r'}) \\cdot \\vu{\\gr}}{\\gr ^2} \\dd \\tau' That does it, in principle. But a little sleight of hand casts this integral into a much more illuminating form. Observing that \\grad ' \\left( \\frac{1}{\\gr} \\right) = \\frac{\\vu{\\gr}}{\\gr ^2} where the differentiation is with respect to the source coordinates (r'), we have V = \\frac{1}{4 \\pi \\epsilon_0} \\int _V \\vec{P} \\cdot \\grad ' \\left( \\frac{1}{\\gr} \\right) \\dd \\tau' Peeling the \\grad leftwards with integration by parts, we have V = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\int _V \\grad' \\cdot \\left( \\frac{\\vec{P}}{\\gr} \\right) \\dd \\tau' - \\int _V \\frac{1}{\\gr} ( \\grad' \\cdot \\vec{P} ) \\dd \\tau' \\right] The left-hand integral is a volume integral of a divergence, so with Gauss's law V = \\frac{1}{4 \\pi \\epsilon_0 } \\oint _S \\frac{1}{\\gr } \\vec{P} \\cdot \\dd \\vec{a'} - \\frac{1}{4 \\pi \\epsilon_0} \\int _V \\frac{1}{\\gr} ( \\grad' \\cdot \\vec{P} ) \\dd \\tau' \\tagl{4.10} The first term looks like the potential of a surface charge \\sigma_b \\equiv \\vec{P} \\cdot \\vu{n} \\tagl{4.11} while the second term looks like the potential of a volume charge \\rho_b \\equiv - \\div \\vec{P} \\tagl{4.12} With these definitions, \\eqref{4.10} becomes V(\\vec{r} = \\frac{1}{4 \\pi \\epsilon_0} \\oint_S \\frac{\\sigma_b}{\\gr} \\dd a' + \\frac{1}{4 \\pi \\epsilon_0} \\int_V \\frac{\\rho_b}{\\gr} \\dd \\tau' \\tagl{4.13} What this means is that the potential (and hence also the field) of a polarized object is the same as that produced by a volume charge density \\rho_b = - \\div \\vec{P} plus a surface charge density \\sigma_b = \\vec{P} \\cdot \\vu{n} . Instead of integrating the contributions of all the infinitesimal dipoles, we could just find those bound charges , and then calculate the fields they produce, in the same way we calculate the field of any other volume and surface charges.","title":"4.2.1: Bound Charges"},{"location":"ch4-2/#example-42","text":"Find the electric field produced by a uniformly polarized sphere of radius R Solution We may as well choose the z axis to coincide with the direction of polarization (Fig 4.9). The volume bound charge density \\rho_b is zero, since \\vec{P} is uniform. The surface bound charge density is then \\sigma_b = \\vec{P} \\cdot \\vu{n} = P \\cos \\theta where \\theta is the usual spherical coordinate. What we want, then is the field produced by a charge density P \\cos \\theta plastered over the surface of a sphere. We happen to have already computed that potential in Exercise 3.9: V(r, \\theta) = \\begin{cases} \\frac{P}{3 \\epsilon_0} r \\cos \\theta, & \\qquad \\text{ for } r \\leq R \\\\ \\frac{P}{3 \\epsilon_0} \\frac{R^3}{r^2} \\cos \\theta, & \\qquad \\text{ for } r \\geq R \\end{cases} Since r \\cos \\theta = z , the field inside the sphere is uniform : \\vec{E} = - \\grad V = - \\frac{P}{3 \\epsilon_0} \\vu{z} = - \\frac{1}{3 \\epsilon_0} \\vec{P} , \\quad \\text{ for } r < R \\tagl{4.14} This is a pretty remarkable result, and will be very useful in what follows. Outside the sphere the potential is the same as that of a perfect dipole at the origin V = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} , \\quad \\text{ for } r \\geq R \\tagl{4.15} whose dipole moment is, not surprisingly, equal to the total dipole moment of the sphere: \\vec{p} = \\frac{4}{3} \\pi R^3 \\vec{P} \\tagl{4.16} The field of the uniformly polarized sphere is shown in Fig 4.10.","title":"Example 4.2"},{"location":"ch4-2/#422-physical-interpretation-of-bound-charges","text":"In the last section we found that the field of a polarized object is identical to the field that would be produced by a certain distribution of \"bound charges,\" \\sigma_b and \\rho_b . But this conclusion emerged in the course of abstract manipulations on the integral in Eq. 4.9, and left us with no clue as to the physical meaning of these bound charges. Indeed, some authors give you the impression that bound charges are in some sense \"fictitious\" - mere bookkeeping devices used to facilitate the calculation of fields. Nothing could be further from the truth: \\rho_b and \\sigma_b represent perfectly genuine accumulations of charge. In this section I'll explain how polarization leads to these charge distributions. The basic idea is very simple: Suppose we have a long string of dipoles, as shown in Fig. 4.11. Along the line, the head of one effectively cancels the tail of its neighbor, but at the ends there are two charges left over: plus at the right end and minus at the left. It is as if we had peeled off an electron at one end and carried it all the way down to the other end, though in fact no single electron made the whole trip - a lot of tiny displacements add up to one large one. We call the net charge at the ends a bound charge to remind ourselves that it cannot be removed; in a dielectric every electron is attached to a specific atom or molecule. But apart from that, bound charge is no different from any other kind. To calculate the actual amount of bound charge resulting from a given polarization, examine a \"tube\" of dielectric parallel to P. The dipole moment of the tiny chunk shown in Fig. 4.12 is P(Ad) , where A is the cross-sectional area of the tube and dis the length of the chunk. In terms of the charge (q) at the end, this same dipole moment can be written qd . The bound charge that piles up at the right end of the tube is therefore q = PA If the ends have been sliced off perpendicularly, the surface charge density is \\sigma_b = \\frac{q}{A} = P For an oblique cut (Fig 4.13), the charge is still the same, but A = A_{end} \\cos \\theta so \\sigma_b = \\frac{1}{A_{end}} = P \\cos \\theta = \\vec{P} \\cdot \\vu{n} The effect of the polarization, then, is to paint a bound charge \\sigma_b = \\vec{P} \\cdot \\vu{n} over the surface of the material. This is exactly what we found by more rigorous means in Sect. 4.2.1. But now we know where the bound charge comes from. If the polarization is nonuniform, we get accumulations of bound charge within the material, as well as on the surface. A glance at Fig. 4.14 suggests that a diverging P results in a pileup of negative charge. Indeed, the net bound charge \\int \\rho_b \\dd \\tau in a given volume is equal and opposite to the amount that has been pushed out through the surface. The latter (by the same reasoning we used before) is \\vec{P} \\cdot \\vu{n} per unit area, so \\int_v \\rho_b \\dd \\tau = - \\oint _S \\vec{P} \\cdot \\dd \\vec{a} = - \\int_V (\\div \\vec{P}) \\dd \\tau Since this is true for any volume, we have \\rho_b = - \\div \\vec{P} confirming, again, the more rigorous conclusion of Sect. 4.2.1.","title":"4.2.2: Physical Interpretation of Bound Charges"},{"location":"ch4-2/#example-43","text":"There is another way of analyzing the uniformly polarized sphere, which nicely illustrates the idea of bound charge. What we have, really, is two spheres of charge: a positive sphere and a negative sphere. Without polarization the two are superimposed and cancel completely. But when the material is uniformly polarized, all the plus charges move slightly upward (the z direction) and all the minus charges move slightly downward (Fig 4.15). The two spheres no longer overlap perfectly: at the top there's a 'cap' of leftover positive charge and at the bottom a cap of negative charge. This 'leftover' charge is precisely the bound surface charge \\sigma_b In Prob 2.18, you calculated the field in the region of overlap between two uniformly charged spheres; the answer was \\vec{E} = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{1 \\vec{d}}{R^3} where q is the total charge of the positive sphere, d is the vector from the negative center to the positive center, and R is the radius of the sphere. We can express this in terms of the polarization of the sphere, \\vec{p} = q \\vec{d} = (\\frac{4}{3} \\pi R^3 )\\vec{P} , as \\vec{E} = - \\frac{1}{3 \\epsilon_0} \\vec{P} Meanwhile, for points outside, it is as though all charge on each sphere were concentrated at the respective center. We have, then, a dipole, with potential V = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} (Remember that d is some small fraction of an atomic radius; Fig 4.15 is grossly exaggerated). These answers agree, of course, wtih the results from Ex 4.2.","title":"Example 4.3"},{"location":"ch4-2/#423-the-field-inside-a-dielectric","text":"I have been sloppy about the distinction between \"pure\" dipoles and \"physical\" dipoles. In developing the theory of bound charges, I assumed we were working with the pure kind - indeed, I stated in Eq 4.8 the formula for the potential of a perfect dipole. And yet an actual polarized dielectric consists of physical dipoles, albeit extremely tiny ones. What is more, I presumed to represent discrete molecular dipoles by a continuous density function P . How should I justify this method? Outside the dielectric there is no real problem: here we are far away from the molecules ( \\gr is many times greater than the separation distance between plus and minus charges), so the dipole potential dominates overwhelmingly and the detailed \"graininess\" of the source is blurred by distance. Inside the dielectric, however, we can hardly pretend to be far from all the dipoles, and the procedure I used in Sect. 4.2.1 is open to serious challenge. In fact, when you stop to think about it, the electric field inside matter must be fantastically complicated, on the microscopic level. If you happen to be very near an electron, the field is gigantic, whereas a short distance away it may be small or may point in a totally different direction. Moreover, an instant later, as the atoms move about, the field will have altered entirely. This true microscopic field would be utterly impossible to calculate, nor would it be of much interest if you could. Just as, for macroscopic purposes, we regard water as a continuous fluid, ignoring its molecular structure, so also we can ignore the microscopic bumps and wrinkles in the electric field inside matter, and concentrate on the macroscopic field. This is defined as the average field over regions large enough to contain many thousands of atoms (so that the uninteresting microscopic fluctuations are smoothed over), and yet small enough to ensure that we do not wash out any significant large-scale variations in the field. (In practice, this means we must average over regions much smaller than the dimensions of the object itself.) Ordinarily, the macroscopic field is what people mean when they speak of \"the\" field inside matter. It remains to show that the macroscopic field is what we actually obtain when we use the methods of Sect. 4.2.1. The argument is subtle, so hang on. Suppose I want to calculate the macroscopic field at some point r within a dielectric. I know I must average the true (microscopic) field over an appropriate volume, so let me draw a small sphere about r , of radius, say, a thousand times the size of a molecule. The macroscopic field at r, then, consists of two parts: the average field over the sphere due to all charges outside, plus the average due to all charges inside: \\vec{E} = \\vec{E}_{out} + \\vec{E}_{in} You proved in problem 3.47 that the average field over a sphere produced by charges outside is equal to the field they produce at the center, so \\vec{E_{out}} is the field at r due to the dipoles exterior to the sphere. These are far enough away that we can safely use Eq 4.9 V_{out} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{outside} \\frac{\\vec{P}(\\vec{r'}) \\cdot \\vu{\\gr}}{\\gr ^2} \\dd \\tau' \\tagl{4.17} The dipoles inside the sphere are too close to treat in this fashion. But fortunately all we need is their average field, which we already know (Eq 3.105) \\vec{E}_{in} = -\\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p}}{R^3} regardless of the details of the charge distribution within the sphere. The only relevant quantity is the total dipole moment, \\vec{p} = (\\frac{4}{3} \\pi R^3 )\\vec{P} : \\vec{E_{in}} = - \\frac{1}{3 \\epsilon_0} \\vec{P} \\tagl{4.18} Now, by assumption, the sphere is small enough that P does not vary significantly over its volume, so the term left out of the integral in \\eqref{4.17} corresponds to the field at the center of a uniformly polarized sphere, to wit: -(1/3\\epsilon_0 )\\vec{P} \\eqref{4.14} . But this is precisely what \\vec{E_{in}} puts back in! The macroscopic field, then, is given by the potential V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\vec{P}(\\vec{r'}) \\cdot \\vu{\\gr}}{\\gr ^2} \\dd \\tau' \\tagl{4.19} where the integral runs over the entire volume of the dielectric. This is, of course, what we used under the assumption of perfect dipoles in Sect 4.2.1; without realizing it, we were correctly calculating the averaged, macroscopic field, for points inside the dielectric. Notice that this argument all revolves around the curious fact that the average field over any sphere (due to the charge inside) is the same as the field at the center of a uniformly polarized sphere with the same total dipole moment. This means that no matter how crazy the actual microscopic charge configuration, we can replace it with a nice smooth distribution of perfect dipoles, if all we care about is the macroscopic (average) field. Incidentally, while the argument ostensibly relies on the spherical shape I chose to average over, the macroscopic field is certainly independent of the geometry of the averaging region, and this is reflected in the final answer \\eqref{4.19} . Presumably one could reproduce the same argument for a cube or ellipsoid or whatever by performing some more grueling calculations.","title":"4.2.3: The Field Inside a Dielectric"},{"location":"ch4-3/","text":"4.3: The Electric Displacement 4.3.1: Gauss's Law in the Presence of Dielectrics In Section 4.2 we found that the effect of polarization is to produce accumulations of (bound) charge, \\rho_b = - \\div \\vec{P} within the dielectric and \\sigma_b = \\vec{P} \\cdot \\vu{n} on the surface. The field due to polarization of the medium is just the field of this bound charge. We are now ready to put it all together: the field attributable to bound charge plus the field due to everything else (which, for want of a better term, we call free charge , \\rho_f ). The free charge might consist of electrons on a conductor or ions embedded in the dielectric material or whatever; any charge, in other words, that is not a result of polarization. Within the dielectric, the total charge density can be written \\rho = \\rho_b + \\rho_f \\tagl{4.20} and Gauss's law reads \\epsilon_0 \\div \\vec{E} = \\rho = \\rho_b + \\rho_f = - \\div \\vec{P} + \\rho_f where E is now the total field, not just that portion generated by polarization. It is convenient to combine the two divergence terms: \\div (\\epsilon_0 \\vec{E} + \\vec{P}) = \\rho_f The expression in parentheses is known as the electric displacement and is designated by the letter D : \\vec{D} \\equiv \\epsilon_0 \\vec{E} + \\vec{P} \\tagl{4.21} In terms of D , Gauss's law then reads \\div \\vec{D} = \\rho_f \\tagl{4.22} or in integral form \\oint \\vec{D} \\cdot \\dd \\vec{a} = Q_{f_{enc}} \\tagl{4.23} where Q_{f_{enc}} denotes the total free charge enclosed in the volume. This is a particularly useful way to express Gauss's law, in the context of dielectrics, because it makes reference only to the free charges , and free charge is the stuff we control. Bound charge comes along for the ride: when we put the free charge in place, a certain polarization automatically arises, by the mechanisms of Sect 4.1, and this polarization produces the bound charge. In a typical problem, therefore, we know \\rho_f , but we do not (initially) know \\rho_b ; \\eqref{4.23} lets us go right to work with the information at hand. In particular, whenever the requisite symmetry is present, we can immediately calculate D by the standard Gauss's law methods. Example 4.4 A long straight wire, carrying uniform line charge \\lambda , is surrounded by rubber insulation out to a radius a (Fig 4.17). Find the electric displacement. Solution Drawing a cylindrical Gaussian surface, of radius s and length L , and applying \\eqref{4.23} we find D( 2\\pi s L) = \\lambda L Therefore \\vec{D} = \\frac{\\lambda}{2 \\pi s} \\vu{s} \\tagl{4.24} Notice that this formula holds both within the insulation and outside it. In the latter region, \\vec{P} = 0 so \\vec{E} =\\frac{1}{\\epsilon_0} \\vec{D} = \\frac{\\lambda}{2 \\pi \\epsilon_0 s} \\vu{s}, \\quad \\text{ for } s > a Inside the rubber, the electric field cannot be determined, since we do not know P . Hold on a tick! We got all the way to a field we can calculate by Gauss's law, but we have left out the surface bound charge \\sigma_b . What happened to it? To be more precise, \\eqref{4.22} works within a dielectric, but we cannot apply Gauss's law precisely at the boundary of the dielectric, because the local \\rho_b blows up there, taking \\div \\vec{E} with it. The polarization drops abruptly to zero outside the material, so its derivative is a delta function. The surface bound charge is precisely this term, so in this sense it is actually included in \\rho_b , but we ordinarily prefer to handle it separately as \\sigma_b . We could even picture the edge of the dielectric as having some finite thickness, within which the polarization drops off to zero (which is probably a more realistic model anyway), in which case there is no \\sigma_b , \\rho_b varies rapidly but smoothly, and Gauss's law can safely be applied everywhere. In any case, we can use \\eqref{4.23} safely without fear of this \"defect.\" 4.3.2: A Deceptive Parallel Our expression for the divergence of the displacement looks just like Gauss's law, only the total charge density \\rho is replaced by the free charge density \\rho_f , and \\vec{D} is substituted for \\epsilon_0 \\vec{E} . For this reason, you may be tempted to conclude that D is \"just like\" E (apart from the factor \\epsilon_0 ), except that its source is \\rho_f instead of \\rho . That is, it's tempting to say \"To solve problems involving dielectrics, you just forget all about the bound charge - calculate the field as you ordinarily would, only call the answer D instead of E .\" This reasoning is seductive, but the conclusion is false; in particular there is no \"Coulomb's law\" for D : \\vec{D}(\\vec{r}) \\neq \\frac{1}{4 \\pi} \\int \\frac{\\vu{\\gr}}{\\gr ^2} \\rho_f(\\vec{r'}) \\dd \\tau' This is because the divergence alone is insufficient to determine a vector field; you need to know its curl as well. One tends to forget this in the case of electrostatics because we usually don't care about the curl of E anyway. But the curl of D is not always zero, even in electrostatics, since there is no reason, in general, to suppose that the curl of P vanishes: \\curl \\vec{D} = \\epsilon_0 (\\curl \\vec{E}) + (\\curl \\vec{P}) = \\curl \\vec{P} \\tagl{4.25} Sometimes it does, but more often it does not. The bar electret of Prob 4.11 is one example of this: here there is no free charge anywhere, so if you really believe that the only source of D is \\rho_f you will be forced to conclude that \\vec{D} = 0 everywhere, and hence that \\vec{E} = (-1 / \\epsilon_0) \\vec{P} inside and \\vec{E} = 0 outside the electret, which is obviously wrong. And because \\curl \\vec{D} \\neq 0 in general, D cannot be expressed as the gradient of a scalar - there is no \"potential\" for D . Advice : When you are asked to compute the electric displacement, first look for symmetry. If the problem exhibits spherical, cylindrical, or plane symmetry, then you can get D directly from Eq. 4.23 by the usual Gauss's law methods. (Evidently in such cases \\curl \\vec{P} is automatically zero, but since symmetry alone dictates the answer, you're not really obliged to worry about the curl.) If the requisite symmetry is absent, you'll have to think of another approach, and, in particular, you must not assume that D is determined exclusively by the free charge. 4.3.3: Boundary Conditions The electrostatic boundary conditions we had in Sect 2.3 can be re-cast in terms of D . \\eqref{4.23} tells us the discontinuity in the component perpendicular to an interface: D_{above} ^{\\perp} - D_{below} ^{\\perp} = \\sigma_f \\tagl{4.26} while \\eqref{4.25} gives the discontinuity in parallel components: \\vec{D}_{above} ^{\\parallel} - \\vec{D}_{below} ^{\\parallel} = \\vec{P}_{above} ^{\\parallel} - \\vec{P}_{below} ^{\\parallel} \\tagl{4.27} In the presence of dielectrics, these are sometimes more useful than the corresponding boundary conditions on E (Eqs 2.31 and 2.32): E_{above} ^{\\perp} - E_{below} ^{\\perp} = \\frac{1}{\\epsilon_0} \\sigma \\tagl{4.28} and \\vec{E}_{above} ^{\\parallel} - \\vec{E}_{below} ^{\\parallel} = 0 \\tagl{4.29}","title":"4.3 - The Electric Displacement"},{"location":"ch4-3/#43-the-electric-displacement","text":"","title":"4.3: The Electric Displacement"},{"location":"ch4-3/#431-gausss-law-in-the-presence-of-dielectrics","text":"In Section 4.2 we found that the effect of polarization is to produce accumulations of (bound) charge, \\rho_b = - \\div \\vec{P} within the dielectric and \\sigma_b = \\vec{P} \\cdot \\vu{n} on the surface. The field due to polarization of the medium is just the field of this bound charge. We are now ready to put it all together: the field attributable to bound charge plus the field due to everything else (which, for want of a better term, we call free charge , \\rho_f ). The free charge might consist of electrons on a conductor or ions embedded in the dielectric material or whatever; any charge, in other words, that is not a result of polarization. Within the dielectric, the total charge density can be written \\rho = \\rho_b + \\rho_f \\tagl{4.20} and Gauss's law reads \\epsilon_0 \\div \\vec{E} = \\rho = \\rho_b + \\rho_f = - \\div \\vec{P} + \\rho_f where E is now the total field, not just that portion generated by polarization. It is convenient to combine the two divergence terms: \\div (\\epsilon_0 \\vec{E} + \\vec{P}) = \\rho_f The expression in parentheses is known as the electric displacement and is designated by the letter D : \\vec{D} \\equiv \\epsilon_0 \\vec{E} + \\vec{P} \\tagl{4.21} In terms of D , Gauss's law then reads \\div \\vec{D} = \\rho_f \\tagl{4.22} or in integral form \\oint \\vec{D} \\cdot \\dd \\vec{a} = Q_{f_{enc}} \\tagl{4.23} where Q_{f_{enc}} denotes the total free charge enclosed in the volume. This is a particularly useful way to express Gauss's law, in the context of dielectrics, because it makes reference only to the free charges , and free charge is the stuff we control. Bound charge comes along for the ride: when we put the free charge in place, a certain polarization automatically arises, by the mechanisms of Sect 4.1, and this polarization produces the bound charge. In a typical problem, therefore, we know \\rho_f , but we do not (initially) know \\rho_b ; \\eqref{4.23} lets us go right to work with the information at hand. In particular, whenever the requisite symmetry is present, we can immediately calculate D by the standard Gauss's law methods.","title":"4.3.1: Gauss's Law in the Presence of Dielectrics"},{"location":"ch4-3/#example-44","text":"A long straight wire, carrying uniform line charge \\lambda , is surrounded by rubber insulation out to a radius a (Fig 4.17). Find the electric displacement. Solution Drawing a cylindrical Gaussian surface, of radius s and length L , and applying \\eqref{4.23} we find D( 2\\pi s L) = \\lambda L Therefore \\vec{D} = \\frac{\\lambda}{2 \\pi s} \\vu{s} \\tagl{4.24} Notice that this formula holds both within the insulation and outside it. In the latter region, \\vec{P} = 0 so \\vec{E} =\\frac{1}{\\epsilon_0} \\vec{D} = \\frac{\\lambda}{2 \\pi \\epsilon_0 s} \\vu{s}, \\quad \\text{ for } s > a Inside the rubber, the electric field cannot be determined, since we do not know P . Hold on a tick! We got all the way to a field we can calculate by Gauss's law, but we have left out the surface bound charge \\sigma_b . What happened to it? To be more precise, \\eqref{4.22} works within a dielectric, but we cannot apply Gauss's law precisely at the boundary of the dielectric, because the local \\rho_b blows up there, taking \\div \\vec{E} with it. The polarization drops abruptly to zero outside the material, so its derivative is a delta function. The surface bound charge is precisely this term, so in this sense it is actually included in \\rho_b , but we ordinarily prefer to handle it separately as \\sigma_b . We could even picture the edge of the dielectric as having some finite thickness, within which the polarization drops off to zero (which is probably a more realistic model anyway), in which case there is no \\sigma_b , \\rho_b varies rapidly but smoothly, and Gauss's law can safely be applied everywhere. In any case, we can use \\eqref{4.23} safely without fear of this \"defect.\"","title":"Example 4.4"},{"location":"ch4-3/#432-a-deceptive-parallel","text":"Our expression for the divergence of the displacement looks just like Gauss's law, only the total charge density \\rho is replaced by the free charge density \\rho_f , and \\vec{D} is substituted for \\epsilon_0 \\vec{E} . For this reason, you may be tempted to conclude that D is \"just like\" E (apart from the factor \\epsilon_0 ), except that its source is \\rho_f instead of \\rho . That is, it's tempting to say \"To solve problems involving dielectrics, you just forget all about the bound charge - calculate the field as you ordinarily would, only call the answer D instead of E .\" This reasoning is seductive, but the conclusion is false; in particular there is no \"Coulomb's law\" for D : \\vec{D}(\\vec{r}) \\neq \\frac{1}{4 \\pi} \\int \\frac{\\vu{\\gr}}{\\gr ^2} \\rho_f(\\vec{r'}) \\dd \\tau' This is because the divergence alone is insufficient to determine a vector field; you need to know its curl as well. One tends to forget this in the case of electrostatics because we usually don't care about the curl of E anyway. But the curl of D is not always zero, even in electrostatics, since there is no reason, in general, to suppose that the curl of P vanishes: \\curl \\vec{D} = \\epsilon_0 (\\curl \\vec{E}) + (\\curl \\vec{P}) = \\curl \\vec{P} \\tagl{4.25} Sometimes it does, but more often it does not. The bar electret of Prob 4.11 is one example of this: here there is no free charge anywhere, so if you really believe that the only source of D is \\rho_f you will be forced to conclude that \\vec{D} = 0 everywhere, and hence that \\vec{E} = (-1 / \\epsilon_0) \\vec{P} inside and \\vec{E} = 0 outside the electret, which is obviously wrong. And because \\curl \\vec{D} \\neq 0 in general, D cannot be expressed as the gradient of a scalar - there is no \"potential\" for D . Advice : When you are asked to compute the electric displacement, first look for symmetry. If the problem exhibits spherical, cylindrical, or plane symmetry, then you can get D directly from Eq. 4.23 by the usual Gauss's law methods. (Evidently in such cases \\curl \\vec{P} is automatically zero, but since symmetry alone dictates the answer, you're not really obliged to worry about the curl.) If the requisite symmetry is absent, you'll have to think of another approach, and, in particular, you must not assume that D is determined exclusively by the free charge.","title":"4.3.2: A Deceptive Parallel"},{"location":"ch4-3/#433-boundary-conditions","text":"The electrostatic boundary conditions we had in Sect 2.3 can be re-cast in terms of D . \\eqref{4.23} tells us the discontinuity in the component perpendicular to an interface: D_{above} ^{\\perp} - D_{below} ^{\\perp} = \\sigma_f \\tagl{4.26} while \\eqref{4.25} gives the discontinuity in parallel components: \\vec{D}_{above} ^{\\parallel} - \\vec{D}_{below} ^{\\parallel} = \\vec{P}_{above} ^{\\parallel} - \\vec{P}_{below} ^{\\parallel} \\tagl{4.27} In the presence of dielectrics, these are sometimes more useful than the corresponding boundary conditions on E (Eqs 2.31 and 2.32): E_{above} ^{\\perp} - E_{below} ^{\\perp} = \\frac{1}{\\epsilon_0} \\sigma \\tagl{4.28} and \\vec{E}_{above} ^{\\parallel} - \\vec{E}_{below} ^{\\parallel} = 0 \\tagl{4.29}","title":"4.3.3: Boundary Conditions"},{"location":"ch4-4/","text":"4.4: Linear Dielectrics 4.4.1: Susceptibility, Permittivity, Dielectric Constant In the first few sections of this chapter we did not commit ourselves as to the cause of P ; we dealt only with the effects of polarization. From the qualitative essence of 4.1, though, we know that the polarization of a dielectric ordinarily results from an electric field, which lines up the atomic or molecular dipoles. For many substances, in fact, the polarization is proportional to the field, provided E is not too strong: \\vec{P} = \\epsilon_0 \\chi_e \\vec{E} \\tagl{4.30} The constant of proportionality, \\chi_e , is called the electric susceptibility of the medium (a factor of \\epsilon_0 has been extracted to make \\chi_e dimensionless). The value of \\chi_e depends on the microscopic structure of the substance in question (and also on external conditions such as temperature). I shall call materials that obey \\eqref{4.30} linear dielectrics . In modern optical applications, especially, nonlinear materials have become increasingly important. For these there is a second term relating P to E - typically a cubic term. In general, Eq 4.30 can be regarded as the first (nonzero) term in the Taylor expansion of P in powers of E . Note that E in \\eqref{4.30} is the total field; it may be due in part to free charges and in part to the polarization itself. If, for instance, we put a piece of dielectric into an external field \\vec{E_0} , we cannot compute P directly from the linear susceptibility relation; the external field will polarize the material, and this polarization will produce its own field, which then contributes to the total field, and this in turn modifies the polarization, which... Breaking out of this infinite regress is not always easy. You'll see some examples in a moment. The simplest approach is to begin with the displacement , at least in those cases where D can be deduced directly from the free charge distribution. In linear media we have \\vec{D} = \\epsilon_0 \\vec{E} + \\vec{P} = \\epsilon_0 \\vec{E} + \\epsilon_0 \\chi_e \\vec{E} = \\epsilon_0 (1 + \\chi_e) \\vec{E} \\tagl{4.31} so D is also proportional to E \\vec{E} = \\epsilon \\vec{E} \\tagl{4.32} where \\epsilon \\equiv \\epsilon_0 (1 + \\chi_e) \\tagl{4.33} This new constant \\epsilon is called the permittivity of the material. (In vacuum, where there is no matter to polarize, the susceptibility is zero, and the permittivity is \\epsilon_0 . That's why \\epsilon_0 is called the permittivity of free space. I dislike the term, for it suggest that the vacuum is just a special kind of linear dielectric, in which the permittivity happens to have the value 8.85 \\times 10^{-12} C^2 / N \\cdot m^2 .) If you remove a factor of \\epsilon_0 , the remaining dimensionless quantity \\epsilon_r = 1 + \\chi _e = \\frac{\\epsilon}{\\epsilon_0} \\tagl{4.34} is called the relative permittivity , or dielectric constant , of the material. Dielectric constants for some common substances are listed in Table 4.2. (Notice that \\epsilon_r is greater than 1, for all ordinary materials.) Of course, the permittivity and the dielectric constant do not convey any information that was not already available in the susceptibility, nor is there anything essentially new in Eq 4.32: the physics of linear dielectrics is all contained in \\eqref{4.30} Example 4.5 A metal sphere of radius a carries a charge Q (Fig 4.20). It is surrounded, out to radius b , by linear dielectric material of permittivity \\epsilon . Find the potential at the center (relative to infinity). Solution To compute V, we need to know E; to find E, we might first try to locate the bound charge; we could get the bound charge from P , but we can't calculate P unless we already know E . What we do know is the free charge, and our arrangement is spherically symmetric, so we can go straight for D using Eq 4.23: \\vec{D} = \\frac{Q}{4 \\pi r^2} \\vu{r}, \\quad \\text{ for all points } r > a Inside the conducting sphere, all our electrostatic fields are zero. We then obtain E via Eq 4.32: \\vec{E} = \\begin{cases} \\frac{Q}{4 \\pi \\epsilon r^2} \\vu{r} & \\quad \\text{ for } a < r < b \\\\ \\frac{Q}{4 \\pi \\epsilon_0 r^2} \\vu{r} & \\quad \\text{ for } r > b \\end{cases} We get the potential at the center by integrating E V = - \\int _{\\infty} ^0 \\vec{E} \\cdot \\dd \\vec{l} = \\\\ - \\int _{\\infty} ^b \\left( \\frac{Q}{4 \\pi \\epsilon_0 r^2} \\right) \\dd r - \\int_b ^a \\left( \\frac{Q}{4 \\pi \\epsilon r^2} \\right) \\dd r\\\\ = \\frac{Q}{4 \\pi } \\left( \\frac{1}{\\epsilon_0 b} + \\frac{1}{\\epsilon a} - \\frac{1}{\\epsilon b} \\right) In this case, we didn't need to compute the polarization or the bound charge explicitly, but we can easily do so now that we have E : \\vec{P} = \\epsilon_0 \\chi_e \\vec{E} = \\frac{\\epsilon_0 \\chi_e Q}{4 \\pi \\epsilon r^2} \\vu{r} within the dielectric, so that \\rho_b = - \\div \\vec{P} = 0 and \\sigma_b = \\vec{P} \\cdot \\vu{n} = \\begin{cases} \\frac{\\epsilon_0 \\chi_e Q}{4 \\pi \\epsilon b^2} & \\qquad \\text{ at the outer surface } \\\\ \\frac{- \\epsilon_0 \\chi_e Q}{4 \\pi \\epsilon a^2} & \\qquad \\text{ at the inner surface } \\end{cases} Notice that the surface bound charge at a is negative ( \\vu{n} points outward with respect to the dielectric, which is + \\vu{n} at b, but -\\vu{r} at a). This is natural, since the charge on the metal sphere attracts its opposite in all the dielectric molecules. It is this layer of negative charge that reduces the field, within the dielectric, from 1 / 4 \\pi \\epsilon_0 (Q / r^2) \\vu{r} to 1 / 4 \\pi \\epsilon (Q / r^2) \\vu{r} . In this respect, a dielectric is rather like an imperfect conductor: on a conducting shell the induced surface charge would be such as to cancel out the field of Q entirely in the region a < r < b ; the dielectric does the best it can, but the cancellation is only partial. Since linear dielectrics give us cases where P and D are proportional to E , you might suppose that linear dielectrics escape the defect in the parallel between E and D . Does it not follow that their curls, like E 's, must vanish? Unfortunately, it does not, for the line integral of P around a closed path that straddles the boundary between one type of material and another need not be zero, even though the integral of E around the same loop must be. The reason is that the proportionality factor \\epsilon_0 \\chi_e is different on the two sides. For instance, at the interface between a polarized dielectric and the vacuum (Fig 4.21), P is zero on one side but not on the other. Around this loop, \\oint \\vec{P} \\cdot \\dd \\vec{l} \\neq 0 , and hence, by Stokes' theorem, the curl of P cannot vanish everywhere within the loop (in fact, it is infinite at the boundary). Of course, if space is entirely filled with a homogeneous linear dielectric, then this objection is void; in this rather special circumstance \\div \\vec{D} = \\rho_f \\quad \\text{and} \\quad \\curl \\vec{D} = 0 so D can be found from the free charge just as though the dielectric were not there: \\vec{D} = \\epsilon_0 \\vec{E_{vac}} where \\vec{E_{vac}} is the field the same charge distribution would produce in the absence of any dielectric. According to \\eqref{4.32} and \\eqref{4.34} , therefore, \\vec{E} = \\frac{1}{\\epsilon} \\vec{D} = \\frac{1}{\\epsilon_r} \\vec{E_{vac}} \\tagl{4.35} Conclusion: when all space is filled with a homogeneous linear dielectric, the field everywhere is simply reduced by a factor of one over the dielectric constant. (Actually it's not necessary for the dielectric to fill all space; in regions where the field is zero anyway, it can hardly matter whether the dielectric is present or not, since there's no polarization in any event.) For example, if a free charge q is embedded in a large dielectric, the field it produces is \\vec{E} = \\frac{1}{4 \\pi \\epsilon} \\frac{q}{r^2} \\vu{r} \\tagl{4.36} (that's \\epsilon , not \\epsilon_0 ), and the force it exerts on nearby charges is reduced accordingly. But it's not that there is anything wrong with Coulomb's law; rather, the polarization of the medium partially \"shields\" the charge, by surrounding it with bound charge of the opposite sign (Fig 4.22) Example 4.6 A parallel-plate capacitor (Fig 4.23) is filled with insulating material of dielectric constant \\epsilon_r . What effect does this have on its capacitance? Solution Since the field is confined to the space between the plates, the dielectric will reduce E , and hence also the potential difference V, by a factor 1 / \\epsilon_r . Accordingly, the capacitance C = Q / V is increased by a factor of the dielectric constant C = \\epsilon_r C_{vac} \\tagl{4.37} This is, in fact, a common way to beef up a capacitor A crystal is generally easier to polarize in some directions than others, and in this case Eq 4.30 is replaced by the general linear relation \\begin{align*} P_x & = \\epsilon_0 (\\chi_{e,xx} E_x + \\chi_{e, xy} E_y + \\chi_{e, xz} E_z) \\\\ P_y & = \\epsilon_0 (\\chi_{e,yx} E_x + \\chi_{e, yy} E_y + \\chi_{e, yz} E_z) \\\\ P_z & = \\epsilon_0 (\\chi_{e,zx} E_x + \\chi_{e, zy} E_y + \\chi_{e, zz} E_z) \\\\ \\end{align*} \\tagl{4.38} just as Eq. 4.1 was superseded by Eq. 4.3 for asymmetrical molecules. The nine coefficients constitute the susceptibility tensor 4.4.2: Boundary Value Problems with Linear Dielectrics In a (homogeneous isotropic) linear dielectric, the bound charge density is proportional to the free charge density \\rho_b = - \\div \\vec{P} = - \\div \\left( \\epsilon_0 \\frac{\\chi_e}{\\epsilon} \\vec{D} \\right) = - \\left( \\frac{\\chi_e}{1 + \\chi_e} \\right) \\rho_f \\tagl{4.39} In particular, unless free charge is actually embedded in the material, \\rho = 0 and any net charge must reside at the surface. Within such a dielectric, then, the potential obeys Laplace's equation, and all the machinery of Chapter 3 carries over. It is convenient, however, to rewrite the boundary conditions in a way that makes reference only to the free charge. Equation 4.26 says \\epsilon_{above} E_{above} ^{\\perp} - \\epsilon_{below} E_{below} ^{\\perp} = \\sigma_f \\tagl{4.40} or, in terms of the potential, \\epsilon_{above} \\pdv{V_{above}}{n} - \\epsilon_{below} \\pdv{V_{below}}{n} = - \\sigma_f \\tagl{4.41} whereas the potential itself is, of course, continuous (Eq 2.34): V_{above} = V_{below} \\tagl{4.42} Example 4.7 A sphere of homogeneous linear dielectric material is placed in an otherwise uniform electric field \\vec{E_0} (Fig 4.27). Find the electric field inside the sphere Solution This is very similar to Ex 3.8, in which an uncharged conducting sphere was introduced into a uniform field. In that case, the field of the induced charge canceled \\vec{E_0} within the sphere. In a dielectric, the cancellation from the bound charge is incomplete. Our problem is to solve Laplace's equation, for V_{in}(r, \\theta) when r \\leq R and V_{out}(r, \\theta) when r \\geq R , subject to the boundary conditions \\tag{i} V_{in} = V_{out} \\qquad \\text{ at } r = R \\tag{ii} \\epsilon \\pdv{V_{in}}{r} = \\epsilon_0 \\pdv{V_{out}}{r} \\qquad \\text{ at } r = R \\tag{iii} V_{out} \\rightarrow - E_0 r \\cos \\theta \\qquad \\text{ for } r \\gg R (The second of these follows from Eq 4.41, since there is no free charge at the surface.) Inside the sphere, Eq 3.65 says V_{in}(r, \\theta) = \\sum_{l=0} ^{\\infty} A_l r^l P_l(\\cos \\theta) \\tagl{4.44} outside the sphere, in view of (iii), we have V_{out}(r, \\theta) = - E_0 r \\cos \\theta + \\sum_{l=0} ^\\infty \\frac{B_l}{r^{l+1}} P_l(\\cos \\theta) \\tagl{4.45} Boundary condition (i) requires that \\sum_{l=0} ^{\\infty} A_l R^l P_l(\\cos \\theta) = - E_0 R \\cos \\theta + \\sum_{l=0} ^\\infty \\frac{B_l}{R^{l+1}} P_l(\\cos \\theta) so A_l R^l = \\frac{B_l}{R_{l+1}}, \\qquad \\text{ for } l \\neq 1 \\\\ A_1 R = - E_0 R + \\frac{B_1}{R^2} \\tagl{4.46} Meanwhile, condition (ii) yields \\epsilon_r \\sum_{l=0} ^\\infty l A_l R^{l-1} P_l (\\cos \\theta) = - E_0 \\cos \\theta - \\sum_{l=0} ^\\infty \\frac{(l+1) B_l}{R^{l+2}} P_l(\\cos theta) so \\begin{align*} \\epsilon_r l A_l R^{l-1} & = - \\frac{(l+1) B_l }{B^{l+2}} , \\text{ for } l \\neq 1 \\\\ \\epsilon_r A_1 & = - E_0 - \\frac{2 B_1}{R^3} \\end{align*} \\tagl{4.47} It follows that A_l = B_l = 0 \\qquad \\text{ for } l \\neq 1\\\\ A_1 = - \\frac{3}{\\epsilon_r + 2} E_0 \\quad B_1 = \\frac{\\epsilon_r - 1}{\\epsilon_r + 2} R^3 E_0 \\tagl 4.48 Evidently V_{in} (r, \\theta) = - \\frac{3 E_0}{\\epsilon_r + 2} r \\cos \\theta = - \\frac{3E_0}{\\epsilon_r + 2} z We should be used to finding that the field within a polarized sphere is uniform, but it's still a surprising result: \\vec{E} = \\frac{3}{\\epsilon_r + 2} \\vec{E_0} \\tagl{4.49} Example 4.8 Suppose the entire region below the plane z = 0 in Fig 4.28 is filled with uniform linear dielectric material of susceptibility \\chi_e . Calculate the force on a point charge q situated a distance d above the origin. Solution The surface bound charge on the xy plane is of opposite sign to q , so the force will be attractive. (In view of Eq 4.39, there is no volume bound charge.) Let us first calculate \\sigma_b , using \\eqref{4.11} and \\eqref{4.30} : \\sigma_b = \\vec{P} \\cdot \\vu{n} = P_z = \\epsilon_0 \\chi_e E_z where E_z is the z-component of the total field inside the dielectric, at z = 0 . This field is due in part to q and in part to the bound charge itself. From Coulomb's law, the former contribution is - \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{(r^2 + d^2)} \\cos \\theta = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd}{(r^2 + d^2)^{3/2}} where r = \\sqrt{x^2 + y^2} is the distance from the origin. We can immediately read off the z-component of the field of the bound charge using Gauss's law as - \\sigma_b / 2 \\epsilon_0 . Thus, \\sigma_b = \\epsilon_0 \\chi_e \\left[ - \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd}{(r^2 + d^2)^{3/2}} - \\frac{\\sigma_b}{2 \\epsilon_0} \\right] which we can solve for \\sigma_b \\sigma_b = - \\frac{1}{2 \\pi} \\left( \\frac{\\chi_e}{\\chi_e + 2} \\right) \\frac{qd}{(r^2 + d^2)^{3/2}} \\tagl{4.50} Apart from the factor \\chi_e / (\\chi_e + 2) this is exactly the same as the induced charge on an infinite conducting plane under similar circumstances (Eq 3.10). Evidently the total bound charge is q_b = - \\left( \\frac{\\chi_e}{\\chi_e + 2} \\right)q \\tagl{4.51} We could, of course, get the field of \\sigma_b by direct integration \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\int \\left( \\frac{\\vu{\\gr}}{\\gr ^2} \\sigma_b \\dd a \\right) But, as in the case of the conducting plane, there is a nicer solution by the method of images. If we replace the dielectric by a single point charge q_b at the image position (0, 0, -d), we have V = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{1}{\\sqrt{x^2 + y^2 +(z-d)^2}} + \\frac{q_b}{\\sqrt{x^2 + y^2 + (z + d)^2}} \\right] \\tagl{4.52} in the region z > 0 . Meanwhile, a charge (q + q_b) at (0, 0, d) yields the potential V = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{q + q_b}{\\sqrt{x^2 + y^2 + (z - d)^2}} \\right] \\tagl{4.53} for the region z < 0 . Taken together, \\eqref{4.52} and \\eqref{4.53} constitute a function that satisfies Poisson's equation with a point charge q at (0, 0, d), which goes to zero at infinity, which is continuous at the boundary z = 0 , and whose normal derivative exhibits the discontinuity appropriate to a surface charge \\sigma_b at z = 0 : - \\epsilon_0 \\left( \\left. \\pdv{V}{z} \\right| _{z = 0+} - \\left. \\pdv{V}{z} \\right|_{z = 0-} \\right) = - \\frac{1}{2 \\pi} \\left( \\frac{\\chi_e}{\\chi_e + 2} \\right) Accordingly, this is the correct potential for our problem. In particular, the force on q is: \\vec{F} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q q_b}{(2d)^2} \\vu{z} = - \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{\\chi_e}{\\chi_e + 2} \\right) \\frac{q^2}{4 d^2} \\vu{z} \\tagl{4.54} I do not claim to have provided a compelling motivation for \\eqref{4.52} and \\eqref{4.53} - like all image solutions, this one owes its justification to the fact that it works : it solves Poisson's equation, and it meets the boundary conditions. Still, discovering an image solution is not entirely a matter of guesswork. There are at least two \"rules of the game\": (1) You must never put an image charge into the region where you're computing the potential. (2) The image charges must add up to the correct total in each region. 4.4.3: Energy in Dielectric Systems It takes work to charge up a capacitor (Eq 2.55): W = \\frac{1}{2} C V^2 If the capacitor is filled with linear dielectric, its capacitance exceeds the vacuum value by a factor of the dielectric constant C = \\epsilon_r C_{vac} as we found in Ex. 4.6. Evidently, the work necessary to charge a dielectric-filled capacitor is increased by the same factor. The reason is pretty clear: you have to pump more (free) charge, to achieve a given potential, because part of the field is canceled off by the bound charges. In chapter 2 we got a general formula for the energy stored in any electrodynamic system W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd \\tau \\tagl{4.55} The case of the dielectric-filled capacitor suggests that this should be changed to W = \\frac{\\epsilon_0}{2} \\int \\epsilon_r E^2 \\dd \\tau = \\frac{1}{2} \\int \\vec{D} \\cdot \\vec{E} \\dd \\tau in the presence of linear dielectrics. To prove it, suppose the dielectric material is fixed in position, and we bring in the free charge, a bit at a time. As \\rho_f is increased by an amount \\Delta \\rho_f , the polarization will change and with it the bound charge distribution; but we're interested only in the work done on the incremental free charge: \\Delta W = \\int (\\Delta \\rho_f) V \\dd \\tau \\tagl{4.56} Since \\div \\vec{D} = \\rho_f, \\Delta \\rho_f = \\div (\\Delta \\vec{D}) , where \\Delta \\vec{D} is the resulting change in D , so \\Delta W = \\int [ \\div ( \\Delta D) ] V \\dd \\tau Now \\div [ (\\Delta D) V ] = [ \\div (\\Delta \\vec{D})] V + \\Delta \\vec{D} \\cdot (\\grad V) and hence, integrating by parts, \\Delta W = \\in \\div [ (\\Delta \\vec{D}) V] \\dd \\tau + \\int ( \\Delta \\vec{D}) \\cdot \\vec{E} \\dd \\tau The divergence theorem turns the first term into a surface integral, which vanishes if we integrate over all space. Therefore, the work done is equal to \\Delta W = \\int (\\Delta \\vec{D}) \\cdot \\vec{E} \\dd \\tau \\tagl{4.57} So far, this applies to any material. In the specific case of a linear dielectric, \\frac{1}{2} \\Delta ( \\vec{D} \\cdot \\vec{E} ) = \\frac{1}{2} \\Delta (\\epsilon E^2) = \\epsilon (\\Delta \\vec{E}) \\cdot \\vec{E} = ( \\Delta \\vec{D}) \\cdot \\vec{E} (for infinitesimal increments). Thus \\Delta W = \\Delta \\left( \\frac{1}{2} \\int \\vec{D} \\cdot \\vec{E} \\dd \\tau \\right) The total work done, then, as we build the free charge up from zero to the final configuration is W = \\frac{1}{2} \\int \\vec{D} \\cdot \\vec{E} \\dd \\tau \\tagl{4.58} as anticipated. It may puzzle you that Eq. 4.55, which we derived quite generally in Chapter 2, does not seem to apply in the presence of dielectrics, where it is replaced by Eq. 4.58. The point is not that one or the other of these equations is wrong, but rather that they address somewhat different questions. The distinction is subtle, so let's go right back to the beginning: What do we mean by \"the energy of a system\"? Answer: It is the work required to assemble the system. Very well - but when dielectrics are involved, there are two quite different ways one might construe this process: We bring in all the charges (free and bound), one by one, with tweezers, and glue each one down in its proper final location. If this is what you mean by \"assemble the system,\" then Eq. 4.55 is your formula for the energy stored. Notice, however, that this will not include the work involved in stretching and twisting the dielectric molecules (if we picture the positive and negative charges as held together by tiny springs, it does not include the spring energy, \\frac{1}{2} k x^2 , associated with polarizing each molecule). With the unpolarized dielectric in place, we bring in the free charges, one by one, allowing the dielectric to respond as it sees fit. If this is what you mean by \"assemble the system\" (and ordinarily it is, since free charge is what we actually push around), then Eq. 4.58 is the formula you want. In this case the \"spring\" energy is included, albeit indirectly, because the force you must apply to the free charge depends on the disposition of the bound charge; as you move the free charge, you are automatically stretching those \"springs.\" Example 4.9 A sphere of radius R is filled with material of dielectric constant \\epsilon_r and uniform embedded free charge \\rho_f . What is the energy of this configuration? Solution From Gauss's law, the displacement is \\vec{D}(r) = \\begin{cases} \\frac{\\rho_f}{3} \\vec{r} & \\qquad ( r < R) \\\\ \\frac{\\rho_f}{3} \\frac{R^3}{r^2} \\vu{r} & \\qquad ( r > R ) \\end{cases} So the electric field is \\vec{E}(r) = \\begin{cases} \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\vec{r} & \\qquad ( r < R) \\\\ \\frac{\\rho_f}{3 \\epsilon_0} \\frac{R^3}{r^2} \\vu{r} & \\qquad ( r > R ) \\end{cases} The purely electrostatic energy is \\begin{align*} W & = \\frac{\\epsilon_0}{2} \\left[ \\left( \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r } \\right) ^2 \\int _0 ^R r^2 4 \\pi r^2 \\dd r + \\left( \\frac{\\rho_f}{3 \\epsilon_0} \\right)^2 R^6 \\int_R ^\\infty \\frac{1}{r^4} 4 \\pi r^2 \\dd r \\right] \\\\ & = \\frac{2 \\pi}{9 \\epsilon_0} \\rho_f ^2 R^5 \\left( \\frac{1}{5 \\epsilon_r ^2} + 1 \\right) \\end{align*} But the total energy (Eq 4.58) is \\begin{align*} W_2 & = \\frac{1}{2} \\left[ \\left( \\frac{\\rho_f}{3} \\right)\\left( \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r } \\right) \\int_0 ^R r^2 4 \\pi r ^2 \\dd r + \\left( \\frac{\\rho_f R^3}{3} \\right) \\left( \\frac{\\rho_f R^3}{3 \\epsilon_0} \\right) \\int _R ^\\infty \\frac{1}{r^4} 4 \\pi r^2 \\dd r \\right] \\\\ & = \\frac{2 \\pi}{9 \\epsilon_0}\\rho_f ^2 R^5 \\left( \\frac{1}{5 \\epsilon_r} + 1 \\right) \\end{align*} Notice that W_1 < W_2 - that's because W_1 does not include the energy involved in stretching the molecules. Let's check that W_2 is the work done on the free charge in assembling the system. We start with the (uncharged, unpolarized) dielectric sphere, and bring in the free charge in infinitesimal installments (dq), filling out the sphere layer by layer. When we have reached radius r' , the electric field is \\vec{E}(r) = \\begin{cases} \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\vec{r} & \\quad (r < r') \\\\ \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\frac{r' ^3}{r^2} \\vu{r} & \\quad (r' < r < R ) \\\\ \\frac{\\rho_f}{3 \\epsilon_0} \\frac{r' ^3}{r^2} \\vu{r} & \\quad ( r > R) \\end{cases} The work required to bring the next dq in from infinity to r' is \\begin{align*} \\dd W & = - \\dd q \\left[ \\int_{\\infty} ^R \\vec{E} \\cdot \\dd \\vec{l} + \\int _R ^{r'} \\vec{E} \\cdot \\dd \\vec{l} \\right] \\\\ & = - \\dd q \\left[ \\frac{\\rho_f r' ^3}{3 \\epsilon_0} \\int_{\\infty} ^R \\frac{1}{r^2} \\dd r + \\frac{\\rho_f r' ^3}{3 \\epsilon_0 \\epsilon_r} \\int _{R} ^{r'} \\frac{1}{r^2} \\dd r \\right] \\\\ & = \\frac{\\rho_f r' ^3}{3 \\epsilon_0} \\left[ \\frac{1}{R} + \\frac{1}{\\epsilon_r} \\left( \\frac{1}{r'} - \\frac{1}{R} \\right) \\right] \\dd q \\end{align*} This increases the radius (r') \\dd q = \\rho_f 4 \\pi r' ^2 \\dd r' so the total work done, in going from r'=0 to r' = R is \\begin{align*} W & = \\frac{4 \\pi \\rho_f ^2}{3 \\epsilon_0} \\left[ \\frac{1}{R} \\left( 1 - \\frac{1}{ \\epsilon_r} \\right) \\int_0 ^R r' ^5 + \\frac{1}{\\epsilon_r} \\int_0 ^R r' ^4 \\dd r' \\right] \\\\ & = \\frac{2 \\pi}{9 \\epsilon_0}\\rho_f ^2 R^5 \\left( \\frac{1}{5 \\epsilon_r} + 1 \\right) = W_2 \\end{align*} Evidently the energy \"stored in the springs\" is W_{sprint} = W_2 - W_1 = \\frac{2 \\pi}{45 \\epsilon_0 \\epsilon_r ^2} \\rho _f ^2 R^5 (\\epsilon_r - 1) I would like to confirm this in an explicit model. Picture the dielectric as a collection of tiny proto-dipoles, each consisting of +q and -q attached to a spring of constant k and equilibrium length 0, so in the absence of any field the positive and negative ends coincide. One end of each dipole is nailed in position (like the nuclei in a solid), but the other end is free to move in response to any imposed field. Let \\dd \\tau be the volume assigned to each proto-dipole (the dipole itself may occupy only a small portion of this space). With the field turned on, the electric force on the free end is balanced by the spring force; the charges separate by a distance d: qE = kd . In our case \\vec{E} = \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\vec{r} The resulting dipole moment is p = qd and the polarization is P = p / \\dd \\tau so k = \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r d^2} P r \\dd \\tau The energy of this particular spring is \\dd W_{spring} = \\frac{1}{2} k d^2 = \\frac{\\rho_f}{6 \\epsilon_0 \\epsilon_r} P r \\dd \\tau and hence the total is W_{spring} = \\frac{\\rho_f}{6 \\epsilon_0 \\epsilon_r} \\int P r \\dd \\tau Now \\vec{P} = \\epsilon_0 \\chi_e \\vec{E} = \\epsilon_0 \\chi_e \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r } \\vec{r} = \\frac{(\\epsilon_r - 1) \\rho_f}{3 \\epsilon_r} \\vec{r} so W_{spring} = \\frac{\\rho_f}{6 \\epsilon_0 \\epsilon_r} \\frac{(\\epsilon_r - 1) \\rho_f}{3 \\epsilon_r} 4 \\pi \\int_0 ^R r^4 \\dd r = \\frac{2 \\pi}{45 \\epsilon_0 \\epsilon_r ^2} \\rho_f ^2 R^5 (\\epsilon_r - 1) and it works out perfectly. It is sometimes alleged that Eq. 4.58 represents the energy even for nonlinear dielectrics, but this is false: To proceed beyond Eq. 4.57, one must assume linearity. In fact, for dissipative systems the whole notion of \"stored energy\" loses its meaning, because the work done depends not only on the final configuration but on how it got there. If the molecular \"springs\" are allowed to have some friction, for instance, then W_{spring} can be made as large as you like, by assembling the charges in such a way that the spring is obliged to expand and contract many times before reaching its final state. In particular, you get nonsensical results if you try to apply Eq. 4.58 to electrets, with frozen-in polarization (see Prob. 4.27). 4.4.4: Forces on Dielectrics Just as a conductor is attracted into an electric field (Eq. 2.51), so too is a dielectric - and for essentially the same reason: the bound charge tends to accumulate near the free charge of the opposite sign. But the calculation of forces on dielectrics can be surprisingly tricky. Consider, for example, the case of a slab of linear dielectric material, partially inserted between the plates of a parallel-plate capacitor (Fig. 4.30). We have always pretended that the field is uniform inside a parallel-plate capacitor, and zero outside. If this were literally true, there would be no net force on the dielectric at all, since the field everywhere would be perpendicular to the plates. However, there is in reality a fringing field around the edges, which for most purposes can be ignored but in this case is responsible for the whole effect. (Indeed, the field could not terminate abruptly at the edge of the capacitor, for if it did, the line integral of E around the closed loop shown in Fig. 4.31 would not be zero.) It is this nonuniform fringing field that pulls the dielectric into the capacitor. Fringing fields are notoriously difficult to calculate; luckily, we can avoid this altogether, by the following ingenious method. Let W be the energy of the system - it depends, of course, on the amount of overlap. If I pull the dielectric out an infinitesimal distance dx, the energy is changed by an amount equal to the work done: \\dd W = F_{me} \\dd x \\tagl{4.59} where F_{me} is the force I mus exert, to counteract the electrical force F on the dielectric. Thus, the electrical force on the slab is F = - \\dv{W}{x} \\tagl{4.60} Now, the energy stored in the capacitor is W = \\frac{1}{2} C V^2 \\tagl{4.61} and the capacitance in this case is C = \\frac{\\epsilon_0 w}{d} (\\epsilon_r l - \\chi_e x) \\tagl{4.62} where l is the length of the plates (Fig 4.30). Let's assume that the total charge on the plates is held constant (Q = CV) , as the dielectric moves. In terms of Q , W = \\frac{1}{2} \\frac{Q^2}{C} \\tagl{4.63} so F = - \\dv{W}{x} = \\frac{1}{2} \\frac{Q^2}{C^2} \\dv{C}{x} = \\frac{1}{2} V^2 \\dv{C}{x} \\tagl{4.64} But \\dv{C}{x} = - \\frac{\\epsilon_0 \\chi_e w}{d} and hence F = - \\frac{\\epsilon_0 \\chi_e w}{2d} V^2 \\tagl{4.65} (The minus sign indicates that the force is in the negative x direction; the dielectric is pulled into the capacitor.) It is a common error to use Eq. 4.61 (with V constant), rather than Eq. 4.63 (with Q constant), in computing the force. One then obtains F = - \\frac{1}{2} V^2 \\dv{C}{x} which is off by a sign. It is, of course, possible to maintain the capacitor at a fixed potential, by connecting it up to a battery. But in that case the battery also does work as the dielectric moves; instead of Eq. 4.59, we now have \\dd W = F_{me} \\dd x + V \\dd Q \\tagl{4.66} where V \\dd Q is the work done by the battery. It follows that F = - \\dv{W}{x} + V \\dv{Q}{x} = - \\frac{1}{2} V^2 \\dv{C}{x} + V^2 \\dv{C}{x} = \\frac{1}{2} V^2 \\dv{C}{x} \\tagl{4.67} the same as before, with the correct sign. Please understand: The force on the dielectric cannot possibly depend on whether you plan to hold Q constant or V constant - it is determined entirely by the distribution of charge, free and bound. It's simpler to calculate the force assuming constant Q , because then you don't have to worry about work done by the battery; but if you insist, it can be done correctly either way. Notice that we were able to determine the force without knowing anything about the fringing fields that are ultimately responsible for it! Of course, it's built into the whole structure of electrostatics that \\curl \\vec{E} = 0 , and hence that the fringing fields must be present; we're not really getting something for nothing here - just cleverly exploiting the internal consistency of the theory. The energy stored in the fringing fields themselves (which was not accounted for in this derivation) stays constant, as the slab moves; what does change is the energy well inside the capacitor, where the field is nice and uniform.","title":"4.4 - Linear Dielectrics"},{"location":"ch4-4/#44-linear-dielectrics","text":"","title":"4.4: Linear Dielectrics"},{"location":"ch4-4/#441-susceptibility-permittivity-dielectric-constant","text":"In the first few sections of this chapter we did not commit ourselves as to the cause of P ; we dealt only with the effects of polarization. From the qualitative essence of 4.1, though, we know that the polarization of a dielectric ordinarily results from an electric field, which lines up the atomic or molecular dipoles. For many substances, in fact, the polarization is proportional to the field, provided E is not too strong: \\vec{P} = \\epsilon_0 \\chi_e \\vec{E} \\tagl{4.30} The constant of proportionality, \\chi_e , is called the electric susceptibility of the medium (a factor of \\epsilon_0 has been extracted to make \\chi_e dimensionless). The value of \\chi_e depends on the microscopic structure of the substance in question (and also on external conditions such as temperature). I shall call materials that obey \\eqref{4.30} linear dielectrics . In modern optical applications, especially, nonlinear materials have become increasingly important. For these there is a second term relating P to E - typically a cubic term. In general, Eq 4.30 can be regarded as the first (nonzero) term in the Taylor expansion of P in powers of E . Note that E in \\eqref{4.30} is the total field; it may be due in part to free charges and in part to the polarization itself. If, for instance, we put a piece of dielectric into an external field \\vec{E_0} , we cannot compute P directly from the linear susceptibility relation; the external field will polarize the material, and this polarization will produce its own field, which then contributes to the total field, and this in turn modifies the polarization, which... Breaking out of this infinite regress is not always easy. You'll see some examples in a moment. The simplest approach is to begin with the displacement , at least in those cases where D can be deduced directly from the free charge distribution. In linear media we have \\vec{D} = \\epsilon_0 \\vec{E} + \\vec{P} = \\epsilon_0 \\vec{E} + \\epsilon_0 \\chi_e \\vec{E} = \\epsilon_0 (1 + \\chi_e) \\vec{E} \\tagl{4.31} so D is also proportional to E \\vec{E} = \\epsilon \\vec{E} \\tagl{4.32} where \\epsilon \\equiv \\epsilon_0 (1 + \\chi_e) \\tagl{4.33} This new constant \\epsilon is called the permittivity of the material. (In vacuum, where there is no matter to polarize, the susceptibility is zero, and the permittivity is \\epsilon_0 . That's why \\epsilon_0 is called the permittivity of free space. I dislike the term, for it suggest that the vacuum is just a special kind of linear dielectric, in which the permittivity happens to have the value 8.85 \\times 10^{-12} C^2 / N \\cdot m^2 .) If you remove a factor of \\epsilon_0 , the remaining dimensionless quantity \\epsilon_r = 1 + \\chi _e = \\frac{\\epsilon}{\\epsilon_0} \\tagl{4.34} is called the relative permittivity , or dielectric constant , of the material. Dielectric constants for some common substances are listed in Table 4.2. (Notice that \\epsilon_r is greater than 1, for all ordinary materials.) Of course, the permittivity and the dielectric constant do not convey any information that was not already available in the susceptibility, nor is there anything essentially new in Eq 4.32: the physics of linear dielectrics is all contained in \\eqref{4.30}","title":"4.4.1: Susceptibility, Permittivity, Dielectric Constant"},{"location":"ch4-4/#example-45","text":"A metal sphere of radius a carries a charge Q (Fig 4.20). It is surrounded, out to radius b , by linear dielectric material of permittivity \\epsilon . Find the potential at the center (relative to infinity). Solution To compute V, we need to know E; to find E, we might first try to locate the bound charge; we could get the bound charge from P , but we can't calculate P unless we already know E . What we do know is the free charge, and our arrangement is spherically symmetric, so we can go straight for D using Eq 4.23: \\vec{D} = \\frac{Q}{4 \\pi r^2} \\vu{r}, \\quad \\text{ for all points } r > a Inside the conducting sphere, all our electrostatic fields are zero. We then obtain E via Eq 4.32: \\vec{E} = \\begin{cases} \\frac{Q}{4 \\pi \\epsilon r^2} \\vu{r} & \\quad \\text{ for } a < r < b \\\\ \\frac{Q}{4 \\pi \\epsilon_0 r^2} \\vu{r} & \\quad \\text{ for } r > b \\end{cases} We get the potential at the center by integrating E V = - \\int _{\\infty} ^0 \\vec{E} \\cdot \\dd \\vec{l} = \\\\ - \\int _{\\infty} ^b \\left( \\frac{Q}{4 \\pi \\epsilon_0 r^2} \\right) \\dd r - \\int_b ^a \\left( \\frac{Q}{4 \\pi \\epsilon r^2} \\right) \\dd r\\\\ = \\frac{Q}{4 \\pi } \\left( \\frac{1}{\\epsilon_0 b} + \\frac{1}{\\epsilon a} - \\frac{1}{\\epsilon b} \\right) In this case, we didn't need to compute the polarization or the bound charge explicitly, but we can easily do so now that we have E : \\vec{P} = \\epsilon_0 \\chi_e \\vec{E} = \\frac{\\epsilon_0 \\chi_e Q}{4 \\pi \\epsilon r^2} \\vu{r} within the dielectric, so that \\rho_b = - \\div \\vec{P} = 0 and \\sigma_b = \\vec{P} \\cdot \\vu{n} = \\begin{cases} \\frac{\\epsilon_0 \\chi_e Q}{4 \\pi \\epsilon b^2} & \\qquad \\text{ at the outer surface } \\\\ \\frac{- \\epsilon_0 \\chi_e Q}{4 \\pi \\epsilon a^2} & \\qquad \\text{ at the inner surface } \\end{cases} Notice that the surface bound charge at a is negative ( \\vu{n} points outward with respect to the dielectric, which is + \\vu{n} at b, but -\\vu{r} at a). This is natural, since the charge on the metal sphere attracts its opposite in all the dielectric molecules. It is this layer of negative charge that reduces the field, within the dielectric, from 1 / 4 \\pi \\epsilon_0 (Q / r^2) \\vu{r} to 1 / 4 \\pi \\epsilon (Q / r^2) \\vu{r} . In this respect, a dielectric is rather like an imperfect conductor: on a conducting shell the induced surface charge would be such as to cancel out the field of Q entirely in the region a < r < b ; the dielectric does the best it can, but the cancellation is only partial. Since linear dielectrics give us cases where P and D are proportional to E , you might suppose that linear dielectrics escape the defect in the parallel between E and D . Does it not follow that their curls, like E 's, must vanish? Unfortunately, it does not, for the line integral of P around a closed path that straddles the boundary between one type of material and another need not be zero, even though the integral of E around the same loop must be. The reason is that the proportionality factor \\epsilon_0 \\chi_e is different on the two sides. For instance, at the interface between a polarized dielectric and the vacuum (Fig 4.21), P is zero on one side but not on the other. Around this loop, \\oint \\vec{P} \\cdot \\dd \\vec{l} \\neq 0 , and hence, by Stokes' theorem, the curl of P cannot vanish everywhere within the loop (in fact, it is infinite at the boundary). Of course, if space is entirely filled with a homogeneous linear dielectric, then this objection is void; in this rather special circumstance \\div \\vec{D} = \\rho_f \\quad \\text{and} \\quad \\curl \\vec{D} = 0 so D can be found from the free charge just as though the dielectric were not there: \\vec{D} = \\epsilon_0 \\vec{E_{vac}} where \\vec{E_{vac}} is the field the same charge distribution would produce in the absence of any dielectric. According to \\eqref{4.32} and \\eqref{4.34} , therefore, \\vec{E} = \\frac{1}{\\epsilon} \\vec{D} = \\frac{1}{\\epsilon_r} \\vec{E_{vac}} \\tagl{4.35} Conclusion: when all space is filled with a homogeneous linear dielectric, the field everywhere is simply reduced by a factor of one over the dielectric constant. (Actually it's not necessary for the dielectric to fill all space; in regions where the field is zero anyway, it can hardly matter whether the dielectric is present or not, since there's no polarization in any event.) For example, if a free charge q is embedded in a large dielectric, the field it produces is \\vec{E} = \\frac{1}{4 \\pi \\epsilon} \\frac{q}{r^2} \\vu{r} \\tagl{4.36} (that's \\epsilon , not \\epsilon_0 ), and the force it exerts on nearby charges is reduced accordingly. But it's not that there is anything wrong with Coulomb's law; rather, the polarization of the medium partially \"shields\" the charge, by surrounding it with bound charge of the opposite sign (Fig 4.22)","title":"Example 4.5"},{"location":"ch4-4/#example-46","text":"A parallel-plate capacitor (Fig 4.23) is filled with insulating material of dielectric constant \\epsilon_r . What effect does this have on its capacitance? Solution Since the field is confined to the space between the plates, the dielectric will reduce E , and hence also the potential difference V, by a factor 1 / \\epsilon_r . Accordingly, the capacitance C = Q / V is increased by a factor of the dielectric constant C = \\epsilon_r C_{vac} \\tagl{4.37} This is, in fact, a common way to beef up a capacitor A crystal is generally easier to polarize in some directions than others, and in this case Eq 4.30 is replaced by the general linear relation \\begin{align*} P_x & = \\epsilon_0 (\\chi_{e,xx} E_x + \\chi_{e, xy} E_y + \\chi_{e, xz} E_z) \\\\ P_y & = \\epsilon_0 (\\chi_{e,yx} E_x + \\chi_{e, yy} E_y + \\chi_{e, yz} E_z) \\\\ P_z & = \\epsilon_0 (\\chi_{e,zx} E_x + \\chi_{e, zy} E_y + \\chi_{e, zz} E_z) \\\\ \\end{align*} \\tagl{4.38} just as Eq. 4.1 was superseded by Eq. 4.3 for asymmetrical molecules. The nine coefficients constitute the susceptibility tensor","title":"Example 4.6"},{"location":"ch4-4/#442-boundary-value-problems-with-linear-dielectrics","text":"In a (homogeneous isotropic) linear dielectric, the bound charge density is proportional to the free charge density \\rho_b = - \\div \\vec{P} = - \\div \\left( \\epsilon_0 \\frac{\\chi_e}{\\epsilon} \\vec{D} \\right) = - \\left( \\frac{\\chi_e}{1 + \\chi_e} \\right) \\rho_f \\tagl{4.39} In particular, unless free charge is actually embedded in the material, \\rho = 0 and any net charge must reside at the surface. Within such a dielectric, then, the potential obeys Laplace's equation, and all the machinery of Chapter 3 carries over. It is convenient, however, to rewrite the boundary conditions in a way that makes reference only to the free charge. Equation 4.26 says \\epsilon_{above} E_{above} ^{\\perp} - \\epsilon_{below} E_{below} ^{\\perp} = \\sigma_f \\tagl{4.40} or, in terms of the potential, \\epsilon_{above} \\pdv{V_{above}}{n} - \\epsilon_{below} \\pdv{V_{below}}{n} = - \\sigma_f \\tagl{4.41} whereas the potential itself is, of course, continuous (Eq 2.34): V_{above} = V_{below} \\tagl{4.42}","title":"4.4.2: Boundary Value Problems with Linear Dielectrics"},{"location":"ch4-4/#example-47","text":"A sphere of homogeneous linear dielectric material is placed in an otherwise uniform electric field \\vec{E_0} (Fig 4.27). Find the electric field inside the sphere Solution This is very similar to Ex 3.8, in which an uncharged conducting sphere was introduced into a uniform field. In that case, the field of the induced charge canceled \\vec{E_0} within the sphere. In a dielectric, the cancellation from the bound charge is incomplete. Our problem is to solve Laplace's equation, for V_{in}(r, \\theta) when r \\leq R and V_{out}(r, \\theta) when r \\geq R , subject to the boundary conditions \\tag{i} V_{in} = V_{out} \\qquad \\text{ at } r = R \\tag{ii} \\epsilon \\pdv{V_{in}}{r} = \\epsilon_0 \\pdv{V_{out}}{r} \\qquad \\text{ at } r = R \\tag{iii} V_{out} \\rightarrow - E_0 r \\cos \\theta \\qquad \\text{ for } r \\gg R (The second of these follows from Eq 4.41, since there is no free charge at the surface.) Inside the sphere, Eq 3.65 says V_{in}(r, \\theta) = \\sum_{l=0} ^{\\infty} A_l r^l P_l(\\cos \\theta) \\tagl{4.44} outside the sphere, in view of (iii), we have V_{out}(r, \\theta) = - E_0 r \\cos \\theta + \\sum_{l=0} ^\\infty \\frac{B_l}{r^{l+1}} P_l(\\cos \\theta) \\tagl{4.45} Boundary condition (i) requires that \\sum_{l=0} ^{\\infty} A_l R^l P_l(\\cos \\theta) = - E_0 R \\cos \\theta + \\sum_{l=0} ^\\infty \\frac{B_l}{R^{l+1}} P_l(\\cos \\theta) so A_l R^l = \\frac{B_l}{R_{l+1}}, \\qquad \\text{ for } l \\neq 1 \\\\ A_1 R = - E_0 R + \\frac{B_1}{R^2} \\tagl{4.46} Meanwhile, condition (ii) yields \\epsilon_r \\sum_{l=0} ^\\infty l A_l R^{l-1} P_l (\\cos \\theta) = - E_0 \\cos \\theta - \\sum_{l=0} ^\\infty \\frac{(l+1) B_l}{R^{l+2}} P_l(\\cos theta) so \\begin{align*} \\epsilon_r l A_l R^{l-1} & = - \\frac{(l+1) B_l }{B^{l+2}} , \\text{ for } l \\neq 1 \\\\ \\epsilon_r A_1 & = - E_0 - \\frac{2 B_1}{R^3} \\end{align*} \\tagl{4.47} It follows that A_l = B_l = 0 \\qquad \\text{ for } l \\neq 1\\\\ A_1 = - \\frac{3}{\\epsilon_r + 2} E_0 \\quad B_1 = \\frac{\\epsilon_r - 1}{\\epsilon_r + 2} R^3 E_0 \\tagl 4.48 Evidently V_{in} (r, \\theta) = - \\frac{3 E_0}{\\epsilon_r + 2} r \\cos \\theta = - \\frac{3E_0}{\\epsilon_r + 2} z We should be used to finding that the field within a polarized sphere is uniform, but it's still a surprising result: \\vec{E} = \\frac{3}{\\epsilon_r + 2} \\vec{E_0} \\tagl{4.49}","title":"Example 4.7"},{"location":"ch4-4/#example-48","text":"Suppose the entire region below the plane z = 0 in Fig 4.28 is filled with uniform linear dielectric material of susceptibility \\chi_e . Calculate the force on a point charge q situated a distance d above the origin. Solution The surface bound charge on the xy plane is of opposite sign to q , so the force will be attractive. (In view of Eq 4.39, there is no volume bound charge.) Let us first calculate \\sigma_b , using \\eqref{4.11} and \\eqref{4.30} : \\sigma_b = \\vec{P} \\cdot \\vu{n} = P_z = \\epsilon_0 \\chi_e E_z where E_z is the z-component of the total field inside the dielectric, at z = 0 . This field is due in part to q and in part to the bound charge itself. From Coulomb's law, the former contribution is - \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{(r^2 + d^2)} \\cos \\theta = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd}{(r^2 + d^2)^{3/2}} where r = \\sqrt{x^2 + y^2} is the distance from the origin. We can immediately read off the z-component of the field of the bound charge using Gauss's law as - \\sigma_b / 2 \\epsilon_0 . Thus, \\sigma_b = \\epsilon_0 \\chi_e \\left[ - \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd}{(r^2 + d^2)^{3/2}} - \\frac{\\sigma_b}{2 \\epsilon_0} \\right] which we can solve for \\sigma_b \\sigma_b = - \\frac{1}{2 \\pi} \\left( \\frac{\\chi_e}{\\chi_e + 2} \\right) \\frac{qd}{(r^2 + d^2)^{3/2}} \\tagl{4.50} Apart from the factor \\chi_e / (\\chi_e + 2) this is exactly the same as the induced charge on an infinite conducting plane under similar circumstances (Eq 3.10). Evidently the total bound charge is q_b = - \\left( \\frac{\\chi_e}{\\chi_e + 2} \\right)q \\tagl{4.51} We could, of course, get the field of \\sigma_b by direct integration \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\int \\left( \\frac{\\vu{\\gr}}{\\gr ^2} \\sigma_b \\dd a \\right) But, as in the case of the conducting plane, there is a nicer solution by the method of images. If we replace the dielectric by a single point charge q_b at the image position (0, 0, -d), we have V = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{1}{\\sqrt{x^2 + y^2 +(z-d)^2}} + \\frac{q_b}{\\sqrt{x^2 + y^2 + (z + d)^2}} \\right] \\tagl{4.52} in the region z > 0 . Meanwhile, a charge (q + q_b) at (0, 0, d) yields the potential V = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{q + q_b}{\\sqrt{x^2 + y^2 + (z - d)^2}} \\right] \\tagl{4.53} for the region z < 0 . Taken together, \\eqref{4.52} and \\eqref{4.53} constitute a function that satisfies Poisson's equation with a point charge q at (0, 0, d), which goes to zero at infinity, which is continuous at the boundary z = 0 , and whose normal derivative exhibits the discontinuity appropriate to a surface charge \\sigma_b at z = 0 : - \\epsilon_0 \\left( \\left. \\pdv{V}{z} \\right| _{z = 0+} - \\left. \\pdv{V}{z} \\right|_{z = 0-} \\right) = - \\frac{1}{2 \\pi} \\left( \\frac{\\chi_e}{\\chi_e + 2} \\right) Accordingly, this is the correct potential for our problem. In particular, the force on q is: \\vec{F} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q q_b}{(2d)^2} \\vu{z} = - \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{\\chi_e}{\\chi_e + 2} \\right) \\frac{q^2}{4 d^2} \\vu{z} \\tagl{4.54} I do not claim to have provided a compelling motivation for \\eqref{4.52} and \\eqref{4.53} - like all image solutions, this one owes its justification to the fact that it works : it solves Poisson's equation, and it meets the boundary conditions. Still, discovering an image solution is not entirely a matter of guesswork. There are at least two \"rules of the game\": (1) You must never put an image charge into the region where you're computing the potential. (2) The image charges must add up to the correct total in each region.","title":"Example 4.8"},{"location":"ch4-4/#443-energy-in-dielectric-systems","text":"It takes work to charge up a capacitor (Eq 2.55): W = \\frac{1}{2} C V^2 If the capacitor is filled with linear dielectric, its capacitance exceeds the vacuum value by a factor of the dielectric constant C = \\epsilon_r C_{vac} as we found in Ex. 4.6. Evidently, the work necessary to charge a dielectric-filled capacitor is increased by the same factor. The reason is pretty clear: you have to pump more (free) charge, to achieve a given potential, because part of the field is canceled off by the bound charges. In chapter 2 we got a general formula for the energy stored in any electrodynamic system W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd \\tau \\tagl{4.55} The case of the dielectric-filled capacitor suggests that this should be changed to W = \\frac{\\epsilon_0}{2} \\int \\epsilon_r E^2 \\dd \\tau = \\frac{1}{2} \\int \\vec{D} \\cdot \\vec{E} \\dd \\tau in the presence of linear dielectrics. To prove it, suppose the dielectric material is fixed in position, and we bring in the free charge, a bit at a time. As \\rho_f is increased by an amount \\Delta \\rho_f , the polarization will change and with it the bound charge distribution; but we're interested only in the work done on the incremental free charge: \\Delta W = \\int (\\Delta \\rho_f) V \\dd \\tau \\tagl{4.56} Since \\div \\vec{D} = \\rho_f, \\Delta \\rho_f = \\div (\\Delta \\vec{D}) , where \\Delta \\vec{D} is the resulting change in D , so \\Delta W = \\int [ \\div ( \\Delta D) ] V \\dd \\tau Now \\div [ (\\Delta D) V ] = [ \\div (\\Delta \\vec{D})] V + \\Delta \\vec{D} \\cdot (\\grad V) and hence, integrating by parts, \\Delta W = \\in \\div [ (\\Delta \\vec{D}) V] \\dd \\tau + \\int ( \\Delta \\vec{D}) \\cdot \\vec{E} \\dd \\tau The divergence theorem turns the first term into a surface integral, which vanishes if we integrate over all space. Therefore, the work done is equal to \\Delta W = \\int (\\Delta \\vec{D}) \\cdot \\vec{E} \\dd \\tau \\tagl{4.57} So far, this applies to any material. In the specific case of a linear dielectric, \\frac{1}{2} \\Delta ( \\vec{D} \\cdot \\vec{E} ) = \\frac{1}{2} \\Delta (\\epsilon E^2) = \\epsilon (\\Delta \\vec{E}) \\cdot \\vec{E} = ( \\Delta \\vec{D}) \\cdot \\vec{E} (for infinitesimal increments). Thus \\Delta W = \\Delta \\left( \\frac{1}{2} \\int \\vec{D} \\cdot \\vec{E} \\dd \\tau \\right) The total work done, then, as we build the free charge up from zero to the final configuration is W = \\frac{1}{2} \\int \\vec{D} \\cdot \\vec{E} \\dd \\tau \\tagl{4.58} as anticipated. It may puzzle you that Eq. 4.55, which we derived quite generally in Chapter 2, does not seem to apply in the presence of dielectrics, where it is replaced by Eq. 4.58. The point is not that one or the other of these equations is wrong, but rather that they address somewhat different questions. The distinction is subtle, so let's go right back to the beginning: What do we mean by \"the energy of a system\"? Answer: It is the work required to assemble the system. Very well - but when dielectrics are involved, there are two quite different ways one might construe this process: We bring in all the charges (free and bound), one by one, with tweezers, and glue each one down in its proper final location. If this is what you mean by \"assemble the system,\" then Eq. 4.55 is your formula for the energy stored. Notice, however, that this will not include the work involved in stretching and twisting the dielectric molecules (if we picture the positive and negative charges as held together by tiny springs, it does not include the spring energy, \\frac{1}{2} k x^2 , associated with polarizing each molecule). With the unpolarized dielectric in place, we bring in the free charges, one by one, allowing the dielectric to respond as it sees fit. If this is what you mean by \"assemble the system\" (and ordinarily it is, since free charge is what we actually push around), then Eq. 4.58 is the formula you want. In this case the \"spring\" energy is included, albeit indirectly, because the force you must apply to the free charge depends on the disposition of the bound charge; as you move the free charge, you are automatically stretching those \"springs.\"","title":"4.4.3: Energy in Dielectric Systems"},{"location":"ch4-4/#example-49","text":"A sphere of radius R is filled with material of dielectric constant \\epsilon_r and uniform embedded free charge \\rho_f . What is the energy of this configuration? Solution From Gauss's law, the displacement is \\vec{D}(r) = \\begin{cases} \\frac{\\rho_f}{3} \\vec{r} & \\qquad ( r < R) \\\\ \\frac{\\rho_f}{3} \\frac{R^3}{r^2} \\vu{r} & \\qquad ( r > R ) \\end{cases} So the electric field is \\vec{E}(r) = \\begin{cases} \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\vec{r} & \\qquad ( r < R) \\\\ \\frac{\\rho_f}{3 \\epsilon_0} \\frac{R^3}{r^2} \\vu{r} & \\qquad ( r > R ) \\end{cases} The purely electrostatic energy is \\begin{align*} W & = \\frac{\\epsilon_0}{2} \\left[ \\left( \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r } \\right) ^2 \\int _0 ^R r^2 4 \\pi r^2 \\dd r + \\left( \\frac{\\rho_f}{3 \\epsilon_0} \\right)^2 R^6 \\int_R ^\\infty \\frac{1}{r^4} 4 \\pi r^2 \\dd r \\right] \\\\ & = \\frac{2 \\pi}{9 \\epsilon_0} \\rho_f ^2 R^5 \\left( \\frac{1}{5 \\epsilon_r ^2} + 1 \\right) \\end{align*} But the total energy (Eq 4.58) is \\begin{align*} W_2 & = \\frac{1}{2} \\left[ \\left( \\frac{\\rho_f}{3} \\right)\\left( \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r } \\right) \\int_0 ^R r^2 4 \\pi r ^2 \\dd r + \\left( \\frac{\\rho_f R^3}{3} \\right) \\left( \\frac{\\rho_f R^3}{3 \\epsilon_0} \\right) \\int _R ^\\infty \\frac{1}{r^4} 4 \\pi r^2 \\dd r \\right] \\\\ & = \\frac{2 \\pi}{9 \\epsilon_0}\\rho_f ^2 R^5 \\left( \\frac{1}{5 \\epsilon_r} + 1 \\right) \\end{align*} Notice that W_1 < W_2 - that's because W_1 does not include the energy involved in stretching the molecules. Let's check that W_2 is the work done on the free charge in assembling the system. We start with the (uncharged, unpolarized) dielectric sphere, and bring in the free charge in infinitesimal installments (dq), filling out the sphere layer by layer. When we have reached radius r' , the electric field is \\vec{E}(r) = \\begin{cases} \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\vec{r} & \\quad (r < r') \\\\ \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\frac{r' ^3}{r^2} \\vu{r} & \\quad (r' < r < R ) \\\\ \\frac{\\rho_f}{3 \\epsilon_0} \\frac{r' ^3}{r^2} \\vu{r} & \\quad ( r > R) \\end{cases} The work required to bring the next dq in from infinity to r' is \\begin{align*} \\dd W & = - \\dd q \\left[ \\int_{\\infty} ^R \\vec{E} \\cdot \\dd \\vec{l} + \\int _R ^{r'} \\vec{E} \\cdot \\dd \\vec{l} \\right] \\\\ & = - \\dd q \\left[ \\frac{\\rho_f r' ^3}{3 \\epsilon_0} \\int_{\\infty} ^R \\frac{1}{r^2} \\dd r + \\frac{\\rho_f r' ^3}{3 \\epsilon_0 \\epsilon_r} \\int _{R} ^{r'} \\frac{1}{r^2} \\dd r \\right] \\\\ & = \\frac{\\rho_f r' ^3}{3 \\epsilon_0} \\left[ \\frac{1}{R} + \\frac{1}{\\epsilon_r} \\left( \\frac{1}{r'} - \\frac{1}{R} \\right) \\right] \\dd q \\end{align*} This increases the radius (r') \\dd q = \\rho_f 4 \\pi r' ^2 \\dd r' so the total work done, in going from r'=0 to r' = R is \\begin{align*} W & = \\frac{4 \\pi \\rho_f ^2}{3 \\epsilon_0} \\left[ \\frac{1}{R} \\left( 1 - \\frac{1}{ \\epsilon_r} \\right) \\int_0 ^R r' ^5 + \\frac{1}{\\epsilon_r} \\int_0 ^R r' ^4 \\dd r' \\right] \\\\ & = \\frac{2 \\pi}{9 \\epsilon_0}\\rho_f ^2 R^5 \\left( \\frac{1}{5 \\epsilon_r} + 1 \\right) = W_2 \\end{align*} Evidently the energy \"stored in the springs\" is W_{sprint} = W_2 - W_1 = \\frac{2 \\pi}{45 \\epsilon_0 \\epsilon_r ^2} \\rho _f ^2 R^5 (\\epsilon_r - 1) I would like to confirm this in an explicit model. Picture the dielectric as a collection of tiny proto-dipoles, each consisting of +q and -q attached to a spring of constant k and equilibrium length 0, so in the absence of any field the positive and negative ends coincide. One end of each dipole is nailed in position (like the nuclei in a solid), but the other end is free to move in response to any imposed field. Let \\dd \\tau be the volume assigned to each proto-dipole (the dipole itself may occupy only a small portion of this space). With the field turned on, the electric force on the free end is balanced by the spring force; the charges separate by a distance d: qE = kd . In our case \\vec{E} = \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r} \\vec{r} The resulting dipole moment is p = qd and the polarization is P = p / \\dd \\tau so k = \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r d^2} P r \\dd \\tau The energy of this particular spring is \\dd W_{spring} = \\frac{1}{2} k d^2 = \\frac{\\rho_f}{6 \\epsilon_0 \\epsilon_r} P r \\dd \\tau and hence the total is W_{spring} = \\frac{\\rho_f}{6 \\epsilon_0 \\epsilon_r} \\int P r \\dd \\tau Now \\vec{P} = \\epsilon_0 \\chi_e \\vec{E} = \\epsilon_0 \\chi_e \\frac{\\rho_f}{3 \\epsilon_0 \\epsilon_r } \\vec{r} = \\frac{(\\epsilon_r - 1) \\rho_f}{3 \\epsilon_r} \\vec{r} so W_{spring} = \\frac{\\rho_f}{6 \\epsilon_0 \\epsilon_r} \\frac{(\\epsilon_r - 1) \\rho_f}{3 \\epsilon_r} 4 \\pi \\int_0 ^R r^4 \\dd r = \\frac{2 \\pi}{45 \\epsilon_0 \\epsilon_r ^2} \\rho_f ^2 R^5 (\\epsilon_r - 1) and it works out perfectly. It is sometimes alleged that Eq. 4.58 represents the energy even for nonlinear dielectrics, but this is false: To proceed beyond Eq. 4.57, one must assume linearity. In fact, for dissipative systems the whole notion of \"stored energy\" loses its meaning, because the work done depends not only on the final configuration but on how it got there. If the molecular \"springs\" are allowed to have some friction, for instance, then W_{spring} can be made as large as you like, by assembling the charges in such a way that the spring is obliged to expand and contract many times before reaching its final state. In particular, you get nonsensical results if you try to apply Eq. 4.58 to electrets, with frozen-in polarization (see Prob. 4.27).","title":"Example 4.9"},{"location":"ch4-4/#444-forces-on-dielectrics","text":"Just as a conductor is attracted into an electric field (Eq. 2.51), so too is a dielectric - and for essentially the same reason: the bound charge tends to accumulate near the free charge of the opposite sign. But the calculation of forces on dielectrics can be surprisingly tricky. Consider, for example, the case of a slab of linear dielectric material, partially inserted between the plates of a parallel-plate capacitor (Fig. 4.30). We have always pretended that the field is uniform inside a parallel-plate capacitor, and zero outside. If this were literally true, there would be no net force on the dielectric at all, since the field everywhere would be perpendicular to the plates. However, there is in reality a fringing field around the edges, which for most purposes can be ignored but in this case is responsible for the whole effect. (Indeed, the field could not terminate abruptly at the edge of the capacitor, for if it did, the line integral of E around the closed loop shown in Fig. 4.31 would not be zero.) It is this nonuniform fringing field that pulls the dielectric into the capacitor. Fringing fields are notoriously difficult to calculate; luckily, we can avoid this altogether, by the following ingenious method. Let W be the energy of the system - it depends, of course, on the amount of overlap. If I pull the dielectric out an infinitesimal distance dx, the energy is changed by an amount equal to the work done: \\dd W = F_{me} \\dd x \\tagl{4.59} where F_{me} is the force I mus exert, to counteract the electrical force F on the dielectric. Thus, the electrical force on the slab is F = - \\dv{W}{x} \\tagl{4.60} Now, the energy stored in the capacitor is W = \\frac{1}{2} C V^2 \\tagl{4.61} and the capacitance in this case is C = \\frac{\\epsilon_0 w}{d} (\\epsilon_r l - \\chi_e x) \\tagl{4.62} where l is the length of the plates (Fig 4.30). Let's assume that the total charge on the plates is held constant (Q = CV) , as the dielectric moves. In terms of Q , W = \\frac{1}{2} \\frac{Q^2}{C} \\tagl{4.63} so F = - \\dv{W}{x} = \\frac{1}{2} \\frac{Q^2}{C^2} \\dv{C}{x} = \\frac{1}{2} V^2 \\dv{C}{x} \\tagl{4.64} But \\dv{C}{x} = - \\frac{\\epsilon_0 \\chi_e w}{d} and hence F = - \\frac{\\epsilon_0 \\chi_e w}{2d} V^2 \\tagl{4.65} (The minus sign indicates that the force is in the negative x direction; the dielectric is pulled into the capacitor.) It is a common error to use Eq. 4.61 (with V constant), rather than Eq. 4.63 (with Q constant), in computing the force. One then obtains F = - \\frac{1}{2} V^2 \\dv{C}{x} which is off by a sign. It is, of course, possible to maintain the capacitor at a fixed potential, by connecting it up to a battery. But in that case the battery also does work as the dielectric moves; instead of Eq. 4.59, we now have \\dd W = F_{me} \\dd x + V \\dd Q \\tagl{4.66} where V \\dd Q is the work done by the battery. It follows that F = - \\dv{W}{x} + V \\dv{Q}{x} = - \\frac{1}{2} V^2 \\dv{C}{x} + V^2 \\dv{C}{x} = \\frac{1}{2} V^2 \\dv{C}{x} \\tagl{4.67} the same as before, with the correct sign. Please understand: The force on the dielectric cannot possibly depend on whether you plan to hold Q constant or V constant - it is determined entirely by the distribution of charge, free and bound. It's simpler to calculate the force assuming constant Q , because then you don't have to worry about work done by the battery; but if you insist, it can be done correctly either way. Notice that we were able to determine the force without knowing anything about the fringing fields that are ultimately responsible for it! Of course, it's built into the whole structure of electrostatics that \\curl \\vec{E} = 0 , and hence that the fringing fields must be present; we're not really getting something for nothing here - just cleverly exploiting the internal consistency of the theory. The energy stored in the fringing fields themselves (which was not accounted for in this derivation) stays constant, as the slab moves; what does change is the energy well inside the capacitor, where the field is nice and uniform.","title":"4.4.4: Forces on Dielectrics"},{"location":"ch5-1/","text":"5.1: The Lorentz Force Law 5.1.1: Magnetic Fields Remember the basic problem of classical electrodynamics: We have a collection of charges q_1, q_2, q_3, \\ldots (the \"source\" charges), and we want to calculate the force they exert on some other charge Q (the \"test\" charge). According to the principle of superposition, it is sufficient to find the force of a single source charge - the total is then the vector sum of all the individual forces. Up to now, we have confined our attention to the simplest case, electrostatics, in which the source charge is at rest (though the test charge need not be). The time has come to consider the forces between charges in motion. To give you some sense of what is in store, imagine that I set up the following demonstration: Two wires hang from the ceiling, a few centimeters apart; when I turn on a current, so that it passes up one wire and back down the other, the wires jump apart - they evidently repel one another (Fig. 5.2(a)). How do we explain this? You might suppose that the battery (or whatever drives the current) is actually charging up the wire, and that the force is simply due to the electrical repulsion of like charges. But this is incorrect. I could hold up a test charge near these wires, and there would be no force on it, for the wires are in fact electrically neutral. (It's true that electrons are flowing down the line - that's what a current is - but there are just as many stationary plus charges as moving minus charges on any given segment.) Moreover, if I hook up my demonstration so as to make the current flow up both wires (Fig. 5.2(b)), they are found to attract! What's going on here? Whatever force accounts for the attraction of parallel currents and the repulsion of anti-parallel ones is not electrostatic in nature. It is our first encounter with a magnetic force. Whereas a stationary charge produces only an electric field E in the space around it, a moving charge generates, in addition, a magnetic field B . In fact, magnetic fields are a lot easier to detect, in practice - all you need is a Boy Scout compass. How these devices work is irrelevant at the moment; it is enough to know that the needle points in the direction of the local magnetic field. Ordinarily, this means north, in response to the earth's magnetic field, but in the laboratory, where typical fields may be hundreds of times stronger than that, the compass indicates the direction of whatever magnetic field is present. Now, if you hold up a tiny compass in the vicinity of a current-carrying wire, you quickly discover a very peculiar thing: The field does not point toward the wire, nor away from it, but rather it circles around the wire. In fact, if you grab the wire with your right hand-thumb in the direction of the current-your fingers curl around in the direction of the magnetic field (Fig. 5.3). How can such a field lead to a force of attraction on a nearby parallel current? At the second wire, the magnetic field points into the page (Fig. 5.4), the current is upward, and yet the resulting force is to the left! It's going to take a strange law to account for these directions. 5.1.2: Magnetic Forces In fact, this contribution of directions is just right for a cross product: the magnetic force on a charge Q , moving with velocity v in a magnetic field B is \\vec{F}_{mag} = Q(\\vec{v} \\cross \\vec{B}) \\tagl{5.1} This is known as the Lorentz force law . In the presence of both electric and magnetic fields, the net force on Q would be \\vec{F} = q[ \\vec{E} + ( \\vec{V} \\cross \\vec{B} ) ] \\tagl{5.2} I do not pretend to have derived \\eqref{5.1} , of course; it is a fundamental axiom of the theory, whose justification is to be found in experiments such as those I described in the previous section. Our main job now is to calculate the magnetic field B (and for that matter the electric field E as well; the rules are more complicated when the source charges are in motion). But before we proceed, it is worthwhile to take a closer look at the Lorentz force law itself; it is a peculiar law, and it leads to some truly bizarre particle trajectories. Example 5.1: Cyclotron Motion We have a charged particle moving in a constant magnetic field. What is the general form of the trajectory? The archetypical motion of a charged particle in a magnetic field is circular, with the magnetic force providing the centripetal acceleration. In Fig 5.5, a uniform magnetic field points into the page; if the charge Q moves counterclockwise, with speed v , around a circle of radius R , the magnetic force points inward, and has a fixed magnitude QvB - just right to sustain uniform circular motion: QvB = m \\frac{v^2}{R} \\quad \\text{ or } \\quad p = Q B R \\tagl{5.3} where m is the particle's mass, and p = mv is its momentum. Equation 5.3 is known as the cyclotron formula because it describes the motion of a particle in a cyclotron - the first of the modern particle accelerators. It also suggests a simple experimental technique for finding the momentum of a charged particle: send it through a region of known magnetic field, and measure the radius of its trajectory. This is in fact the standard means for determining the momenta of elementary particles. I assumed that the charge moves in a plane perpendicular to B . If it starts out with some additional speed v_{\\parallel} parallel to B , this component of the motion is unaffected by the magnetic field, and the particle moves in a helix (Fig 5.6). The radius is given by Eq 5.3, but the velocity in question is now the component perpendicular to B , v_{\\perp} Example 5.2: Cycloid Motion A more exotic trajectory occurs if we include a uniform electric field, at right angles to the magnetic one. Suppose, for instance, that B points in the x-direction, and E in the z-direction, as shown in Fig. 5.7. A positive charge is released from the origin; what path will it follow? Solution Let's think it through qualitatively, first. Initially, the particle is at rest, so the magnetic force is zero, and the electric field accelerates the charge in the z-direction. As it picks up speed, a magnetic force develops which, according to Eq. 5.1, pulls the charge around to the right. The faster it goes, the stronger F_{B} becomes; eventually, it curves the particle back around towards the y axis. At this point the charge is moving against the electrical force, so it begins to slow down - the magnetic force then decreases, and the electrical force takes over, bringing the particle to rest at point a, in Fig. 5.7. There the entire process commences anew, carrying the particle over to point b, and so on. Now let's do it quantitatively. There being no force in the x-direction, the position of the particle at any time t can be described by the vector (0, y(t), z(t) ) ; the velocity is therefore \\vec{v} = (0, \\dot{y}, \\dot{z}) Thus, \\vec{v} \\cross \\vec{B} = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ 0 & \\dot{y} & \\dot{z} \\\\ B & 0 & 0 \\end{vmatrix} = B \\dot{z} \\vu{y} - B \\dot{y} \\vu{z} and hence, applying Newton's second law, \\vec{F} = Q(\\vec{E} + \\vec{v} \\cross \\vec{B}) = Q (E \\vu{z} + B \\dot{z} \\vu{y} - B \\dot{y} \\vu{z}) = m \\vec{a} = m(\\ddot{y} \\vu{y} + \\ddot{z} \\vu{z}) Treating the y and z components separately, QB \\dot{z} = m \\ddot{y} \\qquad Q E - Q B \\dot{y} = m \\ddot{z} Let's define a frequency \\omega = \\frac{Q B}{m} \\tagl{5.4} (This is the cyclotron frequency , at which the particle would revolve in the absence of any electric field.) Then the equations of motion take the form \\ddot{y} = \\omega \\dot{z} \\qquad \\ddot{z} = \\omega \\left( \\frac{E}{B} - \\dot{y} \\right) \\tagl{5.5} We're left with a straightforward ODE, with solution \\begin{align*} y(t) & = C_1 \\cos \\omega t + C_2 \\sin \\omega t + (E / B) t + C_3 \\\\ z(t) & = C_2 \\cos \\omega t - C_1 \\sin \\omega t + C_4 \\end{align*} \\tagl{5.6} But the particle started from rest ( \\dot{y}(0) = \\dot{z}(0) = 0 ) at the origin ( y(0) = z(0) = 0) ; these four conditions determine the constants C_1, C_2, C_3, C_4 : y(t) = \\frac{E}{\\omega B} (\\omega t - \\sin \\omega t) \\qquad z(t) = \\frac{E}{\\omega B} (1 - \\cos \\omega t) \\tagl{5.7} In this form, the answer is not terribly enlightening, but if we let R \\equiv \\frac{E}{\\omega B} \\tagl{5.8} and eliminate the sines and cosines by exploiting the trigonometric identity \\sin ^2 \\omega t + \\cos ^2 \\omega t = 1 , we find that (y - R \\omega t)^2 + (z - R)^2 = R^2 \\tagl{5.9} This is the formula for a circle, of radius R, whose center (0, R \\omega t, R) travels in the y-direction at constant speed u = \\omega R = \\frac{E}{B} \\tagl{5.10} The particle moves as though it were a spot on the rim of a wheel rolling along the y-axis. The curve generated in this way is called a cycloid . Notice that the overall motion is not in the direction of E , as you might suppose, but perpendicular to it. Example 5.2b: Mass Spectrometer Example: Mass Spectrometer Suppose we have particles of several isotopes of a known element, and we wish to know exactly which mass isotopes are present (and separate them out) A mass spectrometer is an instrument which can measure the masses and relative concentrations of atoms and molecules. It makes use of the basic magnetic force on a moving charged particle. First, we ionize the particle, giving it a known net charge. We accelerate the particles through a known voltage into a constant magnetic field perpendicular to the velocity of the particle. The charged particles now undergo cyclotron motion (as we just described) of radius given by m v = q B R \\rightarrow v^2 = \\frac{q^2 B^2 R^2}{m^2} We know that the energy per unit charge imparted by our known voltage difference is q | \\Delta V| = \\frac{1}{2} m v^2 \\rightarrow v^2 = \\frac{2 q | \\Delta V|}{m} We can measure the radius of the cyclotron motion by simply putting a detector wall 1/2 of the way around the circular motion, such that the ions will strike the detector a distance 2R from the output nozzle of the accelerating voltage. Putting our known quantities ( \\Delta V, B ) together with the measured radius of the cyclotron motion, we get \\frac{m}{q} = \\frac{B^2 R^2}{2 | \\Delta V|} One implication of the Lorentz force law deserves special attention: Magnetic forces do no work For the magnetic force is, by definition, always perpendicular to the path of motion. Magnetic forces may alter the direction in which a particle moves, but they cannot speed it up or slow it down. The fact that magnetic forces do no work is an elementary and direct consequence of the Lorentz force law, but there are many situations where it appears so manifestly false that one's confidence is bound to waver. When a magnetic crane lifts the carcass of a junked car, for instance, something is obviously doing work, and it seems perverse to deny that the magnetic force is responsible. Well, perverse or not, deny it we must, and it can be a very subtle matter to figure out who does deserve the credit in such circumstances. We'll see a cute example in the next section, but the full story will have to wait until we hit the key conservation laws much later. 5.1.3: Currents The current in a wire is the charge per unit time passing a given point. By definition, negative charges moving to the left count the same as positive ones to the right. This conveniently reflects the physical fact that almost all phenomena involving moving charges depend on the product of charge and velocity - if you reverse the signs of q and v , you get the same answer, so it doesn't really matter which you have. (The Lorentz force law is a case in point; the Hall effect (Prob. 5.41) is a notorious exception.) In practice, it is ordinarily the negatively charged electrons that do the moving - in the direction opposite to the electric current. To avoid the petty complications this entails, I shall often pretend it's the positive charges that move, as in fact everyone assumed they did for a century or so after Benjamin Franklin established his unfortunate convention. Current is measured in coulombs-per-second, or amperes (A): 1 \\text{ A} = 1 \\text{ C/s} \\tagl{5.12} A line charge \\lambda traveling down a wire at speed v (Fig 5.9) constitutes a current I = \\lambda v \\tagl{5.13} because a segment of length v \\Delta t , carrying charge \\lambda v \\Delta t , passes point P in a time interval \\Delta t . Current is actually a vector \\vec{I} = \\lambda \\vec{v} \\tagl{5.14} Because the path of the flow is dictated by the shape of the wire, one doesn't ordinarily bother to display the direction of I explicitly, but when it comes to surface and volume currents we cannot afford to be so casual, and for the sake of notational consistency it is a good idea to acknowledge the vectorial character of currents right from the start. A neutral wire, of course, contains as many stationary positive charges as mobile negative ones. The former do not contribute to the current-the charge density \\lambda in Eq. 5.13 refers only to the moving charges. In the unusual situation where both types move, I = \\lambda_{+} \\vec{v}_{+} + \\lambda_{-}\\vec{v}_{-} . The magnetic force on a segment of current-carrying wire is \\vec{F}_B = \\int (\\vec{v} \\cross \\vec{B}) \\dd q = \\int (\\vec{v} \\cross \\vec{B}) \\lambda \\dd l = \\int (\\vec{I} \\cross \\vec{B}) \\dd l \\tagl{5.15} Inasmuch as I and dl both point in the same direction, we can just as well write this as \\vec{F}_B = \\int I ( \\dd \\vec{l} \\cross \\vec{B}) \\tagl{5.16} Typically, the current is constant (in magnitude) along the wire, and in that case, I comes outside the integral: \\vec{F}_B = I \\int ( \\dd \\vec{l} \\cross \\vec{B}) \\tagl{5.17} Example 5.3 A rectangular loop of wire, supporting mass m, hangs vertically with one end in a uniform magnetic field B , which points into the page in the shaded region of Fig 5.10. For what current I, in the loop, would the magnetic force upward exactly balance the gravitational force downward? Solution First of all, the current must circulate clockwise, in order for (\\vec{I} \\cross \\vec{b}) in the horizontal segment to point upward. The force is F_B = I B a where a is the width of the loop. The magnetic forces on the two vertical segments cancel. For F_{mag} to balance the weight (mg), we must therefore have I = \\frac{mg}{Ba} \\tagl{5.18} The weight just hangs there, suspended in mid-air! What happens if we now increase the current? Then the upward force exceeds the downward force of gravity, and the loop rises, lifting the weight. Somebody's doing work, and it sure looks as though the magnetic force is responsible. Indeed, one is tempted to write W_{mag} = F_{mag} h = I B a h \\tagl{5.19} where h is the distance the loop rises. But we know that magnetic forces never do work, so what's going on here? Well, when the loop starts to rise, the charges in the wire are no longer moving horizontally - their velocity now acquires an upward component u , the speed of the loop (Fig 5.11), in addition to the horizontal component w associated with the current ( I = \\lambda w ). The magnetic force, which is always perpendicular to the velocity, no longer points straight up, but tilts back. It is perpendicular to the net displacement of the charge (which is in the direction of v ), and therefore it does no work on q . It does have a vertical component ( q w B ); indeed, the net vertical force on the charge (\\lambda a) in the upper segment of the loop is F_{vert} = \\lambda a w B = I B a \\tagl{5.20} (as before); but now it also has a horizontal component (q u B) which opposes the flow of current. Whoever is in charge of maintaining that current, therefore, must now push those charges along, against the backward component of the magnetic force. The total horizontal force on the top segment is F_{horiz} = \\lambda a u B \\tagl{5.21} In a time dt , the charges move a (horizontal) distance w \\dd t , so the work done by this agency (presumably a battery or a generator) is W_{battery} = \\lambda a B \\int u w \\dd t = I B a h which is precisely what we naively attributed to the magnetic force in \\eqref{5.19} . Was work done in this process? Absolutely! Who did it? The battery! What, then, was the role of the magnetic force? Well, it redirected the horizontal force of the battery into the vertical motion of the loop and the weight. It may help to consider a mechanical analogy. Imagine you're sliding a trunk up a frictionless ramp, by pushing on it horizontally with a mop (Fig 5.12). The normal force (N) does no work, because it is perpendicular to the displacement. But it does have a vertical component (which in fact is what lifts the trunk), and a (backward) horizontal component (which you have to overcome by pushing on the mop). Who is doing the work here? You are, obviously - and yet your force (which is purely horizontal) is not (at least, directly) what lifts the box. The normal force plays the same passive (but crucial) role as the magnetic force in Ex 5.3: while doing no work itself, it redirects the efforts of the active agent (you, or the battery, as the case may be), from horizontal to vertical. When charge flows over a surface, we describe it by the surface current density, K , defined as follows: Consider a \"ribbon\" of infinitesimal width \\dd l_{\\perp} , running parallel to the flow (Fig 5.13). If the current in this ribbon is \\dd \\vec{I} , the surface current density is \\vec{K} = \\frac{\\dd \\vec{I}}{\\dd l _{\\perp}} \\tagl{5.22} In words, K is the current per unit width. In particular, if the (mobile) surface charge density is sigma and its velocity is \\vec{v} , then \\vec{K} = \\sigma \\vec{v} \\tagl{5.23} In general, \\vec{K} will vary from point to point over the surface, as \\sigma and/or \\vec{v} changes. The magnetic force on the surface current is \\vec{F}_{mag} = \\int ( \\vec{v} \\cross \\vec{B}) \\sigma \\dd a = \\int (\\vec{K} \\cross \\vec{B}) \\dd a \\tagl{5.24} Caveat: Just as E suffers a discontinuity at a surface charge, so B is discontinuous at a surface current . In \\eqref{5.24} , you must be careful to use the average field, just as we did in Sect 2.5.3. When the flow of charge is distributed throughout a three-dimensional region, we describe it by the volume current density, J , defined as follows: consider a \"tube\" of infinitesimal cross section \\dd a_{\\perp} , running parallel to the flow (Fig 5.14). If the current in this tube is \\dd \\vec{I} , the volume current density is \\vec{J} \\equiv \\dv{\\vec{I}}{a_{\\perp}} \\tagl{5.25} In words, J is the current per unit area. If the (mobile) volume charge density is \\rho and the velocity is \\vec{v} , then \\vec{J} = \\rho \\vec{v} \\tagl{5.26} The magnetic force on a volume current is therefore \\vec{F}_{mag} = \\int (\\vec{v} \\times \\vec{B}) \\rho \\dd \\tau = \\int (\\vec{J} \\times \\vec{B} ) \\dd \\tau \\tagl{5.27} Example 5.4 A current I is uniformly distributed over a wire of circular cross section, with radius a (Fig 5.15). Find the volume current density J . Solution The area perpendicular to the flow is \\pi a^2 , so J = \\frac{I}{\\pi a^2} This was trivial because the current density was uniform Suppose the current density in the wire is proportional to the distance from the axis, J = k s for some constant k . Find the total current in the wire. Because J varies with s , we must integrate \\eqref{5.25} . The current through the shaded patch (Fig 5.16) is J \\dd a_{\\perp} , and \\dd a_{\\perp} = s \\dd s \\dd \\phi so I = \\int (ks) (s \\dd s \\dd \\phi) = 2 \\pi k \\int _0 ^a s^2 \\dd s = \\frac{2 \\pi k a^3}{3} According to Eq. 5.25, the total current crossing a surface S can be written as I = \\int_S J \\dd a_{\\perp} = \\int_S \\vec{J} \\cdot \\dd \\vec{a} \\tagl{5.28} (The dot product serves to pick out the appropriate component of \\dd \\vec{a} ). In particular, the charge per unit time leaving a volume V is \\oint _S \\vec{J} \\cdot \\dd \\vec{a} = \\int_V ( \\div \\vec{J}) \\dd \\tau Because charge is conserved , whatever flows out through the surface must come at the expense of what remains inside: \\int_V (\\div \\vec{J} ) \\dd \\tau = - \\dv{}{t} \\int_V \\rho \\dd \\tau = - \\int _V \\left( \\pdv{\\rho}{t} \\right)\\dd \\tau (The minus sign reflects the fact that an outward flow decreases the charge left in V ). Since this applies to any volume, we conclude that \\div \\vec{J} = - \\pdv{\\rho}{t} \\tagl{5.29} This is the precise mathematical statement of local charge conservation; it is called the continuity equation . For future reference, let us summarize the \"dictionary\" we have implicitly developed for translating equations into the forms appropriate to point, line, surface, and volume currents \\sum_{i = 1} ^n ( ) q_i \\vec{v}_i \\sim \\int_{line} () \\vec{I} \\dd l \\sim \\int_{surf} ( ) \\vec{K} \\dd a \\sim \\int_{vol} ( ) \\vec{J} \\dd \\tau \\tagl{5.30} This correspondence, which is analogous to q \\sim \\lambda \\dd l \\sim \\sigma \\dd a \\sim \\rho \\dd \\tau for the various charge distributions, generates Eqs. 5.15, 5.24, and 5.27 from the original Lorentz force law (5.1).","title":"5.1 - The Lorentz Force Law"},{"location":"ch5-1/#51-the-lorentz-force-law","text":"","title":"5.1: The Lorentz Force Law"},{"location":"ch5-1/#511-magnetic-fields","text":"Remember the basic problem of classical electrodynamics: We have a collection of charges q_1, q_2, q_3, \\ldots (the \"source\" charges), and we want to calculate the force they exert on some other charge Q (the \"test\" charge). According to the principle of superposition, it is sufficient to find the force of a single source charge - the total is then the vector sum of all the individual forces. Up to now, we have confined our attention to the simplest case, electrostatics, in which the source charge is at rest (though the test charge need not be). The time has come to consider the forces between charges in motion. To give you some sense of what is in store, imagine that I set up the following demonstration: Two wires hang from the ceiling, a few centimeters apart; when I turn on a current, so that it passes up one wire and back down the other, the wires jump apart - they evidently repel one another (Fig. 5.2(a)). How do we explain this? You might suppose that the battery (or whatever drives the current) is actually charging up the wire, and that the force is simply due to the electrical repulsion of like charges. But this is incorrect. I could hold up a test charge near these wires, and there would be no force on it, for the wires are in fact electrically neutral. (It's true that electrons are flowing down the line - that's what a current is - but there are just as many stationary plus charges as moving minus charges on any given segment.) Moreover, if I hook up my demonstration so as to make the current flow up both wires (Fig. 5.2(b)), they are found to attract! What's going on here? Whatever force accounts for the attraction of parallel currents and the repulsion of anti-parallel ones is not electrostatic in nature. It is our first encounter with a magnetic force. Whereas a stationary charge produces only an electric field E in the space around it, a moving charge generates, in addition, a magnetic field B . In fact, magnetic fields are a lot easier to detect, in practice - all you need is a Boy Scout compass. How these devices work is irrelevant at the moment; it is enough to know that the needle points in the direction of the local magnetic field. Ordinarily, this means north, in response to the earth's magnetic field, but in the laboratory, where typical fields may be hundreds of times stronger than that, the compass indicates the direction of whatever magnetic field is present. Now, if you hold up a tiny compass in the vicinity of a current-carrying wire, you quickly discover a very peculiar thing: The field does not point toward the wire, nor away from it, but rather it circles around the wire. In fact, if you grab the wire with your right hand-thumb in the direction of the current-your fingers curl around in the direction of the magnetic field (Fig. 5.3). How can such a field lead to a force of attraction on a nearby parallel current? At the second wire, the magnetic field points into the page (Fig. 5.4), the current is upward, and yet the resulting force is to the left! It's going to take a strange law to account for these directions.","title":"5.1.1: Magnetic Fields"},{"location":"ch5-1/#512-magnetic-forces","text":"In fact, this contribution of directions is just right for a cross product: the magnetic force on a charge Q , moving with velocity v in a magnetic field B is \\vec{F}_{mag} = Q(\\vec{v} \\cross \\vec{B}) \\tagl{5.1} This is known as the Lorentz force law . In the presence of both electric and magnetic fields, the net force on Q would be \\vec{F} = q[ \\vec{E} + ( \\vec{V} \\cross \\vec{B} ) ] \\tagl{5.2} I do not pretend to have derived \\eqref{5.1} , of course; it is a fundamental axiom of the theory, whose justification is to be found in experiments such as those I described in the previous section. Our main job now is to calculate the magnetic field B (and for that matter the electric field E as well; the rules are more complicated when the source charges are in motion). But before we proceed, it is worthwhile to take a closer look at the Lorentz force law itself; it is a peculiar law, and it leads to some truly bizarre particle trajectories.","title":"5.1.2: Magnetic Forces"},{"location":"ch5-1/#example-51-cyclotron-motion","text":"We have a charged particle moving in a constant magnetic field. What is the general form of the trajectory? The archetypical motion of a charged particle in a magnetic field is circular, with the magnetic force providing the centripetal acceleration. In Fig 5.5, a uniform magnetic field points into the page; if the charge Q moves counterclockwise, with speed v , around a circle of radius R , the magnetic force points inward, and has a fixed magnitude QvB - just right to sustain uniform circular motion: QvB = m \\frac{v^2}{R} \\quad \\text{ or } \\quad p = Q B R \\tagl{5.3} where m is the particle's mass, and p = mv is its momentum. Equation 5.3 is known as the cyclotron formula because it describes the motion of a particle in a cyclotron - the first of the modern particle accelerators. It also suggests a simple experimental technique for finding the momentum of a charged particle: send it through a region of known magnetic field, and measure the radius of its trajectory. This is in fact the standard means for determining the momenta of elementary particles. I assumed that the charge moves in a plane perpendicular to B . If it starts out with some additional speed v_{\\parallel} parallel to B , this component of the motion is unaffected by the magnetic field, and the particle moves in a helix (Fig 5.6). The radius is given by Eq 5.3, but the velocity in question is now the component perpendicular to B , v_{\\perp}","title":"Example 5.1: Cyclotron Motion"},{"location":"ch5-1/#example-52-cycloid-motion","text":"A more exotic trajectory occurs if we include a uniform electric field, at right angles to the magnetic one. Suppose, for instance, that B points in the x-direction, and E in the z-direction, as shown in Fig. 5.7. A positive charge is released from the origin; what path will it follow? Solution Let's think it through qualitatively, first. Initially, the particle is at rest, so the magnetic force is zero, and the electric field accelerates the charge in the z-direction. As it picks up speed, a magnetic force develops which, according to Eq. 5.1, pulls the charge around to the right. The faster it goes, the stronger F_{B} becomes; eventually, it curves the particle back around towards the y axis. At this point the charge is moving against the electrical force, so it begins to slow down - the magnetic force then decreases, and the electrical force takes over, bringing the particle to rest at point a, in Fig. 5.7. There the entire process commences anew, carrying the particle over to point b, and so on. Now let's do it quantitatively. There being no force in the x-direction, the position of the particle at any time t can be described by the vector (0, y(t), z(t) ) ; the velocity is therefore \\vec{v} = (0, \\dot{y}, \\dot{z}) Thus, \\vec{v} \\cross \\vec{B} = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ 0 & \\dot{y} & \\dot{z} \\\\ B & 0 & 0 \\end{vmatrix} = B \\dot{z} \\vu{y} - B \\dot{y} \\vu{z} and hence, applying Newton's second law, \\vec{F} = Q(\\vec{E} + \\vec{v} \\cross \\vec{B}) = Q (E \\vu{z} + B \\dot{z} \\vu{y} - B \\dot{y} \\vu{z}) = m \\vec{a} = m(\\ddot{y} \\vu{y} + \\ddot{z} \\vu{z}) Treating the y and z components separately, QB \\dot{z} = m \\ddot{y} \\qquad Q E - Q B \\dot{y} = m \\ddot{z} Let's define a frequency \\omega = \\frac{Q B}{m} \\tagl{5.4} (This is the cyclotron frequency , at which the particle would revolve in the absence of any electric field.) Then the equations of motion take the form \\ddot{y} = \\omega \\dot{z} \\qquad \\ddot{z} = \\omega \\left( \\frac{E}{B} - \\dot{y} \\right) \\tagl{5.5} We're left with a straightforward ODE, with solution \\begin{align*} y(t) & = C_1 \\cos \\omega t + C_2 \\sin \\omega t + (E / B) t + C_3 \\\\ z(t) & = C_2 \\cos \\omega t - C_1 \\sin \\omega t + C_4 \\end{align*} \\tagl{5.6} But the particle started from rest ( \\dot{y}(0) = \\dot{z}(0) = 0 ) at the origin ( y(0) = z(0) = 0) ; these four conditions determine the constants C_1, C_2, C_3, C_4 : y(t) = \\frac{E}{\\omega B} (\\omega t - \\sin \\omega t) \\qquad z(t) = \\frac{E}{\\omega B} (1 - \\cos \\omega t) \\tagl{5.7} In this form, the answer is not terribly enlightening, but if we let R \\equiv \\frac{E}{\\omega B} \\tagl{5.8} and eliminate the sines and cosines by exploiting the trigonometric identity \\sin ^2 \\omega t + \\cos ^2 \\omega t = 1 , we find that (y - R \\omega t)^2 + (z - R)^2 = R^2 \\tagl{5.9} This is the formula for a circle, of radius R, whose center (0, R \\omega t, R) travels in the y-direction at constant speed u = \\omega R = \\frac{E}{B} \\tagl{5.10} The particle moves as though it were a spot on the rim of a wheel rolling along the y-axis. The curve generated in this way is called a cycloid . Notice that the overall motion is not in the direction of E , as you might suppose, but perpendicular to it.","title":"Example 5.2: Cycloid Motion"},{"location":"ch5-1/#example-52b-mass-spectrometer","text":"Example: Mass Spectrometer Suppose we have particles of several isotopes of a known element, and we wish to know exactly which mass isotopes are present (and separate them out) A mass spectrometer is an instrument which can measure the masses and relative concentrations of atoms and molecules. It makes use of the basic magnetic force on a moving charged particle. First, we ionize the particle, giving it a known net charge. We accelerate the particles through a known voltage into a constant magnetic field perpendicular to the velocity of the particle. The charged particles now undergo cyclotron motion (as we just described) of radius given by m v = q B R \\rightarrow v^2 = \\frac{q^2 B^2 R^2}{m^2} We know that the energy per unit charge imparted by our known voltage difference is q | \\Delta V| = \\frac{1}{2} m v^2 \\rightarrow v^2 = \\frac{2 q | \\Delta V|}{m} We can measure the radius of the cyclotron motion by simply putting a detector wall 1/2 of the way around the circular motion, such that the ions will strike the detector a distance 2R from the output nozzle of the accelerating voltage. Putting our known quantities ( \\Delta V, B ) together with the measured radius of the cyclotron motion, we get \\frac{m}{q} = \\frac{B^2 R^2}{2 | \\Delta V|} One implication of the Lorentz force law deserves special attention: Magnetic forces do no work For the magnetic force is, by definition, always perpendicular to the path of motion. Magnetic forces may alter the direction in which a particle moves, but they cannot speed it up or slow it down. The fact that magnetic forces do no work is an elementary and direct consequence of the Lorentz force law, but there are many situations where it appears so manifestly false that one's confidence is bound to waver. When a magnetic crane lifts the carcass of a junked car, for instance, something is obviously doing work, and it seems perverse to deny that the magnetic force is responsible. Well, perverse or not, deny it we must, and it can be a very subtle matter to figure out who does deserve the credit in such circumstances. We'll see a cute example in the next section, but the full story will have to wait until we hit the key conservation laws much later.","title":"Example 5.2b: Mass Spectrometer"},{"location":"ch5-1/#513-currents","text":"The current in a wire is the charge per unit time passing a given point. By definition, negative charges moving to the left count the same as positive ones to the right. This conveniently reflects the physical fact that almost all phenomena involving moving charges depend on the product of charge and velocity - if you reverse the signs of q and v , you get the same answer, so it doesn't really matter which you have. (The Lorentz force law is a case in point; the Hall effect (Prob. 5.41) is a notorious exception.) In practice, it is ordinarily the negatively charged electrons that do the moving - in the direction opposite to the electric current. To avoid the petty complications this entails, I shall often pretend it's the positive charges that move, as in fact everyone assumed they did for a century or so after Benjamin Franklin established his unfortunate convention. Current is measured in coulombs-per-second, or amperes (A): 1 \\text{ A} = 1 \\text{ C/s} \\tagl{5.12} A line charge \\lambda traveling down a wire at speed v (Fig 5.9) constitutes a current I = \\lambda v \\tagl{5.13} because a segment of length v \\Delta t , carrying charge \\lambda v \\Delta t , passes point P in a time interval \\Delta t . Current is actually a vector \\vec{I} = \\lambda \\vec{v} \\tagl{5.14} Because the path of the flow is dictated by the shape of the wire, one doesn't ordinarily bother to display the direction of I explicitly, but when it comes to surface and volume currents we cannot afford to be so casual, and for the sake of notational consistency it is a good idea to acknowledge the vectorial character of currents right from the start. A neutral wire, of course, contains as many stationary positive charges as mobile negative ones. The former do not contribute to the current-the charge density \\lambda in Eq. 5.13 refers only to the moving charges. In the unusual situation where both types move, I = \\lambda_{+} \\vec{v}_{+} + \\lambda_{-}\\vec{v}_{-} . The magnetic force on a segment of current-carrying wire is \\vec{F}_B = \\int (\\vec{v} \\cross \\vec{B}) \\dd q = \\int (\\vec{v} \\cross \\vec{B}) \\lambda \\dd l = \\int (\\vec{I} \\cross \\vec{B}) \\dd l \\tagl{5.15} Inasmuch as I and dl both point in the same direction, we can just as well write this as \\vec{F}_B = \\int I ( \\dd \\vec{l} \\cross \\vec{B}) \\tagl{5.16} Typically, the current is constant (in magnitude) along the wire, and in that case, I comes outside the integral: \\vec{F}_B = I \\int ( \\dd \\vec{l} \\cross \\vec{B}) \\tagl{5.17}","title":"5.1.3: Currents"},{"location":"ch5-1/#example-53","text":"A rectangular loop of wire, supporting mass m, hangs vertically with one end in a uniform magnetic field B , which points into the page in the shaded region of Fig 5.10. For what current I, in the loop, would the magnetic force upward exactly balance the gravitational force downward? Solution First of all, the current must circulate clockwise, in order for (\\vec{I} \\cross \\vec{b}) in the horizontal segment to point upward. The force is F_B = I B a where a is the width of the loop. The magnetic forces on the two vertical segments cancel. For F_{mag} to balance the weight (mg), we must therefore have I = \\frac{mg}{Ba} \\tagl{5.18} The weight just hangs there, suspended in mid-air! What happens if we now increase the current? Then the upward force exceeds the downward force of gravity, and the loop rises, lifting the weight. Somebody's doing work, and it sure looks as though the magnetic force is responsible. Indeed, one is tempted to write W_{mag} = F_{mag} h = I B a h \\tagl{5.19} where h is the distance the loop rises. But we know that magnetic forces never do work, so what's going on here? Well, when the loop starts to rise, the charges in the wire are no longer moving horizontally - their velocity now acquires an upward component u , the speed of the loop (Fig 5.11), in addition to the horizontal component w associated with the current ( I = \\lambda w ). The magnetic force, which is always perpendicular to the velocity, no longer points straight up, but tilts back. It is perpendicular to the net displacement of the charge (which is in the direction of v ), and therefore it does no work on q . It does have a vertical component ( q w B ); indeed, the net vertical force on the charge (\\lambda a) in the upper segment of the loop is F_{vert} = \\lambda a w B = I B a \\tagl{5.20} (as before); but now it also has a horizontal component (q u B) which opposes the flow of current. Whoever is in charge of maintaining that current, therefore, must now push those charges along, against the backward component of the magnetic force. The total horizontal force on the top segment is F_{horiz} = \\lambda a u B \\tagl{5.21} In a time dt , the charges move a (horizontal) distance w \\dd t , so the work done by this agency (presumably a battery or a generator) is W_{battery} = \\lambda a B \\int u w \\dd t = I B a h which is precisely what we naively attributed to the magnetic force in \\eqref{5.19} . Was work done in this process? Absolutely! Who did it? The battery! What, then, was the role of the magnetic force? Well, it redirected the horizontal force of the battery into the vertical motion of the loop and the weight. It may help to consider a mechanical analogy. Imagine you're sliding a trunk up a frictionless ramp, by pushing on it horizontally with a mop (Fig 5.12). The normal force (N) does no work, because it is perpendicular to the displacement. But it does have a vertical component (which in fact is what lifts the trunk), and a (backward) horizontal component (which you have to overcome by pushing on the mop). Who is doing the work here? You are, obviously - and yet your force (which is purely horizontal) is not (at least, directly) what lifts the box. The normal force plays the same passive (but crucial) role as the magnetic force in Ex 5.3: while doing no work itself, it redirects the efforts of the active agent (you, or the battery, as the case may be), from horizontal to vertical. When charge flows over a surface, we describe it by the surface current density, K , defined as follows: Consider a \"ribbon\" of infinitesimal width \\dd l_{\\perp} , running parallel to the flow (Fig 5.13). If the current in this ribbon is \\dd \\vec{I} , the surface current density is \\vec{K} = \\frac{\\dd \\vec{I}}{\\dd l _{\\perp}} \\tagl{5.22} In words, K is the current per unit width. In particular, if the (mobile) surface charge density is sigma and its velocity is \\vec{v} , then \\vec{K} = \\sigma \\vec{v} \\tagl{5.23} In general, \\vec{K} will vary from point to point over the surface, as \\sigma and/or \\vec{v} changes. The magnetic force on the surface current is \\vec{F}_{mag} = \\int ( \\vec{v} \\cross \\vec{B}) \\sigma \\dd a = \\int (\\vec{K} \\cross \\vec{B}) \\dd a \\tagl{5.24} Caveat: Just as E suffers a discontinuity at a surface charge, so B is discontinuous at a surface current . In \\eqref{5.24} , you must be careful to use the average field, just as we did in Sect 2.5.3. When the flow of charge is distributed throughout a three-dimensional region, we describe it by the volume current density, J , defined as follows: consider a \"tube\" of infinitesimal cross section \\dd a_{\\perp} , running parallel to the flow (Fig 5.14). If the current in this tube is \\dd \\vec{I} , the volume current density is \\vec{J} \\equiv \\dv{\\vec{I}}{a_{\\perp}} \\tagl{5.25} In words, J is the current per unit area. If the (mobile) volume charge density is \\rho and the velocity is \\vec{v} , then \\vec{J} = \\rho \\vec{v} \\tagl{5.26} The magnetic force on a volume current is therefore \\vec{F}_{mag} = \\int (\\vec{v} \\times \\vec{B}) \\rho \\dd \\tau = \\int (\\vec{J} \\times \\vec{B} ) \\dd \\tau \\tagl{5.27}","title":"Example 5.3"},{"location":"ch5-1/#example-54","text":"A current I is uniformly distributed over a wire of circular cross section, with radius a (Fig 5.15). Find the volume current density J . Solution The area perpendicular to the flow is \\pi a^2 , so J = \\frac{I}{\\pi a^2} This was trivial because the current density was uniform Suppose the current density in the wire is proportional to the distance from the axis, J = k s for some constant k . Find the total current in the wire. Because J varies with s , we must integrate \\eqref{5.25} . The current through the shaded patch (Fig 5.16) is J \\dd a_{\\perp} , and \\dd a_{\\perp} = s \\dd s \\dd \\phi so I = \\int (ks) (s \\dd s \\dd \\phi) = 2 \\pi k \\int _0 ^a s^2 \\dd s = \\frac{2 \\pi k a^3}{3} According to Eq. 5.25, the total current crossing a surface S can be written as I = \\int_S J \\dd a_{\\perp} = \\int_S \\vec{J} \\cdot \\dd \\vec{a} \\tagl{5.28} (The dot product serves to pick out the appropriate component of \\dd \\vec{a} ). In particular, the charge per unit time leaving a volume V is \\oint _S \\vec{J} \\cdot \\dd \\vec{a} = \\int_V ( \\div \\vec{J}) \\dd \\tau Because charge is conserved , whatever flows out through the surface must come at the expense of what remains inside: \\int_V (\\div \\vec{J} ) \\dd \\tau = - \\dv{}{t} \\int_V \\rho \\dd \\tau = - \\int _V \\left( \\pdv{\\rho}{t} \\right)\\dd \\tau (The minus sign reflects the fact that an outward flow decreases the charge left in V ). Since this applies to any volume, we conclude that \\div \\vec{J} = - \\pdv{\\rho}{t} \\tagl{5.29} This is the precise mathematical statement of local charge conservation; it is called the continuity equation . For future reference, let us summarize the \"dictionary\" we have implicitly developed for translating equations into the forms appropriate to point, line, surface, and volume currents \\sum_{i = 1} ^n ( ) q_i \\vec{v}_i \\sim \\int_{line} () \\vec{I} \\dd l \\sim \\int_{surf} ( ) \\vec{K} \\dd a \\sim \\int_{vol} ( ) \\vec{J} \\dd \\tau \\tagl{5.30} This correspondence, which is analogous to q \\sim \\lambda \\dd l \\sim \\sigma \\dd a \\sim \\rho \\dd \\tau for the various charge distributions, generates Eqs. 5.15, 5.24, and 5.27 from the original Lorentz force law (5.1).","title":"Example 5.4"},{"location":"ch5-2/","text":"5.2: The Biot-Savart Law 5.2.1: Steady Currents Stationary charges produce electric fields that are constant in time; hence the term electrostatics . Steady currents produce magnetic fields that are constant in time; the theory of steady currents is called magnetostatics . By \" steady current \" I mean a continuous flow that has been going on forever, without change and without piling up anywhere (some people call them \"stationary currents\"; to my ear, that's a contradiction in terms). Formally, electro/magnetostatics is the regime \\pdv{\\rho}{t} = 0, \\quad \\pdv{\\vec{J}}{t} = 0 \\tagl{5.32} at all places and all times. Of course, there's no such thing in practice as a truly steady current, any more than there is a truly stationary charge. In this sense, both electrostatics and magnetostatics describe artificial worlds that exist only in textbooks. However, they represent suitable approximations as long as the actual fluctuations are remote, or gradual - in fact, for most purposes magnetostatics applies very well to household currents, which alternate 120 times per second! Notice that a moving point charge cannot possibly constitute a steady current. If it's here one instant, it's gone the next. This may seem like a minor thing to you, but it's a major headache for me. I developed each topic in electrostatics by starting out with the simple case of a point charge at rest; then I generalized to an arbitrary charge distribution by invoking the superposition principle. This approach is not open to us in magnetostatics because a moving point charge does not produce a static field in the first place. We are forced to deal with extended current distributions right from the start, and, as a result, the arguments are bound to be more cumbersome. When a steady current flows in a wire, its magnitude I must be the same all along the line; otherwise, charge would be piling up somewhere, and it wouldn't be a steady current. More generally, since \\partial \\rho / \\partial t = 0 in magnetostatics, the continuity equation (5.29) becomes \\grad \\vec{J} = 0 \\tagl{5.33} 5.2.2: The Magnetic Field of a Steady Current The magnetic field of a steady line current is given by the Biot-Savart law : \\vec{B}(r) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{I} \\cross \\vu{\\gr}}{\\gr ^2} \\dd l' = \\frac{ \\mu_0}{4 \\pi } I \\int \\frac{\\dd \\vec{l'} \\cross \\vu{\\gr}}{\\gr ^2} \\tagl{5.34} The integration is along the current path, in the direction of the flow; \\dd \\vec{l'} is an element of length along the wire, and \\vu{\\gr} , as always, is the vector from the source to the point r (Fig 5.17). The constant \\mu_0 is called the permeability of free space : \\mu_0 = 4 \\pi \\times 10^{-7} \\text{N} / \\text{A}^2 \\tagl{5.35} This is an exact number, not an empirical constant. It serves to define the ampereThese units are such that B itself comes out in newtons per ampere-meter (as required by the Lorentz force law), or teslas (T): 1 \\text{T} = 1 \\text{N} / (\\text{A} \\cdot \\text{m}) \\tagl{5.36} As the starting point for magnetostatics, the Biot-Savart law plays a role analogous to Coulomb's law in electrostatics. Indeed, the 1/ \\gr ^2 dependence is common to both laws Example 5.5 Find the magnetic field a distance s from a long straight wire carrying a steady current I (Fig 5.18). Solution In the diagram, \\dd \\vec{l'} \\cross \\gr points out of the page, and has the magnitude \\dd l' \\sin \\alpha = \\dd l' \\cos \\theta Also, l' = s \\tan \\theta , so \\dd l' = \\frac{s}{ \\cos ^2 \\theta} \\dd \\theta and s = \\gr \\cos \\theta , so \\frac{1}{\\gr^2} = \\frac{\\cos ^2 \\theta}{s^2} The Biot-Savart law gives the magnetic field as \\begin{align*} B & = \\frac{\\mu_0 I}{4 \\pi} \\int _{\\theta_1}^{\\theta_2} \\left( \\frac{\\cos ^2 \\theta}{s^2} \\right) \\left( \\frac{s}{\\cos ^2 \\theta} \\right) \\cos \\theta \\dd \\theta \\\\ & = \\frac{\\mu_0}{4 \\pi s} \\int _{\\theta_1}^{\\theta_2} \\cos \\theta \\dd \\theta \\\\ & = \\frac{\\mu_0 I}{4 \\pi s} (\\sin \\theta_2 - \\sin \\theta_1) \\tagl{5.37} \\end{align*} That is the field of any straight segment of wire, in terms of the initial and final angles \\theta_1 and \\theta_2 (Fig 5.19). Of course, a finite segment by itself could never support a steady current (where would the charge go when it got to the end?), but it might be a piece of some closed circuit, and \\eqref{5.37} would then represent its contribution to the total field. In the case of an infinite wire, \\theta_1 = - \\pi / 2 and \\theta_2 = \\pi / 2 so we obtain B = \\frac{\\mu_0 I}{2 \\pi s} \\tagl{5.38} Notice that the field is inversely proportional to the distance from the wire - just like the electric field of an infinite line charge. In the region below the wire, B points into the page, and in general, it \"circles around\" the wire, in accordance with the right-hand rule \\vec{B} = \\frac{\\mu_0 I}{2 \\pi s} \\vu{\\phi} \\tagl{5.39} As an application, let's find the force of attraction between two long, parallel wires a distance d apart, carrying currents I_1 and I_2 (Fig 5.20). The field at (2) due to (1) is B = \\frac{\\mu_0 I_1}{2 \\pi d} and it points into the page. The Lorentz force law (in the form appropriate to line currents) predicts a force directed upwards (1), of magnitude F = I_2 \\left( \\frac{\\mu_0 I_1}{2 \\pi d} \\right) \\int \\dd l The total force, not surprisingly, is infinite, but the force per unit length is f = \\frac{ \\mu_0}{2 \\pi } \\frac{I_1 I_2}{d} \\tagl{5.40} If the currents are antiparallel (one up, one down), the force is repulsive - consistent again with the qualitative observations of Sect 5.1.1. Example 5.6 Find the magnetic field a distance z above the center of a circular loop of radius R , which carries a steady current I (Fig 5.21) Solution The field \\dd \\vec{B} attributable to the segment \\dd \\vec{l'} points as shown. As we integrate \\dd \\vec{l'} around the loop, \\dd \\vec{B} sweeps out a cone. The horizontal components cancel, and the vertical components combine, to give B(z) = \\frac{\\mu_0 }{4 \\pi} I \\int \\frac{\\dd l'}{\\gr ^2} \\cos \\theta (Notice that dl' and \\gr are perpendicular, in this case; the factor of \\cos \\theta projects out the vertical component.) Now, \\cos \\theta and \\gr ^2 are constants, and \\int dl' is simply the circumference 2 \\pi R , so B(z) = \\frac{\\mu_0 I}{4 \\pi} \\left( \\frac{\\cos \\theta}{\\gr ^2} \\right) 2 \\pi R = \\frac{\\mu_0 I}{2} \\frac{R^2}{(R^2 + z^2) ^{3/2}} \\tagl{5.41} For surface and volume currents, the Biot-Savart law becomes \\vec{B}(r) = \\frac{\\mu_0}{4 \\pi } \\int \\frac{\\vec{K}(r') \\times \\vu{\\gr}}{\\gr ^2} \\dd a' and \\vec{B}(r) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{J}(r') \\times \\vu{\\gr}}{\\gr ^2} \\dd \\tau' \\tagl{5.42} respectively. You might be tempted to write down the corresponding formula for a moving point charge, using the \"dictionary\" \\vec{B}(r) = \\frac{\\mu_0}{4 \\pi} \\frac{q \\vec{v} \\times \\vu{\\gr}}{\\gr ^2} but this is simply wrong . As I mentioned earlier, a point charge does not constitute a steady current, and the Biot-Savart law, which only holds for steady currents, does not correctly determine its field. The superposition principle applies to magnetic fields just as it does to electric firlds: if you have a collection of source currents, the net field is the (vector) sum of the fields due to each of them taken separately.","title":"5.2 - The Biot-Savart Law"},{"location":"ch5-2/#52-the-biot-savart-law","text":"","title":"5.2: The Biot-Savart Law"},{"location":"ch5-2/#521-steady-currents","text":"Stationary charges produce electric fields that are constant in time; hence the term electrostatics . Steady currents produce magnetic fields that are constant in time; the theory of steady currents is called magnetostatics . By \" steady current \" I mean a continuous flow that has been going on forever, without change and without piling up anywhere (some people call them \"stationary currents\"; to my ear, that's a contradiction in terms). Formally, electro/magnetostatics is the regime \\pdv{\\rho}{t} = 0, \\quad \\pdv{\\vec{J}}{t} = 0 \\tagl{5.32} at all places and all times. Of course, there's no such thing in practice as a truly steady current, any more than there is a truly stationary charge. In this sense, both electrostatics and magnetostatics describe artificial worlds that exist only in textbooks. However, they represent suitable approximations as long as the actual fluctuations are remote, or gradual - in fact, for most purposes magnetostatics applies very well to household currents, which alternate 120 times per second! Notice that a moving point charge cannot possibly constitute a steady current. If it's here one instant, it's gone the next. This may seem like a minor thing to you, but it's a major headache for me. I developed each topic in electrostatics by starting out with the simple case of a point charge at rest; then I generalized to an arbitrary charge distribution by invoking the superposition principle. This approach is not open to us in magnetostatics because a moving point charge does not produce a static field in the first place. We are forced to deal with extended current distributions right from the start, and, as a result, the arguments are bound to be more cumbersome. When a steady current flows in a wire, its magnitude I must be the same all along the line; otherwise, charge would be piling up somewhere, and it wouldn't be a steady current. More generally, since \\partial \\rho / \\partial t = 0 in magnetostatics, the continuity equation (5.29) becomes \\grad \\vec{J} = 0 \\tagl{5.33}","title":"5.2.1: Steady Currents"},{"location":"ch5-2/#522-the-magnetic-field-of-a-steady-current","text":"The magnetic field of a steady line current is given by the Biot-Savart law : \\vec{B}(r) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{I} \\cross \\vu{\\gr}}{\\gr ^2} \\dd l' = \\frac{ \\mu_0}{4 \\pi } I \\int \\frac{\\dd \\vec{l'} \\cross \\vu{\\gr}}{\\gr ^2} \\tagl{5.34} The integration is along the current path, in the direction of the flow; \\dd \\vec{l'} is an element of length along the wire, and \\vu{\\gr} , as always, is the vector from the source to the point r (Fig 5.17). The constant \\mu_0 is called the permeability of free space : \\mu_0 = 4 \\pi \\times 10^{-7} \\text{N} / \\text{A}^2 \\tagl{5.35} This is an exact number, not an empirical constant. It serves to define the ampereThese units are such that B itself comes out in newtons per ampere-meter (as required by the Lorentz force law), or teslas (T): 1 \\text{T} = 1 \\text{N} / (\\text{A} \\cdot \\text{m}) \\tagl{5.36} As the starting point for magnetostatics, the Biot-Savart law plays a role analogous to Coulomb's law in electrostatics. Indeed, the 1/ \\gr ^2 dependence is common to both laws","title":"5.2.2: The Magnetic Field of a Steady Current"},{"location":"ch5-2/#example-55","text":"Find the magnetic field a distance s from a long straight wire carrying a steady current I (Fig 5.18). Solution In the diagram, \\dd \\vec{l'} \\cross \\gr points out of the page, and has the magnitude \\dd l' \\sin \\alpha = \\dd l' \\cos \\theta Also, l' = s \\tan \\theta , so \\dd l' = \\frac{s}{ \\cos ^2 \\theta} \\dd \\theta and s = \\gr \\cos \\theta , so \\frac{1}{\\gr^2} = \\frac{\\cos ^2 \\theta}{s^2} The Biot-Savart law gives the magnetic field as \\begin{align*} B & = \\frac{\\mu_0 I}{4 \\pi} \\int _{\\theta_1}^{\\theta_2} \\left( \\frac{\\cos ^2 \\theta}{s^2} \\right) \\left( \\frac{s}{\\cos ^2 \\theta} \\right) \\cos \\theta \\dd \\theta \\\\ & = \\frac{\\mu_0}{4 \\pi s} \\int _{\\theta_1}^{\\theta_2} \\cos \\theta \\dd \\theta \\\\ & = \\frac{\\mu_0 I}{4 \\pi s} (\\sin \\theta_2 - \\sin \\theta_1) \\tagl{5.37} \\end{align*} That is the field of any straight segment of wire, in terms of the initial and final angles \\theta_1 and \\theta_2 (Fig 5.19). Of course, a finite segment by itself could never support a steady current (where would the charge go when it got to the end?), but it might be a piece of some closed circuit, and \\eqref{5.37} would then represent its contribution to the total field. In the case of an infinite wire, \\theta_1 = - \\pi / 2 and \\theta_2 = \\pi / 2 so we obtain B = \\frac{\\mu_0 I}{2 \\pi s} \\tagl{5.38} Notice that the field is inversely proportional to the distance from the wire - just like the electric field of an infinite line charge. In the region below the wire, B points into the page, and in general, it \"circles around\" the wire, in accordance with the right-hand rule \\vec{B} = \\frac{\\mu_0 I}{2 \\pi s} \\vu{\\phi} \\tagl{5.39} As an application, let's find the force of attraction between two long, parallel wires a distance d apart, carrying currents I_1 and I_2 (Fig 5.20). The field at (2) due to (1) is B = \\frac{\\mu_0 I_1}{2 \\pi d} and it points into the page. The Lorentz force law (in the form appropriate to line currents) predicts a force directed upwards (1), of magnitude F = I_2 \\left( \\frac{\\mu_0 I_1}{2 \\pi d} \\right) \\int \\dd l The total force, not surprisingly, is infinite, but the force per unit length is f = \\frac{ \\mu_0}{2 \\pi } \\frac{I_1 I_2}{d} \\tagl{5.40} If the currents are antiparallel (one up, one down), the force is repulsive - consistent again with the qualitative observations of Sect 5.1.1.","title":"Example 5.5"},{"location":"ch5-2/#example-56","text":"Find the magnetic field a distance z above the center of a circular loop of radius R , which carries a steady current I (Fig 5.21) Solution The field \\dd \\vec{B} attributable to the segment \\dd \\vec{l'} points as shown. As we integrate \\dd \\vec{l'} around the loop, \\dd \\vec{B} sweeps out a cone. The horizontal components cancel, and the vertical components combine, to give B(z) = \\frac{\\mu_0 }{4 \\pi} I \\int \\frac{\\dd l'}{\\gr ^2} \\cos \\theta (Notice that dl' and \\gr are perpendicular, in this case; the factor of \\cos \\theta projects out the vertical component.) Now, \\cos \\theta and \\gr ^2 are constants, and \\int dl' is simply the circumference 2 \\pi R , so B(z) = \\frac{\\mu_0 I}{4 \\pi} \\left( \\frac{\\cos \\theta}{\\gr ^2} \\right) 2 \\pi R = \\frac{\\mu_0 I}{2} \\frac{R^2}{(R^2 + z^2) ^{3/2}} \\tagl{5.41} For surface and volume currents, the Biot-Savart law becomes \\vec{B}(r) = \\frac{\\mu_0}{4 \\pi } \\int \\frac{\\vec{K}(r') \\times \\vu{\\gr}}{\\gr ^2} \\dd a' and \\vec{B}(r) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{J}(r') \\times \\vu{\\gr}}{\\gr ^2} \\dd \\tau' \\tagl{5.42} respectively. You might be tempted to write down the corresponding formula for a moving point charge, using the \"dictionary\" \\vec{B}(r) = \\frac{\\mu_0}{4 \\pi} \\frac{q \\vec{v} \\times \\vu{\\gr}}{\\gr ^2} but this is simply wrong . As I mentioned earlier, a point charge does not constitute a steady current, and the Biot-Savart law, which only holds for steady currents, does not correctly determine its field. The superposition principle applies to magnetic fields just as it does to electric firlds: if you have a collection of source currents, the net field is the (vector) sum of the fields due to each of them taken separately.","title":"Example 5.6"},{"location":"ch5-3/","text":"5.3: The Divergence and Curl of B 5.3.1: Straight-Line Currents The magnetic field of an infinite straight wire is shown in Fig 5.27 (the current is coming out of the page). At a glance, it is clear that this field has a nonzero curl (something you'll never see in an electrostatic field); let's calculate it. According to Eq. 5.38, the integral of B around a circular path of radius s , centered at the wire, is \\oint \\vec{B} \\cdot \\dd l = \\oint \\frac{\\mu_0 I}{2 \\pi s} = \\frac{\\mu_0 I}{2 \\pi s } \\oint \\dd l = \\mu_0 I Notice that the answer is independent of s ; that's because B decreases at the same rate as the circumference increases. In fact, it doesn't have to be a circle; any old loop that encloses the wire would give the same answer. For if we use cylindrical coordinates (s, \\phi, z) , with the current flowing along the z axis, \\vec{B} = (\\mu_0 I / 2 \\pi s) \\vu{\\phi} and \\dd \\vec{l} = \\dd s \\, \\vu{s} + s \\dd \\phi \\, \\vu{\\phi} + \\dd z \\, \\vu{z} , so \\oint \\vec{B} \\cdot \\dd l = \\frac{\\mu_0 I}{2 \\pi} \\oint \\frac{1}{s} s \\dd \\phi = \\frac{\\mu_0 I}{2 \\pi } \\int _0 ^{2 \\pi} \\dd \\phi = \\mu_0 I This assumes that the loop encircles the wire exactly once; if it went around twice, then \\phi would go from 0 to 4 \\pi , and if it didn't enclose the wire at all, then \\phi would go from \\phi_1 to \\phi_2 and back again, with \\int \\dd \\phi = 0 (Fig 5.28). Now suppose we have a bundle of straight wires. Each wire that passes through our loop contributes \\mu_0 I , and those outside contribute nothing (Fig 5.29). The line integral will then be \\oint \\vec{B} \\cdot \\dd l = \\mu_0 I_{enc} \\tagl{5.44} where I_{enc} stands for the total current enclosed by the integration path. If the flow of charge is represented by a volume current density J , the enclosed current is I_{enc} = \\int \\vec{J} \\cdot \\dd \\vec{a} \\tagl{5.45} with the integral taken over any surface bounded by the loop. Applying Stokes' theorem to Eq 5.44, then \\int (\\curl \\vec{B}) \\cdot \\dd \\vec{a} = \\mu_0 \\int \\vec{J} \\cdot \\dd \\vec{a} and hence \\curl \\vec{B} = \\mu_0 \\vec{J} With minimal labor, we have actually obtained the general formula for the curl of B . But our derivation is seriously flawed by the restriction to infinite straight line currents (and combinations thereof). Most current configurations cannot be constructed out of infinite straight wires, and we have no right to assume that Eq. 5.46 applies to them. So the next section is devoted to the formal derivation of the divergence and curl of B, starting from the Biot-Savart law itself. 5.3.2: The Divergence and Curl of B The Biot-Savart law for the general case of a volume current reads \\vec{B}(\\vec{r}) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{J}(r') \\times \\vu{\\gr}}{\\gr ^2} \\dd \\tau' \\tagl{5.47} This formula gives the magnetic field at a point r = (x, y, z) in terms of an integral over the current distribution \\vec{J} (x', y', z') (Fig 5.30). It is best to be absolutely explicit at this stage: B is a function of (x, y, z) J is a function of (x', y', z') \\vu{\\gr} = (x - x') \\vu{x} + (y - y') \\vu{y} + (z - z') \\vu{z} \\dd \\tau' = dx' dy' dz' The integration is over the primed coordinates; the divergence and the curl of B are with respect to the unprimed coordinates. Applying the divergence to \\eqref{5.47} we obtain \\div \\vec{B} = \\frac{\\mu_0 }{4 \\pi } \\int \\div \\left( \\vec{J} \\times \\frac{\\vu{\\gr}}{\\gr^2} \\right)\\dd \\tau' \\tagl{5.48} With one of our product rules for divergences \\div \\left( \\vec{J} \\times \\frac{\\vu{\\gr}}{\\gr^2} \\right) = \\frac{\\vu{\\gr}}{\\gr^2} \\cdot (\\curl \\vec{J}) - \\vec{J} \\cdot \\left( \\curl \\frac{ \\vu{\\gr}}{\\gr ^2} \\right) \\tagl{5.49} But \\curl \\vec{J} = 0 because J doesn't depend on the unprimed variables, while \\curl ( \\vu{\\gr} / \\gr ^2) = 0 (we've already explicitly worked that in Chapter 1), so \\div \\vec{B} = 0 \\tagl{5.50} Evidently, the divergence of the magnetic field is zero. Applying the curl to Eq 5.47, we obtain \\curl \\vec{B} = \\frac{\\mu_0}{4 \\pi} \\int \\curl \\left( \\vec{J} \\times \\frac{\\vu{\\gr}}{\\gr ^2} \\right) \\dd \\tau' \\tagl{5.51} Again, our strategy is to expand the integrand, using the appropriate product rule - in this case one of those for curls \\curl \\left( \\vec{J} \\times \\frac{\\vu{\\gr}}{\\gr^2} \\right) = \\vec{J} \\left( \\div \\frac{\\vu{\\gr}}{\\gr ^2} \\right) - ( \\vec{J} \\cdot \\grad) \\frac{\\vu{\\gr}}{\\gr^2} \\tagl{5.52} (I have dropped terms involving derivatives of J , because J does not depend on x, y, z.) The second term integrates to zero, as we'll see in the next paragraph. The first term involves the divergence we were at pains to calculate in Chapter 1 (Eq. 1.100): \\div \\left( \\frac{\\vu{\\gr}}{\\gr^2} \\right) = 4 \\pi \\delta ^2(\\gr) \\tagl{5.53} Thus \\curl \\vec{B} = \\frac{\\mu_0}{4 \\pi} \\int \\vec{J}(r') 4 \\pi \\delta ^3 (\\vec{r} - \\vec{r'} \\dd \\tau' = \\mu_0 \\vec{J} (\\vec{r}) which confirms that Eq. 5.46 is not restricted to straight-line currents, but holds quite generally in magnetostatics. To complete the argument, however, we must check that the second term in Eq 5.52 integrates to zero. Because the derivative acts only on \\vu{\\gr} / \\gr ^2 , we can switch from \\grad to \\grad' at the cost of a minus sign -(\\vec{J} \\cdot \\grad) \\frac{\\vu{\\gr}}{\\gr^2} = (\\vec{J} \\cdot \\grad' ) \\frac{\\vu{\\gr}}{\\gr^2} \\tagl{5.54} The x component, in particular, is (\\vec{J} \\cdot \\grad') \\left( \\frac{x - x' }{\\gr ^3} \\right) = \\grad' \\cdot \\left( \\frac{(x - x')}{\\gr ^3} \\vec{J} \\right) - \\left( x - x' \\gr ^3 \\right)(\\grad' \\cdot \\vec{J}) Now, for steady currents the divergence of J is zero, so \\left( - (\\vec{J} \\cdot \\grad) \\frac{\\vu{\\gr}}{\\gr^2} \\right)_{x} = \\grad' \\cdot \\left( \\frac{(x - x')}{\\gr ^3} \\vec{J} \\right) and therefore this contribution to the integral can be written \\int_{V} \\grad' \\cdot \\left( \\frac{(x - x')}{\\gr ^3} \\vec{J} \\right) \\dd \\tau' = \\oint_S \\frac{(x - x')}{\\gr^3} \\vec{J} \\cdot \\dd \\vec{a'} \\tagl{5.55} (The reason for switching from \\grad to \\grad' was to allow this integration by parts). But what region are we integrating over? Well, it's the volume that appears in the Biot-Savart law (Eq. 5.47) - large enough, that is, to include all the current. You can make it bigger than that, if you like; \\vec{J} = 0 out there anyway, so it will add nothing to the integral. The essential point is that on the boundary the current is zero (all current is safely inside) and hence the surface integral (Eq. 5.55) vanishes. 5.3.3: Ampere's Law The equation for the curl of B , \\curl \\vec{B} = \\mu_0 \\vec{J} \\tagl{5.56} is called Ampere's Law (in differential form). It can be converted to integral form by the usual device of applying one of the fundamental theorems - in this case Stokes' theorem: \\int (\\curl \\vec{B}) \\cdot \\dd \\vec{a} = \\oint \\vec{B} \\cdot \\dd \\vec{l} = \\mu_0 \\int \\vec{J} \\cdot \\dd \\vec{a} Now, \\int \\vec{J} \\cdot \\dd \\vec{a} is the total current passing through the surface (Fig. 5.31), which we call I_{enc} (the current enclosed by the Amperian loop). Thus \\oint \\vec{B} \\cdot \\dd \\vec{l} = \\mu_0 I_{enc} \\tagl{5.57} This is the integral version of Ampere's law; it generalizes Eq 5.44 to arbitrary steady currents. Notice that Eq 5.57 inherits the sign ambiguity of Stokes' theorem: which way around the loop am I supposed to go? And which direction through the surface corresponds to a \"positive\" current? The resolution, as always, is the right-hand rule: If the fingers of your right hand indicate the direction of integration around the boundary, then your thumb defines the direction of a positive current. Just as the Biot-Savart law plays a role in magnetostatics that Coulomb's law assumed in electrostatics, so Ampere's plays the part of Gauss's. In particular, for currents with appropriate symmetry, Ampere's law in integral form offers a lovely and extraordinarily efficient way of calculating the magnetic field. Example 5.7 Find the magnetic field a distance s from a long straight wire (Fig 5.32), carrying a steady current I (the same problem from the previous section, in which we used the Biot-Savart law) Solution We know the direction of B is \"circumferential,\" circling around the wire as indicated by the right-hand rule. By symmetry, the magnitude of B is constant around an Amperian loop of radius s , centered on the wire. So Ampere's law gives \\oint \\vec{B} \\cdot \\dd \\vec{l} = B \\oint dl = 2 \\pi s B = \\mu_0 I_{enc} = \\mu_0 I or B = \\frac{\\mu_0 I}{2 \\pi s} This is the same answer we got before (Eq. 5.38), but it was waay easier to obtain this time. Example 5.8 Find the magnetic field of an infinite uniform surface current \\vec{K} = K \\vu{x} , flowing over the xy plane (Fig 5.33) Solution First of all, what is the direction of B? Could it have any x component? No: A glance at the Biot-Savart law (Eq. 5.42) reveals that B is perpendicular to K. Could it have a z component? No again. You could confirm this by noting that any vertical contribution from a filament at +y is canceled by the corresponding filament at - y. But there is a nicer argument: Suppose the field pointed away from the plane. By reversing the direction of the current, I could make it point toward the plane (in the Biot-Savart law, changing the sign of the current switches the sign of the field). But the z component of B cannot possibly depend on the direction of the current in the xy plane. (Think about it!) So B can only have a y component, and a quick check with your right hand should convince you that it points to the left above the plane and to the right below it. With this in mind, we draw a rectangular Amperian loop as shown in Fig. 5.33, parallel to the yz plane and extending an equal distance above and below the surface. Applying Ampere's law, \\oint \\vec{B} \\cdot \\dd \\vec{l} = 2 B l = \\mu_0 I_{enc} = \\mu_0 K l (One Bl comes from the top segment and the other from the bottom), so B = (\\mu_0 / 2) K , or, more precisely \\vec{B} = \\begin{cases} +(\\mu_0 / 2) K \\vu{y} \\quad & \\text{ for } z < 0 \\\\ - (\\mu_0 / 2) K \\vu{y} \\quad & \\text{ for } z > 0 \\end{cases} \\tagl{5.58} Notice that the field is independent of the distance from the plane, just like the electric field of a uniform surface charge. Example 5.9 Find the magnetic field of a very long solenoid, consisting of n closely wound turns per unit length on a cylinder of radius R, each carrying a steady current I (Fig 5.34). [The point of making the windings so close is that one can then pretend each turn is circular. If this troubles you (after all, there is a net current I in the direction of the solenoid's axis, no matter how tight the winding), picture instead a sheet of aluminum foil wrapped around the cylinder, carrying the equivalent of a uniform surface current K = nI (Fig 5.35). Or make a double winding, going up to one end and then - always in the same sense - going back down again, thereby eliminating the net longitudinal current. But, in truth, this is all unnecessary fastidiousness, for the field inside a solenoid is huge (relatively speaking), and the field of the longitudinal current is at most a tiny refinement.] Solution First of all, what is the direction of B ? Could it have a radial component? No. For suppose B_s were positive; if we reversed the direction of the current, B_s would then be negative. But switching I is physically equivalent to turning the solenoid upside down, and that certainly should not alter the radial field. How about a \"circumferential\" component? No. For B_{\\phi} would be constant around an Amperian loop concentric with the solenoid (Fig. 5.36), and hence \\oint \\vec{B} \\cdot \\dd \\vec{l} = B_{\\phi} (2 \\pi s) = \\mu_0 I_{enc} = 0 since the loop encloses no current. So, the magnetic field of an infinite, closely wound solenoid runs parallel to the axis. From the right-hand rule, we expect that it points inside the solenoid and downward outside. Moreover, it certainly approaches zero as you go very far away. With this in mind, let's apply Ampere's law to the two rectangular loops in Fig 5.37. Loop 1 lies entirely outside the solenoid, with its sides at distances a and b from the axis: \\oint \\vec{B} \\cdot \\dd \\vec{l} = [ B(a) - B(b)] L = \\mu_0 I_{enc} = 0 so B(a) = B(b) Evidently the field outside does not depend on the distance from the axis. But we agreed that it goes to zero for large s. It must therefore be zero everywhere! (This astonishing result can also be derived from the Biot-Savart law, of course, but it's much more difficult. See Prob. 5.46.) As for loop 2, which is half inside and half outside, Ampere's law gives \\oint \\vec{B} \\cdot \\dd \\vec{l} = B L = \\mu_0 I_{enc} = \\mu_0 n I L where B is the field inside the solenoid. (The right side of the loop contributes nothing, since B = 0 out there.) Conclusion: \\vec{B} = \\begin{cases} \\mu_0 n I \\vu{z} & \\quad \\text{ inside the solenoid } \\\\ 0 & \\quad \\text{ outside the solenoid } \\end{cases} \\tagl{5.59} Notice that the field inside is uniform - it doesn't depend on the distance from the axis. In this sense the solenoid is to magnetostatics what the parallel-plate capacitor is to electrostatics: a simple device for producing strong uniform fields. Like Gauss's law, Ampere's law is always true (for steady currents), but it is not always useful. Only when the symmetry of the problem enables you to pull B outside the integral \\oint \\vec{B} \\cdot \\dd \\vec{l} can you calculate the magnetic field from Ampere's law. When it does work, it's by far the fastest method; when it doesn't, you have to fall back on the Biot-Savart law. The current configurations that can be handled by Ampere's law are Infinite straight lines (Ex 5.7) Infinite planes (Ex 5.8) Infinite solenoids (Ex 5.9) Toroids (Ex 5.10) The last of these is a surprising and elegant application of Ampere's law. As in Exs. 5.8 and 5.9, the hard part is figuring out the direction of the field (which we will now have done, once and for all, for each of the four geometries); the actual application of Ampere's law takes only one line. Example 5.10 A toroidal coil consists of a circular ring, or 'donut,' around which a long wire is wrapped (Fig 5.38). The winding is uniform and tight enough so that each turn can be considered a plane closed loop. The cross-sectional shape of the coil is immaterial; I made it rectangular in Fig 5.38 for the sake of simplicity, but it could just as well be circular or even some weird asymmetrical form, as in Fig 5.39, as long as the shape remains the same all the way around the ring. In that case, it follows that the magnetic field of the toroid is circumferential at all points, both inside and outside the coil. Proof According to the Biot-Savart law, the field at r due to the current element at r' is \\dd \\vec{B} = \\frac{\\mu_0 }{4 \\pi} \\frac{\\vec{I} \\cross \\vu{\\gr}}{\\gr ^3} \\dd l' We may as well put r in the xz plane (Fig 5.39), so its Cartesian components are (x, 0, z), while the source coordinates are \\vec{r'} = (s' \\cos \\phi', s' \\sin \\phi', z') Then \\gr = (x - s' \\cos \\phi', -s' \\sin \\phi', z - z') Since the current has no \\phi component, \\vec{I} = I_s \\vu{x} + I_z \\vu{z} , or (in Cartesian coordinates) \\vec{I} = (I_s \\cos \\phi', I_s \\sin ' , I_z) Accordingly, \\vec{I} \\cross \\vec{\\gr} = \\begin{bmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ I_s \\cos \\phi' & I_s \\sin \\phi' & I_z \\\\ (x - s' \\cos \\phi') & (- s' \\sin \\phi') & (z - z') \\end{bmatrix} \\\\ = [ \\sin \\phi' (I_s (z - z') + s' I_z)] \\vu{x} + [I_z (x - s' \\cos \\phi') - I_s \\cos \\phi' (z - z')] \\vu{y} + [-I_s x \\sin \\phi']\\vu{z} But there is a symmetrically situated current element at r'' , with the same s' , the same \\gr , the same dl' , the same I_s , and the same I_z , but negative \\phi' (Fig 5.39). Because \\sin \\phi' changes sign, the \\vu{x} and \\vu{z} contributions from \\vu{r'} and \\vu{r''} cancel each other, leaving only a \\vu{y} term. Thus the field at \\vu{r} is in the \\vu{y} direction, and in general the field points in the \\vu{\\phi} direction. Now that we know the field is circumferential, determining its magnitude is simple: just apply Ampere's law to a circle of radius s about the axis of the toroid B 2 \\pi s = \\mu_0 I_{enc} and hence \\vec{B}(\\vec{r}) = \\begin{cases} \\frac{\\mu_0 N I}{2 \\pi s} \\vu{\\phi} \\quad & \\text{ for points inside the coil} \\\\ 0 \\quad & \\text{ for points outside the coil} \\end{cases} \\tagl{5.60} where N is the total number of turns. 5.3.4: Comparison of Magnetostatics and Electrostatics The divergence and curl of the electrostatic field are \\begin{cases} \\div \\vec{E} = \\frac{1}{\\epsilon_0} \\rho \\quad & \\text{(Gauss's law)}\\\\ \\curl \\vec{E} = 0 \\quad & \\text{(no name)} \\end{cases} These are Maxwell's equations for electrostatics. Together with the boundary condition \\vec{E} \\rightarrow 0 far from all charges, Maxwell's equations determine the field, if the source charge density \\rho is given; they contain essentially the same information as Coulomb's law plus the principle of superposition. The divergence and curl of the magnetostatic field are \\begin{cases} \\div \\vec{B} = 0 \\quad & \\text{(no name)} \\\\ \\curl \\vec{B} = \\mu_0 \\vec{J} \\quad & \\text{(Ampere's Law)} \\end{cases} These are Maxwell's equations for magnetostatics. Again, together with the boundary condition \\vec{B} \\rightarrow 0 far from all currents, Maxwell's equations determine the magnetic field; they are equivalent to the Biot-Savart law (plus superposition). Maxwell's equations and the force law \\vec{F} = Q(\\vec{E} + \\vec{v} \\cross \\vec{B}) constitute the most elegant formulation of electrostatics and magnetostatics. The electric field diverges away from a (positive) charge; the magnetic field line curls around a current (Fig. 5.44). Electric field lines originate on positive charges and terminate on negative ones; magnetic field lines do not begin or end anywhere - to do so would require a nonzero divergence. They typically form closed loops or extend out to infinity. To put it another way, there are no point sources for B , as there are for E ; there exists no magnetic analog to electric charge. This is the physical content of the statement \\div \\vec{B} = 0 . Coulomb and others believed that magnetism was produced by magnetic charges (magnetic monopoles, as we would now call them), and in some older books you will still find references to a magnetic version of Coulomb's law, giving the force of attraction or repulsion between them. It was Ampere who first speculated that all magnetic effects are attributable to electric charges in motion (currents). As far as we know, Ampere was right; nevertheless, it remains an open experimental question whether magnetic monopoles exist in nature (they are obviously pretty rare, or somebody would have found one), and in fact some recent elementary particle theories require them. For our purposes, though, B is divergenceless, and there are no magnetic monopoles. It takes a moving electric charge to produce a magnetic field, and it takes another moving electric charge to \"feel\" a magnetic field. Typically, electric forces are enormously larger than magnetic ones. That's not something intrinsic to the theory; it has to do with the sizes of the fundamental constants \\epsilon_0 and \\mu_0 . In general, it is only when both the source charges and the test charge are moving at velocities comparable to the speed of light that the magnetic force approaches the electric force in strength. (Problems 5.13 and 5.17 illustrate this rule.) How is it, then, that we notice magnetic effects at all? The answer is that both in the production of a magnetic field (Biot-Savart) and in its detection (Lorentz), it is the current that matters, and we can compensate for a smallish velocity by pouring huge amounts of charge down the wire. Ordinarily, this charge would simultaneously generate so large an electric force as to swamp the magnetic one. But if we arrange to keep the wire neutral, by embedding in it an equal quantity of opposite charge at rest, the electric field cancels out, leaving the magnetic field to stand alone. It sounds very elaborate, but of course this is precisely what happens in an ordinary current carrying wire.","title":"5.3 - The Divergence and Curl of B"},{"location":"ch5-3/#53-the-divergence-and-curl-of-b","text":"","title":"5.3: The Divergence and Curl of B"},{"location":"ch5-3/#531-straight-line-currents","text":"The magnetic field of an infinite straight wire is shown in Fig 5.27 (the current is coming out of the page). At a glance, it is clear that this field has a nonzero curl (something you'll never see in an electrostatic field); let's calculate it. According to Eq. 5.38, the integral of B around a circular path of radius s , centered at the wire, is \\oint \\vec{B} \\cdot \\dd l = \\oint \\frac{\\mu_0 I}{2 \\pi s} = \\frac{\\mu_0 I}{2 \\pi s } \\oint \\dd l = \\mu_0 I Notice that the answer is independent of s ; that's because B decreases at the same rate as the circumference increases. In fact, it doesn't have to be a circle; any old loop that encloses the wire would give the same answer. For if we use cylindrical coordinates (s, \\phi, z) , with the current flowing along the z axis, \\vec{B} = (\\mu_0 I / 2 \\pi s) \\vu{\\phi} and \\dd \\vec{l} = \\dd s \\, \\vu{s} + s \\dd \\phi \\, \\vu{\\phi} + \\dd z \\, \\vu{z} , so \\oint \\vec{B} \\cdot \\dd l = \\frac{\\mu_0 I}{2 \\pi} \\oint \\frac{1}{s} s \\dd \\phi = \\frac{\\mu_0 I}{2 \\pi } \\int _0 ^{2 \\pi} \\dd \\phi = \\mu_0 I This assumes that the loop encircles the wire exactly once; if it went around twice, then \\phi would go from 0 to 4 \\pi , and if it didn't enclose the wire at all, then \\phi would go from \\phi_1 to \\phi_2 and back again, with \\int \\dd \\phi = 0 (Fig 5.28). Now suppose we have a bundle of straight wires. Each wire that passes through our loop contributes \\mu_0 I , and those outside contribute nothing (Fig 5.29). The line integral will then be \\oint \\vec{B} \\cdot \\dd l = \\mu_0 I_{enc} \\tagl{5.44} where I_{enc} stands for the total current enclosed by the integration path. If the flow of charge is represented by a volume current density J , the enclosed current is I_{enc} = \\int \\vec{J} \\cdot \\dd \\vec{a} \\tagl{5.45} with the integral taken over any surface bounded by the loop. Applying Stokes' theorem to Eq 5.44, then \\int (\\curl \\vec{B}) \\cdot \\dd \\vec{a} = \\mu_0 \\int \\vec{J} \\cdot \\dd \\vec{a} and hence \\curl \\vec{B} = \\mu_0 \\vec{J} With minimal labor, we have actually obtained the general formula for the curl of B . But our derivation is seriously flawed by the restriction to infinite straight line currents (and combinations thereof). Most current configurations cannot be constructed out of infinite straight wires, and we have no right to assume that Eq. 5.46 applies to them. So the next section is devoted to the formal derivation of the divergence and curl of B, starting from the Biot-Savart law itself.","title":"5.3.1: Straight-Line Currents"},{"location":"ch5-3/#532-the-divergence-and-curl-of-b","text":"The Biot-Savart law for the general case of a volume current reads \\vec{B}(\\vec{r}) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{J}(r') \\times \\vu{\\gr}}{\\gr ^2} \\dd \\tau' \\tagl{5.47} This formula gives the magnetic field at a point r = (x, y, z) in terms of an integral over the current distribution \\vec{J} (x', y', z') (Fig 5.30). It is best to be absolutely explicit at this stage: B is a function of (x, y, z) J is a function of (x', y', z') \\vu{\\gr} = (x - x') \\vu{x} + (y - y') \\vu{y} + (z - z') \\vu{z} \\dd \\tau' = dx' dy' dz' The integration is over the primed coordinates; the divergence and the curl of B are with respect to the unprimed coordinates. Applying the divergence to \\eqref{5.47} we obtain \\div \\vec{B} = \\frac{\\mu_0 }{4 \\pi } \\int \\div \\left( \\vec{J} \\times \\frac{\\vu{\\gr}}{\\gr^2} \\right)\\dd \\tau' \\tagl{5.48} With one of our product rules for divergences \\div \\left( \\vec{J} \\times \\frac{\\vu{\\gr}}{\\gr^2} \\right) = \\frac{\\vu{\\gr}}{\\gr^2} \\cdot (\\curl \\vec{J}) - \\vec{J} \\cdot \\left( \\curl \\frac{ \\vu{\\gr}}{\\gr ^2} \\right) \\tagl{5.49} But \\curl \\vec{J} = 0 because J doesn't depend on the unprimed variables, while \\curl ( \\vu{\\gr} / \\gr ^2) = 0 (we've already explicitly worked that in Chapter 1), so \\div \\vec{B} = 0 \\tagl{5.50} Evidently, the divergence of the magnetic field is zero. Applying the curl to Eq 5.47, we obtain \\curl \\vec{B} = \\frac{\\mu_0}{4 \\pi} \\int \\curl \\left( \\vec{J} \\times \\frac{\\vu{\\gr}}{\\gr ^2} \\right) \\dd \\tau' \\tagl{5.51} Again, our strategy is to expand the integrand, using the appropriate product rule - in this case one of those for curls \\curl \\left( \\vec{J} \\times \\frac{\\vu{\\gr}}{\\gr^2} \\right) = \\vec{J} \\left( \\div \\frac{\\vu{\\gr}}{\\gr ^2} \\right) - ( \\vec{J} \\cdot \\grad) \\frac{\\vu{\\gr}}{\\gr^2} \\tagl{5.52} (I have dropped terms involving derivatives of J , because J does not depend on x, y, z.) The second term integrates to zero, as we'll see in the next paragraph. The first term involves the divergence we were at pains to calculate in Chapter 1 (Eq. 1.100): \\div \\left( \\frac{\\vu{\\gr}}{\\gr^2} \\right) = 4 \\pi \\delta ^2(\\gr) \\tagl{5.53} Thus \\curl \\vec{B} = \\frac{\\mu_0}{4 \\pi} \\int \\vec{J}(r') 4 \\pi \\delta ^3 (\\vec{r} - \\vec{r'} \\dd \\tau' = \\mu_0 \\vec{J} (\\vec{r}) which confirms that Eq. 5.46 is not restricted to straight-line currents, but holds quite generally in magnetostatics. To complete the argument, however, we must check that the second term in Eq 5.52 integrates to zero. Because the derivative acts only on \\vu{\\gr} / \\gr ^2 , we can switch from \\grad to \\grad' at the cost of a minus sign -(\\vec{J} \\cdot \\grad) \\frac{\\vu{\\gr}}{\\gr^2} = (\\vec{J} \\cdot \\grad' ) \\frac{\\vu{\\gr}}{\\gr^2} \\tagl{5.54} The x component, in particular, is (\\vec{J} \\cdot \\grad') \\left( \\frac{x - x' }{\\gr ^3} \\right) = \\grad' \\cdot \\left( \\frac{(x - x')}{\\gr ^3} \\vec{J} \\right) - \\left( x - x' \\gr ^3 \\right)(\\grad' \\cdot \\vec{J}) Now, for steady currents the divergence of J is zero, so \\left( - (\\vec{J} \\cdot \\grad) \\frac{\\vu{\\gr}}{\\gr^2} \\right)_{x} = \\grad' \\cdot \\left( \\frac{(x - x')}{\\gr ^3} \\vec{J} \\right) and therefore this contribution to the integral can be written \\int_{V} \\grad' \\cdot \\left( \\frac{(x - x')}{\\gr ^3} \\vec{J} \\right) \\dd \\tau' = \\oint_S \\frac{(x - x')}{\\gr^3} \\vec{J} \\cdot \\dd \\vec{a'} \\tagl{5.55} (The reason for switching from \\grad to \\grad' was to allow this integration by parts). But what region are we integrating over? Well, it's the volume that appears in the Biot-Savart law (Eq. 5.47) - large enough, that is, to include all the current. You can make it bigger than that, if you like; \\vec{J} = 0 out there anyway, so it will add nothing to the integral. The essential point is that on the boundary the current is zero (all current is safely inside) and hence the surface integral (Eq. 5.55) vanishes.","title":"5.3.2: The Divergence and Curl of B"},{"location":"ch5-3/#533-amperes-law","text":"The equation for the curl of B , \\curl \\vec{B} = \\mu_0 \\vec{J} \\tagl{5.56} is called Ampere's Law (in differential form). It can be converted to integral form by the usual device of applying one of the fundamental theorems - in this case Stokes' theorem: \\int (\\curl \\vec{B}) \\cdot \\dd \\vec{a} = \\oint \\vec{B} \\cdot \\dd \\vec{l} = \\mu_0 \\int \\vec{J} \\cdot \\dd \\vec{a} Now, \\int \\vec{J} \\cdot \\dd \\vec{a} is the total current passing through the surface (Fig. 5.31), which we call I_{enc} (the current enclosed by the Amperian loop). Thus \\oint \\vec{B} \\cdot \\dd \\vec{l} = \\mu_0 I_{enc} \\tagl{5.57} This is the integral version of Ampere's law; it generalizes Eq 5.44 to arbitrary steady currents. Notice that Eq 5.57 inherits the sign ambiguity of Stokes' theorem: which way around the loop am I supposed to go? And which direction through the surface corresponds to a \"positive\" current? The resolution, as always, is the right-hand rule: If the fingers of your right hand indicate the direction of integration around the boundary, then your thumb defines the direction of a positive current. Just as the Biot-Savart law plays a role in magnetostatics that Coulomb's law assumed in electrostatics, so Ampere's plays the part of Gauss's. In particular, for currents with appropriate symmetry, Ampere's law in integral form offers a lovely and extraordinarily efficient way of calculating the magnetic field.","title":"5.3.3: Ampere's Law"},{"location":"ch5-3/#example-57","text":"Find the magnetic field a distance s from a long straight wire (Fig 5.32), carrying a steady current I (the same problem from the previous section, in which we used the Biot-Savart law) Solution We know the direction of B is \"circumferential,\" circling around the wire as indicated by the right-hand rule. By symmetry, the magnitude of B is constant around an Amperian loop of radius s , centered on the wire. So Ampere's law gives \\oint \\vec{B} \\cdot \\dd \\vec{l} = B \\oint dl = 2 \\pi s B = \\mu_0 I_{enc} = \\mu_0 I or B = \\frac{\\mu_0 I}{2 \\pi s} This is the same answer we got before (Eq. 5.38), but it was waay easier to obtain this time.","title":"Example 5.7"},{"location":"ch5-3/#example-58","text":"Find the magnetic field of an infinite uniform surface current \\vec{K} = K \\vu{x} , flowing over the xy plane (Fig 5.33) Solution First of all, what is the direction of B? Could it have any x component? No: A glance at the Biot-Savart law (Eq. 5.42) reveals that B is perpendicular to K. Could it have a z component? No again. You could confirm this by noting that any vertical contribution from a filament at +y is canceled by the corresponding filament at - y. But there is a nicer argument: Suppose the field pointed away from the plane. By reversing the direction of the current, I could make it point toward the plane (in the Biot-Savart law, changing the sign of the current switches the sign of the field). But the z component of B cannot possibly depend on the direction of the current in the xy plane. (Think about it!) So B can only have a y component, and a quick check with your right hand should convince you that it points to the left above the plane and to the right below it. With this in mind, we draw a rectangular Amperian loop as shown in Fig. 5.33, parallel to the yz plane and extending an equal distance above and below the surface. Applying Ampere's law, \\oint \\vec{B} \\cdot \\dd \\vec{l} = 2 B l = \\mu_0 I_{enc} = \\mu_0 K l (One Bl comes from the top segment and the other from the bottom), so B = (\\mu_0 / 2) K , or, more precisely \\vec{B} = \\begin{cases} +(\\mu_0 / 2) K \\vu{y} \\quad & \\text{ for } z < 0 \\\\ - (\\mu_0 / 2) K \\vu{y} \\quad & \\text{ for } z > 0 \\end{cases} \\tagl{5.58} Notice that the field is independent of the distance from the plane, just like the electric field of a uniform surface charge.","title":"Example 5.8"},{"location":"ch5-3/#example-59","text":"Find the magnetic field of a very long solenoid, consisting of n closely wound turns per unit length on a cylinder of radius R, each carrying a steady current I (Fig 5.34). [The point of making the windings so close is that one can then pretend each turn is circular. If this troubles you (after all, there is a net current I in the direction of the solenoid's axis, no matter how tight the winding), picture instead a sheet of aluminum foil wrapped around the cylinder, carrying the equivalent of a uniform surface current K = nI (Fig 5.35). Or make a double winding, going up to one end and then - always in the same sense - going back down again, thereby eliminating the net longitudinal current. But, in truth, this is all unnecessary fastidiousness, for the field inside a solenoid is huge (relatively speaking), and the field of the longitudinal current is at most a tiny refinement.] Solution First of all, what is the direction of B ? Could it have a radial component? No. For suppose B_s were positive; if we reversed the direction of the current, B_s would then be negative. But switching I is physically equivalent to turning the solenoid upside down, and that certainly should not alter the radial field. How about a \"circumferential\" component? No. For B_{\\phi} would be constant around an Amperian loop concentric with the solenoid (Fig. 5.36), and hence \\oint \\vec{B} \\cdot \\dd \\vec{l} = B_{\\phi} (2 \\pi s) = \\mu_0 I_{enc} = 0 since the loop encloses no current. So, the magnetic field of an infinite, closely wound solenoid runs parallel to the axis. From the right-hand rule, we expect that it points inside the solenoid and downward outside. Moreover, it certainly approaches zero as you go very far away. With this in mind, let's apply Ampere's law to the two rectangular loops in Fig 5.37. Loop 1 lies entirely outside the solenoid, with its sides at distances a and b from the axis: \\oint \\vec{B} \\cdot \\dd \\vec{l} = [ B(a) - B(b)] L = \\mu_0 I_{enc} = 0 so B(a) = B(b) Evidently the field outside does not depend on the distance from the axis. But we agreed that it goes to zero for large s. It must therefore be zero everywhere! (This astonishing result can also be derived from the Biot-Savart law, of course, but it's much more difficult. See Prob. 5.46.) As for loop 2, which is half inside and half outside, Ampere's law gives \\oint \\vec{B} \\cdot \\dd \\vec{l} = B L = \\mu_0 I_{enc} = \\mu_0 n I L where B is the field inside the solenoid. (The right side of the loop contributes nothing, since B = 0 out there.) Conclusion: \\vec{B} = \\begin{cases} \\mu_0 n I \\vu{z} & \\quad \\text{ inside the solenoid } \\\\ 0 & \\quad \\text{ outside the solenoid } \\end{cases} \\tagl{5.59} Notice that the field inside is uniform - it doesn't depend on the distance from the axis. In this sense the solenoid is to magnetostatics what the parallel-plate capacitor is to electrostatics: a simple device for producing strong uniform fields. Like Gauss's law, Ampere's law is always true (for steady currents), but it is not always useful. Only when the symmetry of the problem enables you to pull B outside the integral \\oint \\vec{B} \\cdot \\dd \\vec{l} can you calculate the magnetic field from Ampere's law. When it does work, it's by far the fastest method; when it doesn't, you have to fall back on the Biot-Savart law. The current configurations that can be handled by Ampere's law are Infinite straight lines (Ex 5.7) Infinite planes (Ex 5.8) Infinite solenoids (Ex 5.9) Toroids (Ex 5.10) The last of these is a surprising and elegant application of Ampere's law. As in Exs. 5.8 and 5.9, the hard part is figuring out the direction of the field (which we will now have done, once and for all, for each of the four geometries); the actual application of Ampere's law takes only one line.","title":"Example 5.9"},{"location":"ch5-3/#example-510","text":"A toroidal coil consists of a circular ring, or 'donut,' around which a long wire is wrapped (Fig 5.38). The winding is uniform and tight enough so that each turn can be considered a plane closed loop. The cross-sectional shape of the coil is immaterial; I made it rectangular in Fig 5.38 for the sake of simplicity, but it could just as well be circular or even some weird asymmetrical form, as in Fig 5.39, as long as the shape remains the same all the way around the ring. In that case, it follows that the magnetic field of the toroid is circumferential at all points, both inside and outside the coil. Proof According to the Biot-Savart law, the field at r due to the current element at r' is \\dd \\vec{B} = \\frac{\\mu_0 }{4 \\pi} \\frac{\\vec{I} \\cross \\vu{\\gr}}{\\gr ^3} \\dd l' We may as well put r in the xz plane (Fig 5.39), so its Cartesian components are (x, 0, z), while the source coordinates are \\vec{r'} = (s' \\cos \\phi', s' \\sin \\phi', z') Then \\gr = (x - s' \\cos \\phi', -s' \\sin \\phi', z - z') Since the current has no \\phi component, \\vec{I} = I_s \\vu{x} + I_z \\vu{z} , or (in Cartesian coordinates) \\vec{I} = (I_s \\cos \\phi', I_s \\sin ' , I_z) Accordingly, \\vec{I} \\cross \\vec{\\gr} = \\begin{bmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ I_s \\cos \\phi' & I_s \\sin \\phi' & I_z \\\\ (x - s' \\cos \\phi') & (- s' \\sin \\phi') & (z - z') \\end{bmatrix} \\\\ = [ \\sin \\phi' (I_s (z - z') + s' I_z)] \\vu{x} + [I_z (x - s' \\cos \\phi') - I_s \\cos \\phi' (z - z')] \\vu{y} + [-I_s x \\sin \\phi']\\vu{z} But there is a symmetrically situated current element at r'' , with the same s' , the same \\gr , the same dl' , the same I_s , and the same I_z , but negative \\phi' (Fig 5.39). Because \\sin \\phi' changes sign, the \\vu{x} and \\vu{z} contributions from \\vu{r'} and \\vu{r''} cancel each other, leaving only a \\vu{y} term. Thus the field at \\vu{r} is in the \\vu{y} direction, and in general the field points in the \\vu{\\phi} direction. Now that we know the field is circumferential, determining its magnitude is simple: just apply Ampere's law to a circle of radius s about the axis of the toroid B 2 \\pi s = \\mu_0 I_{enc} and hence \\vec{B}(\\vec{r}) = \\begin{cases} \\frac{\\mu_0 N I}{2 \\pi s} \\vu{\\phi} \\quad & \\text{ for points inside the coil} \\\\ 0 \\quad & \\text{ for points outside the coil} \\end{cases} \\tagl{5.60} where N is the total number of turns.","title":"Example 5.10"},{"location":"ch5-3/#534-comparison-of-magnetostatics-and-electrostatics","text":"The divergence and curl of the electrostatic field are \\begin{cases} \\div \\vec{E} = \\frac{1}{\\epsilon_0} \\rho \\quad & \\text{(Gauss's law)}\\\\ \\curl \\vec{E} = 0 \\quad & \\text{(no name)} \\end{cases} These are Maxwell's equations for electrostatics. Together with the boundary condition \\vec{E} \\rightarrow 0 far from all charges, Maxwell's equations determine the field, if the source charge density \\rho is given; they contain essentially the same information as Coulomb's law plus the principle of superposition. The divergence and curl of the magnetostatic field are \\begin{cases} \\div \\vec{B} = 0 \\quad & \\text{(no name)} \\\\ \\curl \\vec{B} = \\mu_0 \\vec{J} \\quad & \\text{(Ampere's Law)} \\end{cases} These are Maxwell's equations for magnetostatics. Again, together with the boundary condition \\vec{B} \\rightarrow 0 far from all currents, Maxwell's equations determine the magnetic field; they are equivalent to the Biot-Savart law (plus superposition). Maxwell's equations and the force law \\vec{F} = Q(\\vec{E} + \\vec{v} \\cross \\vec{B}) constitute the most elegant formulation of electrostatics and magnetostatics. The electric field diverges away from a (positive) charge; the magnetic field line curls around a current (Fig. 5.44). Electric field lines originate on positive charges and terminate on negative ones; magnetic field lines do not begin or end anywhere - to do so would require a nonzero divergence. They typically form closed loops or extend out to infinity. To put it another way, there are no point sources for B , as there are for E ; there exists no magnetic analog to electric charge. This is the physical content of the statement \\div \\vec{B} = 0 . Coulomb and others believed that magnetism was produced by magnetic charges (magnetic monopoles, as we would now call them), and in some older books you will still find references to a magnetic version of Coulomb's law, giving the force of attraction or repulsion between them. It was Ampere who first speculated that all magnetic effects are attributable to electric charges in motion (currents). As far as we know, Ampere was right; nevertheless, it remains an open experimental question whether magnetic monopoles exist in nature (they are obviously pretty rare, or somebody would have found one), and in fact some recent elementary particle theories require them. For our purposes, though, B is divergenceless, and there are no magnetic monopoles. It takes a moving electric charge to produce a magnetic field, and it takes another moving electric charge to \"feel\" a magnetic field. Typically, electric forces are enormously larger than magnetic ones. That's not something intrinsic to the theory; it has to do with the sizes of the fundamental constants \\epsilon_0 and \\mu_0 . In general, it is only when both the source charges and the test charge are moving at velocities comparable to the speed of light that the magnetic force approaches the electric force in strength. (Problems 5.13 and 5.17 illustrate this rule.) How is it, then, that we notice magnetic effects at all? The answer is that both in the production of a magnetic field (Biot-Savart) and in its detection (Lorentz), it is the current that matters, and we can compensate for a smallish velocity by pouring huge amounts of charge down the wire. Ordinarily, this charge would simultaneously generate so large an electric force as to swamp the magnetic one. But if we arrange to keep the wire neutral, by embedding in it an equal quantity of opposite charge at rest, the electric field cancels out, leaving the magnetic field to stand alone. It sounds very elaborate, but of course this is precisely what happens in an ordinary current carrying wire.","title":"5.3.4: Comparison of Magnetostatics and Electrostatics"},{"location":"ch5-4/","text":"5.4: Magnetic Vector Potential 5.4.1: The Vector Potential Just as \\curl \\vec{E} = 0 permitted us to introduce a scalar potential (V) in electrostatics, \\vec{E} = - \\grad \\vec{V} so \\div \\vec{B} = 0 invites the introduction of a vector potential A in magnetostatics \\vec{B} = \\curl \\vec{A} \\tagl{5.61} We were allowed to define these potentials based on our extended proof of the Helmholtz theorem (back in Section 1.6). The potential formulation automatically takes care of \\div \\vec{B} = 0 since the divergence of a curl is always zero; there remains Ampere's law: \\curl \\vec{B} = \\curl (\\curl \\vec{A}) = \\grad (\\div \\vec{A}) - \\grad ^2 \\vec{A} = \\mu_0 \\vec{J} \\tagl{5.62} Now, the electric potential had a built-in ambiguity: you can add to V any function whose gradient is zero (which is to say, a constant), without altering the physical quantity E . Likewise, you can add to A any function whose curl vanishes (which is to say, the gradient of any scalar), with no effect on B . We can exploit this freedom to eliminate the divergence of A : \\div \\vec{A} = 0 \\tagl{5.63} To prove that this is always possible, suppose that our original potential \\vec{A_0} is not divergenceless. If we add to it the gradient of \\lambda (\\vec{A} = \\vec{A}_0 + \\grad \\lambda) , the new divergence is \\div \\vec{A} = \\div \\vec{A_0} + \\grad ^2 \\lambda We can accommodate Eq. 5.63, then, if a function \\lambda can be found that satisfies \\grad ^2 \\lambda = - \\div \\vec{A_0} But this is mathematically identical to Poisson's equation \\grad ^2 \\lambda = - \\frac{\\rho}{\\epsilon_0} with \\div \\vec{A}_0 in place of \\rho / \\epsilon_0 as the \"source.\" And we know how to solve Poisson's equation - that's what electrostatics is all about. In particular, if \\rho goes to infinity, the solution is Eq. 2.29: V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho}{\\gr} \\dd \\tau' and by the same token, if \\div \\vec{A}_0 goes to zero at infinity, then \\lambda = \\frac{1}{4 \\pi} \\int \\frac{\\div \\vec{A}_0}{\\gr } \\dd \\tau' If \\div \\vec{A}_0 does not go to zero at infinity, then we'll have to use other means to discover the appropriate \\lambda , just as we get the electric potential by other means when the charge distribution extends to infinity. But the essential point remains: It is always possible to make the vector potential divergenceless. To put it the other way round,: the definition \\vec{B} = \\curl \\vec{A} specifies the curl of A , but it doesn't say anything about the divergence - we are at liberty to pick that as we see fit, and zero is ordinarily the simplest choice. With this condition on A , Ampere's law becomes \\grad ^2 \\vec{A} = - \\mu_0 \\vec{J} \\tagl{5.64} This again is nothing but Poisson's equation, or rather it is three of them, one for each Cartesian coordinate. In Cartesian coordinates, \\grad ^2 \\vec{A} = (\\grad ^2 A_x) \\vu{x} + (\\grad ^2 A_y) \\vu{y} + (\\grad ^2 A_z) \\vu{z} , so 5.64 reduces to \\grad ^2 A_x = - \\mu_0 J_x , \\grad ^2 A_y = - \\mu_0 J_y , and \\grad ^2 A_z = - \\mu_0 J_z . In curvilinear coordinates, the unit vectors themselves are functions of position, and must be differentiated, so it is not the case, for example, that \\grad ^2 A_r = - \\mu_0 J_r . Remember that even if you plan to evaluate integrals such as 5.65 using curvilinear coordinates, you must first express \\vec{J} in terms of its Cartesian components. Assuming J goes to zero at infinity, we can read off the solution \\vec{A} (\\vec{r}) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{J}(r')}{\\gr} \\dd \\tau' \\tagl{5.65} For line and surface currents, \\vec{A} = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{I}}{\\gr } \\dd l' = \\frac{\\mu_0 \\vec{I}}{4 \\pi} \\int \\frac{1}{\\gr} \\dd \\vec{l}'; \\qquad \\vec{A} = \\frac{\\mu_0}{4\\pi} \\int \\frac{\\vec{K}}{\\gr} \\dd a' \\tagl{5.66} (If the current does not go to zero at infinity, we have to find other ways to get A ; some of these are explored in Exercise 5.12 and in the problems at the end of the section.) It must be said that A is not as useful as V. For one thing, it's still a vector, and although Eqs. 5.65 and 5.66 are somewhat easier to work with than the Biot-Savart law, you still have to fuss with components. It would be nice if we could get away with a scalar potential \\vec{B} = - \\grad U but this is incompatible with Ampere's law, since the curl of a gradient is always zero. (A magnetostatic scalar potential can be used, if you stick scrupulously to simply-connected, current-free regions, but as a theoretical tool, it is of limited interest. See problem 5.29.) Moreover, since magnetic forces do no work, A does not admit a simple physical interpretation in terms of potential energy per unit charge. (In some contexts it can be interpreted as the momentum per unit charge.) Nevertheless, the vector potential has substantial theoretical importance, as we shall see in chapter 10. Example 5.11 A spherical shell of radius R, carrying a uniform surface charge \\sigma , is set spinning at angular velocity \\omega . Find the vector potential it produces at r (Fig 5.45). Solution While it might seem natural to set the polar axis along \\omega , in fact the integration is easier if we let r lie on the z axis, so that \\omega is tilted at an angle \\psi . We may as well orient the x axis so that \\omega lies in the xz plane, as shown in Fig 5.46. According to Eq. 5.66, \\vec{A}(r) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{K(r')}}{\\gr} \\dd a' where \\vec{K} = \\sigma \\vec{v} , \\gr = \\sqrt{R^2 + r^2 - 2 R r \\cos \\theta'} , and \\dd a' = R^2 \\sin \\theta' \\dd \\theta ' \\dd \\phi' \\ . Now the velocity of a point r' in a rotating rigid body is \\vec{\\omega} \\cross \\vec{r'} ; in this case, \\begin{align*} \\vec{v} & = \\vec{\\omega} \\cross \\vec{r'} \\\\ & = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ \\omega \\sin \\psi & 0 & \\omega \\cos \\psi \\\\ R \\sin \\theta' \\cos \\phi' & R \\sin \\theta' \\sin \\phi' & R \\cos \\theta' \\end{vmatrix} \\\\ & = R \\omega [ - (\\cos \\psi \\sin \\theta' \\sin \\phi') \\vu{x} \\\\ & \\qquad + (\\cos \\psi \\sin \\theta' \\cos \\phi' - \\sin \\psi \\cos \\theta') \\vu{y} \\\\ & \\qquad + (\\sin \\psi \\sin \\theta' \\sin \\phi') \\vu{z} ] \\end{align*} Notice that each of these terms, save one, involves either \\sin \\phi' or \\cos \\phi' . Since \\int _0 ^{2 \\pi} \\sin \\phi' \\dd \\phi' = \\int_0 ^{2\\pi} \\cos \\phi' \\dd \\phi' = 0 such terms do not contribute to the integral. There remains \\vec{A}(r) = - \\frac{\\mu_0 R^3 \\sigma \\omega \\sin \\psi}{2} \\left( \\int_0 ^{\\pi} \\frac{\\cos \\theta' \\sin \\theta'}{\\sqrt{R^2 + r^2 - 2 R r \\cos \\theta'}} \\dd \\theta' \\right) \\vu{y} Letting u \\equiv \\cos \\theta' , the integral becomes \\int_{-1} ^{+1} \\frac{u}{\\sqrt{R^2 + r^2 - 2 R r u}} \\dd u = \\left. - \\frac{(R^2 + r^2 + R r u)}{3 R^2 r^2} \\sqrt{R^2 + r^2 - 2 R r u} \\right|_{-1} ^{+1} \\\\ = - \\frac{1}{3 R^2 r^2} \\left[ (R^2 + r^2 + R r ) | R - r| - (R^2 + r^2 - Rr)(R + r) \\right] If the point r lies inside the sphere, then R > r and this expression reduces to (2r / 3R^2) ; if r lies outside the sphere, so that R < r , it reduces to (2R / 3r^2) . Noting that (\\vec{\\omega} \\cross \\vec{r}) = - \\omega r \\sin \\psi \\vu{y} , we have finally \\vec{A}(r) = \\begin{cases} \\frac{\\mu_0 R \\sigma}{3} (\\vec{\\omega} \\cross \\vec{r}), \\qquad & \\text{ for points inside the sphere} \\\\ \\frac{\\mu_0 R^4 \\sigma}{3 r^3} (\\vec{\\omega} \\cross \\vec{r}, \\qquad & \\text{ for points outside the sphere} \\end{cases} \\tagl{5.68} Having evaluated the integral, I revert to the \"natural\" coordinates of Fig. 5.45, in which \\vec{\\omega} coincides with the z axis and the point r is at (r, \\theta, \\phi) : \\vec{A}(r, \\theta, \\phi) = \\begin{cases} \\frac{\\mu_0 R \\sigma \\omega }{3} r \\sin \\theta \\vu{\\phi}, \\qquad & (r \\leq R) \\\\ \\frac{\\mu_0 R^4 \\omega \\sigma}{3 r^3} \\frac{\\sin \\theta}{r^2} \\vu{\\phi} , \\qquad & (r \\geq R) \\end{cases} \\tagl{5.69} Curiously, the field inside this spherical shell is uniform: \\vec{B} = \\curl \\vec{A} = \\frac{2 \\mu_0 R \\omega \\sigma}{3} (\\cos \\theta \\vu{r} - \\sin \\theta \\vu{\\theta}) = \\frac{2}{3} \\mu_0 R \\omega \\vu{z} = \\frac{2}{3} \\mu_0 \\sigma R \\vec{\\omega} \\tagl{5.70} Example 5.12 Find the vector potential of an infinite solenoid with n turns per unit length, radius R, and current I Solution This time we cannot use Eq 5.66, since the current itself extends to infinity. But there is a cute method that does the job. Notice that \\oint \\vec{A} \\cdot \\dd \\vec{l} = \\int (\\curl \\vec{A}) \\cdot \\dd \\vec{a} = \\int \\vec{B} \\cdot \\dd \\vec{a} = \\Phi \\tagl{5.71} where \\Phi is the flux of B through the loop in question. This is reminiscent of Ampere's law in integral form (Eq. 5.57) \\oint \\vec{B} \\cdot \\dd \\vec{a} = \\mu_0 I_{enc} In fact, it's the same equation, with \\vec{B} \\rightarrow \\vec{A} and \\mu_0 I_{enc} \\rightarrow \\Phi . If symmetry permits, we can determine A from \\Phi in the same way we got B from I_{enc} , in section 5.3.3. The present problem (with a uniform longitudinal magnetic field \\mu_0 n I inside the solenoid and no field outside) is analogous to the Ampere's law problem of a fat wire carrying a uniformly distributed current. The vector potential is \"circumferential\" (it mimics the magnetic field in the analog); using a circular \"Amperian loop\" at radius s inside the solenoid, we have \\oint \\vec{A} \\dd \\vec{l} = A (2 \\pi s) = \\int \\vec{B} \\cdot \\dd \\vec{a} = \\mu_0 n I (\\pi s^2) so \\vec{A} = \\frac{\\mu_0 n I}{2} s \\vu{\\phi}, \\quad \\text{ for } s \\leq R \\tagl{5.72} For an Amperian loop outside the solenoid, the flux is \\int \\vec{B} \\cdot \\dd \\vec{a} = \\mu_0 n I (\\pi R^2) since the field only extends out to R. Thus \\vec{A} = \\frac{\\mu_0 n I}{2} \\frac{R^2}{s} \\vu{\\phi} \\quad \\text{ for } s \\geq R \\tagl{5.73} To check our work, we can make sure that \\curl \\vec{A} = \\vec{B} and \\div \\vec{A} = 0 . Inside the solenoid, \\begin{align*} \\div \\vec{A} & = \\frac{1}{s} \\pdv{}{s} (s A_s) + \\frac{1}{s} \\pdv{A_\\phi}{\\phi} + \\pdv{A_z}{z} \\\\ & = \\frac{1}{s} \\pdv{}{\\phi} \\frac{\\mu_0 n I}{2} s = 0 \\\\ \\curl \\vec{A} & = \\left( \\frac{1}{s} \\pdv{A_z}{\\phi} - \\pdv{A_{\\phi}}{z} \\right) \\vu{s} \\\\ & + \\left( \\pdv{A_s}{z} - \\pdv{A_z}{s} \\right) \\vu{\\phi} \\\\ & + \\frac{1}{s} \\left[ \\pdv{}{s} (s A_{\\phi}) - \\pdv{A_s}{\\phi} \\right] \\vu{z} \\\\ & = \\frac{\\mu_0 n I}{2} \\frac{1}{s} \\left( \\pdv{}{s} s^2 \\right) \\\\ & = \\mu_0 n I \\end{align*} Outside the solenoid, \\begin{align*} \\div \\vec{A} & = \\frac{1}{s} \\pdv{}{s} (s A_s) + \\frac{1}{s} \\pdv{A_\\phi}{\\phi} + \\pdv{A_z}{z} \\\\ & = \\frac{1}{s} \\pdv{}{\\phi} \\frac{\\mu_0 n I}{2} \\frac{R^2}{s} = 0 \\\\ \\curl \\vec{A} & = \\left( \\frac{1}{s} \\pdv{A_z}{\\phi} - \\pdv{A_{\\phi}}{z} \\right) \\vu{s} \\\\ & + \\left( \\pdv{A_s}{z} - \\pdv{A_z}{s} \\right) \\vu{\\phi} \\\\ & + \\frac{1}{s} \\left[ \\pdv{}{s} (s A_{\\phi}) - \\pdv{A_s}{\\phi} \\right] \\vu{z} \\\\ & = \\frac{\\mu_0 n I}{2} \\frac{1}{s} \\left( \\pdv{}{s} R^2 \\right) \\\\ & = 0 \\end{align*} which is just the answer we got in section 5.3 by Biot-Savart. Typically, the direction of A mimics the direction of the current. For instance, both were azimuthal in Exs. 5.11 and 5.12. Indeed, if all the current flows in one direction, then Eq. 5.65 suggests that A must point that way too. Thus the potential of a finite segment of straight wire (Prob. 5.23) is in the direction of the current. Of course, if the current extends to infinity you can't use Eq. 5.65 in the first place (see Probs. 5.26 and 5.27). Moreover, you can always add an arbitrary constant vector to A - this is analogous to changing the reference point for V , and it won't affect the divergence or curl of A , which is all that matters (in Eq. 5.65 we have chosen the constant so that A goes to zero at infinity). In principle you could even use a vector potential that is not divergenceless, in which case all bets are off. Despite these caveats, the essential point remains: Ordinarily the direction of A will match the direction of the current. 5.4.2: Boundary Conditions In Chapter 2, we had a triangular diagram to summarize the relations among the three fundamental quantities in electrostatics: the charge density \\rho , the electric field E , and the potential V . A similar figure can be constructed for magnetostatics (Fig 5.48), relating the current density J , the field B , and the potential A . There is one \"missing link\" in the diagram: the equation for A in terms of B . It's unlikely you would ever need such a formula, but in case you are interested, see Probs. 5.52 and 5.53 Just as the electric field suffers a discontinuity at a surface charge, so the magnetic field is discontinuous at a surface current. Only this time it is the tangential component that changes. For if we apply Eq. 5.50 in integral form \\oint \\vec{B} \\cdot \\dd \\vec{a} = 0 to a wafer-thin pillbox straddling the surface (Fig 5.49), we get B_{above} ^\\perp = B_{below} ^\\perp \\tagl{5.74} As for the tangential components, an Amperian loop running perpendicular to the current (Fig 5.50) yields \\oint \\vec{B} \\cdot \\dd \\vec{l} = \\left( B_{above}^\\parallel - B_{below} ^{\\parallel} \\right) l = \\mu_0 I_{enc} = \\mu_0 K l or B_{above}^\\parallel - B_{below} ^{\\parallel} = \\mu_0 K \\tagl{5.75} Thus the component of B that is parallel to the surface but perpendicular to the current is discontinuous in the amount \\mu_0 L . A similar Amperian loop running parallel to the current reveals that the parallel component is continuous. These results can be summarized in a single formula \\vec{B}_{above} - \\vec{B}_{below} = \\mu_0 ( \\vec{K} \\cross \\vu{n} ) \\tagl{5.76} where \\vu{n} is perpendicular to the surface, pointing \"upward.\" Like the scalar potential in electrostatics, the vector potential is continuous across any boundary: \\vec{A}_{above} = \\vec{A}_{below} \\tagl{5.77} for \\div \\vec{A} = 0 guarantees that the normal component is continuous, and \\curl \\vec{A} = \\vec{B} in the form \\oint \\vec{A} \\cdot \\dd \\vec{l} = \\int \\vec{B} \\cdot \\dd \\vec{a} = \\Phi means that the tangential components are continuous (the flux through an Amperian loop of vanishing thickness is zero). But the derivative of A inherits the discontinuity of B : \\pdv{A_{above}}{n} - \\pdv{A_{below}}{n} = - \\mu_0 \\vec{K} \\tagl{5.78} 5.4.3: Multipole Expansion of the Vector Potential If you want an approximate formula for the vector potential of a localized current distribution, valid at distant points, a multipole expansion is in order. Remember: the idea of a multipole expansion is to write the potential in the form of a power series in 1/r , where r is the distance to the point in question (Fig 5.51); if r is sufficiently large, the series will be dominated by the lowest nonvanishing contribution, and the higher terms can be ignored. As we found in Section 3.4.1, \\frac{1}{\\gr} = \\frac{1}{r} \\sum_{n=0}^\\infty \\left( \\frac{r'}{r} \\right)^n P_n (\\cos \\alpha) \\tagl{5.79} where \\alpha is the angle between r and r' . Accordingly, the vector potential of a current loop can be written \\vec{A}(r) = \\frac{\\mu_0 I}{4 \\pi} \\oint \\frac{1}{\\gr} \\dd \\vec{l'} = \\frac{\\mu_0 I}{4 \\pi} \\sum_{n=0} ^\\infty \\frac{1}{r^{n+1}} \\oint (r')^n P_n (\\cos \\alpha) \\dd \\vec{l'} \\tagl{5.80} or, more explicitly, \\begin{align*} \\vec{A}(r) & = \\frac{\\mu_0 I}{4 \\pi} \\left[ \\frac{1}{r} \\oint \\dd \\vec{l'} + \\frac{1}{r^2} \\oint r' \\cos \\alpha \\dd \\vec{l'} \\right. \\\\ & \\left. \\quad + \\frac{1}{r^3} \\oint (r')^2 \\left( \\frac{3}{2} \\cos ^2 \\alpha - \\frac{1}{2} \\right) \\dd \\vec{l'} + \\ldots \\right] \\end{align*} \\tagl{5.81} As with the multipole expansion of V , we call the first term (which goes like 1/r ) the monopole term, the second (which goes like 1/r^2 ) dipole , the third quadrupole , and so on. Now, the magnetic monopole term is always zero, for the integral is just the total vector displacement around a closed loop \\oint \\dd \\vec{l'} = 0 \\tagl{5.82} This reflects the fact that there are no magnetic monopoles in nature (an assumption contained in Maxwell's equation \\div \\vec{B} = 0 , on which the entire theory of vector potential is predicated). In the absence of any monopole contribution, the dominant term is the dipole (except in the rare case where it, too, vanishes): \\vec{A}_{dip} = \\frac{\\mu_0 I}{4 \\pi r^2} \\oint r' \\cos \\alpha \\dd \\vec{l'} = \\frac{\\mu_0 I}{4 \\pi r^2} \\oint (\\vu{r} \\cdot \\vec{r'} ) \\dd \\vec{l'} \\tagl{5.83} This integral can be rewritten in a more illuminating way if we invoke Eq. 1.108 with \\vec{c} = \\vu{r} : \\oint (\\vu{r} \\cdot \\vec{r'}) \\dd \\vec{l'} = - \\vu{r} \\cross \\int \\dd \\vec{a'} \\tagl{5.84} Then \\vec{A}_{dip} (r) = \\frac{\\mu_0}{4 \\pi} \\frac{\\vec{m} \\cross \\vu{r}}{r^2} \\tagl{5.85} where we define the magnetic dipole moment m : \\vec{m} \\equiv I \\int \\dd \\vec{a} = I \\vec{a} \\tagl{5.86} Here a is the \"vector area\" of the loop (Problem 1.62); if the loop is flat, a is the ordinary area enclosed, with the direction assigned by the usual right-hand rule (fingers in the direction of the current). Example 5.13 Find the magnetic dipole moment of the 'bookend-shaped' loop shown in Fig 5.52. All sides have length w , and it carries a current I . Solution This wire could be considered the superposition of two plane square loops (Fig 5.53). The \"extra\" sides (AB) cancel when the two are put together, since the currents flow in opposite directions. The net magnetic dipole moment is \\vec{m} = I w^2 \\vu{y} + I w^2 \\vu{z} Its magnitude is \\sqrt{2} I w^2 and it points along the 45^{\\circ} line z = y It is clear from Eq. 5.86 that the magnetic dipole moment is independent of the choice of origin. You may remember that the electric dipole moment is independent of the origin only when the total charge vanishes (Sect. 3.4.3). Since the magnetic monopole moment is always zero, it is not really surprising that the magnetic dipole moment is always independent of origin. Although the dipole term dominates the multipole expansion (unless \\vec{m} = 0 ) and thus offers a good approximation to the true potential, it is not ordinarily the exact potential; there will be quadrupole, octopole, and higher contributions. You might ask, is it possible to devise a current distribution whose potential is \"pure\" dipole - for which Eq. 5.85 is exact? Well, yes and no: like the electrical analog, it can be done, but the model is a bit contrived. To begin with, you must take an infinitesimally small loop at the origin, but then, in order to keep the dipole moment finite, you have to crank the current up to infinity, with the product m = I a held fixed. In practice, the dipole potential is a suitable approximation whenever the distance r greatly exceeds the size of the loop. The magnetic field of a (perfect) dipole is easiest to calculate if we put \\vec{m} at the origin and let it point in the z-direction (Fig 5.54). According to Eq. 5.85, the potential at point (r, \\theta, \\phi) is \\vec{A}_{dip} = \\frac{\\mu_0}{4 \\pi} \\frac{m \\sin \\theta}{r^2} \\vu{\\phi} \\tagl{5.87} and hence \\vec{B}_{dip} = \\curl \\vec{A} = \\frac{\\mu_0 m}{4 \\pi r^3} (2 \\cos \\theta \\vu{r} + \\sin \\theta \\vu{\\theta}) \\tagl{5.88} Unsurprisingly, this is identical in structure to the field of an electric dipole (Eq. 3.103)! (Up close, however, the field of a physical magnetic dipole - a small current loop - looks quite different from the field of a physical electric dipole - plus and minus charges a short distance apart. Compare Fig 5.55 with Fig 3.37.)","title":"5.4 - Magnetic Vector Potential"},{"location":"ch5-4/#54-magnetic-vector-potential","text":"","title":"5.4: Magnetic Vector Potential"},{"location":"ch5-4/#541-the-vector-potential","text":"Just as \\curl \\vec{E} = 0 permitted us to introduce a scalar potential (V) in electrostatics, \\vec{E} = - \\grad \\vec{V} so \\div \\vec{B} = 0 invites the introduction of a vector potential A in magnetostatics \\vec{B} = \\curl \\vec{A} \\tagl{5.61} We were allowed to define these potentials based on our extended proof of the Helmholtz theorem (back in Section 1.6). The potential formulation automatically takes care of \\div \\vec{B} = 0 since the divergence of a curl is always zero; there remains Ampere's law: \\curl \\vec{B} = \\curl (\\curl \\vec{A}) = \\grad (\\div \\vec{A}) - \\grad ^2 \\vec{A} = \\mu_0 \\vec{J} \\tagl{5.62} Now, the electric potential had a built-in ambiguity: you can add to V any function whose gradient is zero (which is to say, a constant), without altering the physical quantity E . Likewise, you can add to A any function whose curl vanishes (which is to say, the gradient of any scalar), with no effect on B . We can exploit this freedom to eliminate the divergence of A : \\div \\vec{A} = 0 \\tagl{5.63} To prove that this is always possible, suppose that our original potential \\vec{A_0} is not divergenceless. If we add to it the gradient of \\lambda (\\vec{A} = \\vec{A}_0 + \\grad \\lambda) , the new divergence is \\div \\vec{A} = \\div \\vec{A_0} + \\grad ^2 \\lambda We can accommodate Eq. 5.63, then, if a function \\lambda can be found that satisfies \\grad ^2 \\lambda = - \\div \\vec{A_0} But this is mathematically identical to Poisson's equation \\grad ^2 \\lambda = - \\frac{\\rho}{\\epsilon_0} with \\div \\vec{A}_0 in place of \\rho / \\epsilon_0 as the \"source.\" And we know how to solve Poisson's equation - that's what electrostatics is all about. In particular, if \\rho goes to infinity, the solution is Eq. 2.29: V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho}{\\gr} \\dd \\tau' and by the same token, if \\div \\vec{A}_0 goes to zero at infinity, then \\lambda = \\frac{1}{4 \\pi} \\int \\frac{\\div \\vec{A}_0}{\\gr } \\dd \\tau' If \\div \\vec{A}_0 does not go to zero at infinity, then we'll have to use other means to discover the appropriate \\lambda , just as we get the electric potential by other means when the charge distribution extends to infinity. But the essential point remains: It is always possible to make the vector potential divergenceless. To put it the other way round,: the definition \\vec{B} = \\curl \\vec{A} specifies the curl of A , but it doesn't say anything about the divergence - we are at liberty to pick that as we see fit, and zero is ordinarily the simplest choice. With this condition on A , Ampere's law becomes \\grad ^2 \\vec{A} = - \\mu_0 \\vec{J} \\tagl{5.64} This again is nothing but Poisson's equation, or rather it is three of them, one for each Cartesian coordinate. In Cartesian coordinates, \\grad ^2 \\vec{A} = (\\grad ^2 A_x) \\vu{x} + (\\grad ^2 A_y) \\vu{y} + (\\grad ^2 A_z) \\vu{z} , so 5.64 reduces to \\grad ^2 A_x = - \\mu_0 J_x , \\grad ^2 A_y = - \\mu_0 J_y , and \\grad ^2 A_z = - \\mu_0 J_z . In curvilinear coordinates, the unit vectors themselves are functions of position, and must be differentiated, so it is not the case, for example, that \\grad ^2 A_r = - \\mu_0 J_r . Remember that even if you plan to evaluate integrals such as 5.65 using curvilinear coordinates, you must first express \\vec{J} in terms of its Cartesian components. Assuming J goes to zero at infinity, we can read off the solution \\vec{A} (\\vec{r}) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{J}(r')}{\\gr} \\dd \\tau' \\tagl{5.65} For line and surface currents, \\vec{A} = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{I}}{\\gr } \\dd l' = \\frac{\\mu_0 \\vec{I}}{4 \\pi} \\int \\frac{1}{\\gr} \\dd \\vec{l}'; \\qquad \\vec{A} = \\frac{\\mu_0}{4\\pi} \\int \\frac{\\vec{K}}{\\gr} \\dd a' \\tagl{5.66} (If the current does not go to zero at infinity, we have to find other ways to get A ; some of these are explored in Exercise 5.12 and in the problems at the end of the section.) It must be said that A is not as useful as V. For one thing, it's still a vector, and although Eqs. 5.65 and 5.66 are somewhat easier to work with than the Biot-Savart law, you still have to fuss with components. It would be nice if we could get away with a scalar potential \\vec{B} = - \\grad U but this is incompatible with Ampere's law, since the curl of a gradient is always zero. (A magnetostatic scalar potential can be used, if you stick scrupulously to simply-connected, current-free regions, but as a theoretical tool, it is of limited interest. See problem 5.29.) Moreover, since magnetic forces do no work, A does not admit a simple physical interpretation in terms of potential energy per unit charge. (In some contexts it can be interpreted as the momentum per unit charge.) Nevertheless, the vector potential has substantial theoretical importance, as we shall see in chapter 10.","title":"5.4.1: The Vector Potential"},{"location":"ch5-4/#example-511","text":"A spherical shell of radius R, carrying a uniform surface charge \\sigma , is set spinning at angular velocity \\omega . Find the vector potential it produces at r (Fig 5.45). Solution While it might seem natural to set the polar axis along \\omega , in fact the integration is easier if we let r lie on the z axis, so that \\omega is tilted at an angle \\psi . We may as well orient the x axis so that \\omega lies in the xz plane, as shown in Fig 5.46. According to Eq. 5.66, \\vec{A}(r) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{K(r')}}{\\gr} \\dd a' where \\vec{K} = \\sigma \\vec{v} , \\gr = \\sqrt{R^2 + r^2 - 2 R r \\cos \\theta'} , and \\dd a' = R^2 \\sin \\theta' \\dd \\theta ' \\dd \\phi' \\ . Now the velocity of a point r' in a rotating rigid body is \\vec{\\omega} \\cross \\vec{r'} ; in this case, \\begin{align*} \\vec{v} & = \\vec{\\omega} \\cross \\vec{r'} \\\\ & = \\begin{vmatrix} \\vu{x} & \\vu{y} & \\vu{z} \\\\ \\omega \\sin \\psi & 0 & \\omega \\cos \\psi \\\\ R \\sin \\theta' \\cos \\phi' & R \\sin \\theta' \\sin \\phi' & R \\cos \\theta' \\end{vmatrix} \\\\ & = R \\omega [ - (\\cos \\psi \\sin \\theta' \\sin \\phi') \\vu{x} \\\\ & \\qquad + (\\cos \\psi \\sin \\theta' \\cos \\phi' - \\sin \\psi \\cos \\theta') \\vu{y} \\\\ & \\qquad + (\\sin \\psi \\sin \\theta' \\sin \\phi') \\vu{z} ] \\end{align*} Notice that each of these terms, save one, involves either \\sin \\phi' or \\cos \\phi' . Since \\int _0 ^{2 \\pi} \\sin \\phi' \\dd \\phi' = \\int_0 ^{2\\pi} \\cos \\phi' \\dd \\phi' = 0 such terms do not contribute to the integral. There remains \\vec{A}(r) = - \\frac{\\mu_0 R^3 \\sigma \\omega \\sin \\psi}{2} \\left( \\int_0 ^{\\pi} \\frac{\\cos \\theta' \\sin \\theta'}{\\sqrt{R^2 + r^2 - 2 R r \\cos \\theta'}} \\dd \\theta' \\right) \\vu{y} Letting u \\equiv \\cos \\theta' , the integral becomes \\int_{-1} ^{+1} \\frac{u}{\\sqrt{R^2 + r^2 - 2 R r u}} \\dd u = \\left. - \\frac{(R^2 + r^2 + R r u)}{3 R^2 r^2} \\sqrt{R^2 + r^2 - 2 R r u} \\right|_{-1} ^{+1} \\\\ = - \\frac{1}{3 R^2 r^2} \\left[ (R^2 + r^2 + R r ) | R - r| - (R^2 + r^2 - Rr)(R + r) \\right] If the point r lies inside the sphere, then R > r and this expression reduces to (2r / 3R^2) ; if r lies outside the sphere, so that R < r , it reduces to (2R / 3r^2) . Noting that (\\vec{\\omega} \\cross \\vec{r}) = - \\omega r \\sin \\psi \\vu{y} , we have finally \\vec{A}(r) = \\begin{cases} \\frac{\\mu_0 R \\sigma}{3} (\\vec{\\omega} \\cross \\vec{r}), \\qquad & \\text{ for points inside the sphere} \\\\ \\frac{\\mu_0 R^4 \\sigma}{3 r^3} (\\vec{\\omega} \\cross \\vec{r}, \\qquad & \\text{ for points outside the sphere} \\end{cases} \\tagl{5.68} Having evaluated the integral, I revert to the \"natural\" coordinates of Fig. 5.45, in which \\vec{\\omega} coincides with the z axis and the point r is at (r, \\theta, \\phi) : \\vec{A}(r, \\theta, \\phi) = \\begin{cases} \\frac{\\mu_0 R \\sigma \\omega }{3} r \\sin \\theta \\vu{\\phi}, \\qquad & (r \\leq R) \\\\ \\frac{\\mu_0 R^4 \\omega \\sigma}{3 r^3} \\frac{\\sin \\theta}{r^2} \\vu{\\phi} , \\qquad & (r \\geq R) \\end{cases} \\tagl{5.69} Curiously, the field inside this spherical shell is uniform: \\vec{B} = \\curl \\vec{A} = \\frac{2 \\mu_0 R \\omega \\sigma}{3} (\\cos \\theta \\vu{r} - \\sin \\theta \\vu{\\theta}) = \\frac{2}{3} \\mu_0 R \\omega \\vu{z} = \\frac{2}{3} \\mu_0 \\sigma R \\vec{\\omega} \\tagl{5.70}","title":"Example 5.11"},{"location":"ch5-4/#example-512","text":"Find the vector potential of an infinite solenoid with n turns per unit length, radius R, and current I Solution This time we cannot use Eq 5.66, since the current itself extends to infinity. But there is a cute method that does the job. Notice that \\oint \\vec{A} \\cdot \\dd \\vec{l} = \\int (\\curl \\vec{A}) \\cdot \\dd \\vec{a} = \\int \\vec{B} \\cdot \\dd \\vec{a} = \\Phi \\tagl{5.71} where \\Phi is the flux of B through the loop in question. This is reminiscent of Ampere's law in integral form (Eq. 5.57) \\oint \\vec{B} \\cdot \\dd \\vec{a} = \\mu_0 I_{enc} In fact, it's the same equation, with \\vec{B} \\rightarrow \\vec{A} and \\mu_0 I_{enc} \\rightarrow \\Phi . If symmetry permits, we can determine A from \\Phi in the same way we got B from I_{enc} , in section 5.3.3. The present problem (with a uniform longitudinal magnetic field \\mu_0 n I inside the solenoid and no field outside) is analogous to the Ampere's law problem of a fat wire carrying a uniformly distributed current. The vector potential is \"circumferential\" (it mimics the magnetic field in the analog); using a circular \"Amperian loop\" at radius s inside the solenoid, we have \\oint \\vec{A} \\dd \\vec{l} = A (2 \\pi s) = \\int \\vec{B} \\cdot \\dd \\vec{a} = \\mu_0 n I (\\pi s^2) so \\vec{A} = \\frac{\\mu_0 n I}{2} s \\vu{\\phi}, \\quad \\text{ for } s \\leq R \\tagl{5.72} For an Amperian loop outside the solenoid, the flux is \\int \\vec{B} \\cdot \\dd \\vec{a} = \\mu_0 n I (\\pi R^2) since the field only extends out to R. Thus \\vec{A} = \\frac{\\mu_0 n I}{2} \\frac{R^2}{s} \\vu{\\phi} \\quad \\text{ for } s \\geq R \\tagl{5.73} To check our work, we can make sure that \\curl \\vec{A} = \\vec{B} and \\div \\vec{A} = 0 . Inside the solenoid, \\begin{align*} \\div \\vec{A} & = \\frac{1}{s} \\pdv{}{s} (s A_s) + \\frac{1}{s} \\pdv{A_\\phi}{\\phi} + \\pdv{A_z}{z} \\\\ & = \\frac{1}{s} \\pdv{}{\\phi} \\frac{\\mu_0 n I}{2} s = 0 \\\\ \\curl \\vec{A} & = \\left( \\frac{1}{s} \\pdv{A_z}{\\phi} - \\pdv{A_{\\phi}}{z} \\right) \\vu{s} \\\\ & + \\left( \\pdv{A_s}{z} - \\pdv{A_z}{s} \\right) \\vu{\\phi} \\\\ & + \\frac{1}{s} \\left[ \\pdv{}{s} (s A_{\\phi}) - \\pdv{A_s}{\\phi} \\right] \\vu{z} \\\\ & = \\frac{\\mu_0 n I}{2} \\frac{1}{s} \\left( \\pdv{}{s} s^2 \\right) \\\\ & = \\mu_0 n I \\end{align*} Outside the solenoid, \\begin{align*} \\div \\vec{A} & = \\frac{1}{s} \\pdv{}{s} (s A_s) + \\frac{1}{s} \\pdv{A_\\phi}{\\phi} + \\pdv{A_z}{z} \\\\ & = \\frac{1}{s} \\pdv{}{\\phi} \\frac{\\mu_0 n I}{2} \\frac{R^2}{s} = 0 \\\\ \\curl \\vec{A} & = \\left( \\frac{1}{s} \\pdv{A_z}{\\phi} - \\pdv{A_{\\phi}}{z} \\right) \\vu{s} \\\\ & + \\left( \\pdv{A_s}{z} - \\pdv{A_z}{s} \\right) \\vu{\\phi} \\\\ & + \\frac{1}{s} \\left[ \\pdv{}{s} (s A_{\\phi}) - \\pdv{A_s}{\\phi} \\right] \\vu{z} \\\\ & = \\frac{\\mu_0 n I}{2} \\frac{1}{s} \\left( \\pdv{}{s} R^2 \\right) \\\\ & = 0 \\end{align*} which is just the answer we got in section 5.3 by Biot-Savart. Typically, the direction of A mimics the direction of the current. For instance, both were azimuthal in Exs. 5.11 and 5.12. Indeed, if all the current flows in one direction, then Eq. 5.65 suggests that A must point that way too. Thus the potential of a finite segment of straight wire (Prob. 5.23) is in the direction of the current. Of course, if the current extends to infinity you can't use Eq. 5.65 in the first place (see Probs. 5.26 and 5.27). Moreover, you can always add an arbitrary constant vector to A - this is analogous to changing the reference point for V , and it won't affect the divergence or curl of A , which is all that matters (in Eq. 5.65 we have chosen the constant so that A goes to zero at infinity). In principle you could even use a vector potential that is not divergenceless, in which case all bets are off. Despite these caveats, the essential point remains: Ordinarily the direction of A will match the direction of the current.","title":"Example 5.12"},{"location":"ch5-4/#542-boundary-conditions","text":"In Chapter 2, we had a triangular diagram to summarize the relations among the three fundamental quantities in electrostatics: the charge density \\rho , the electric field E , and the potential V . A similar figure can be constructed for magnetostatics (Fig 5.48), relating the current density J , the field B , and the potential A . There is one \"missing link\" in the diagram: the equation for A in terms of B . It's unlikely you would ever need such a formula, but in case you are interested, see Probs. 5.52 and 5.53 Just as the electric field suffers a discontinuity at a surface charge, so the magnetic field is discontinuous at a surface current. Only this time it is the tangential component that changes. For if we apply Eq. 5.50 in integral form \\oint \\vec{B} \\cdot \\dd \\vec{a} = 0 to a wafer-thin pillbox straddling the surface (Fig 5.49), we get B_{above} ^\\perp = B_{below} ^\\perp \\tagl{5.74} As for the tangential components, an Amperian loop running perpendicular to the current (Fig 5.50) yields \\oint \\vec{B} \\cdot \\dd \\vec{l} = \\left( B_{above}^\\parallel - B_{below} ^{\\parallel} \\right) l = \\mu_0 I_{enc} = \\mu_0 K l or B_{above}^\\parallel - B_{below} ^{\\parallel} = \\mu_0 K \\tagl{5.75} Thus the component of B that is parallel to the surface but perpendicular to the current is discontinuous in the amount \\mu_0 L . A similar Amperian loop running parallel to the current reveals that the parallel component is continuous. These results can be summarized in a single formula \\vec{B}_{above} - \\vec{B}_{below} = \\mu_0 ( \\vec{K} \\cross \\vu{n} ) \\tagl{5.76} where \\vu{n} is perpendicular to the surface, pointing \"upward.\" Like the scalar potential in electrostatics, the vector potential is continuous across any boundary: \\vec{A}_{above} = \\vec{A}_{below} \\tagl{5.77} for \\div \\vec{A} = 0 guarantees that the normal component is continuous, and \\curl \\vec{A} = \\vec{B} in the form \\oint \\vec{A} \\cdot \\dd \\vec{l} = \\int \\vec{B} \\cdot \\dd \\vec{a} = \\Phi means that the tangential components are continuous (the flux through an Amperian loop of vanishing thickness is zero). But the derivative of A inherits the discontinuity of B : \\pdv{A_{above}}{n} - \\pdv{A_{below}}{n} = - \\mu_0 \\vec{K} \\tagl{5.78}","title":"5.4.2: Boundary Conditions"},{"location":"ch5-4/#543-multipole-expansion-of-the-vector-potential","text":"If you want an approximate formula for the vector potential of a localized current distribution, valid at distant points, a multipole expansion is in order. Remember: the idea of a multipole expansion is to write the potential in the form of a power series in 1/r , where r is the distance to the point in question (Fig 5.51); if r is sufficiently large, the series will be dominated by the lowest nonvanishing contribution, and the higher terms can be ignored. As we found in Section 3.4.1, \\frac{1}{\\gr} = \\frac{1}{r} \\sum_{n=0}^\\infty \\left( \\frac{r'}{r} \\right)^n P_n (\\cos \\alpha) \\tagl{5.79} where \\alpha is the angle between r and r' . Accordingly, the vector potential of a current loop can be written \\vec{A}(r) = \\frac{\\mu_0 I}{4 \\pi} \\oint \\frac{1}{\\gr} \\dd \\vec{l'} = \\frac{\\mu_0 I}{4 \\pi} \\sum_{n=0} ^\\infty \\frac{1}{r^{n+1}} \\oint (r')^n P_n (\\cos \\alpha) \\dd \\vec{l'} \\tagl{5.80} or, more explicitly, \\begin{align*} \\vec{A}(r) & = \\frac{\\mu_0 I}{4 \\pi} \\left[ \\frac{1}{r} \\oint \\dd \\vec{l'} + \\frac{1}{r^2} \\oint r' \\cos \\alpha \\dd \\vec{l'} \\right. \\\\ & \\left. \\quad + \\frac{1}{r^3} \\oint (r')^2 \\left( \\frac{3}{2} \\cos ^2 \\alpha - \\frac{1}{2} \\right) \\dd \\vec{l'} + \\ldots \\right] \\end{align*} \\tagl{5.81} As with the multipole expansion of V , we call the first term (which goes like 1/r ) the monopole term, the second (which goes like 1/r^2 ) dipole , the third quadrupole , and so on. Now, the magnetic monopole term is always zero, for the integral is just the total vector displacement around a closed loop \\oint \\dd \\vec{l'} = 0 \\tagl{5.82} This reflects the fact that there are no magnetic monopoles in nature (an assumption contained in Maxwell's equation \\div \\vec{B} = 0 , on which the entire theory of vector potential is predicated). In the absence of any monopole contribution, the dominant term is the dipole (except in the rare case where it, too, vanishes): \\vec{A}_{dip} = \\frac{\\mu_0 I}{4 \\pi r^2} \\oint r' \\cos \\alpha \\dd \\vec{l'} = \\frac{\\mu_0 I}{4 \\pi r^2} \\oint (\\vu{r} \\cdot \\vec{r'} ) \\dd \\vec{l'} \\tagl{5.83} This integral can be rewritten in a more illuminating way if we invoke Eq. 1.108 with \\vec{c} = \\vu{r} : \\oint (\\vu{r} \\cdot \\vec{r'}) \\dd \\vec{l'} = - \\vu{r} \\cross \\int \\dd \\vec{a'} \\tagl{5.84} Then \\vec{A}_{dip} (r) = \\frac{\\mu_0}{4 \\pi} \\frac{\\vec{m} \\cross \\vu{r}}{r^2} \\tagl{5.85} where we define the magnetic dipole moment m : \\vec{m} \\equiv I \\int \\dd \\vec{a} = I \\vec{a} \\tagl{5.86} Here a is the \"vector area\" of the loop (Problem 1.62); if the loop is flat, a is the ordinary area enclosed, with the direction assigned by the usual right-hand rule (fingers in the direction of the current).","title":"5.4.3: Multipole Expansion of the Vector Potential"},{"location":"ch5-4/#example-513","text":"Find the magnetic dipole moment of the 'bookend-shaped' loop shown in Fig 5.52. All sides have length w , and it carries a current I . Solution This wire could be considered the superposition of two plane square loops (Fig 5.53). The \"extra\" sides (AB) cancel when the two are put together, since the currents flow in opposite directions. The net magnetic dipole moment is \\vec{m} = I w^2 \\vu{y} + I w^2 \\vu{z} Its magnitude is \\sqrt{2} I w^2 and it points along the 45^{\\circ} line z = y It is clear from Eq. 5.86 that the magnetic dipole moment is independent of the choice of origin. You may remember that the electric dipole moment is independent of the origin only when the total charge vanishes (Sect. 3.4.3). Since the magnetic monopole moment is always zero, it is not really surprising that the magnetic dipole moment is always independent of origin. Although the dipole term dominates the multipole expansion (unless \\vec{m} = 0 ) and thus offers a good approximation to the true potential, it is not ordinarily the exact potential; there will be quadrupole, octopole, and higher contributions. You might ask, is it possible to devise a current distribution whose potential is \"pure\" dipole - for which Eq. 5.85 is exact? Well, yes and no: like the electrical analog, it can be done, but the model is a bit contrived. To begin with, you must take an infinitesimally small loop at the origin, but then, in order to keep the dipole moment finite, you have to crank the current up to infinity, with the product m = I a held fixed. In practice, the dipole potential is a suitable approximation whenever the distance r greatly exceeds the size of the loop. The magnetic field of a (perfect) dipole is easiest to calculate if we put \\vec{m} at the origin and let it point in the z-direction (Fig 5.54). According to Eq. 5.85, the potential at point (r, \\theta, \\phi) is \\vec{A}_{dip} = \\frac{\\mu_0}{4 \\pi} \\frac{m \\sin \\theta}{r^2} \\vu{\\phi} \\tagl{5.87} and hence \\vec{B}_{dip} = \\curl \\vec{A} = \\frac{\\mu_0 m}{4 \\pi r^3} (2 \\cos \\theta \\vu{r} + \\sin \\theta \\vu{\\theta}) \\tagl{5.88} Unsurprisingly, this is identical in structure to the field of an electric dipole (Eq. 3.103)! (Up close, however, the field of a physical magnetic dipole - a small current loop - looks quite different from the field of a physical electric dipole - plus and minus charges a short distance apart. Compare Fig 5.55 with Fig 3.37.)","title":"Example 5.13"},{"location":"ch6-1/","text":"6.1: Magnetization 6.1.1: Diamagnets, Paramagnets, Ferromagnets If you ask the average person what \"magnetism\" is, you will probably be told about refrigerator decorations, compass needles, and the North Pole - none of which has any obvious connection with moving charges or current-carrying wires. Yet all magnetic phenomena are due to electric charges in motion, and in fact, if you could examine a piece of magnetic material on an atomic scale you would find tiny currents: electrons orbiting around nuclei and spinning about their axes. For macroscopic purposes, these current loops are so small that we may treat them as magnetic dipoles. Ordinarily, they cancel each other out because of the random orientation of the atoms. But when a magnetic field is applied, a net alignment of these magnetic dipoles occurs, and the medium becomes magnetically polarized, or magnetized. Unlike electric polarization, which is almost always in the same direction as E , some materials acquire a magnetization parallel to B (paramagnets) and some opposite to B (diamagnets). A few substances (called ferromagnets, in deference to the most common example, iron) retain their magnetization even after the external field has been removed - for these, the magnetization is not determined by the present field but by the whole magnetic \"history\" of the object. Permanent magnets made of iron are the most familiar examples of magnetism, but from a theoretical point of view they are the most complicated; I'll save ferromagnetism for the end of the chapter, and begin with qualitative models of paramagnetism and diamagnetism. 6.1.2: Torques and Forces on Magnetic Dipoles A magnetic dipole experiences a torque in a magnetic field, just as an electric dipole does in an electric field. Let's calculate the torque on a rectangular current loop in a uniform field B . (Since any current loop could be built up from infinitesimal rectangles, with all the \"internal\" sides canceling, as indicated in Fig. 6.1, there is no real loss of generality here; but if you prefer to start from scratch with an arbitrary shape, see Prob. 6.2.) Center the loop at the origin, and tilt it an angle \\theta from the z axis towards the y axis (Fig. 6.2). Let B point in the z direction. The forces on the two sloping sides cancel (they tend to stretch the loop, but they don't rotate it). The forces on the \"horizontal\" sides are likewise equal and opposite (so the net force on the loop is zero), but they do generate a torque: \\vec{N} = a F \\sin \\theta \\vu{x} The magnitude of the force on each of these segments is F = I b B and therefore \\vec{N} = I a b B \\sin \\theta \\vu{x} = m B \\sin \\theta \\vu{x} or \\vec{N} = \\vec{m} \\cross \\vec{B} \\tagl{6.1} where m = I a b is the magnetic dipole moment of the loop. Equation 6.1 gives the torque on any localized current distribution, in the presence of a uniform field; in a non-uniform field it is the exact torque (about the center) for a perfect dipole of infinitesimal size. Notice that Eq. 6.1 is identical in form to the electrical analog, Eq. 4.4: \\vec{N} = \\vec{p} \\cross \\vec{E} . In particular, the torque is again in such a direction as to line the dipole up parallel to the field. It is this torque that accounts for paramagnetism. Since every electron constitutes a magnetic dipole (picture it, if you wish, as a tiny spinning sphere of charge), you might expect paramagnetism to be a universal phenomenon. Actually, quantum mechanics (specifically, the Pauli exclusion principle) tends to lock the electrons within a given atom together in pairs with opposing spins, and this effectively neutralizes the torque on the combination. As a result, paramagnetism most often occurs in atoms or molecules with an odd number of electrons, where the \"extra\" unpaired member is subject to the magnetic torque. Even here, the alignment is far from complete, since random thermal collisions tend to destroy the order. In a uniform field, the net force on a current loop is zero \\vec{F} = I \\oint ( \\dd \\vec{l} \\cross \\vec{B}) = I \\left( \\oint \\dd \\vec{l} \\right) \\cross \\vec{B} = 0 the constant B comes outside the integral, and the net displacement \\oint \\dd \\vec{l} around a closed loop vanishes. In a nonuniform field, this is no longer the case. For example, suppose a circular wire ring of radius R, carrying a current I , is suspended above a short solenoid in the \"fringing\" region (Fig 6.3). Here B has a radial component, and there is a net downward force on the loop (Fig 6.4) F = 2 \\pi I R B \\cos \\theta \\tagl{6.2} For an infinitesimal loop, with dipole moment m , in a field B , the force is \\vec{F} = \\grad (\\vec{m} \\cdot \\vec{B}) \\tagl{6.3} Once again the magnetic formula is identical to its electrical \"twin,\" if we write the latter in the form \\vec{F} = \\grad ( \\vec{p} \\cdot \\vec{E}) . If you're starting to get a sense of deja vu, perhaps you will have more respect for those early physicists who thought magnetic dipoles consisted of positive and negative magnetic \"charges\" (north and south \"poles,\" they called them), separated by a small distance, just like electric dipoles (Fig. 6.5(a)). They wrote down a \"Coulomb's law\" for the attraction and repulsion of these poles, and developed the whole of magnetostatics in exact analogy to electrostatics. It's not a bad model, for many purposes - it gives the correct field of a dipole (at least, away from the origin), the right torque on a dipole (at least, on a stationary dipole), and the proper force on a dipole (at least, in the absence of external currents). But it's bad physics, because there's no such thing as a single magnetic north pole or south pole. If you break a bar magnet in half, you don't get a north pole in one hand and a south pole in the other; you get two complete magnets. Magnetism is not due to magnetic monopoles, but rather to moving electric charges; magnetic dipoles are tiny current loops (Fig. 6.5(c)), and it's an extraordinary thing, really, that the formulas involving m bear any resemblance to the corresponding formulas for p . Sometimes it is easier to think in terms of the \"Gilbert\" model of a magnetic dipole (separated monopoles), instead of the physically correct \"Ampere\" model (current loop). Indeed, this picture occasionally offers a quick and clever solution to an otherwise cumbersome problem (you just copy the corresponding result from electrostatics, changing \\vec{p} to \\vec{m} , 1 / \\epsilon_0 to \\mu_0 , and \\vec{E} to \\vec{B} ). But whenever the close-up features of the dipole come into play, the two models can yield strikingly different answers. My advice is to use the Gilbert model, if you like, to get an intuitive \"feel\" for a problem, but never rely on it for quantitative results. 6.1.3: Effect of a Magnetic Field on Atomic Orbits Electrons not only spin; they also revolve around the nucleus - for simplicity, let's assume the orbit is a circle of radius R (Fig. 6.9). Although technically this orbital motion does not constitute a steady current, in practice the period T = 2 \\pi R / v is so short that unless you blink awfully fast, it's going to look like a steady current: I = \\frac{-e}{T} = - \\frac{ev}{2 \\pi R} (The minus sign accounts for the negative charge of the electron.) Accordingly, the orbital dipole moment (I \\pi R^2) is \\vec{m} = - \\frac{1}{2} e v R \\vu{z} \\tagl{6.4} Like any other magnetic dipole, this one is subject to a torque \\vec{m} \\cross \\vec{B} when you turn on a magnetic field. But it's a lot harder to tilt the entire orbit than it is the spin, so the orbital contribution to paramagnetism is small. There is, however, a more significant effect on the orbital motion: The electron speeds up or slows down, depending on the orientation of B . For whereas the centripetal acceleration v^2 / R is ordinarily sustained by electrical forces alone, \\frac{1}{4 \\pi \\epsilon_0} \\frac{e^2}{R^2} = m_e \\frac{v^2}{R} \\tagl{6.5} in the presence of a magnetic field there is an additional force, -e (\\vec{v} \\cross \\vec{B}) . (To avoid confusion with the magnetic dipole moment m , we write the mass of the electron as m_e .) For the sake of argument, let's say that B is perpendicular to the plane of orbit, as shown in Fig 6.10; then \\frac{1}{4 \\pi \\epsilon_0} \\frac{e^2}{R^2} + e \\overline{v} B = m_e \\frac{\\overline{v}^2}{R} \\tagl{6.6} Under these conditions, the new speed \\overline{v} is greater than v e \\overline{v} B = \\frac{m_e}{R} (\\overline{v} ^2 - v^2) = \\frac{m_e}{R} (\\overline{v} + v)(overline{v} - v) or, assuming the change in \\Delta v = \\overline{v} - v is small, \\Delta v = \\frac{e R B}{2 m_e} \\tagl{6.7} When B is turned on, then, the electron speeds up. A change in orbital speed means a change in the dipole moment (Eq. 6.4): \\Delta \\vec{m} = - \\frac{1}{2} e (\\Delta v) R \\vu{z} = - \\frac{e^2 R^2}{4 m_e} \\vec{B} \\tagl{6.8} Notice that the change in m is opposite to the direction of B . (An electron circling the other way would have a dipole moment pointing upward, but such an orbit would be slowed down by the field, so the change is still opposite to B .) Ordinarily, the electron orbits are randomly oriented, and the orbital dipole moments cancel out. But in the presence of a magnetic field, each atom picks up a little \"extra\" dipole moment, and these increments are all anti-parallel to the field. This is the mechanism responsible for diamagnetism. It is a universal phenomenon, affecting all atoms. However, it is typically much weaker than paramagnetism, and is therefore observed mainly in atoms with even numbers of electrons, where paramagnetism is usually absent. In deriving Eq. 6.8, I assumed that the orbit remains circular, with its original radius R . I cannot offer a justification for this at the present stage. If the atom is stationary while the field is turned on, then my assumption can be proved - this is not magnetostatics, however, and the details will have to await Chapter 7 (see Prob. 7.52). If the atom is moved into the field, the situation is enormously more complicated. But never mind - I'm only trying to give you a qualitative account of diamagnetism. Assume, if you prefer, that the velocity remains the same while the radius changes - the formula (Eq. 6.8) is altered (by a factor of 2), but the qualitative conclusion is unaffected. The truth is that this classical model is fundamentally flawed (diamagnetism is really a quantum phenomenon), so there's not much point in refining the details. What is important is the empirical fact that in diamagnetic materials the induced dipole moments point opposite to the magnetic field. 6.1.4: Magnetization In the presence of a magnetic field, matter becomes magnetized ; that is, upon microscopic examination, it will be found to contain many tiny dipoles, with a net alignment along some direction. We have discussed two mechanisms that account for this magnetic polarization: (1) paramagnetism (the dipoles associated with the spins of unpaired electrons experience a torque tending to line them up parallel to the field) and (2) diamagnetism (the orbital speed of the electrons is altered in such a way as to change the orbital dipole moment in a direction opposite to the field). Whatever the cause, we describe the state of magnetic polarization by the vector quantity \\vec{M} \\equiv \\text{ magnetic dipole moment per unit volume } \\tagl{6.9} M is called the magnetization ; it plays a role analogous to the polarization P in electrostatics. In the following section, we will not worry about how the magnetization got there - it could be paramagnetism, diamagnetism, or even ferromagnetism - we shall take M as a given, and calculate the field this magnetization itself produces. Incidentally, it may have surprised you to learn that materials other than the famous ferromagnetic trio (iron, nickel, and cobalt) are affected by a magnetic field at all. You cannot, of course, pick up a piece of wood or aluminum with a magnet. The reason is that diamagnetism and paramagnetism are extremely weak: It takes a delicate experiment and a powerful magnet to detect them at all. If you were to suspend a piece of paramagnetic material above a solenoid, as in Fig. 6.3, the induced magnetization would be upward, and hence the force downward. By contrast, the magnetization of a diamagnetic object would be downward and the force upward. In general, when a sample is placed in a region of nonuniform field, the paramagnet is attracted into the field, whereas the diamagnet is repelled away. But the actual forces are pitifully weak - in a typical experimental arrangement the force on a comparable sample of iron would be 10^4 or 10^5 times as great. That's why it was reasonable for us to calculate the field inside a piece of copper wire, say, in Chapter 5, without worrying about the effects of magnetization.","title":"6.1 - Magnetization"},{"location":"ch6-1/#61-magnetization","text":"","title":"6.1: Magnetization"},{"location":"ch6-1/#611-diamagnets-paramagnets-ferromagnets","text":"If you ask the average person what \"magnetism\" is, you will probably be told about refrigerator decorations, compass needles, and the North Pole - none of which has any obvious connection with moving charges or current-carrying wires. Yet all magnetic phenomena are due to electric charges in motion, and in fact, if you could examine a piece of magnetic material on an atomic scale you would find tiny currents: electrons orbiting around nuclei and spinning about their axes. For macroscopic purposes, these current loops are so small that we may treat them as magnetic dipoles. Ordinarily, they cancel each other out because of the random orientation of the atoms. But when a magnetic field is applied, a net alignment of these magnetic dipoles occurs, and the medium becomes magnetically polarized, or magnetized. Unlike electric polarization, which is almost always in the same direction as E , some materials acquire a magnetization parallel to B (paramagnets) and some opposite to B (diamagnets). A few substances (called ferromagnets, in deference to the most common example, iron) retain their magnetization even after the external field has been removed - for these, the magnetization is not determined by the present field but by the whole magnetic \"history\" of the object. Permanent magnets made of iron are the most familiar examples of magnetism, but from a theoretical point of view they are the most complicated; I'll save ferromagnetism for the end of the chapter, and begin with qualitative models of paramagnetism and diamagnetism.","title":"6.1.1: Diamagnets, Paramagnets, Ferromagnets"},{"location":"ch6-1/#612-torques-and-forces-on-magnetic-dipoles","text":"A magnetic dipole experiences a torque in a magnetic field, just as an electric dipole does in an electric field. Let's calculate the torque on a rectangular current loop in a uniform field B . (Since any current loop could be built up from infinitesimal rectangles, with all the \"internal\" sides canceling, as indicated in Fig. 6.1, there is no real loss of generality here; but if you prefer to start from scratch with an arbitrary shape, see Prob. 6.2.) Center the loop at the origin, and tilt it an angle \\theta from the z axis towards the y axis (Fig. 6.2). Let B point in the z direction. The forces on the two sloping sides cancel (they tend to stretch the loop, but they don't rotate it). The forces on the \"horizontal\" sides are likewise equal and opposite (so the net force on the loop is zero), but they do generate a torque: \\vec{N} = a F \\sin \\theta \\vu{x} The magnitude of the force on each of these segments is F = I b B and therefore \\vec{N} = I a b B \\sin \\theta \\vu{x} = m B \\sin \\theta \\vu{x} or \\vec{N} = \\vec{m} \\cross \\vec{B} \\tagl{6.1} where m = I a b is the magnetic dipole moment of the loop. Equation 6.1 gives the torque on any localized current distribution, in the presence of a uniform field; in a non-uniform field it is the exact torque (about the center) for a perfect dipole of infinitesimal size. Notice that Eq. 6.1 is identical in form to the electrical analog, Eq. 4.4: \\vec{N} = \\vec{p} \\cross \\vec{E} . In particular, the torque is again in such a direction as to line the dipole up parallel to the field. It is this torque that accounts for paramagnetism. Since every electron constitutes a magnetic dipole (picture it, if you wish, as a tiny spinning sphere of charge), you might expect paramagnetism to be a universal phenomenon. Actually, quantum mechanics (specifically, the Pauli exclusion principle) tends to lock the electrons within a given atom together in pairs with opposing spins, and this effectively neutralizes the torque on the combination. As a result, paramagnetism most often occurs in atoms or molecules with an odd number of electrons, where the \"extra\" unpaired member is subject to the magnetic torque. Even here, the alignment is far from complete, since random thermal collisions tend to destroy the order. In a uniform field, the net force on a current loop is zero \\vec{F} = I \\oint ( \\dd \\vec{l} \\cross \\vec{B}) = I \\left( \\oint \\dd \\vec{l} \\right) \\cross \\vec{B} = 0 the constant B comes outside the integral, and the net displacement \\oint \\dd \\vec{l} around a closed loop vanishes. In a nonuniform field, this is no longer the case. For example, suppose a circular wire ring of radius R, carrying a current I , is suspended above a short solenoid in the \"fringing\" region (Fig 6.3). Here B has a radial component, and there is a net downward force on the loop (Fig 6.4) F = 2 \\pi I R B \\cos \\theta \\tagl{6.2} For an infinitesimal loop, with dipole moment m , in a field B , the force is \\vec{F} = \\grad (\\vec{m} \\cdot \\vec{B}) \\tagl{6.3} Once again the magnetic formula is identical to its electrical \"twin,\" if we write the latter in the form \\vec{F} = \\grad ( \\vec{p} \\cdot \\vec{E}) . If you're starting to get a sense of deja vu, perhaps you will have more respect for those early physicists who thought magnetic dipoles consisted of positive and negative magnetic \"charges\" (north and south \"poles,\" they called them), separated by a small distance, just like electric dipoles (Fig. 6.5(a)). They wrote down a \"Coulomb's law\" for the attraction and repulsion of these poles, and developed the whole of magnetostatics in exact analogy to electrostatics. It's not a bad model, for many purposes - it gives the correct field of a dipole (at least, away from the origin), the right torque on a dipole (at least, on a stationary dipole), and the proper force on a dipole (at least, in the absence of external currents). But it's bad physics, because there's no such thing as a single magnetic north pole or south pole. If you break a bar magnet in half, you don't get a north pole in one hand and a south pole in the other; you get two complete magnets. Magnetism is not due to magnetic monopoles, but rather to moving electric charges; magnetic dipoles are tiny current loops (Fig. 6.5(c)), and it's an extraordinary thing, really, that the formulas involving m bear any resemblance to the corresponding formulas for p . Sometimes it is easier to think in terms of the \"Gilbert\" model of a magnetic dipole (separated monopoles), instead of the physically correct \"Ampere\" model (current loop). Indeed, this picture occasionally offers a quick and clever solution to an otherwise cumbersome problem (you just copy the corresponding result from electrostatics, changing \\vec{p} to \\vec{m} , 1 / \\epsilon_0 to \\mu_0 , and \\vec{E} to \\vec{B} ). But whenever the close-up features of the dipole come into play, the two models can yield strikingly different answers. My advice is to use the Gilbert model, if you like, to get an intuitive \"feel\" for a problem, but never rely on it for quantitative results.","title":"6.1.2: Torques and Forces on Magnetic Dipoles"},{"location":"ch6-1/#613-effect-of-a-magnetic-field-on-atomic-orbits","text":"Electrons not only spin; they also revolve around the nucleus - for simplicity, let's assume the orbit is a circle of radius R (Fig. 6.9). Although technically this orbital motion does not constitute a steady current, in practice the period T = 2 \\pi R / v is so short that unless you blink awfully fast, it's going to look like a steady current: I = \\frac{-e}{T} = - \\frac{ev}{2 \\pi R} (The minus sign accounts for the negative charge of the electron.) Accordingly, the orbital dipole moment (I \\pi R^2) is \\vec{m} = - \\frac{1}{2} e v R \\vu{z} \\tagl{6.4} Like any other magnetic dipole, this one is subject to a torque \\vec{m} \\cross \\vec{B} when you turn on a magnetic field. But it's a lot harder to tilt the entire orbit than it is the spin, so the orbital contribution to paramagnetism is small. There is, however, a more significant effect on the orbital motion: The electron speeds up or slows down, depending on the orientation of B . For whereas the centripetal acceleration v^2 / R is ordinarily sustained by electrical forces alone, \\frac{1}{4 \\pi \\epsilon_0} \\frac{e^2}{R^2} = m_e \\frac{v^2}{R} \\tagl{6.5} in the presence of a magnetic field there is an additional force, -e (\\vec{v} \\cross \\vec{B}) . (To avoid confusion with the magnetic dipole moment m , we write the mass of the electron as m_e .) For the sake of argument, let's say that B is perpendicular to the plane of orbit, as shown in Fig 6.10; then \\frac{1}{4 \\pi \\epsilon_0} \\frac{e^2}{R^2} + e \\overline{v} B = m_e \\frac{\\overline{v}^2}{R} \\tagl{6.6} Under these conditions, the new speed \\overline{v} is greater than v e \\overline{v} B = \\frac{m_e}{R} (\\overline{v} ^2 - v^2) = \\frac{m_e}{R} (\\overline{v} + v)(overline{v} - v) or, assuming the change in \\Delta v = \\overline{v} - v is small, \\Delta v = \\frac{e R B}{2 m_e} \\tagl{6.7} When B is turned on, then, the electron speeds up. A change in orbital speed means a change in the dipole moment (Eq. 6.4): \\Delta \\vec{m} = - \\frac{1}{2} e (\\Delta v) R \\vu{z} = - \\frac{e^2 R^2}{4 m_e} \\vec{B} \\tagl{6.8} Notice that the change in m is opposite to the direction of B . (An electron circling the other way would have a dipole moment pointing upward, but such an orbit would be slowed down by the field, so the change is still opposite to B .) Ordinarily, the electron orbits are randomly oriented, and the orbital dipole moments cancel out. But in the presence of a magnetic field, each atom picks up a little \"extra\" dipole moment, and these increments are all anti-parallel to the field. This is the mechanism responsible for diamagnetism. It is a universal phenomenon, affecting all atoms. However, it is typically much weaker than paramagnetism, and is therefore observed mainly in atoms with even numbers of electrons, where paramagnetism is usually absent. In deriving Eq. 6.8, I assumed that the orbit remains circular, with its original radius R . I cannot offer a justification for this at the present stage. If the atom is stationary while the field is turned on, then my assumption can be proved - this is not magnetostatics, however, and the details will have to await Chapter 7 (see Prob. 7.52). If the atom is moved into the field, the situation is enormously more complicated. But never mind - I'm only trying to give you a qualitative account of diamagnetism. Assume, if you prefer, that the velocity remains the same while the radius changes - the formula (Eq. 6.8) is altered (by a factor of 2), but the qualitative conclusion is unaffected. The truth is that this classical model is fundamentally flawed (diamagnetism is really a quantum phenomenon), so there's not much point in refining the details. What is important is the empirical fact that in diamagnetic materials the induced dipole moments point opposite to the magnetic field.","title":"6.1.3: Effect of a Magnetic Field on Atomic Orbits"},{"location":"ch6-1/#614-magnetization","text":"In the presence of a magnetic field, matter becomes magnetized ; that is, upon microscopic examination, it will be found to contain many tiny dipoles, with a net alignment along some direction. We have discussed two mechanisms that account for this magnetic polarization: (1) paramagnetism (the dipoles associated with the spins of unpaired electrons experience a torque tending to line them up parallel to the field) and (2) diamagnetism (the orbital speed of the electrons is altered in such a way as to change the orbital dipole moment in a direction opposite to the field). Whatever the cause, we describe the state of magnetic polarization by the vector quantity \\vec{M} \\equiv \\text{ magnetic dipole moment per unit volume } \\tagl{6.9} M is called the magnetization ; it plays a role analogous to the polarization P in electrostatics. In the following section, we will not worry about how the magnetization got there - it could be paramagnetism, diamagnetism, or even ferromagnetism - we shall take M as a given, and calculate the field this magnetization itself produces. Incidentally, it may have surprised you to learn that materials other than the famous ferromagnetic trio (iron, nickel, and cobalt) are affected by a magnetic field at all. You cannot, of course, pick up a piece of wood or aluminum with a magnet. The reason is that diamagnetism and paramagnetism are extremely weak: It takes a delicate experiment and a powerful magnet to detect them at all. If you were to suspend a piece of paramagnetic material above a solenoid, as in Fig. 6.3, the induced magnetization would be upward, and hence the force downward. By contrast, the magnetization of a diamagnetic object would be downward and the force upward. In general, when a sample is placed in a region of nonuniform field, the paramagnet is attracted into the field, whereas the diamagnet is repelled away. But the actual forces are pitifully weak - in a typical experimental arrangement the force on a comparable sample of iron would be 10^4 or 10^5 times as great. That's why it was reasonable for us to calculate the field inside a piece of copper wire, say, in Chapter 5, without worrying about the effects of magnetization.","title":"6.1.4: Magnetization"},{"location":"ch6-2/","text":"6.2: The Field of a Magnetized Object 6.2.1: Bound Currents Suppose we have a piece of magnetized material; the magnetic dipole moment per unit volume, M , is given. What field does this object produce? Well, the vector potential of a single dipole m is given by Eq. 5.85 \\vec{A}_{dip} (r) = \\frac{\\mu_0}{4 \\pi} \\frac{\\vec{m} \\cross \\vu{r}}{r^2} In the magnetized object, each volume element \\dd \\tau' carries a dipole moment M \\dd \\tau' , so the total vector potential is (Fig 6.11) \\vec{A}(r) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{M}(r') \\cross \\vu{\\gr}}{\\gr ^2} \\dd \\tau' \\tagl{6.11} That does it, in principle. But, as in the electrical case, the integral can be cast in a more illuminating form by exploiting the identity \\grad ' \\frac{1}{\\gr} = \\frac{\\vu{\\gr}}{\\gr^2} With this, \\vec{A}(r) = \\frac{\\mu_0}{4 \\pi} \\int \\left[ \\vec{M}(r') \\cross \\left( \\grad ' \\frac{1}{\\gr} \\right) \\right] \\dd \\tau' Integrating by parts gives \\vec{A}(r) = \\frac{\\mu_0}{4 \\pi} \\left( \\int \\frac{1}{\\gr} [ \\grad ' \\cross \\vec{M}(r') ] \\dd \\tau' - \\int \\grad' \\cross \\left[ \\frac{\\vec{M}(r')}{\\gr} \\right] \\dd \\tau' \\right) Problem 1.61 invites us to express the latter as a surface integral \\vec{A}(r) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{1}{\\gr} [ \\grad' \\cross \\vec{M}(r')] \\dd \\tau' + \\frac{\\mu_0}{4 \\pi} \\oint \\frac{1}{\\gr} [ \\vec{M}(r') \\cross \\dd \\vec{a'} ] \\tagl{6.12} The first term looks just like the potential of a volume current \\vec{J}_b = \\curl \\vec{M} \\tagl{6.13} while the second looks like the potential of a surface current \\vec{K_b} = \\vec{M} \\cross \\vu{n} \\tagl{6.14} where \\vu{n} is the normal unit vector. With these definitions, \\vec{A}(r) = \\frac{\\mu_0}{4\\pi} \\int_V \\frac{\\vec{J_b}(r')}{\\gr} \\dd \\tau' + \\frac{\\mu_0}{4 \\pi} \\oint _S \\frac{\\vec{K_b}(r')}{\\gr} \\dd a' \\tagl{6.15} What this means is that the potential (and hence also the field) of a magnetized object is the same as would be produced by a volume current \\vec{J_b} = \\curl \\vec{M} throughout the material, plus a surface current \\vec{K_b} = \\vec{M} \\cross \\vu{n} , on the boundary. Instead of integrating the contributions of all the infinitesimal dipoles, using Eq. 6.11, we first determine the bound currents, and then find the field they produce, in the same way we would calculate the field of any other volume and surface currents. Notice the striking parallel with the electrical case: there the field of a polarized object was the same as that of a bound volume charge \\rho_b = - \\div \\vec{P} plus a bound surface charge \\sigma_b = \\vec{P} \\cdot \\vu{n} . Example 6.1 Find the magnetic field of a uniformly magnetized sphere Solution Choosing the z axis along the direction of M (Fig 6.12), we have \\vec{J}_b = \\curl \\vec{M} = 0, \\qquad \\vec{K}_b = \\vec{M} \\cross \\vu{n} = M \\sin \\theta \\vu{\\phi} With no bound current density, we can replace our magnetized sphere by a spinning surface current density. Conveniently enough, \\vec{M} \\cross \\vu{n} looks a whole lot like the current density of a rotating spherical shell of uniform surface charge \\sigma \\vec{K} = \\sigma \\vec{v} = \\sigma \\omega R \\sin \\theta \\vu{\\phi} which we already worked out in Exercise 5.11. So, identifying \\sigma R \\vec{\\omega} \\rightarrow \\vec{M} , we can write down \\vec{B} = \\frac{2}{3} \\mu_0 \\vec{M} \\tagl{6.16} inside the sphere, and outside the sphere the field is that of a perfect dipole \\vec{m} = \\frac{4}{3} \\pi R^3 \\vec{M} \\rightarrow \\quad \\vec{B} = \\frac{4 \\mu_0 R^3}{9 r^3} (2 \\cos \\theta \\vu{r} + \\sin \\theta \\vu{\\theta} ) Notice that the internal field is uniform, like the electric field inside a uniformly polarized sphere (Eq. 4.14), although the actual formulas for the two cases are curiously different ( \\frac{2}{3} in place of -\\frac{1}{3} ). The external fields are also analogous: pure dipole in both instances. 6.2.2: Physical Interpretation of Bound Currents In the last section, we found that the field of a magnetized object is identical to the field that would be produced by a certain distribution of \"bound\" currents, \\vec{J}_b and \\vec{K}_b . I want to show you how these bound currents arise physically. This will be a heuristic argument - the rigorous derivation has already been given. Figure 6.15 depicts a thin slab of uniformly magnetized material, with the dipoles represented by tiny current loops. Notice that all the \"internal\" currents cancel: every time there is one going to the right, a contiguous one is going to the left. However, at the edge there is no adjacent loop to do the canceling. The whole thing, then, is equivalent to a single ribbon of current I flowing around the boundary (Fig. 6.16). What is this current, in terms of M ? Say that each of the tiny loops has area a and thickness t (Fig 6.17). In terms of the magnetization, its dipole moment is m = M a t . In terms of the circulating current I , however, m = I a . Therefore I = M t , so the surface current is K_b = I / t = M . Using the outward-drawn unit vector \\vu{n} (Fig 6.16), the direction of \\vec{K_b} is conveniently indicated by the cross product: \\vec{K_b} = \\vec{M} \\cross \\vu{n} (This expression also records the fact that there is no current on the top or bottom surface of the slab; here M is parallel to \\vu{n} , so the cross product vanishes.) This bound surface current is exactly what we obtained in Sect. 6.2.1. It is a peculiar kind of current, in the sense that no single charge makes the whole trip - on the contrary, each charge moves only in a tiny little loop within a single atom. Nevertheless, the net effect is a macroscopic current flowing over the surface of the magnetized object. We call it a \"bound\" current to remind ourselves that every charge is attached to a particular atom, but it's a perfectly genuine current, and it produces a magnetic field in the same way any other current does. When the magnetization is nonuniform, the internal currents no longer cancel. Figure 6.18(a) shows two adjacent chunks of magnetized material, with a larger arrow on the one to the right suggesting greater magnetization at that point. On the surface where they join, there is a net current in the x direction, given by I_x = [M_z (y + \\dd y) - M_z (y)] \\dd z = \\pdv{M_z}{y} \\dd y \\dd z The corresponding volume current density is therefore (J_b)_x = \\pdv{M_z}{y} By the same token, a nonuniform magnetization in the y direction would contribute an amount (J_b)_x = \\pdv{M_z}{y} - \\pdv{M_y}{z} In general, then \\vec{J_b} = \\curl \\vec{M} consistent, again, with the result of Section 6.2.1. Incidentally, like any other steady current, \\vec{J_b} should obey the conservation law 5.33 \\div \\vec{J_b} = 0 Does it? Yes, for the divergence of a curl is always zero. 6.2.3: The Magnetic Field Inside Matter Like the electric field, the actual microscopic magnetic field inside matter fluctuates wildly from point to point and instant to instant. When we speak of \"the\" magnetic field in matter, we mean the macroscopic field: the average over regions large enough to contain many atoms. (The magnetization M is \"smoothed out\" in the same sense.) It is this macroscopic field that one obtains when the methods of Sect. 6.2.1 are applied to points inside magnetized material, as you can prove for yourself in the following problem. Problem 6.11 In Sect 6.2.1, we began with the potential of a perfect dipole (Eq. 6.10), whereas in fact we are dealing with physical dipoles. Show, by the method of Section 4.2.3, that we nonetheless get the correct macroscopic field. As in Section 4.2.3, we want the average of \\vec{B} = \\vec{B}_{out} + \\vec{B}_{in} , where \\vec{B}_{out} is due to molecules outside a small sphere around point P, and \\vec{B}_{in} is due to molecules inside the sphere. The average of \\vec{B}_{out} is the same as the field at the center, and for this it is OK to use Equation 6.10, since the center is \"far\" from all the molecules in question: \\vec{A}_{out} = \\frac{\\mu_0}{4 \\pi} \\int_{outside} \\frac{\\vec{M} \\cross \\vu{\\gr}}{\\gr ^2} \\dd \\tau The average of \\vec{B}_{in} is \\frac{\\mu_0}{4 \\pi} \\frac{2 \\vec{m}}{R^3} - Equation 5.93 - where \\vec{m} = \\frac{4}{3} \\pi R^3 \\vec{M} . Thus the average \\vec{B}_{in} is 2 \\mu_0 \\vec{M}/3 . But what is left out of the integral \\vec{A}_{out} is the contribution of a uniformly magnetized sphere, to wit 2 \\mu_0 \\vec{M} / 3 (Equation 6.16), and this is precisely what \\vec{B}_{in} puts back in. So we'll get the correct macroscopic field using Equation 6.10.","title":"6.2 - The Field of a Magnetized Object"},{"location":"ch6-2/#62-the-field-of-a-magnetized-object","text":"","title":"6.2: The Field of a Magnetized Object"},{"location":"ch6-2/#621-bound-currents","text":"Suppose we have a piece of magnetized material; the magnetic dipole moment per unit volume, M , is given. What field does this object produce? Well, the vector potential of a single dipole m is given by Eq. 5.85 \\vec{A}_{dip} (r) = \\frac{\\mu_0}{4 \\pi} \\frac{\\vec{m} \\cross \\vu{r}}{r^2} In the magnetized object, each volume element \\dd \\tau' carries a dipole moment M \\dd \\tau' , so the total vector potential is (Fig 6.11) \\vec{A}(r) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{\\vec{M}(r') \\cross \\vu{\\gr}}{\\gr ^2} \\dd \\tau' \\tagl{6.11} That does it, in principle. But, as in the electrical case, the integral can be cast in a more illuminating form by exploiting the identity \\grad ' \\frac{1}{\\gr} = \\frac{\\vu{\\gr}}{\\gr^2} With this, \\vec{A}(r) = \\frac{\\mu_0}{4 \\pi} \\int \\left[ \\vec{M}(r') \\cross \\left( \\grad ' \\frac{1}{\\gr} \\right) \\right] \\dd \\tau' Integrating by parts gives \\vec{A}(r) = \\frac{\\mu_0}{4 \\pi} \\left( \\int \\frac{1}{\\gr} [ \\grad ' \\cross \\vec{M}(r') ] \\dd \\tau' - \\int \\grad' \\cross \\left[ \\frac{\\vec{M}(r')}{\\gr} \\right] \\dd \\tau' \\right) Problem 1.61 invites us to express the latter as a surface integral \\vec{A}(r) = \\frac{\\mu_0}{4 \\pi} \\int \\frac{1}{\\gr} [ \\grad' \\cross \\vec{M}(r')] \\dd \\tau' + \\frac{\\mu_0}{4 \\pi} \\oint \\frac{1}{\\gr} [ \\vec{M}(r') \\cross \\dd \\vec{a'} ] \\tagl{6.12} The first term looks just like the potential of a volume current \\vec{J}_b = \\curl \\vec{M} \\tagl{6.13} while the second looks like the potential of a surface current \\vec{K_b} = \\vec{M} \\cross \\vu{n} \\tagl{6.14} where \\vu{n} is the normal unit vector. With these definitions, \\vec{A}(r) = \\frac{\\mu_0}{4\\pi} \\int_V \\frac{\\vec{J_b}(r')}{\\gr} \\dd \\tau' + \\frac{\\mu_0}{4 \\pi} \\oint _S \\frac{\\vec{K_b}(r')}{\\gr} \\dd a' \\tagl{6.15} What this means is that the potential (and hence also the field) of a magnetized object is the same as would be produced by a volume current \\vec{J_b} = \\curl \\vec{M} throughout the material, plus a surface current \\vec{K_b} = \\vec{M} \\cross \\vu{n} , on the boundary. Instead of integrating the contributions of all the infinitesimal dipoles, using Eq. 6.11, we first determine the bound currents, and then find the field they produce, in the same way we would calculate the field of any other volume and surface currents. Notice the striking parallel with the electrical case: there the field of a polarized object was the same as that of a bound volume charge \\rho_b = - \\div \\vec{P} plus a bound surface charge \\sigma_b = \\vec{P} \\cdot \\vu{n} .","title":"6.2.1: Bound Currents"},{"location":"ch6-2/#example-61","text":"Find the magnetic field of a uniformly magnetized sphere Solution Choosing the z axis along the direction of M (Fig 6.12), we have \\vec{J}_b = \\curl \\vec{M} = 0, \\qquad \\vec{K}_b = \\vec{M} \\cross \\vu{n} = M \\sin \\theta \\vu{\\phi} With no bound current density, we can replace our magnetized sphere by a spinning surface current density. Conveniently enough, \\vec{M} \\cross \\vu{n} looks a whole lot like the current density of a rotating spherical shell of uniform surface charge \\sigma \\vec{K} = \\sigma \\vec{v} = \\sigma \\omega R \\sin \\theta \\vu{\\phi} which we already worked out in Exercise 5.11. So, identifying \\sigma R \\vec{\\omega} \\rightarrow \\vec{M} , we can write down \\vec{B} = \\frac{2}{3} \\mu_0 \\vec{M} \\tagl{6.16} inside the sphere, and outside the sphere the field is that of a perfect dipole \\vec{m} = \\frac{4}{3} \\pi R^3 \\vec{M} \\rightarrow \\quad \\vec{B} = \\frac{4 \\mu_0 R^3}{9 r^3} (2 \\cos \\theta \\vu{r} + \\sin \\theta \\vu{\\theta} ) Notice that the internal field is uniform, like the electric field inside a uniformly polarized sphere (Eq. 4.14), although the actual formulas for the two cases are curiously different ( \\frac{2}{3} in place of -\\frac{1}{3} ). The external fields are also analogous: pure dipole in both instances.","title":"Example 6.1"},{"location":"ch6-2/#622-physical-interpretation-of-bound-currents","text":"In the last section, we found that the field of a magnetized object is identical to the field that would be produced by a certain distribution of \"bound\" currents, \\vec{J}_b and \\vec{K}_b . I want to show you how these bound currents arise physically. This will be a heuristic argument - the rigorous derivation has already been given. Figure 6.15 depicts a thin slab of uniformly magnetized material, with the dipoles represented by tiny current loops. Notice that all the \"internal\" currents cancel: every time there is one going to the right, a contiguous one is going to the left. However, at the edge there is no adjacent loop to do the canceling. The whole thing, then, is equivalent to a single ribbon of current I flowing around the boundary (Fig. 6.16). What is this current, in terms of M ? Say that each of the tiny loops has area a and thickness t (Fig 6.17). In terms of the magnetization, its dipole moment is m = M a t . In terms of the circulating current I , however, m = I a . Therefore I = M t , so the surface current is K_b = I / t = M . Using the outward-drawn unit vector \\vu{n} (Fig 6.16), the direction of \\vec{K_b} is conveniently indicated by the cross product: \\vec{K_b} = \\vec{M} \\cross \\vu{n} (This expression also records the fact that there is no current on the top or bottom surface of the slab; here M is parallel to \\vu{n} , so the cross product vanishes.) This bound surface current is exactly what we obtained in Sect. 6.2.1. It is a peculiar kind of current, in the sense that no single charge makes the whole trip - on the contrary, each charge moves only in a tiny little loop within a single atom. Nevertheless, the net effect is a macroscopic current flowing over the surface of the magnetized object. We call it a \"bound\" current to remind ourselves that every charge is attached to a particular atom, but it's a perfectly genuine current, and it produces a magnetic field in the same way any other current does. When the magnetization is nonuniform, the internal currents no longer cancel. Figure 6.18(a) shows two adjacent chunks of magnetized material, with a larger arrow on the one to the right suggesting greater magnetization at that point. On the surface where they join, there is a net current in the x direction, given by I_x = [M_z (y + \\dd y) - M_z (y)] \\dd z = \\pdv{M_z}{y} \\dd y \\dd z The corresponding volume current density is therefore (J_b)_x = \\pdv{M_z}{y} By the same token, a nonuniform magnetization in the y direction would contribute an amount (J_b)_x = \\pdv{M_z}{y} - \\pdv{M_y}{z} In general, then \\vec{J_b} = \\curl \\vec{M} consistent, again, with the result of Section 6.2.1. Incidentally, like any other steady current, \\vec{J_b} should obey the conservation law 5.33 \\div \\vec{J_b} = 0 Does it? Yes, for the divergence of a curl is always zero.","title":"6.2.2: Physical Interpretation of Bound Currents"},{"location":"ch6-2/#623-the-magnetic-field-inside-matter","text":"Like the electric field, the actual microscopic magnetic field inside matter fluctuates wildly from point to point and instant to instant. When we speak of \"the\" magnetic field in matter, we mean the macroscopic field: the average over regions large enough to contain many atoms. (The magnetization M is \"smoothed out\" in the same sense.) It is this macroscopic field that one obtains when the methods of Sect. 6.2.1 are applied to points inside magnetized material, as you can prove for yourself in the following problem.","title":"6.2.3: The Magnetic Field Inside Matter"},{"location":"ch6-2/#problem-611","text":"In Sect 6.2.1, we began with the potential of a perfect dipole (Eq. 6.10), whereas in fact we are dealing with physical dipoles. Show, by the method of Section 4.2.3, that we nonetheless get the correct macroscopic field. As in Section 4.2.3, we want the average of \\vec{B} = \\vec{B}_{out} + \\vec{B}_{in} , where \\vec{B}_{out} is due to molecules outside a small sphere around point P, and \\vec{B}_{in} is due to molecules inside the sphere. The average of \\vec{B}_{out} is the same as the field at the center, and for this it is OK to use Equation 6.10, since the center is \"far\" from all the molecules in question: \\vec{A}_{out} = \\frac{\\mu_0}{4 \\pi} \\int_{outside} \\frac{\\vec{M} \\cross \\vu{\\gr}}{\\gr ^2} \\dd \\tau The average of \\vec{B}_{in} is \\frac{\\mu_0}{4 \\pi} \\frac{2 \\vec{m}}{R^3} - Equation 5.93 - where \\vec{m} = \\frac{4}{3} \\pi R^3 \\vec{M} . Thus the average \\vec{B}_{in} is 2 \\mu_0 \\vec{M}/3 . But what is left out of the integral \\vec{A}_{out} is the contribution of a uniformly magnetized sphere, to wit 2 \\mu_0 \\vec{M} / 3 (Equation 6.16), and this is precisely what \\vec{B}_{in} puts back in. So we'll get the correct macroscopic field using Equation 6.10.","title":"Problem 6.11"},{"location":"ch6-3/","text":"6.3: The Auxiliary Field H 6.3.1: Ampere's Law in Magnetized Materials In Section 6.2, we found that the effect of magnetization is to establish bound currents \\vec{J_b} = \\curl \\vec{M} within the material and \\vec{K_b} = \\vec{M} \\cross \\vu{n} on the surface. The field due to magnetization of the medium is just the field produced by these bound currents. We are now ready to put everything together: the field attributable to bound currents, plus the field due to everything else - which I will call the free current . The free current might flow through wires embedded in the magnetized substance or, if the latter is a conductor, through the material itself. In any event, the total current can be written as \\vec{J} = \\vec{J_b} + \\vec{J_f} \\tagl{6.17} There is no new physics in Eq. 6.17; it is simply a convenience to separate the current into these two parts, because they got there by quite different means: the free current is there because somebody hooked up a wire to a battery - it involves actual transport of charge; the bound current is there because of magnetization - it results from the conspiracy of many aligned atomic dipoles. In view of Eqs. 6.13 and 6.17, Ampere's law can be written \\frac{1}{\\mu_0} (\\curl \\vec{B}) = \\vec{J} = \\vec{J_b} + \\vec{J_f} = \\vec{J_f} + (\\curl \\vec{M}) or, collecting together the two curls: \\curl \\left( \\frac{1}{\\mu_0} \\vec{B} - \\vec{M} \\right) = \\vec{J_f} The quantity in parentheses is designated by the letter H : \\vec{H} \\equiv \\frac{1}{\\mu_0} \\vec{B} - \\vec{M} \\tagl{6.18} In terms of H , then, Ampere's law reads \\curl \\vec{H} = \\vec{J_f} \\tagl{6.19} or, in integral form, \\oint \\vec{H} \\cdot \\dd \\vec{l} = I _{f, enc} \\tagl{6.20} where I_{f, enc} is the total free current passing through the Amperian loop. H plays a role in magnetostatics analogous to D in electroostatics: Just as D allowed us to write Gauss's law in terms of the free charge alone, H permits us to express Ampere's law in terms of the free current alone - and free current is what we control directly. Bound current, like bound charge, comes along for the ride - the material gets magnetized, and this results in bound currents; we cannot turn them on or off independently, as we can the free currents. In applying Eq. 6.20, all we need to worry about is the free current, which we know about because we put it there. In particular, when symmetry permits, we can calculate H immediately from Eq. 6.20 by the usual Ampere's law methods. (For example, problems 6.7 and 6.8 can be done in one line by noting that \\vec{H} = 0 .) Example 6.2 A long copper rod of radius R carries a uniformly distributed (free) current I (Fig 6.19). Find H inside and outside the rod. Solution Copper is weakly diamagnetic, so the dipoles will line up opposite to the field. This results in a bound current running antiparallel to I , within the wire, and parallel to I along the surface (Fig 6.20). Just how great these bound currents will be we are not yet in a position to say - but in order to calculate H it is sufficient to realize that all the currents are longitudinal, so \\vec{B} , \\vec{M} , and therefore also \\vec{H} , are circumferential. Applying Eq. 6.20 to an Amperian loop of radius s < R , H (2 \\pi s) = I_{f, enc} = I \\frac{\\pi s^2}{\\pi R^2} so, inside the wire, \\vec{H} = \\frac{I}{2 \\pi R^2} s \\vu{\\phi} \\quad (s \\leq R) \\tagl{6.21} Outside the wire, \\vec{H} = \\frac{I}{2 \\pi s} \\vu{\\phi} \\quad (s \\geq R) \\tagl{6.22} Outside of the wire (as always in empty space) \\vec{M} = 0 , so \\vec{B} = \\mu_0 \\vec{H} = \\frac{\\mu_0 I}{2 \\pi s} \\vu{\\phi} \\quad (s \\geq R) the same as for a nonmagnetized wire (Ex. 5.7). Within the wire, we are not yet quite ready to determine B , since we have no way of knowing M . (In practice, the magnetization of copper is so small that for most purposes we can ignore it altogether.) As it turns, out, H is a more useful quantity than D . In the laboratory, you will frequently hear people talking about H (more often even than B ), but you will never hear anyone speak of D (only E ). The reason is this: to build an electromagnet you run a certain (free) current through a coil. The current is the thing you read on the dial, and this determines H (or at any rate, the line integral of H ); B depends on the specific materials you used and even, if iron is present, on the history of your magnet. On the other hand, if you want to set up an electric field, you do not plaster a known free charge on the plates of a parallel plate capacitor; rather, you connect them to a battery of known voltage. It's the potential difference you read on your dial, and that determines E (or rather, the line integral of E ); D depends on the details of the dielectric you're using. If it were easy to measure charge, and hard to measure potential, then you'd find experimentalists talking about D instead of E . So the relative familiarity of H as contrasted with D derives from purely practical considerations; theoretically, they're on an equal footing. Many authors call H , not B , the \"magnetic field.\" Then they have to invent a new word for B : the \"flux density,\" or magnetic \"induction\" (an absurd choice, since that term already has at least two other meanings in electrodynamics). Anyway, B is indisputably the fundamental quantity, so I shall continue to call it the \"magnetic field,\" as everyone does in the spoken language. H has no sensible name: just call it \" H .\" 6.3.2: A Deceptive Parallel Equation 6.19 looks just like Ampere's original law, except that the total current is replaced by the free current, and \\vec{B} is replaced by \\mu_0 \\vec{H} . As in the case of D , however, I must warn you against reading too much into this correspondence. It does not say that \\mu_0 \\vec{H} is \"just like \\vec{B} , only its source is \\vec{J_f} instead of \\vec{J} .\" For the curl alone does not determine a vector field - you must also know the divergence. And whereas \\div \\vec{B} = 0 , the divergence of H is not, in general, zero. In fact, from Eq. 6.18, \\div \\vec{H} = - \\div \\vec{M} \\tagl{6.23} Only when the divergence of M vanishes is the parallel between \\vec{B} and \\mu_0 \\vec{H} faithful. If you think I'm being pedantic, consider the example of a bar magnet - a short cylinder of iron that carries a permanent uniform magnetization M parallel to its axis. In this case there is no free current anywhere, and a naive application of Eq. 6.20 might lead you to suppose that \\vec{H} = 0 , and hence that \\vec{B} = \\mu_0 M inside the magnet and \\vec{B} = 0 outside, which is nonsense. It is quite true that the curl of H vanishes everywhere, but the divergence does not (check top and bottom surfaces of the magnet!). Advice: When you are asked to find B or H in a problem involving magnetic materials, first look for symmetry. If the problem exhibits cylindrical, plane, solenoidal, or toroidal symmetry, then you can get H directly from Eq. 6.20 by the usual Ampere's law methods. (Evidently, in such cases \\div \\vec{M} is automatically zero, since the free current alone determines the answer.) If the requisite symmetry is absent, you'll have to think of another approach, and in particular you must not assume that H is zero just because there is no free current in sight. 6.3.3: Boundary Conditions The magnetostatic boundary conditions of Section 5.4.2 can be rewritten in terms of H and the free current. From Eq. 6.23 it follows that H_{above} ^\\perp - H_{below} ^\\perp = -(M_{above} ^\\perp - M_{below} ^\\perp) \\tagl{6.24} while Eq. 6.19 says \\vec{H}_{above} ^\\parallel - \\vec{H}_{below} ^\\parallel = \\vec{K}_f \\cross \\vu{n} \\tagl{6.25} In the presence of materials, these are sometimes more useful than the corresponding boundary conditions on B (Eqs. 5.74 and 5.76) B_{above} ^\\perp - B_{below} ^\\perp = 0 \\tagl{6.26} and \\vec{B}_{above} ^\\parallel - \\vec{B}_{below} ^\\parallel = \\mu_0 (\\vec{K} \\cross \\vu{n} ) \\tagl{6.27}","title":"6.3 - The Auxiliary Field"},{"location":"ch6-3/#63-the-auxiliary-field-h","text":"","title":"6.3: The Auxiliary Field H"},{"location":"ch6-3/#631-amperes-law-in-magnetized-materials","text":"In Section 6.2, we found that the effect of magnetization is to establish bound currents \\vec{J_b} = \\curl \\vec{M} within the material and \\vec{K_b} = \\vec{M} \\cross \\vu{n} on the surface. The field due to magnetization of the medium is just the field produced by these bound currents. We are now ready to put everything together: the field attributable to bound currents, plus the field due to everything else - which I will call the free current . The free current might flow through wires embedded in the magnetized substance or, if the latter is a conductor, through the material itself. In any event, the total current can be written as \\vec{J} = \\vec{J_b} + \\vec{J_f} \\tagl{6.17} There is no new physics in Eq. 6.17; it is simply a convenience to separate the current into these two parts, because they got there by quite different means: the free current is there because somebody hooked up a wire to a battery - it involves actual transport of charge; the bound current is there because of magnetization - it results from the conspiracy of many aligned atomic dipoles. In view of Eqs. 6.13 and 6.17, Ampere's law can be written \\frac{1}{\\mu_0} (\\curl \\vec{B}) = \\vec{J} = \\vec{J_b} + \\vec{J_f} = \\vec{J_f} + (\\curl \\vec{M}) or, collecting together the two curls: \\curl \\left( \\frac{1}{\\mu_0} \\vec{B} - \\vec{M} \\right) = \\vec{J_f} The quantity in parentheses is designated by the letter H : \\vec{H} \\equiv \\frac{1}{\\mu_0} \\vec{B} - \\vec{M} \\tagl{6.18} In terms of H , then, Ampere's law reads \\curl \\vec{H} = \\vec{J_f} \\tagl{6.19} or, in integral form, \\oint \\vec{H} \\cdot \\dd \\vec{l} = I _{f, enc} \\tagl{6.20} where I_{f, enc} is the total free current passing through the Amperian loop. H plays a role in magnetostatics analogous to D in electroostatics: Just as D allowed us to write Gauss's law in terms of the free charge alone, H permits us to express Ampere's law in terms of the free current alone - and free current is what we control directly. Bound current, like bound charge, comes along for the ride - the material gets magnetized, and this results in bound currents; we cannot turn them on or off independently, as we can the free currents. In applying Eq. 6.20, all we need to worry about is the free current, which we know about because we put it there. In particular, when symmetry permits, we can calculate H immediately from Eq. 6.20 by the usual Ampere's law methods. (For example, problems 6.7 and 6.8 can be done in one line by noting that \\vec{H} = 0 .)","title":"6.3.1: Ampere's Law in Magnetized Materials"},{"location":"ch6-3/#example-62","text":"A long copper rod of radius R carries a uniformly distributed (free) current I (Fig 6.19). Find H inside and outside the rod. Solution Copper is weakly diamagnetic, so the dipoles will line up opposite to the field. This results in a bound current running antiparallel to I , within the wire, and parallel to I along the surface (Fig 6.20). Just how great these bound currents will be we are not yet in a position to say - but in order to calculate H it is sufficient to realize that all the currents are longitudinal, so \\vec{B} , \\vec{M} , and therefore also \\vec{H} , are circumferential. Applying Eq. 6.20 to an Amperian loop of radius s < R , H (2 \\pi s) = I_{f, enc} = I \\frac{\\pi s^2}{\\pi R^2} so, inside the wire, \\vec{H} = \\frac{I}{2 \\pi R^2} s \\vu{\\phi} \\quad (s \\leq R) \\tagl{6.21} Outside the wire, \\vec{H} = \\frac{I}{2 \\pi s} \\vu{\\phi} \\quad (s \\geq R) \\tagl{6.22} Outside of the wire (as always in empty space) \\vec{M} = 0 , so \\vec{B} = \\mu_0 \\vec{H} = \\frac{\\mu_0 I}{2 \\pi s} \\vu{\\phi} \\quad (s \\geq R) the same as for a nonmagnetized wire (Ex. 5.7). Within the wire, we are not yet quite ready to determine B , since we have no way of knowing M . (In practice, the magnetization of copper is so small that for most purposes we can ignore it altogether.) As it turns, out, H is a more useful quantity than D . In the laboratory, you will frequently hear people talking about H (more often even than B ), but you will never hear anyone speak of D (only E ). The reason is this: to build an electromagnet you run a certain (free) current through a coil. The current is the thing you read on the dial, and this determines H (or at any rate, the line integral of H ); B depends on the specific materials you used and even, if iron is present, on the history of your magnet. On the other hand, if you want to set up an electric field, you do not plaster a known free charge on the plates of a parallel plate capacitor; rather, you connect them to a battery of known voltage. It's the potential difference you read on your dial, and that determines E (or rather, the line integral of E ); D depends on the details of the dielectric you're using. If it were easy to measure charge, and hard to measure potential, then you'd find experimentalists talking about D instead of E . So the relative familiarity of H as contrasted with D derives from purely practical considerations; theoretically, they're on an equal footing. Many authors call H , not B , the \"magnetic field.\" Then they have to invent a new word for B : the \"flux density,\" or magnetic \"induction\" (an absurd choice, since that term already has at least two other meanings in electrodynamics). Anyway, B is indisputably the fundamental quantity, so I shall continue to call it the \"magnetic field,\" as everyone does in the spoken language. H has no sensible name: just call it \" H .\"","title":"Example 6.2"},{"location":"ch6-3/#632-a-deceptive-parallel","text":"Equation 6.19 looks just like Ampere's original law, except that the total current is replaced by the free current, and \\vec{B} is replaced by \\mu_0 \\vec{H} . As in the case of D , however, I must warn you against reading too much into this correspondence. It does not say that \\mu_0 \\vec{H} is \"just like \\vec{B} , only its source is \\vec{J_f} instead of \\vec{J} .\" For the curl alone does not determine a vector field - you must also know the divergence. And whereas \\div \\vec{B} = 0 , the divergence of H is not, in general, zero. In fact, from Eq. 6.18, \\div \\vec{H} = - \\div \\vec{M} \\tagl{6.23} Only when the divergence of M vanishes is the parallel between \\vec{B} and \\mu_0 \\vec{H} faithful. If you think I'm being pedantic, consider the example of a bar magnet - a short cylinder of iron that carries a permanent uniform magnetization M parallel to its axis. In this case there is no free current anywhere, and a naive application of Eq. 6.20 might lead you to suppose that \\vec{H} = 0 , and hence that \\vec{B} = \\mu_0 M inside the magnet and \\vec{B} = 0 outside, which is nonsense. It is quite true that the curl of H vanishes everywhere, but the divergence does not (check top and bottom surfaces of the magnet!). Advice: When you are asked to find B or H in a problem involving magnetic materials, first look for symmetry. If the problem exhibits cylindrical, plane, solenoidal, or toroidal symmetry, then you can get H directly from Eq. 6.20 by the usual Ampere's law methods. (Evidently, in such cases \\div \\vec{M} is automatically zero, since the free current alone determines the answer.) If the requisite symmetry is absent, you'll have to think of another approach, and in particular you must not assume that H is zero just because there is no free current in sight.","title":"6.3.2: A Deceptive Parallel"},{"location":"ch6-3/#633-boundary-conditions","text":"The magnetostatic boundary conditions of Section 5.4.2 can be rewritten in terms of H and the free current. From Eq. 6.23 it follows that H_{above} ^\\perp - H_{below} ^\\perp = -(M_{above} ^\\perp - M_{below} ^\\perp) \\tagl{6.24} while Eq. 6.19 says \\vec{H}_{above} ^\\parallel - \\vec{H}_{below} ^\\parallel = \\vec{K}_f \\cross \\vu{n} \\tagl{6.25} In the presence of materials, these are sometimes more useful than the corresponding boundary conditions on B (Eqs. 5.74 and 5.76) B_{above} ^\\perp - B_{below} ^\\perp = 0 \\tagl{6.26} and \\vec{B}_{above} ^\\parallel - \\vec{B}_{below} ^\\parallel = \\mu_0 (\\vec{K} \\cross \\vu{n} ) \\tagl{6.27}","title":"6.3.3: Boundary Conditions"},{"location":"ch6-4/","text":"6.4: Linear and Nonlinear Media 6.4.1: Magnetic Susceptibility and Permeability In paramagnetic and diamagnetic materials, the magnetization is sustained by the field; when B is removed, M disappears. In fact, for most substances the magnetization is proportional to the field, provided the field is not too strong. For notational consistency with the electrical case (Eq. 4.30), I should express the proportionality thus \\vec{M} = \\frac{1}{\\mu_0} \\chi_m \\vec{B} \\tagl{6.28} but custom dictates that it be written in terms of H , instead of B : \\vec{M} = \\chi_m \\vec{H} The constant of proportionality \\chi_m is called the magnetic susceptibility ; it is a dimensionless quantity that varies from one substance to another - positive for paramagnets and negative for diamagnets. Typical values are around 10^{-5} . Materials that obey Eq. 6.29 are called linear media . In view of Eq. 6.18, \\vec{B} = \\mu_0 (\\vec{H} + \\vec{M}) = \\mu_0 (1 + \\chi_m) \\vec{H} \\tagl{6.30} for linear media. Thus B is also proportional to H : \\vec{B} = \\mu \\vec{H} \\tagl{6.31} where \\mu \\equiv \\mu_0 (1 + \\chi_m) \\tagl{6.32} \\mu is called the permeability of the material. In a vacuum, where there is no matter to magnetize, the susceptibility \\chi_m vanishes, and the permeability is \\mu_0 . That's why \\mu_0 is called the permeability of free space . Example 6.3 An infinite solenoid (n turns per unit length, current I ), is filled with linear material of susceptibility \\chi_m . Find the magnetic field inside the solenoid. Solution Since B is due in part to the bound currents (which we don't yet know), we cannot compute it directly. However, this is one of those symmetrical cases in which we can get H from the free current alone, using Ampere's law in the form of Eq. 6.20 \\vec{H} = n I \\vu{z} According to Eq. 6.31, then \\vec{B} = \\mu_0(1 + \\chi_m) n I \\vu{z} If the medium is paramagnetic, the field is slightly enhanced; if it's diamagnetic, the field is somewhat reduced. This reflects the fact that the bound surface current \\vec{K}_b = \\vec{M} \\cross \\vu{n} = \\chi_m (\\vec{H} \\cross \\vu{n} ) = \\chi_m n I \\vu{\\phi} is in the same direction as I , in the former case ( \\chi_m > 0 ), and opposite in the latter ( \\chi_m < 0 ). You might suppose that linear media escape the defect in the parallel between B and H : since M and H are now proportional to B , does it not follow that their divergence, like B 's, must always vanish? Unfortunately, it does not; at the boundary between two materials of different permeability, the divergence of M can actually be infinite. For instance, at the end of a cylinder of linear paramagnetic material, M is zero on one side but not on the other. For the \"Gaussian pillbox\" shown in Fig 6.23, \\oint \\vec{M} \\cdot \\dd \\vec{a} \\neq 0 , and hence, by the divergence theorem, \\div \\vec{M} cannot vanish everywhere within it. Incidentally, the volume bound current density in a homogeneous linear material is proportional to the free current density: \\vec{J}_b = \\curl \\vec{M} = \\curl (\\chi_m \\vec{H}) = \\chi_m \\vec{J_f} \\tagl{6.33} In particular, unless free current actually flows through the material, all bound current will be at the surface. 6.4.2: Ferromagnetism In a linear medium, the alignment of atomic dipoles is maintained by a magnetic field imposed from the outside. Ferromagnets - which are emphatically not linear - require no external fields to sustain magnetization; the alignment is \"frozen in.\" Like paramagnetism, ferromagnetism involves the magnetic dipoles associated with the spins of unpaired electrons. The new feature, which makes ferromagnetism so different from paramagnetism, is the interaction between nearby dipoles: In a ferromagnet, each dipole \"likes\" to point in the same direction as its neighbors. The reason for this preference is essentially quantum mechanical, and I won't try to explain it here; it is enough to know that the correlation is so strong as to align virtually 100% of the unpaired electron spins. If you could somehow magnify a piece of iron and \"see\" the individual dipoles as tiny arrows, it would look something like Fig. 6.25, with all the spins pointing the same way. But if that is true, why isn't every wrench and nail a powerful magnet? The answer is that the alignment occurs in relatively small patches, called domains . Each domain contains billions of dipoles, all lined up (these domains are actually visible under a microscope, given suitable etching techniques - see Fig 6.26), but the domains themselves are randomly oriented. The household wrench contains an enormous number of domains, and their magnetic fields cancel, so the wrench as a whole is not magnetized. (Actually, the orientation of domains is not completely random; within a given crystal, there may be some preferential alignment along the crystal axes. But there will be just as many domains pointing one way as the other, so there is still no large-scale magnetization. Moreover, the crystals themselves are randomly oriented within any sizable chunk of metal.) How, then, would you produce a permanent magnet, such as they sell in toy stores? If you put a piece of iron into a strong magnetic field, the torque \\vec{N} = \\vec{m} \\cross \\vec{B} tends to align the dipoles parallel to the field. Since they like to stay parallel to their neighbors, most of the dipoles will resist this torque. However, at the boundary between two domains, there are competing neighbors, and the torque will throw its weight on the side of the domain most nearly parallel to the field; this domain will win some converts, at the expense of the less favorably oriented one. The net effect of the magnetic field, then, is to move the domain boundaries. Domains parallel to the field grow, and the others shrink. If the field is strong enough, one domain takes over entirely, and the iron is said to be saturated. It turns out that this process (the shifting of domain boundaries in response to an external field) is not entirely reversible: When the field is switched off, there will be some return to randomly oriented domains, but it is far from complete; there remains a preponderance of domains in the original direction. You now have a permanent magnet. A simple way to accomplish this, in practice, is to wrap a coil of wire around the object to be magnetized (Fig. 6.27). Run a current I through the coil; this provides the external magnetic field (pointing to the left in the diagram). As you increase the current, the field increases, the domain boundaries move, and the magnetization grows. Eventually, you reach the saturation point, with all the dipoles aligned, and a further increase in current has no effect on M (Fig. 6.28, point b). Now suppose you reduce the current. Instead of retracing the path back to \\vec{M} = 0 , there is only a partial return to randomly oriented domains; M decreases, but even with the current off there is some residual magnetization (point c). The wrench is now a permanent magnet. If you want to eliminate the remaining magnetization, you'll have to run a current backwards through the coil (a negative I ). Now the external field points to the right, and as you increase I (negatively), M drops down to zero (point d). If you turn I still higher, you soon reach saturation in the other direction-all the dipoles now pointing to the right (e). At this stage, switching off the current will leave the wrench with a permanent magnetization to the right (point f). To complete the story, turn I on again in the positive sense: M returns to zero (point g), and eventually to the forward saturation point (b). The path we have traced out is called a hysteresis loop. Notice that the magnetization of the wrench depends not only on the applied field (that is, on/), but also on its previous magnetic \"history.\" For instance, at three different times in our experiment the current was zero (a, c, and f), yet the magnetization was different for each of them. Actually, it is customary to draw hysteresis loops as plots of B against H , rather than M against I . (If our coil is approximated by a long solenoid, with n turns per unit length, then H = nI , so H and I are proportional. Meanwhile, \\vec{B} = \\mu_0 (\\vec{H} + \\vec{M}) , but in practice M is huge compared to H , so to all intents and purposes B is proportional to M .) To make the units consistent (teslas), I have plotted (\\mu_0 H) horizontally (Fig. 6.29); notice, however, that the vertical scale is 10^4 times greater than the horizontal one. Roughly speaking, \\mu_0 \\vec{H} is the field our coil would have produced in the absence of any iron; B is what we actually got, and compared to \\mu_0 \\vec{H} , it is gigantic. A little current goes a long way, when you have ferromagnetic materials around. That's why anyone who wants to make a powerful electromagnet will wrap the coil around an iron core. It doesn't take much of an external field to move the domain boundaries, and when you do that, you have all the dipoles in the iron working with you. One final point about ferromagnetism: It all follows, remember, from the fact that the dipoles within a given domain line up parallel to one another. Random thermal motions compete with this ordering, but as long as the temperature doesn't get too high, they cannot budge the dipoles out of line. It's not surprising, though, that very high temperatures do destroy the alignment. What is surprising is that this occurs at a precise temperature ( 770^\\circ C , for iron). Below this temperature (called the Curie point), iron is ferromagnetic; above, it is paramagnetic. The Curie point is rather like the boiling point or the freezing point in that there is no gradual transition from ferro- to para-magnetic behavior, any more than there is between water and ice. These abrupt changes in the properties of a substance, occurring at sharply defined temperatures, are known in statistical mechanics as phase transitions.","title":"6.4 - Linear and Nonlinear Media"},{"location":"ch6-4/#64-linear-and-nonlinear-media","text":"","title":"6.4: Linear and Nonlinear Media"},{"location":"ch6-4/#641-magnetic-susceptibility-and-permeability","text":"In paramagnetic and diamagnetic materials, the magnetization is sustained by the field; when B is removed, M disappears. In fact, for most substances the magnetization is proportional to the field, provided the field is not too strong. For notational consistency with the electrical case (Eq. 4.30), I should express the proportionality thus \\vec{M} = \\frac{1}{\\mu_0} \\chi_m \\vec{B} \\tagl{6.28} but custom dictates that it be written in terms of H , instead of B : \\vec{M} = \\chi_m \\vec{H} The constant of proportionality \\chi_m is called the magnetic susceptibility ; it is a dimensionless quantity that varies from one substance to another - positive for paramagnets and negative for diamagnets. Typical values are around 10^{-5} . Materials that obey Eq. 6.29 are called linear media . In view of Eq. 6.18, \\vec{B} = \\mu_0 (\\vec{H} + \\vec{M}) = \\mu_0 (1 + \\chi_m) \\vec{H} \\tagl{6.30} for linear media. Thus B is also proportional to H : \\vec{B} = \\mu \\vec{H} \\tagl{6.31} where \\mu \\equiv \\mu_0 (1 + \\chi_m) \\tagl{6.32} \\mu is called the permeability of the material. In a vacuum, where there is no matter to magnetize, the susceptibility \\chi_m vanishes, and the permeability is \\mu_0 . That's why \\mu_0 is called the permeability of free space .","title":"6.4.1: Magnetic Susceptibility and Permeability"},{"location":"ch6-4/#example-63","text":"An infinite solenoid (n turns per unit length, current I ), is filled with linear material of susceptibility \\chi_m . Find the magnetic field inside the solenoid. Solution Since B is due in part to the bound currents (which we don't yet know), we cannot compute it directly. However, this is one of those symmetrical cases in which we can get H from the free current alone, using Ampere's law in the form of Eq. 6.20 \\vec{H} = n I \\vu{z} According to Eq. 6.31, then \\vec{B} = \\mu_0(1 + \\chi_m) n I \\vu{z} If the medium is paramagnetic, the field is slightly enhanced; if it's diamagnetic, the field is somewhat reduced. This reflects the fact that the bound surface current \\vec{K}_b = \\vec{M} \\cross \\vu{n} = \\chi_m (\\vec{H} \\cross \\vu{n} ) = \\chi_m n I \\vu{\\phi} is in the same direction as I , in the former case ( \\chi_m > 0 ), and opposite in the latter ( \\chi_m < 0 ). You might suppose that linear media escape the defect in the parallel between B and H : since M and H are now proportional to B , does it not follow that their divergence, like B 's, must always vanish? Unfortunately, it does not; at the boundary between two materials of different permeability, the divergence of M can actually be infinite. For instance, at the end of a cylinder of linear paramagnetic material, M is zero on one side but not on the other. For the \"Gaussian pillbox\" shown in Fig 6.23, \\oint \\vec{M} \\cdot \\dd \\vec{a} \\neq 0 , and hence, by the divergence theorem, \\div \\vec{M} cannot vanish everywhere within it. Incidentally, the volume bound current density in a homogeneous linear material is proportional to the free current density: \\vec{J}_b = \\curl \\vec{M} = \\curl (\\chi_m \\vec{H}) = \\chi_m \\vec{J_f} \\tagl{6.33} In particular, unless free current actually flows through the material, all bound current will be at the surface.","title":"Example 6.3"},{"location":"ch6-4/#642-ferromagnetism","text":"In a linear medium, the alignment of atomic dipoles is maintained by a magnetic field imposed from the outside. Ferromagnets - which are emphatically not linear - require no external fields to sustain magnetization; the alignment is \"frozen in.\" Like paramagnetism, ferromagnetism involves the magnetic dipoles associated with the spins of unpaired electrons. The new feature, which makes ferromagnetism so different from paramagnetism, is the interaction between nearby dipoles: In a ferromagnet, each dipole \"likes\" to point in the same direction as its neighbors. The reason for this preference is essentially quantum mechanical, and I won't try to explain it here; it is enough to know that the correlation is so strong as to align virtually 100% of the unpaired electron spins. If you could somehow magnify a piece of iron and \"see\" the individual dipoles as tiny arrows, it would look something like Fig. 6.25, with all the spins pointing the same way. But if that is true, why isn't every wrench and nail a powerful magnet? The answer is that the alignment occurs in relatively small patches, called domains . Each domain contains billions of dipoles, all lined up (these domains are actually visible under a microscope, given suitable etching techniques - see Fig 6.26), but the domains themselves are randomly oriented. The household wrench contains an enormous number of domains, and their magnetic fields cancel, so the wrench as a whole is not magnetized. (Actually, the orientation of domains is not completely random; within a given crystal, there may be some preferential alignment along the crystal axes. But there will be just as many domains pointing one way as the other, so there is still no large-scale magnetization. Moreover, the crystals themselves are randomly oriented within any sizable chunk of metal.) How, then, would you produce a permanent magnet, such as they sell in toy stores? If you put a piece of iron into a strong magnetic field, the torque \\vec{N} = \\vec{m} \\cross \\vec{B} tends to align the dipoles parallel to the field. Since they like to stay parallel to their neighbors, most of the dipoles will resist this torque. However, at the boundary between two domains, there are competing neighbors, and the torque will throw its weight on the side of the domain most nearly parallel to the field; this domain will win some converts, at the expense of the less favorably oriented one. The net effect of the magnetic field, then, is to move the domain boundaries. Domains parallel to the field grow, and the others shrink. If the field is strong enough, one domain takes over entirely, and the iron is said to be saturated. It turns out that this process (the shifting of domain boundaries in response to an external field) is not entirely reversible: When the field is switched off, there will be some return to randomly oriented domains, but it is far from complete; there remains a preponderance of domains in the original direction. You now have a permanent magnet. A simple way to accomplish this, in practice, is to wrap a coil of wire around the object to be magnetized (Fig. 6.27). Run a current I through the coil; this provides the external magnetic field (pointing to the left in the diagram). As you increase the current, the field increases, the domain boundaries move, and the magnetization grows. Eventually, you reach the saturation point, with all the dipoles aligned, and a further increase in current has no effect on M (Fig. 6.28, point b). Now suppose you reduce the current. Instead of retracing the path back to \\vec{M} = 0 , there is only a partial return to randomly oriented domains; M decreases, but even with the current off there is some residual magnetization (point c). The wrench is now a permanent magnet. If you want to eliminate the remaining magnetization, you'll have to run a current backwards through the coil (a negative I ). Now the external field points to the right, and as you increase I (negatively), M drops down to zero (point d). If you turn I still higher, you soon reach saturation in the other direction-all the dipoles now pointing to the right (e). At this stage, switching off the current will leave the wrench with a permanent magnetization to the right (point f). To complete the story, turn I on again in the positive sense: M returns to zero (point g), and eventually to the forward saturation point (b). The path we have traced out is called a hysteresis loop. Notice that the magnetization of the wrench depends not only on the applied field (that is, on/), but also on its previous magnetic \"history.\" For instance, at three different times in our experiment the current was zero (a, c, and f), yet the magnetization was different for each of them. Actually, it is customary to draw hysteresis loops as plots of B against H , rather than M against I . (If our coil is approximated by a long solenoid, with n turns per unit length, then H = nI , so H and I are proportional. Meanwhile, \\vec{B} = \\mu_0 (\\vec{H} + \\vec{M}) , but in practice M is huge compared to H , so to all intents and purposes B is proportional to M .) To make the units consistent (teslas), I have plotted (\\mu_0 H) horizontally (Fig. 6.29); notice, however, that the vertical scale is 10^4 times greater than the horizontal one. Roughly speaking, \\mu_0 \\vec{H} is the field our coil would have produced in the absence of any iron; B is what we actually got, and compared to \\mu_0 \\vec{H} , it is gigantic. A little current goes a long way, when you have ferromagnetic materials around. That's why anyone who wants to make a powerful electromagnet will wrap the coil around an iron core. It doesn't take much of an external field to move the domain boundaries, and when you do that, you have all the dipoles in the iron working with you. One final point about ferromagnetism: It all follows, remember, from the fact that the dipoles within a given domain line up parallel to one another. Random thermal motions compete with this ordering, but as long as the temperature doesn't get too high, they cannot budge the dipoles out of line. It's not surprising, though, that very high temperatures do destroy the alignment. What is surprising is that this occurs at a precise temperature ( 770^\\circ C , for iron). Below this temperature (called the Curie point), iron is ferromagnetic; above, it is paramagnetic. The Curie point is rather like the boiling point or the freezing point in that there is no gradual transition from ferro- to para-magnetic behavior, any more than there is between water and ice. These abrupt changes in the properties of a substance, occurring at sharply defined temperatures, are known in statistical mechanics as phase transitions.","title":"6.4.2: Ferromagnetism"},{"location":"ch7-1/","text":"Electromotive Force 7.1.1: Ohm's Law To make a current flow, you have to push on the charges. How fast they move, in response to a given push, depends on the nature of the material. For most sub- stances, the current density \\vec{J} is proportional to the force per unit charge, \\vec{f} : \\vec{J} = \\sigma \\vec{f} \\tagl{7.1} The proportionality factor \\sigma (not to be confused with surface charge) is an empirical constant that varies from one material to another; it's called the conductivity of the medium. Actually, the handbooks usually list the reciprocal of \\sigma , called the resistivity: \\rho = 1 / \\sigma (not to be confused with charge density - I'm sorry, but we're running out of Greek letters, and this is the standard notation). Some typical values are listed in Table 7.1. Notice that even insulators conduct slightly, though the conductivity of a metal is astronomically greater; in fact, for most purposes metals can be regarded as perfect conductors, with \\sigma = \\infty , while for insulators we can pretend \\sigma = 0 . In principle, the force that drives the charges to produce the current could be anything - chemical, gravitational, or trained ants with tiny harnesses. For our purposes, though, it's usually an electromagnetic force that does the job. In this case \\eqref{7.1} becomes \\vec{J} = \\sigma (\\vec{E} + \\vec{v} \\cross \\vec{B}) \\tagl{7.2} Ordinarily, the velocity of the charges is sufficiently small that the second term can be ignored: \\vec{J} = \\sigma \\vec{E} \\tagl{7.3} (However, in plasmas, for instance, the magnetic contribution to \\vec{f} can be significant.) Equation 7.3 is called Ohm's law , thought the physics behind it is really contained in Eq. 7.1, of which 7.3 is just a special case. Remember back in chapter 2 when we talked about conductors and determined that \\vec{E} = 0 inside a conductor? Well, that was the case for stationary charges ( \\vec{J} = 0) . Moreover, for perfect conductors \\vec{E} = \\vec{J} / \\sigma = 0 even if current is flowing. In practice, metals are such good conductors that the electric field required to drive currents in them is negligible. Thus we routinely treat the connecting wires in electric circuits (for example) as equipotentials. Resistors , by contrast, are made of poorly conducting materials. Example 7.1 A cylindrical resistor of cross-sectional area A and length L is made from material with conductivity \\sigma . (See Fig. 7.1; as indicated, the cross section need not be circular, but I do assume it is the same all the way down.) If we stipulate that the potential is constant over each end, and the potential difference between the ends is V , what current flows? Solution As it turns out, the electric field is uniform within the wire (I'll prove this in a moment). It follows from Eq. 7.3 that the current density is also uniform, so I = J A = \\sigma E A = \\frac{\\sigma A}{L} V Example 7.2 Two long coaxial metal cylinders (radii a and b ) are separated by material of conductivity \\sigma (Fig. 7.2). If they are maintained at a potential difference V , what current flows from one to the other, in a length L ? Solution The field between the cylinders is \\vec{E} = \\frac{\\lambda}{2 \\pi \\epsilon_0 s} \\vu{s} where \\lambda is the charge per unit length on the inner cylinder. The current is therefore I = \\int \\vec{J} \\cdot \\dd \\vec{a} = \\sigma \\int \\vec{E} \\cdot \\dd \\vec{a} = \\frac{\\sigma}{\\epsilon_0} \\lambda L (The integral is over any surface enclosing the inner cylinder.) Meanwhile, the potential difference between the cylinders is V = - \\int_b ^a \\vec{E} \\cdot \\dd \\vec{l} = \\frac{\\lambda}{2 \\pi \\epsilon_0} \\ln \\left( \\frac{b}{a} \\right) so I = \\frac{2 \\pi \\sigma L}{\\ln (b / a)} V As these examples illustrate, the total current flowing from one electrode to the other is proportional to the potential difference between them: V = I R \\tagl{7.4} This, of course, is the more familiar version of Ohm's law. The constant of proportionality R is called the resistance; it's a function of the geometry of the arrangement and the conductivity of the medium between the electrodes. (In Ex. 7.1, R = L / \\sigma A ; in Ex. 7.2, R = \\ln (b /a ) / 2 \\pi \\sigma L ) Resistance is measured in ohms ( \\Omega ): an ohm is a volt per ampere. Notice that the proportionality between V and I is a direct consequence of Eq. 7.3: if you want to double V , you simply double the charge on the electrodes - that doubles \\vec{E} , which (for an ohmic material) doubles \\vec{J} , which doubles \\vec{I} . For steady currents and uniform conductivity, \\div \\vec{E} = \\frac{1}{\\sigma} \\div \\vec{J} = 0 \\tagl{7.5} and therefore the charge density is zero; any unbalanced charge resides on the surface. (We proved this long ago, for the case of stationary charges, using the fact that \\vec{E} = 0 ; evidently, it is still true when the charges are allowed to move.) It follows, in particular, that Laplace's equation holds within a homogeneous ohmic material carrying a steady current, so all the tools and tricks of Chapter 3 are available for calculating the potential. Example 7.3 I asserted that the field in exercise 7.1 is uniform. Let's prove it Solution Within the cylinder V obeys Laplace's equation. What are the boundary conditions? At the left end the potential is constant - we may as well set it equal to zero. At the right end the potential is likewise constant - call it V_0 . On the cylindrical surface, \\vec{J} \\cdot \\vu{n} = 0 , or else charge would be leaking out into the surrounding space (which we take to be nonconducting). Therefore \\vec{E} \\cdot \\vu{n} = 0 , and hence \\pdv{V}{n} = 0 . With V or its normal derivative specified on all surfaces, the potential is uniquely determined (Prob. 3.5). But it's easy to guess one potential that obeys Laplace's equation and fits these boundary conditions: V(z) = \\frac{V_0 z}{L} where z is measured along the axis. The uniqueness theorem guarantees that this is the solution. The corresponding field is \\vec{E} = - \\grad V = - \\frac{V_0}{L} \\vu{z} which is indeed uniform. Contrast the enormously more difficult problem that arises if the conducting material is removed, leaving only a metal plate at either end (Fig 7.3). Evidently in the present case charge arranges itself over the surface of the wire in just such a way as to produce a nice uniform field within. I don't suppose there is any formula in physics more familiar than Ohm's law, and yet it's not really a true law, in the sense of Coulomb's or Ampere's; rather, it is a \"rule of thumb\" that applies pretty well to many substances. You're not going to win a Nobel prize for finding an exception. In fact, when you stop to think about it, it's a little surprising that Ohm's law ever holds. After all, a given field \\vec{E} produces a force q \\vec{E} (on a charge q ), and according to Newton's second law, the charge will accelerate. But if the charges are accelerating, why doesn't the current increase with time, growing larger and larger the longer you leave the field on? Ohm's law implies, on the contrary, that a constant field produces a constant current, which suggests a constant velocity. Isn't that a contradiction to Newton's law? No, for we are forgetting the frequent collisions electrons make as they pass down the wire. It's a little like this: Suppose you're driving down a street with a stop sign at every intersection, so that, although you accelerate constantly in between, you are obliged to start all over again with each new block. Your average speed is then a constant, in spite of the fact that (save for the periodic abrupt stops) you are always accelerating. If the length of a block is \\lambda and your acceleration is a , the time it takes to go a block is t = \\sqrt{ \\frac{2 \\lambda}{a} } and hence your average velocity is v_{ave} = \\frac{1}{2} a t = \\sqrt{\\frac{\\lambda a}{2} } But wait! That's no good either! It says that the velocity is proportional to the square root of the acceleration, and therefore that the current should be proportional to the square root of the field! There's another twist to the story: In practice, the charges are already moving very fast because of their thermal energy. But the thermal velocities v_{th} have random directions, and average to zero. The drift velocity we are concerned with is a tiny extra bit (Prob. 5.20). So the time between collisions is actually much shorter than we supposed; if we assume for the sake of argument that all charges travel the same distance \\lambda between collisions, then t = \\frac{\\lambda}{v_{th}} and therefore v_{ave} = \\frac{1}{2} a t = \\frac{a \\lambda}{2 v_{th}} If there are n molecules per unit volume, and f free electrons per molecule, each with charge q and mass m , the current density is \\vec{J} = n f q \\vec{v}_{ave} = \\frac{n f q \\lambda}{2 v_{th}} \\frac{\\vec{F}}{m} = \\left( \\frac{n f \\lambda q^2}{2 m v_{th}} \\right) \\vec{E} \\tagl{7.6} We can't claim that Eq. 7.6 is an accurate formula for the conductivity, but it does have all the basic ingredients, and it correctly predicts that conductivity is proportional to the density of the moving charges, and (ordinarily), decreases with increasing temperature. As a result of all the collisions, the work done by the electrical force is converted into heat in the resistor. Since the work done per unit charge is V and the charge flowing per unit time is I , the power delivered is P = V I = I^2 R \\tagl{7.7} This is the Joule heating law . With I in amperes and R in ohms, P comes out in watts (joules per second). 7.1.2: Electromotive Force If you think about a typical electric circuit - a battery hooked up to a light bulb, say (Fig. 7.7 ) - a perplexing question arises: In practice, the current is the same all the way around the loop; why is this the case, when the only obvious driving force is inside the battery? Off hand, you might expect a large current in the battery and none at all in the lamp. Who's doing the pushing, in the rest of the circuit, and how does it happen that this push is exactly right to produce the same current in each segment? What's more, given that the charges in a typical wire move (literally) at a snail's pace (see Prob. 5.20), why doesn't it take half an hour for the current to reach the light bulb? How do all the charges know to start moving at the same instant? Answer: If the current were not the same all the way around (for instance, during the first split second after the switch is closed), then charge would be piling up somewhere, and - here's the crucial point - the electric field of this accumulating charge is in such a direction as to even out the flow. Suppose, for instance, that the current into the bend in Fig. 7.8 is greater than the current out. Then charge piles up at the \"knee,\" and this produces a field aiming away from the kink. This field opposes the current flowing in (slowing it down) and promotes the current flowing out (speeding it up) until these currents are equal, at which point there is no further accumulation of charge, and equilibrium is established. It's a beautiful system, automatically self-correcting to keep the current uniform, and it does it all so quickly that, in practice, you can safely assume the current is the same all around the circuit, even in systems that oscillate at radio frequencies. There are really two forces involved in driving current around a circuit: the source, \\vec{f}_S , which is ordinarily confined to one portion of the loop (a battery, say), and an electrostatic force, which serves to smooth out the flow and communicate the influence of the source to distant parts of the circuit: \\vec{f} = \\vec{f}_S + \\vec{E} \\tagl{7.8} The physical agency responsible for \\vec{f}_S can be many different things: in a battery it's a chemical force; in a piezoelectric crystal mechanical pressure is converted into an electrical impulse; in a thermocouple it's a temperature gradient that does the job; in a photoelectric cell it's light; and in a Van de Graaff generator the electrons are literally loaded onto a conveyor belt and swept along. Whatever the mechanism, its net effect is determined by the line integral off around the circuit: \\mathcal{E} = \\oint \\vec{f} \\cdot \\dd \\vec{l} = \\oint \\vec{f}_S \\cdot \\dd \\vec{l} \\tagl{7.9} ( Because \\oint \\vec{E} \\cdot \\dd \\vec{l} = 0 for electrostatic fields, it doesn't matter whether you use \\vec{f} or \\vec{f}_S ). \\mathcal{E} is called the electromotive force, or emf , of the circuit. It's a lousy term, since this is not a force at all - it's the integral of a force per unit charge. Some people would prefer the word electromotance , but emf is so established that I think we'd better stick with it. Within an ideal source of emf (a resistanceless battery, for instance), the net force on the charges is zero (Eq. 7.1 with \\sigma = \\infty ), so \\vec{E} = - \\vec{f}_S . The potential difference between the terminals (a and b) is therefore V = - \\int _a ^b \\vec{E} \\cdot \\dd \\vec{l} = \\int_a ^b \\vec{f}_S \\cdot \\dd \\vec{l} = \\oint \\vec{f}_S \\cdot \\dd \\vec{l} = \\mathcal{E} \\tagl{7.10} (we can extend the integral to the entire loop because \\vec{f}_s = 0 outside the source). The function of a battery, then, is to establish and maintain a voltage difference equal to the electromotive force (a 6V battery, for example, holds the positive terminal 6V above the negative terminal). The resulting electrostatic field drives current around the rest of the circuit (notice, however, that inside the battery \\vec{f}_S drives current in the direction opposite to \\vec{E} ). Because it's the line integral of \\vec{f}_S , \\mathcal{E} can be interpreted as the work done per unit charge, by the source - indeed, in some books the electromotive force is defined this way. However, as you'll soon see, there is some subtlety involved in this interpretation, so I prefer Eq. 7.9. 7.1.3: Motional emf In the last section, I listed several possible sources of electromotive force, batteries being the most familiar. But I did not mention the commonest one of all: the generator. Generators exploit motional emfs, which arise when you move a wire through a magnetic field. Figure 7.10 suggests a primitive model for a generator. In the shaded region there is a uniform magnetic field \\vec{B} , pointing into the page, and the resistor R represents whatever it is (maybe a light bulb or a toaster) we're trying to drive current through. If the entire loop is pulled to the right with speed v , the charges in segment ab experience a magnetic force whose vertical component q v B drives current around the loop, in the clockwise direction. The emf is \\mathcal{E} = \\oint \\vec{f}_{mag} \\cdot \\dd \\vec{l} = v B h \\tagl{7.11} Notice that the integral you perform to calculate \\mathcal{E} (Eq. 7.9 or 7.11) is carried out at one instance in time - take a \"snapshot\" of the loop, if you like, and work from that. Thus \\dd \\vec{l} , for the segment ab in Fig 7.10, points straight up, even though the loop is moving to the right. You can't quarrel with this - it's simply the way emf is defined - but it's important to be clear about it. In particular, although the magnetic force is responsible for establishing the emf, it is not doing any work - magnetic forces never do work. Who, then, is supplying the energy that heats the resistor? Answer: The person who's pulling on the loop. With the current flowing, the free charges in segment ab have a vertical velocity (call it \\vec{u} ) in addition to the horizontal velocity \\vec{v} they inherit from the motion of the loop. Accordingly, the magnetic force has a component q u \\vec{B} to the left. To counteract this, the person pulling on the wire must exert a force per unit charge f_{pull} = u B to the right (Fig 7.11). This force is transmitted to the charge by the structure of the wire. Meanwhile, the particle is usually moving in the direction of the resultant velocity \\vec{w} , and the distance it goes is (h / \\cos \\theta) . The work done per unit charge is therefore \\int f_{pull} \\cdot \\dd \\vec{l} = (u B) \\left( \\frac{h}{\\cos \\theta} \\right)\\sin \\theta = v B h = \\mathcal{E} ( \\sin \\theta coming from the dot product). As it turns out, then, the work done per unit charge is exactly equal to the emf, though the integrals are taken along entirely different paths (Fig. 7.12), and completely different forces are involved. To calculate the emf, you integrate around the loop at one instant, but to calculate the work done you follow a charge in its journey around the loop; f_{pull} contributes nothing to the emf, because it is perpendicular to the wire, whereas f_{mag} contributes nothing to work because it is perpendicular to the motion of the charge. There is a particularly nice way of expressing the emf generated in a moving loop. Let \\Phi be the flux of \\vec{B} through the loop: \\Phi \\equiv \\int \\vec{B} \\cdot \\dd \\vec{a} \\tagl{7.12} For the rectangular loop in Fig 7.10, \\Phi = B h x As the loop moves, the flux decreases: \\dv{\\Phi}{t} = B h \\dv{x}{t} = - B h v (The minus sign accounts for the fact that dx/dt is negative.) But this is precisely the emf (Eq. 7.11); evidently the emf generated in the loop is minus the rate of change of flux through the loop: \\mathcal{E} = - \\dv{\\Phi}{t} \\tagl{7.13} This is the flux rule for motional emf. Apart from its delightful simplicity, the flux rule has the virtue of applying to non-rectangular loops moving in arbitrary directions through nonuniform magnetic fields; in fact, the loop need not even maintain a fixed shape. Proof Proof. Figure 7.13 shows a loop of wire at time t , and also a short time \\dd t later. Suppose we compute the flux at time t , using surface S , and the flux at time t + \\dd t , using the surface consisting of S plus the \"ribbon\" that connects the new position of the loop to the old. The change in flux, then, is \\dd \\Phi = \\Phi(t + \\dd t) - \\Phi(t) = \\Phi_{ribbon} = \\int_{ribbon} \\vec{B} \\cdot \\dd \\vec{a} Focus your attention on point P : in time \\dd t it moves to P' . Let \\vec{v} be the velocity of the wire, and \\vec{u} the velocity of a charge down the wire; \\vec{w} = \\vec{v} + \\vec{u} is the resultant velocity of a charge at P . The infinitesimal element of area on the ribbon can be written as \\dd \\vec{a} = ( \\vec{v} \\cross \\dd \\vec{l} ) \\dd t (see inset in Fig 7.13). Therefore \\dv{\\Phi}{t} = \\oint \\vec{B} \\cdot ( \\vec{v} \\cross \\dd \\vec{l}) Since \\vec{w} = ( \\vec{v} + \\vec{u}) and \\vec{u} is parallel to \\dd \\vec{l} , we can just as well write \\dv{\\Phi}{t} = \\oint \\vec{B} \\cdot ( \\vec{w} \\cross \\dd \\vec{l}) Now, the scalar triple product can be rewritten: \\vec{B} \\cdot ( \\vec{w} \\cross \\dd \\vec{l}) = - ( \\vec{w} \\cross \\vec{B}) \\cdot \\dd \\vec{l} so \\dv{\\Phi}{t} = - \\oint ( \\vec{w} \\cross \\vec{B}) \\cdot \\dd \\vec{l} But (\\vec{w} \\cross \\vec{B}) is the magnetic force per unit charge, \\vec{f}_{mag} , so \\dv{\\Phi}{t} = - \\oint \\vec{f}_{mag} \\cdot \\dd \\vec{l} and the integral of \\vec{f}_{mag} is the emf: \\mathcal{E} = - \\dv{\\Phi}{t} There is a sign ambiguity in the definition of emf (Eq. 7.9): Which way around the loop are you supposed to integrate? There is a compensatory ambiguity in the definition of flux (Eq. 7.12): Which is the positive direction for \\dd \\vec{a} ? In applying the flux rule, sign consistency is governed (as always) by your right hand: If your fingers define the positive direction around the loop, then your thumb indicates the direction of \\dd \\vec{a} . Should the emf come out negative, it means the current will flow in in the negative direction around the circuit. The flux rule is a nifty short-cut for calculating motional emfs. It does not contain any new physics - just the Lorentz force law. But it can lead to error or ambiguity if you're not careful. The flux rule assumes you have a single wire loop - it can move, rotate, stretch, or distort (continuously), but beware of switches, sliding contacts, or extended conductors allowing a variety of current paths. A standard \"flux rule paradox\" involves the circuit in Figure 7.14. When the switch is thrown (from a to b) the flux through the circuit doubles, but there's no motional emf (no conductor moving through a magnetic field), and the ammeter (A) records no current. Example 7.4 A metal disk of radius a rotates with angular velocity \\omega about a vertical axis, through a uniform field \\vec{B} , pointing up. A circuit is made by connecting one end of a resistor to the axle and the other end to a sliding contact, which touches the outer edge of the disk (Fig 7.15). Find the current in the resistor Solution The speed of a point on the disk at a distance s from the axis is v = \\omega s , so the force per unit charge is \\vec{f}_{mag} = \\vec{v} \\cross \\vec{B} = \\omega s B \\vu{s} . The emf is therefore \\mathcal{E} = \\int _0 ^a f_{mag} \\dd s = \\omega B \\int_0 ^a s \\dd s = \\frac{\\omega B a^2}{2} and the current is I = \\frac{\\mathcal{E}}{R} = \\frac{\\omega B a^2}{2 R} Example 7.4 (the Faraday disk , or Faraday dynamo ) involves a motional emf that you can't calculate (at least, not directly) from the flux rule. The flux rule assumes the current flows along a well-defined path, whereas in this example the current spreads out over the whole disk. It's not even clear what the \"flux through the circuit\" would mean in this context. Even more tricky is the case of eddy currents. Take a chunk of aluminum (say), and shake it around in a nonuniform magnetic field. Currents will be generated in the material, and you will feel a kind of \"viscous drag\" - as though you were pulling the block through molasses (this is the force I called \\vec{f}_{pull} in the discussion of motional emf). Eddy currents are notoriously difficult to calculate, but easy and dramatic to demonstrate. You may have witnessed the classic experiment in which an aluminum disk mounted as a pendulum on a horizontal axis swings down and passes between the poles of a magnet (Fig. 7.16a). When it enters the field region it suddenly slows way down. To confirm that eddy currents are responsible, one repeats the demonstration using a disk that has many slots cut in it, to prevent the flow of large-scale currents (Fig. 7.16b). This time the disk swings freely, unimpeded by the field.","title":"7.1 - Electromotive Force"},{"location":"ch7-1/#electromotive-force","text":"","title":"Electromotive Force"},{"location":"ch7-1/#711-ohms-law","text":"To make a current flow, you have to push on the charges. How fast they move, in response to a given push, depends on the nature of the material. For most sub- stances, the current density \\vec{J} is proportional to the force per unit charge, \\vec{f} : \\vec{J} = \\sigma \\vec{f} \\tagl{7.1} The proportionality factor \\sigma (not to be confused with surface charge) is an empirical constant that varies from one material to another; it's called the conductivity of the medium. Actually, the handbooks usually list the reciprocal of \\sigma , called the resistivity: \\rho = 1 / \\sigma (not to be confused with charge density - I'm sorry, but we're running out of Greek letters, and this is the standard notation). Some typical values are listed in Table 7.1. Notice that even insulators conduct slightly, though the conductivity of a metal is astronomically greater; in fact, for most purposes metals can be regarded as perfect conductors, with \\sigma = \\infty , while for insulators we can pretend \\sigma = 0 . In principle, the force that drives the charges to produce the current could be anything - chemical, gravitational, or trained ants with tiny harnesses. For our purposes, though, it's usually an electromagnetic force that does the job. In this case \\eqref{7.1} becomes \\vec{J} = \\sigma (\\vec{E} + \\vec{v} \\cross \\vec{B}) \\tagl{7.2} Ordinarily, the velocity of the charges is sufficiently small that the second term can be ignored: \\vec{J} = \\sigma \\vec{E} \\tagl{7.3} (However, in plasmas, for instance, the magnetic contribution to \\vec{f} can be significant.) Equation 7.3 is called Ohm's law , thought the physics behind it is really contained in Eq. 7.1, of which 7.3 is just a special case. Remember back in chapter 2 when we talked about conductors and determined that \\vec{E} = 0 inside a conductor? Well, that was the case for stationary charges ( \\vec{J} = 0) . Moreover, for perfect conductors \\vec{E} = \\vec{J} / \\sigma = 0 even if current is flowing. In practice, metals are such good conductors that the electric field required to drive currents in them is negligible. Thus we routinely treat the connecting wires in electric circuits (for example) as equipotentials. Resistors , by contrast, are made of poorly conducting materials.","title":"7.1.1: Ohm's Law"},{"location":"ch7-1/#example-71","text":"A cylindrical resistor of cross-sectional area A and length L is made from material with conductivity \\sigma . (See Fig. 7.1; as indicated, the cross section need not be circular, but I do assume it is the same all the way down.) If we stipulate that the potential is constant over each end, and the potential difference between the ends is V , what current flows? Solution As it turns out, the electric field is uniform within the wire (I'll prove this in a moment). It follows from Eq. 7.3 that the current density is also uniform, so I = J A = \\sigma E A = \\frac{\\sigma A}{L} V","title":"Example 7.1"},{"location":"ch7-1/#example-72","text":"Two long coaxial metal cylinders (radii a and b ) are separated by material of conductivity \\sigma (Fig. 7.2). If they are maintained at a potential difference V , what current flows from one to the other, in a length L ? Solution The field between the cylinders is \\vec{E} = \\frac{\\lambda}{2 \\pi \\epsilon_0 s} \\vu{s} where \\lambda is the charge per unit length on the inner cylinder. The current is therefore I = \\int \\vec{J} \\cdot \\dd \\vec{a} = \\sigma \\int \\vec{E} \\cdot \\dd \\vec{a} = \\frac{\\sigma}{\\epsilon_0} \\lambda L (The integral is over any surface enclosing the inner cylinder.) Meanwhile, the potential difference between the cylinders is V = - \\int_b ^a \\vec{E} \\cdot \\dd \\vec{l} = \\frac{\\lambda}{2 \\pi \\epsilon_0} \\ln \\left( \\frac{b}{a} \\right) so I = \\frac{2 \\pi \\sigma L}{\\ln (b / a)} V As these examples illustrate, the total current flowing from one electrode to the other is proportional to the potential difference between them: V = I R \\tagl{7.4} This, of course, is the more familiar version of Ohm's law. The constant of proportionality R is called the resistance; it's a function of the geometry of the arrangement and the conductivity of the medium between the electrodes. (In Ex. 7.1, R = L / \\sigma A ; in Ex. 7.2, R = \\ln (b /a ) / 2 \\pi \\sigma L ) Resistance is measured in ohms ( \\Omega ): an ohm is a volt per ampere. Notice that the proportionality between V and I is a direct consequence of Eq. 7.3: if you want to double V , you simply double the charge on the electrodes - that doubles \\vec{E} , which (for an ohmic material) doubles \\vec{J} , which doubles \\vec{I} . For steady currents and uniform conductivity, \\div \\vec{E} = \\frac{1}{\\sigma} \\div \\vec{J} = 0 \\tagl{7.5} and therefore the charge density is zero; any unbalanced charge resides on the surface. (We proved this long ago, for the case of stationary charges, using the fact that \\vec{E} = 0 ; evidently, it is still true when the charges are allowed to move.) It follows, in particular, that Laplace's equation holds within a homogeneous ohmic material carrying a steady current, so all the tools and tricks of Chapter 3 are available for calculating the potential.","title":"Example 7.2"},{"location":"ch7-1/#example-73","text":"I asserted that the field in exercise 7.1 is uniform. Let's prove it Solution Within the cylinder V obeys Laplace's equation. What are the boundary conditions? At the left end the potential is constant - we may as well set it equal to zero. At the right end the potential is likewise constant - call it V_0 . On the cylindrical surface, \\vec{J} \\cdot \\vu{n} = 0 , or else charge would be leaking out into the surrounding space (which we take to be nonconducting). Therefore \\vec{E} \\cdot \\vu{n} = 0 , and hence \\pdv{V}{n} = 0 . With V or its normal derivative specified on all surfaces, the potential is uniquely determined (Prob. 3.5). But it's easy to guess one potential that obeys Laplace's equation and fits these boundary conditions: V(z) = \\frac{V_0 z}{L} where z is measured along the axis. The uniqueness theorem guarantees that this is the solution. The corresponding field is \\vec{E} = - \\grad V = - \\frac{V_0}{L} \\vu{z} which is indeed uniform. Contrast the enormously more difficult problem that arises if the conducting material is removed, leaving only a metal plate at either end (Fig 7.3). Evidently in the present case charge arranges itself over the surface of the wire in just such a way as to produce a nice uniform field within. I don't suppose there is any formula in physics more familiar than Ohm's law, and yet it's not really a true law, in the sense of Coulomb's or Ampere's; rather, it is a \"rule of thumb\" that applies pretty well to many substances. You're not going to win a Nobel prize for finding an exception. In fact, when you stop to think about it, it's a little surprising that Ohm's law ever holds. After all, a given field \\vec{E} produces a force q \\vec{E} (on a charge q ), and according to Newton's second law, the charge will accelerate. But if the charges are accelerating, why doesn't the current increase with time, growing larger and larger the longer you leave the field on? Ohm's law implies, on the contrary, that a constant field produces a constant current, which suggests a constant velocity. Isn't that a contradiction to Newton's law? No, for we are forgetting the frequent collisions electrons make as they pass down the wire. It's a little like this: Suppose you're driving down a street with a stop sign at every intersection, so that, although you accelerate constantly in between, you are obliged to start all over again with each new block. Your average speed is then a constant, in spite of the fact that (save for the periodic abrupt stops) you are always accelerating. If the length of a block is \\lambda and your acceleration is a , the time it takes to go a block is t = \\sqrt{ \\frac{2 \\lambda}{a} } and hence your average velocity is v_{ave} = \\frac{1}{2} a t = \\sqrt{\\frac{\\lambda a}{2} } But wait! That's no good either! It says that the velocity is proportional to the square root of the acceleration, and therefore that the current should be proportional to the square root of the field! There's another twist to the story: In practice, the charges are already moving very fast because of their thermal energy. But the thermal velocities v_{th} have random directions, and average to zero. The drift velocity we are concerned with is a tiny extra bit (Prob. 5.20). So the time between collisions is actually much shorter than we supposed; if we assume for the sake of argument that all charges travel the same distance \\lambda between collisions, then t = \\frac{\\lambda}{v_{th}} and therefore v_{ave} = \\frac{1}{2} a t = \\frac{a \\lambda}{2 v_{th}} If there are n molecules per unit volume, and f free electrons per molecule, each with charge q and mass m , the current density is \\vec{J} = n f q \\vec{v}_{ave} = \\frac{n f q \\lambda}{2 v_{th}} \\frac{\\vec{F}}{m} = \\left( \\frac{n f \\lambda q^2}{2 m v_{th}} \\right) \\vec{E} \\tagl{7.6} We can't claim that Eq. 7.6 is an accurate formula for the conductivity, but it does have all the basic ingredients, and it correctly predicts that conductivity is proportional to the density of the moving charges, and (ordinarily), decreases with increasing temperature. As a result of all the collisions, the work done by the electrical force is converted into heat in the resistor. Since the work done per unit charge is V and the charge flowing per unit time is I , the power delivered is P = V I = I^2 R \\tagl{7.7} This is the Joule heating law . With I in amperes and R in ohms, P comes out in watts (joules per second).","title":"Example 7.3"},{"location":"ch7-1/#712-electromotive-force","text":"If you think about a typical electric circuit - a battery hooked up to a light bulb, say (Fig. 7.7 ) - a perplexing question arises: In practice, the current is the same all the way around the loop; why is this the case, when the only obvious driving force is inside the battery? Off hand, you might expect a large current in the battery and none at all in the lamp. Who's doing the pushing, in the rest of the circuit, and how does it happen that this push is exactly right to produce the same current in each segment? What's more, given that the charges in a typical wire move (literally) at a snail's pace (see Prob. 5.20), why doesn't it take half an hour for the current to reach the light bulb? How do all the charges know to start moving at the same instant? Answer: If the current were not the same all the way around (for instance, during the first split second after the switch is closed), then charge would be piling up somewhere, and - here's the crucial point - the electric field of this accumulating charge is in such a direction as to even out the flow. Suppose, for instance, that the current into the bend in Fig. 7.8 is greater than the current out. Then charge piles up at the \"knee,\" and this produces a field aiming away from the kink. This field opposes the current flowing in (slowing it down) and promotes the current flowing out (speeding it up) until these currents are equal, at which point there is no further accumulation of charge, and equilibrium is established. It's a beautiful system, automatically self-correcting to keep the current uniform, and it does it all so quickly that, in practice, you can safely assume the current is the same all around the circuit, even in systems that oscillate at radio frequencies. There are really two forces involved in driving current around a circuit: the source, \\vec{f}_S , which is ordinarily confined to one portion of the loop (a battery, say), and an electrostatic force, which serves to smooth out the flow and communicate the influence of the source to distant parts of the circuit: \\vec{f} = \\vec{f}_S + \\vec{E} \\tagl{7.8} The physical agency responsible for \\vec{f}_S can be many different things: in a battery it's a chemical force; in a piezoelectric crystal mechanical pressure is converted into an electrical impulse; in a thermocouple it's a temperature gradient that does the job; in a photoelectric cell it's light; and in a Van de Graaff generator the electrons are literally loaded onto a conveyor belt and swept along. Whatever the mechanism, its net effect is determined by the line integral off around the circuit: \\mathcal{E} = \\oint \\vec{f} \\cdot \\dd \\vec{l} = \\oint \\vec{f}_S \\cdot \\dd \\vec{l} \\tagl{7.9} ( Because \\oint \\vec{E} \\cdot \\dd \\vec{l} = 0 for electrostatic fields, it doesn't matter whether you use \\vec{f} or \\vec{f}_S ). \\mathcal{E} is called the electromotive force, or emf , of the circuit. It's a lousy term, since this is not a force at all - it's the integral of a force per unit charge. Some people would prefer the word electromotance , but emf is so established that I think we'd better stick with it. Within an ideal source of emf (a resistanceless battery, for instance), the net force on the charges is zero (Eq. 7.1 with \\sigma = \\infty ), so \\vec{E} = - \\vec{f}_S . The potential difference between the terminals (a and b) is therefore V = - \\int _a ^b \\vec{E} \\cdot \\dd \\vec{l} = \\int_a ^b \\vec{f}_S \\cdot \\dd \\vec{l} = \\oint \\vec{f}_S \\cdot \\dd \\vec{l} = \\mathcal{E} \\tagl{7.10} (we can extend the integral to the entire loop because \\vec{f}_s = 0 outside the source). The function of a battery, then, is to establish and maintain a voltage difference equal to the electromotive force (a 6V battery, for example, holds the positive terminal 6V above the negative terminal). The resulting electrostatic field drives current around the rest of the circuit (notice, however, that inside the battery \\vec{f}_S drives current in the direction opposite to \\vec{E} ). Because it's the line integral of \\vec{f}_S , \\mathcal{E} can be interpreted as the work done per unit charge, by the source - indeed, in some books the electromotive force is defined this way. However, as you'll soon see, there is some subtlety involved in this interpretation, so I prefer Eq. 7.9.","title":"7.1.2: Electromotive Force"},{"location":"ch7-1/#713-motional-emf","text":"In the last section, I listed several possible sources of electromotive force, batteries being the most familiar. But I did not mention the commonest one of all: the generator. Generators exploit motional emfs, which arise when you move a wire through a magnetic field. Figure 7.10 suggests a primitive model for a generator. In the shaded region there is a uniform magnetic field \\vec{B} , pointing into the page, and the resistor R represents whatever it is (maybe a light bulb or a toaster) we're trying to drive current through. If the entire loop is pulled to the right with speed v , the charges in segment ab experience a magnetic force whose vertical component q v B drives current around the loop, in the clockwise direction. The emf is \\mathcal{E} = \\oint \\vec{f}_{mag} \\cdot \\dd \\vec{l} = v B h \\tagl{7.11} Notice that the integral you perform to calculate \\mathcal{E} (Eq. 7.9 or 7.11) is carried out at one instance in time - take a \"snapshot\" of the loop, if you like, and work from that. Thus \\dd \\vec{l} , for the segment ab in Fig 7.10, points straight up, even though the loop is moving to the right. You can't quarrel with this - it's simply the way emf is defined - but it's important to be clear about it. In particular, although the magnetic force is responsible for establishing the emf, it is not doing any work - magnetic forces never do work. Who, then, is supplying the energy that heats the resistor? Answer: The person who's pulling on the loop. With the current flowing, the free charges in segment ab have a vertical velocity (call it \\vec{u} ) in addition to the horizontal velocity \\vec{v} they inherit from the motion of the loop. Accordingly, the magnetic force has a component q u \\vec{B} to the left. To counteract this, the person pulling on the wire must exert a force per unit charge f_{pull} = u B to the right (Fig 7.11). This force is transmitted to the charge by the structure of the wire. Meanwhile, the particle is usually moving in the direction of the resultant velocity \\vec{w} , and the distance it goes is (h / \\cos \\theta) . The work done per unit charge is therefore \\int f_{pull} \\cdot \\dd \\vec{l} = (u B) \\left( \\frac{h}{\\cos \\theta} \\right)\\sin \\theta = v B h = \\mathcal{E} ( \\sin \\theta coming from the dot product). As it turns out, then, the work done per unit charge is exactly equal to the emf, though the integrals are taken along entirely different paths (Fig. 7.12), and completely different forces are involved. To calculate the emf, you integrate around the loop at one instant, but to calculate the work done you follow a charge in its journey around the loop; f_{pull} contributes nothing to the emf, because it is perpendicular to the wire, whereas f_{mag} contributes nothing to work because it is perpendicular to the motion of the charge. There is a particularly nice way of expressing the emf generated in a moving loop. Let \\Phi be the flux of \\vec{B} through the loop: \\Phi \\equiv \\int \\vec{B} \\cdot \\dd \\vec{a} \\tagl{7.12} For the rectangular loop in Fig 7.10, \\Phi = B h x As the loop moves, the flux decreases: \\dv{\\Phi}{t} = B h \\dv{x}{t} = - B h v (The minus sign accounts for the fact that dx/dt is negative.) But this is precisely the emf (Eq. 7.11); evidently the emf generated in the loop is minus the rate of change of flux through the loop: \\mathcal{E} = - \\dv{\\Phi}{t} \\tagl{7.13} This is the flux rule for motional emf. Apart from its delightful simplicity, the flux rule has the virtue of applying to non-rectangular loops moving in arbitrary directions through nonuniform magnetic fields; in fact, the loop need not even maintain a fixed shape. Proof Proof. Figure 7.13 shows a loop of wire at time t , and also a short time \\dd t later. Suppose we compute the flux at time t , using surface S , and the flux at time t + \\dd t , using the surface consisting of S plus the \"ribbon\" that connects the new position of the loop to the old. The change in flux, then, is \\dd \\Phi = \\Phi(t + \\dd t) - \\Phi(t) = \\Phi_{ribbon} = \\int_{ribbon} \\vec{B} \\cdot \\dd \\vec{a} Focus your attention on point P : in time \\dd t it moves to P' . Let \\vec{v} be the velocity of the wire, and \\vec{u} the velocity of a charge down the wire; \\vec{w} = \\vec{v} + \\vec{u} is the resultant velocity of a charge at P . The infinitesimal element of area on the ribbon can be written as \\dd \\vec{a} = ( \\vec{v} \\cross \\dd \\vec{l} ) \\dd t (see inset in Fig 7.13). Therefore \\dv{\\Phi}{t} = \\oint \\vec{B} \\cdot ( \\vec{v} \\cross \\dd \\vec{l}) Since \\vec{w} = ( \\vec{v} + \\vec{u}) and \\vec{u} is parallel to \\dd \\vec{l} , we can just as well write \\dv{\\Phi}{t} = \\oint \\vec{B} \\cdot ( \\vec{w} \\cross \\dd \\vec{l}) Now, the scalar triple product can be rewritten: \\vec{B} \\cdot ( \\vec{w} \\cross \\dd \\vec{l}) = - ( \\vec{w} \\cross \\vec{B}) \\cdot \\dd \\vec{l} so \\dv{\\Phi}{t} = - \\oint ( \\vec{w} \\cross \\vec{B}) \\cdot \\dd \\vec{l} But (\\vec{w} \\cross \\vec{B}) is the magnetic force per unit charge, \\vec{f}_{mag} , so \\dv{\\Phi}{t} = - \\oint \\vec{f}_{mag} \\cdot \\dd \\vec{l} and the integral of \\vec{f}_{mag} is the emf: \\mathcal{E} = - \\dv{\\Phi}{t} There is a sign ambiguity in the definition of emf (Eq. 7.9): Which way around the loop are you supposed to integrate? There is a compensatory ambiguity in the definition of flux (Eq. 7.12): Which is the positive direction for \\dd \\vec{a} ? In applying the flux rule, sign consistency is governed (as always) by your right hand: If your fingers define the positive direction around the loop, then your thumb indicates the direction of \\dd \\vec{a} . Should the emf come out negative, it means the current will flow in in the negative direction around the circuit. The flux rule is a nifty short-cut for calculating motional emfs. It does not contain any new physics - just the Lorentz force law. But it can lead to error or ambiguity if you're not careful. The flux rule assumes you have a single wire loop - it can move, rotate, stretch, or distort (continuously), but beware of switches, sliding contacts, or extended conductors allowing a variety of current paths. A standard \"flux rule paradox\" involves the circuit in Figure 7.14. When the switch is thrown (from a to b) the flux through the circuit doubles, but there's no motional emf (no conductor moving through a magnetic field), and the ammeter (A) records no current.","title":"7.1.3: Motional emf"},{"location":"ch7-1/#example-74","text":"A metal disk of radius a rotates with angular velocity \\omega about a vertical axis, through a uniform field \\vec{B} , pointing up. A circuit is made by connecting one end of a resistor to the axle and the other end to a sliding contact, which touches the outer edge of the disk (Fig 7.15). Find the current in the resistor Solution The speed of a point on the disk at a distance s from the axis is v = \\omega s , so the force per unit charge is \\vec{f}_{mag} = \\vec{v} \\cross \\vec{B} = \\omega s B \\vu{s} . The emf is therefore \\mathcal{E} = \\int _0 ^a f_{mag} \\dd s = \\omega B \\int_0 ^a s \\dd s = \\frac{\\omega B a^2}{2} and the current is I = \\frac{\\mathcal{E}}{R} = \\frac{\\omega B a^2}{2 R} Example 7.4 (the Faraday disk , or Faraday dynamo ) involves a motional emf that you can't calculate (at least, not directly) from the flux rule. The flux rule assumes the current flows along a well-defined path, whereas in this example the current spreads out over the whole disk. It's not even clear what the \"flux through the circuit\" would mean in this context. Even more tricky is the case of eddy currents. Take a chunk of aluminum (say), and shake it around in a nonuniform magnetic field. Currents will be generated in the material, and you will feel a kind of \"viscous drag\" - as though you were pulling the block through molasses (this is the force I called \\vec{f}_{pull} in the discussion of motional emf). Eddy currents are notoriously difficult to calculate, but easy and dramatic to demonstrate. You may have witnessed the classic experiment in which an aluminum disk mounted as a pendulum on a horizontal axis swings down and passes between the poles of a magnet (Fig. 7.16a). When it enters the field region it suddenly slows way down. To confirm that eddy currents are responsible, one repeats the demonstration using a disk that has many slots cut in it, to prevent the flow of large-scale currents (Fig. 7.16b). This time the disk swings freely, unimpeded by the field.","title":"Example 7.4"},{"location":"ch7-2/","text":"7.2: Electromagnetic Induction 7.2.1: Faraday's Law In 1831 Michael Faraday reported on a series of experiments, including three that (with some violence to history) can be characterized as follows: Experiment 1: . He pulled a loop of wire to the right through a magnetic field (Fig 7.21a). A current flowed in the loop. Experiment 2: He moved the magnet to the left, holding the loop still (Fig 7.21b). Again, a current flowed in the loop. Experiment 3: With both the loop and the magnet at rest (Fig 7.21c), he changed the strength of the field (he used an electromagnet, and varied the current in the coil). Once again, current flowed in the loop. The first experiment, of course, is a straightforward case of motional emf; according to the flux rule: \\mathcal{E} = - \\dv{\\Phi}{t} I don't think it will surprise you to learn that exactly the same emf arises in Experiment 2 - all that really matters is the relative motion of the magnet and the loop. Indeed, in the light of special relativity it has to be so. But Faraday knew nothing of relativity, and in classical electrodynamics this simple reciprocity is a remarkable coincidence. For if the loop moves, it's a magnetic force that sets up the emf, but if the loop is stationary, the force cannot be magnetic - stationary charges experience no magnetic forces. In that case, what is responsible? What sort of field exerts a force on charges at rest? Well, electric fields do, of course, but in this case there doesn't seem to be any electric field in sight. Faraday had an ingenious inspiration: \\textbf{A changing magnetic field induces an electric field} It is this induced electric field that accounts for the emf in Experiment 2. Indeed, if (as Faraday found empirically) the emf is again equal to the rate of change of the flux, \\mathcal{E} = \\oint \\vec{E} \\cdot \\dd \\vec{l} = - \\dv{\\Phi}{t} \\tagl{7.14} then \\vec{E} is related to the change in \\vec{B} by the equation \\oint \\vec{E} \\cdot \\dd \\vec{l} = - \\int \\pdv{\\vec{B}}{t} \\cdot \\dd \\vec{a} \\tagl{7.15} This is Faraday's law , in integral form. We can convert it to differential form by applying Stokes' theorem: \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} \\tagl{7.16} Note that Faraday's law reduces to the old rule \\oint \\vec{E} \\cdot \\dd \\vec{l} = 0 (or, in differential form, \\curl \\vec{E} = 0 ) in the static case (constant \\vec{B} ), as, of course, it should. In Experiment 3, the magnetic field changes for entirely different reasons, but according to Faraday's law an electric field will again be induced, giving rise to an emf - d \\Phi / dt . Indeed, one can subsume all three cases (and for that matter any combination of them) into a kind of universal flux rule : Whenever (and for whatever reason) the magnetic flux through a loop changes, an emf \\mathcal{E} = - \\pdv{\\Phi}{t} \\tagl{7.17} will appear in the loop. Many people call this \"Faraday's law.\" Maybe I'm overly fastidious, but I find this confusing. There are really two totally different mechanisms underlying Eq. 7.17, and to identify them both as \"Faraday's law\" is a little like saying that because identical twins look alike we ought to call them by the same name. In Faraday's first experiment, it's the Lorentz force law at work; the emf is magnetic. But in the other two it's an electric field (induced by the changing magnetic field) that does the job. Viewed in this light, it is quite astonishing that all three processes yield the same formula for the emf. In fact, it was precisely this \"coincidence\" that led Einstein to the special theory of relativity - he sought a deeper understanding of what is, in classical electrodynamics, a peculiar accident. But that's a story for chapter 12. In the meantime, I shall reserve the term \"Faraday's law\" for electric fields induced by changing magnetic fields, and I do not regard Experiment 1 as an instance of Faraday's law. Example 7.5 A long cylindrical magnet of length L and radius a carries a uniform magnetization \\vec{M} parallel to its axis. It passes at constant velocity \\vec{v} through a circular wire ring of slightly larger diameter (Fig. 7.22). Graph the emf induced in the ring, as a function of time. Solution The magnetic field is the same as that of a long solenoid with surface current \\vec{K}_b = M \\vu{\\phi} . So the field inside is \\vec{B} = \\mu_0 \\vec{M} , except near the ends, where it starts to spread out. The flux through the ring is zero when the magnet is far away; it builds up to a maximum of \\mu_0 M \\pi a^2 as the leading end passes through; and it drops back to zero as the trailing end emerges (Fig. 7.23a). The emf is (minus) the derivative of \\Phi with respect to time, so it consists of two spikes, as shown in Fig. 7.23b. Keeping track of the signs in Faraday's law can be a real headache. For instance, in Ex. 7.5 we would like to know which way around the ring the induced current flows. In principle, the right-hand rule does the job (we called \\Phi positive to the left, in Fig. 7.22, so the positive direction for current in the ring is counter-clockwise, as viewed from the left; since the first spike in Fig. 7.23b is negative, the first current pulse flows clockwise, and the second counterclockwise). But there's a handy rule, called Lenz's law, whose sole purpose is to help you get the directions right: Nature abhors a change in flux The induced current will flow in such a direction that the flux it produces tends to cancel the change. (As the front end of the magnet in Ex. 7.5 enters the ring, the flux increases, so the current in the ring must generate a field to the right - it therefore flows clockwise.) Notice that it is the change in flux, not the flux itself, that nature abhors (when the tail end of the magnet exits the ring, the flux drops, so the induced current flows counterclockwise, in an effort to restore it). Faraday induction is a kind of \"inertial\" phenomenon: A conducting loop \"likes\" to maintain a constant flux through it; if you try to change the flux, the loop responds by sending a current around in such a direction as to frustrate your efforts. (It doesn't succeed completely; the flux produced by the induced current is typically only a tiny fraction of the original. All Lenz's law tells you is the direction of the flow.) Example 7.6 The 'jumping ring' demonstration. If you wind a solenoidal coil around an iron core (the iron is there to beef up the magnetic field), place a metal ring on top, and plug it in, the ring will jump several feet in the air (Fig 7.24). Why? Solution Before you turn on the current, the flux through the ring was zero. Afterward a flux appeared (upward in the diagram), and the emf generated in the ring led to a current (in the ring) which, according to Lenz's law, was in such a direction that its field tended to cancel this new flux. This means that the current in the loop is opposite to the current in the solenoid. As opposite currents repel (as we saw in our Biot-Savart calculations in the last chapter), the ring flies off. 7.2.2: The Induced Electric Field Faraday's law generalizes the electrostatic rule \\curl \\vec{E} = 0 to the time-dependent regime. The divergence of \\vec{E} is still given by Gauss's law ( \\div \\vec{E} = \\frac{1}{\\epsilon_0} \\rho ). If \\vec{E} is a pure Faraday field (due exclusively to a changing \\vec{B} , with \\rho = 0 ), then \\div \\vec{E} = 0 \\qquad \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} This is mathematically identical to magnetostatics \\div \\vec{B} = 0 \\qquad \\curl \\vec{B} = \\mu_0 \\vec{J} Conclusion: Faraday-induced electric fields are determined by -(\\partial \\vec{B} / \\partial t) in exactly the same way as magnetostatic fields are determined by \\mu_0 \\vec{J} . The analog to Biot-Savart is \\vec{E} = - \\frac{1}{4 \\pi} \\int \\frac{(\\partial \\vec{B} / \\partial t) \\cross \\vu{\\gr}}{\\gr ^2} \\dd \\tau = - \\frac{1}{4 \\pi} \\pdv{}{t} \\int \\frac{\\vec{B} \\cross \\vu{\\gr}}{\\gr ^2} \\dd \\tau \\tagl{7.18} and if symmetry permits, we can use all the tricks associated with Ampere's law in integral form ( \\oint \\vec{B} \\cdot \\dd \\vec{l} = \\mu_0 I_{enc} ), only now it's Faraday's law in integral form: \\oint \\vec{E} \\cdot \\dd \\vec{l} = - \\dv{\\Phi}{t} \\tagl{7.19} The rate of change of (magnetic) flux through the Amperian loop plays the role formerly assigned to \\mu_0 I_{enc} . Example 7.7 A uniform magnetic field \\vec{B}(t) , pointing straight up, fills the shaded circular region of Fig. 7.25. If \\vec{B} is changing with time, what is the induced electric field? Solution \\vec{E} points in the circumferential direction, just like the magnetic field inside a long straight wire carrying a uniform current density. Draw an Amperian loop of radius s , and apply Faraday's law: \\oint \\vec{E} \\cdot \\dd \\vec{l} = E ( 2 \\pi s) = - \\dv{\\Phi}{t} = - \\dv{}{t} \\left( \\pi s^2 B(t) \\right) = - \\pi s^2 \\dv{B}{t} Therefore \\vec{E} = - \\frac{s}{2} \\dv{B}{t} \\vu{\\phi} If \\vec{B} is increasing, \\vec{E} runs clockwise, as viewed from above. Example 7.8 A line charge \\lambda is glued to the rim of a wheel of radius b , which is then suspended horizontally, as shown in Fig 7.26, so that it is free to rotate (the spokes are made of some nonconducting material - wood, maybe). In the central region, out to radius a , there is a uniform magnetic field \\vec{B}_0 , pointing up. Now someone turns the field off. What happens? Solution The changing magnetic field will induce an electric field, curling around the axis of the wheel. This electric field exerts a force on the charges at the rim, and the wheel starts to turn. According to Lenz's law, it will rotate in such a direction that its field tends to restore the upward flux. The motion, then, is counterclockwise, as viewed frrom above. Faraday's law, applied to the loop at radius b , says \\oint \\vec{E} \\cdot \\dd \\vec{l} = E ( 2 \\pi b) = - \\dv{\\Phi}{t} = - \\pi ^2 \\dv{B}{t} or \\vec{E} = - \\frac{a^2}{2b} \\dv{B}{t} \\vu{\\phi} The torque on a segment of length \\dd \\vec{l} is (\\vec{r} \\cross \\vec{F}) , or b \\lambda E \\dd l . The total torque on the wheel is therefore N = b \\lambda \\left( - \\dv{\\Phi}{t} = - \\pi ^2 \\dv{B}{t} \\right) \\oint \\dd l = - b \\lambda \\pi a^2 \\dv{B}{t} and the angular momentum imparted to the wheel is \\int N \\dd t = - \\lambda \\pi a^2 b \\int_{B_0} ^0 \\dd B = \\lambda \\pi a^2 b B_0 It doesn't matter how quickly or slowly you tum off the field; the resulting angular velocity of the wheel is the same regardless. (If you find yourself wondering where the angular momentum came from, you're getting ahead of the story! Wait for the next chapter.) Note that it's the electric field that did the rotating. To convince you of this, I deliberately set things up so that the magnetic field is zero at the location of the charge. The experimenter may tell you she never put in any electric field - all she did was switch off the magnetic field. But when she did that, an electric field automatically appeared, and it's this electric field that turned the wheel. I must warn you, now, of a small fraud that tarnishes many applications of Faraday's law: Electromagnetic induction, of course, occurs only when the magnetic fields are changing, and yet we would like to use the apparatus of magnetostatics (Ampere's law, the Biot-Savart law, and the rest) to calculate those magnetic fields. Technically, any result derived in this way is only approximately correct. But in practice the error is usually negligible, unless the field fluctuates extremely rapidly, or you are interested in points very far from the source. Even the case of a wire snipped by a pair of scissors (Prob. 7.18) is static enough for Ampere's law to apply. This regime, in which magnetostatic rules can be used to calculate the magnetic field on the right hand side of Faraday's law, is called quasistatic. Generally speaking, it is only when we come to electromagnetic waves and radiation that we must worry seriously about the breakdown of magnetostatics itself. Example 7.9 An infinitely long straight wire carries a slowly varying current I(t) . Determine the induced electric field, as a function of the distance s from the wire Solution In the quasistatic approximation, the magnetic field is (\\mu_0 I / 2 \\pi s) and it circles around the wire. Like the B -field of a solenoid, E here runs parallel to the axis. For the rectangular \"Amperian loop\" in Fig 7.27, Faraday's law gives: \\begin{align*} \\oint \\vec{E} \\cdot \\dd \\vec{l} & = E (s_0) l - E(s) l \\\\ & = - \\dv{}{t} \\int \\vec{B} \\cdot \\dd \\vec{a} \\\\ & = - \\frac{\\mu_0 l }{2\\pi} \\dv{I}{t} \\int _{s_0} ^s \\frac{1}{s} \\dd s' \\\\ & = - \\frac{\\mu_0 l}{2 \\pi} \\dv{I}{t} ( \\ln \\, s - \\ln \\, s_0 ) \\end{align*} Thus \\vec{E}(s) = \\left( \\frac{\\mu_0}{2\\pi} \\dv{I}{t} \\ln \\, s + K \\right) \\vu{z} \\tagl{7.20} where K is a constant (that is to say, it is independent of s - it might still be a function of t ). The actual value of K depends on the whole history of the function I(t) - we'll see some examples in Chapter 10. Equation 7.20 has the particular implication that E blows up as s goes to infinity. That can't be true... What's gone wrong? Answer: we have overstepped the limits of the quasistatic approximation. As we shall see in Chapter 9, electromagnetic \"news\" travels at the speed of light, and at large distances B depends not on the current now , but on the current as it was at some earlier time (indeed, a whole range of earlier times, since different points on the wire are different distances away). If \\tau is the time it takes I to change substantially, then the quasistatic approximation should hold only for s \\ll c \\tau \\tagl{7.21} and hence Eq. 7.20 simply does not apply, at extremely large s . 7.2.3: Inductance Suppose you have two loops of wire, at rest (Fig 7.30). If you run a steady current I_1 around loop 1, it produces a magnetic field \\vec{B}_1 . Some of the field lines pass through loop 2; let \\Phi_2 be the flux of \\vec{B}_1 through 2. You might have a tough time actually calculating \\vec{B_1} , but a glance at the Biot-Savart law, \\vec{B}_1 = \\frac{\\mu_0}{4 \\pi} I_1 \\oint \\frac{\\dd \\vec{l}_1 \\cross \\vu{\\gr}}{\\gr^2} reveals one significant fact about this field: It is proportional to the current I_1 . Therefore, so too is the flux through loop 2: \\Phi_2 = \\int \\vec{B}_1 \\cdot \\dd \\vec{a}_2 Thus \\Phi_2 = M_{21} I_1 \\tagl{7.22} where M_{21} is the constant of proportionality; it is known as the mutual inductance of the two loops. There is a cute formula for the mutual inductance, which you can derive by expressing the flux in terms of the vector potential, and invoking Stokes' theorem: \\Phi_2 = \\int \\vec{B}_1 \\cdot \\dd \\vec{a}_2 = \\int (\\curl \\vec{A}_1) \\cdot \\dd \\vec{a}_2 = \\oint \\vec{A}_1 \\cdot \\dd \\vec{l}_2 Now, according to Eq. 5.66, \\vec{A}_1 = \\frac{\\mu_0 I_1}{4 \\pi} \\oint \\frac{\\dd \\vec{l}_1}{\\gr} and hence \\Phi_2 = \\frac{\\mu_0 I_1}{4 \\pi} \\oint \\left( \\oint \\frac{\\dd \\vec{l}_1}{\\gr} \\right) \\cdot \\dd \\vec{l}_2 Evidently M_{21} = \\frac{\\mu_0}{4 \\pi} \\oint \\oint \\frac{\\dd \\vec{l}_1 \\cdot \\dd \\vec{l}_2}{\\gr} \\tagl{7.23} This is the Neumann formula ; it involves a double line integral - one integration around loop 1, the other around loop 2 (Fig 7.31). It's not very useful for practical calculations, but it does reveal two important things about the mutual inductance: M_{21} is a purely geometrical quantity, having to do with the sizes, shapes, and relative positions of the two loops. The integral in Eq. 7.23 is unchanged if we switch the roles of loops 1 and 2; it follows that M_{21} = M_{12} \\tagl{7.24} This is an astonishing conclusion: Whatever the shapes and positions of the loops, the flux through 2 when we run a current I around 1 is identical to the flux through 1 when we send the same current I around 2. We may as well drop the subscripts and call them both M. Example 7.10 A short solenoid (length l and radius a , with n_1 turns per unit length) lies on the axis of a very long solenoid (radius b , n_2 turns per unit length) as shown in Fig 7.32. Current I flows in the short solenoid. What is the flux through the long solenoid? Solution Since the inner solenoid is short, it has a very complicated field; moreover, it puts a different flux through each turn of the outer solenoid. It would be a miserable task to compute the total flux this way. However, if we exploit the equality of the mutual inductances, the problem becomes very easy. Just look at the reverse situation: run the current I through the outer solenoid, and calculate the flux through the inner one. The field inside the long solenoid is constant B = \\mu_0 n_2 I so the flux through a single loop of the short solenoid is B \\pi a^2 = \\mu_0 n_2 I \\pi a^2 There are n_1 l turns in all, so the total flux through the inner solenoid is \\Phi = \\mu_0 \\pi a^2 n_1 n_2 I This is also the flux a current I in the short solenoid would put through the long one, which is what we set out to find. Incidentally, the mutual inductance, in this case, is M = \\mu_0 \\pi a^2 n_1 n_2 l Suppose now, that you vary the current in loop 1. The flux through loop 2 will vary accordingly, and Faraday's law says this changing flux will induce an emf in loop 2: \\mathcal{E}_2 = - \\dv{\\Phi_2}{t} = - M \\dv{I_1}{t} \\tagl{7.25} (In quoting Eq. 7.22 - which was based on the Biot-Savart law - I am tacitly assuming that the currents change slowly enough for the system to be considered quasistatic.) What a remarkable thing: Every time you change the current in loop 1, and induced current flows in loop 2 - even though there are no wires connecting them! Come to think of it, a changing current not only induces an emf in any nearby loops, it also induces an emf in the source loop itself (Fig 7.33). Once again, the field (and therefore the flux) is proportional to the current \\Phi = L I \\tagl{7.26} The constant of proportionality L is called the self inductance (or simply the inductance ) of the loop. As with M , it depends on the geometry (side and shape ) of the loop. If the current changes, the emf induced in the loop is \\mathcal{E} = - L \\dv{I}{t} \\tagl{7.27} Inductance is measured in henries (H); a henry is a volt-second per ampere. Example 7.11 Find the self inductance of a toroidal coil with rectangular cross-section (inner radius a , outer radius b , height h ), that carries a total of N turns. Solution The magnetic field inside of a toroid is B = \\frac{\\mu_0 N I}{2 \\pi s} The flux through a single turn (Fig 7.34) is \\int \\vec B \\cdot \\dd \\vec a = \\frac{\\mu_0 N I}{2 \\pi} h \\int_a ^b \\frac{1}{s} = \\frac{\\mu_0 N I h}{2 \\pi} \\ln \\frac{b}{a} The total flux is N times this, so the self-inductance (Eq. 7.26) is L = \\frac{\\mu_0 N^2 h}{2 \\pi} \\ln \\left( \\frac{b}{a} \\right) \\tagl{7.28} Inductance (like capacitance) is an intrinsically positive quantity. Lenz's law, which is enforced by the minus sign in Eq. 7.27, dictates that the emf is in such a direction as to oppose any change in current. For this reason, it is called a back emf . Whenever you try to alter the current in a wire, you must fight against this back emf. Inductance plays somewhat the same role in electric currents that mass plays in mechanical systems: The greater L , the harder it is to change the current, just as the larger the mass, the harder it is to change an object's velocity. Example 7.12 Suppose a current I is flowing around a loop, when someone suddenly cuts the wire. The current drops \"instantaneously\" to zero. This generates a whopping back emf, for although I may be small, d I / d t is enormous. (That's why you sometimes draw a spark when you unplug an iron or toaster - electromagnetic induction is desperately trying to keep the current going, even if it has to jump the gap in the circuit.) Nothing so dramatic occurs when you plug in a toaster or iron. In this case induction opposes the sudden increase in current, prescribing instead a smooth and continuous buildup. Suppose, for instance, that a battery (which supplies a constant emf \\mathcal{E}_0 ) is connected to a circuit of resistance R and inductance L (Fig. 7.35). What current flows? Solution The total emf in this circuit is \\mathcal{E}_0 from the battery plus -L (\\dv{I}{t}) from the inductance. Ohm's law says \\mathcal{E}_0 - L \\dv{I}{t} = I R This is a first-order differential equation for I as a function of time. The general solution is I(t) = \\frac{\\mathcal{E}_0}{R} + k e^{-(R/L) t} where k is a constant to be determined by the initial conditions. In particular, if you close the switch at time t = 0 , so I(0) = 0 , then k = - \\mathcal{E}_0 / R , and I(t) = \\frac{\\mathcal{E}_0}{R} \\left( 1 - e^{-(R/L) t} \\right) \\tagl{7.29} This function is plotted in Fig 7.36. Had there been no inductance in the circuit, the current would have jumped immediately to \\mathcal{E}_0 / R . In practice, every circuit has some self-inductance, and the current approaches \\mathcal{E}_0 / R asymptotically. The quantity \\tau = L / R is the time constant for an LR circuit; it tells you how long the current takes to reach a substantial fraction (1 - 1/e) of its final value. 7.2.4: Energy in Magnetic Fields It takes a certain amount of energy to start a current flowing in a circuit. I'm not talking about the energy delivered to the resistors and converted into heat - that is irretrievably lost, as far as the circuit is concerned, and can be large or small, depending on how long you let the current run. What I am concerned with, rather, is the work you must do against the back emf to get the current going. This is fixed amount, and it is recoverable: you get it back when the current is turned off. In the meantime, it represents energy latent in the circuit; as we'll see in a moment, it can be regarded as energy stored in the magnetic field. The work done on a unit charge, against the back emf, in one trip around the circuit is - \\mathcal{E} (the minus sign records the fact that this is the work done by you against the emf, not the work done by the emf). The amount of charge per unit time passing down the wire is I. So the total work done per unit time is \\dv{W}{t} = - \\mathcal{E}I = L I \\dv{I}{t} If we start with zero current and build it up to a final value I, the work done (integrating the last equation over time) is W = \\frac{1}{2} L I^2 \\tagl{7.30} So, this is the energy stored in an inductor, or in any loop that has an inductance L . It does not depend on how long we take to crank up the current, only on the geometry of the loop (in the form of L ) and the final current I . This is only really sensible for a system of conducting loops, but we can be a bit more general. We can express W by recalling that the flux \\Phi through a loop (which is LI ) is \\Phi = \\int \\vec{B} \\cdot \\dd \\vec{a} = \\int (\\curl \\vec{A}) \\cdot \\dd \\vec{a} = \\oint \\vec{A} \\cdot \\dd \\vec{l} where the line integral is around the perimeter of the loop. So, we have LI = \\oint \\vec{A} \\cdot \\dd \\vec{l} and therefore W = \\frac{1}{2} I \\oint \\vec{A} \\cdot \\dd \\vec{l} = \\frac{1}{2} \\oint (\\vec{A} \\cdot \\vec{I}) \\dd l \\tagl{7.31} We can pretty obviously generalize this to volume currents W = \\frac{1}{2} \\int _V (\\vec{A} \\cdot \\vec{J}) \\dd \\tau \\tagl{7.32} But we can do one better, expressing W entirely in terms of the magnetic field: \\curl \\vec{B} = \\mu_0 \\vec{J} lets us eliminate the current density from the picture W = \\frac{1}{2 \\mu_0} \\int \\vec{A} \\cdot (\\curl \\vec{B}) \\dd \\tau \\tagl{7.33} Integration by parts gets us to slap the derivative from B to A \\div (\\vec{A} \\cross \\vec{B}) = \\vec{B} \\cdot (\\curl \\vec{A}) - \\vec{A} \\cdot (\\curl \\vec{B}) so \\vec{A} \\cdot (\\curl \\vec{B}) = \\vec{B} \\cdot \\vec{B} - \\div (\\vec{A} \\cross \\vec{B} Consequently W = \\frac{1}{2\\mu_0} \\left( \\int B^2 \\dd \\tau - \\int \\div (\\vec{A} \\cross \\vec{B} \\dd \\tau \\right) \\\\ = \\frac{1}{2\\mu_0} \\left( \\int _V B^2 \\dd \\tau - \\oint_S (\\vec{A} \\cross \\vec{B} ) \\cdot \\dd \\vec{a} \\right) \\tagl{7.34} Now, the integration in Eq. 7.32 is to be taken over the entire volume occupied by the current. But any region larger than this will do just as well, for \\vec{J} is zero out there anyway. In Eq. 7.34, the larger the region we pick the greater is the contribution from the volume integral, and therefore the smaller is that of the surface integral (this makes sense: as the surface gets farther from the current, both A and B decrease). In particular, if we agree to integrate over all space, then the surface integral goes to zero, and we are left with W = \\frac{1}{2 \\mu_0} \\int _{\\text{all space}} B^2 \\dd \\tau \\tagl{7.35} In view of this result, we say the energy is \"stored in the magnetic field,\" in the amount (B^2 / 2 \\mu_0) per unit volume. This is a nice way to think of it, though someone looking at Eq. 7.32 might prefer to say that the energy is stored in the current distribution, in the amount \\frac{1}{2} (\\vec{A} \\cdot \\vec{J}) per unit volume. The distinction is one of bookkeeping; the important quantity is the total energy W , and we need not worry about where (if anywhere) the energy is \"located.\" You might find it strange that it takes energy to set up a magnetic field - after all, magnetic fields themselves do no work. The point is that producing a magnetic field, where previously there was none, requires changing the field, and a changing B-field, according to Faraday, induces an electric field. The latter, of course, can do work. In the beginning, there is no \\vec{E} , and at the end there is no \\vec{E} ; but in between, while \\vec{B} is building up, there is an \\vec{E} , and it is against this that the work is done. (You see why I could not calculate the energy stored in a magnetostatic field back in Chapter 5.) In the light of this, it is extraordinary how similar the magnetic energy formulas are to their electrostatic counterparts: Example 7.13 A long coaxial cable carries current I (the current flows down the surface of the inner cylinder, radius a , and back along the outer cylinder, radius b ) as shown in Fig 7.40. Find the magnetic energy stored in a section of length l Solution Ampere's law will tell us that B between the surfaces is \\vec{B} = \\frac{\\mu_0 I}{2 \\pi s } \\vu{\\phi} and outside the cable, the field is zero. Eq. 7.35 gives us the volume energy density \\frac{1}{2 \\mu_0} \\left( \\frac{\\mu_0 I}{2 \\pi s} \\right)^2 = \\frac{\\mu_0 I^2}{8 \\pi^2 s^2} The energy in a shell of length l , radius s and thickness \\dd s is \\left( \\frac{\\mu_0 I^2}{8 \\pi ^2 s^2} \\right)2 \\pi l s \\, \\dd s = \\frac{\\mu_0 I^2 l}{4 \\pi} \\left( \\frac{\\dd s}{s} \\right) Integrating from a to b , we have W = \\frac{\\mu_0 I^2 l}{4 \\pi} \\ln \\left( \\frac{b}{a} \\right) Incidentally, this suggests a very simple way to calculate the self-inductance of the cable. According to Eq. 7.30, the energy can also be written as \\frac{1}{2} L I^2 . Comparing the two expressions, L = \\frac{\\mu_0 l}{2 \\pi} \\ln \\left( \\frac{b}{a} \\right) This method of calculating the self-inductance is especially useful when the current is not confined to a single path, but spreads over some surface or volume, so that different parts of the current enclose different amounts of flux. In such cases, it can be very tricky to get the inductance directly from Eq. 7.26, and it is best to let 7.30 define L","title":"7.2 - Electromagnetic Induction"},{"location":"ch7-2/#72-electromagnetic-induction","text":"","title":"7.2: Electromagnetic Induction"},{"location":"ch7-2/#721-faradays-law","text":"In 1831 Michael Faraday reported on a series of experiments, including three that (with some violence to history) can be characterized as follows: Experiment 1: . He pulled a loop of wire to the right through a magnetic field (Fig 7.21a). A current flowed in the loop. Experiment 2: He moved the magnet to the left, holding the loop still (Fig 7.21b). Again, a current flowed in the loop. Experiment 3: With both the loop and the magnet at rest (Fig 7.21c), he changed the strength of the field (he used an electromagnet, and varied the current in the coil). Once again, current flowed in the loop. The first experiment, of course, is a straightforward case of motional emf; according to the flux rule: \\mathcal{E} = - \\dv{\\Phi}{t} I don't think it will surprise you to learn that exactly the same emf arises in Experiment 2 - all that really matters is the relative motion of the magnet and the loop. Indeed, in the light of special relativity it has to be so. But Faraday knew nothing of relativity, and in classical electrodynamics this simple reciprocity is a remarkable coincidence. For if the loop moves, it's a magnetic force that sets up the emf, but if the loop is stationary, the force cannot be magnetic - stationary charges experience no magnetic forces. In that case, what is responsible? What sort of field exerts a force on charges at rest? Well, electric fields do, of course, but in this case there doesn't seem to be any electric field in sight. Faraday had an ingenious inspiration: \\textbf{A changing magnetic field induces an electric field} It is this induced electric field that accounts for the emf in Experiment 2. Indeed, if (as Faraday found empirically) the emf is again equal to the rate of change of the flux, \\mathcal{E} = \\oint \\vec{E} \\cdot \\dd \\vec{l} = - \\dv{\\Phi}{t} \\tagl{7.14} then \\vec{E} is related to the change in \\vec{B} by the equation \\oint \\vec{E} \\cdot \\dd \\vec{l} = - \\int \\pdv{\\vec{B}}{t} \\cdot \\dd \\vec{a} \\tagl{7.15} This is Faraday's law , in integral form. We can convert it to differential form by applying Stokes' theorem: \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} \\tagl{7.16} Note that Faraday's law reduces to the old rule \\oint \\vec{E} \\cdot \\dd \\vec{l} = 0 (or, in differential form, \\curl \\vec{E} = 0 ) in the static case (constant \\vec{B} ), as, of course, it should. In Experiment 3, the magnetic field changes for entirely different reasons, but according to Faraday's law an electric field will again be induced, giving rise to an emf - d \\Phi / dt . Indeed, one can subsume all three cases (and for that matter any combination of them) into a kind of universal flux rule : Whenever (and for whatever reason) the magnetic flux through a loop changes, an emf \\mathcal{E} = - \\pdv{\\Phi}{t} \\tagl{7.17} will appear in the loop. Many people call this \"Faraday's law.\" Maybe I'm overly fastidious, but I find this confusing. There are really two totally different mechanisms underlying Eq. 7.17, and to identify them both as \"Faraday's law\" is a little like saying that because identical twins look alike we ought to call them by the same name. In Faraday's first experiment, it's the Lorentz force law at work; the emf is magnetic. But in the other two it's an electric field (induced by the changing magnetic field) that does the job. Viewed in this light, it is quite astonishing that all three processes yield the same formula for the emf. In fact, it was precisely this \"coincidence\" that led Einstein to the special theory of relativity - he sought a deeper understanding of what is, in classical electrodynamics, a peculiar accident. But that's a story for chapter 12. In the meantime, I shall reserve the term \"Faraday's law\" for electric fields induced by changing magnetic fields, and I do not regard Experiment 1 as an instance of Faraday's law.","title":"7.2.1: Faraday's Law"},{"location":"ch7-2/#example-75","text":"A long cylindrical magnet of length L and radius a carries a uniform magnetization \\vec{M} parallel to its axis. It passes at constant velocity \\vec{v} through a circular wire ring of slightly larger diameter (Fig. 7.22). Graph the emf induced in the ring, as a function of time. Solution The magnetic field is the same as that of a long solenoid with surface current \\vec{K}_b = M \\vu{\\phi} . So the field inside is \\vec{B} = \\mu_0 \\vec{M} , except near the ends, where it starts to spread out. The flux through the ring is zero when the magnet is far away; it builds up to a maximum of \\mu_0 M \\pi a^2 as the leading end passes through; and it drops back to zero as the trailing end emerges (Fig. 7.23a). The emf is (minus) the derivative of \\Phi with respect to time, so it consists of two spikes, as shown in Fig. 7.23b. Keeping track of the signs in Faraday's law can be a real headache. For instance, in Ex. 7.5 we would like to know which way around the ring the induced current flows. In principle, the right-hand rule does the job (we called \\Phi positive to the left, in Fig. 7.22, so the positive direction for current in the ring is counter-clockwise, as viewed from the left; since the first spike in Fig. 7.23b is negative, the first current pulse flows clockwise, and the second counterclockwise). But there's a handy rule, called Lenz's law, whose sole purpose is to help you get the directions right: Nature abhors a change in flux The induced current will flow in such a direction that the flux it produces tends to cancel the change. (As the front end of the magnet in Ex. 7.5 enters the ring, the flux increases, so the current in the ring must generate a field to the right - it therefore flows clockwise.) Notice that it is the change in flux, not the flux itself, that nature abhors (when the tail end of the magnet exits the ring, the flux drops, so the induced current flows counterclockwise, in an effort to restore it). Faraday induction is a kind of \"inertial\" phenomenon: A conducting loop \"likes\" to maintain a constant flux through it; if you try to change the flux, the loop responds by sending a current around in such a direction as to frustrate your efforts. (It doesn't succeed completely; the flux produced by the induced current is typically only a tiny fraction of the original. All Lenz's law tells you is the direction of the flow.)","title":"Example 7.5"},{"location":"ch7-2/#example-76","text":"The 'jumping ring' demonstration. If you wind a solenoidal coil around an iron core (the iron is there to beef up the magnetic field), place a metal ring on top, and plug it in, the ring will jump several feet in the air (Fig 7.24). Why? Solution Before you turn on the current, the flux through the ring was zero. Afterward a flux appeared (upward in the diagram), and the emf generated in the ring led to a current (in the ring) which, according to Lenz's law, was in such a direction that its field tended to cancel this new flux. This means that the current in the loop is opposite to the current in the solenoid. As opposite currents repel (as we saw in our Biot-Savart calculations in the last chapter), the ring flies off.","title":"Example 7.6"},{"location":"ch7-2/#722-the-induced-electric-field","text":"Faraday's law generalizes the electrostatic rule \\curl \\vec{E} = 0 to the time-dependent regime. The divergence of \\vec{E} is still given by Gauss's law ( \\div \\vec{E} = \\frac{1}{\\epsilon_0} \\rho ). If \\vec{E} is a pure Faraday field (due exclusively to a changing \\vec{B} , with \\rho = 0 ), then \\div \\vec{E} = 0 \\qquad \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} This is mathematically identical to magnetostatics \\div \\vec{B} = 0 \\qquad \\curl \\vec{B} = \\mu_0 \\vec{J} Conclusion: Faraday-induced electric fields are determined by -(\\partial \\vec{B} / \\partial t) in exactly the same way as magnetostatic fields are determined by \\mu_0 \\vec{J} . The analog to Biot-Savart is \\vec{E} = - \\frac{1}{4 \\pi} \\int \\frac{(\\partial \\vec{B} / \\partial t) \\cross \\vu{\\gr}}{\\gr ^2} \\dd \\tau = - \\frac{1}{4 \\pi} \\pdv{}{t} \\int \\frac{\\vec{B} \\cross \\vu{\\gr}}{\\gr ^2} \\dd \\tau \\tagl{7.18} and if symmetry permits, we can use all the tricks associated with Ampere's law in integral form ( \\oint \\vec{B} \\cdot \\dd \\vec{l} = \\mu_0 I_{enc} ), only now it's Faraday's law in integral form: \\oint \\vec{E} \\cdot \\dd \\vec{l} = - \\dv{\\Phi}{t} \\tagl{7.19} The rate of change of (magnetic) flux through the Amperian loop plays the role formerly assigned to \\mu_0 I_{enc} .","title":"7.2.2: The Induced Electric Field"},{"location":"ch7-2/#example-77","text":"A uniform magnetic field \\vec{B}(t) , pointing straight up, fills the shaded circular region of Fig. 7.25. If \\vec{B} is changing with time, what is the induced electric field? Solution \\vec{E} points in the circumferential direction, just like the magnetic field inside a long straight wire carrying a uniform current density. Draw an Amperian loop of radius s , and apply Faraday's law: \\oint \\vec{E} \\cdot \\dd \\vec{l} = E ( 2 \\pi s) = - \\dv{\\Phi}{t} = - \\dv{}{t} \\left( \\pi s^2 B(t) \\right) = - \\pi s^2 \\dv{B}{t} Therefore \\vec{E} = - \\frac{s}{2} \\dv{B}{t} \\vu{\\phi} If \\vec{B} is increasing, \\vec{E} runs clockwise, as viewed from above.","title":"Example 7.7"},{"location":"ch7-2/#example-78","text":"A line charge \\lambda is glued to the rim of a wheel of radius b , which is then suspended horizontally, as shown in Fig 7.26, so that it is free to rotate (the spokes are made of some nonconducting material - wood, maybe). In the central region, out to radius a , there is a uniform magnetic field \\vec{B}_0 , pointing up. Now someone turns the field off. What happens? Solution The changing magnetic field will induce an electric field, curling around the axis of the wheel. This electric field exerts a force on the charges at the rim, and the wheel starts to turn. According to Lenz's law, it will rotate in such a direction that its field tends to restore the upward flux. The motion, then, is counterclockwise, as viewed frrom above. Faraday's law, applied to the loop at radius b , says \\oint \\vec{E} \\cdot \\dd \\vec{l} = E ( 2 \\pi b) = - \\dv{\\Phi}{t} = - \\pi ^2 \\dv{B}{t} or \\vec{E} = - \\frac{a^2}{2b} \\dv{B}{t} \\vu{\\phi} The torque on a segment of length \\dd \\vec{l} is (\\vec{r} \\cross \\vec{F}) , or b \\lambda E \\dd l . The total torque on the wheel is therefore N = b \\lambda \\left( - \\dv{\\Phi}{t} = - \\pi ^2 \\dv{B}{t} \\right) \\oint \\dd l = - b \\lambda \\pi a^2 \\dv{B}{t} and the angular momentum imparted to the wheel is \\int N \\dd t = - \\lambda \\pi a^2 b \\int_{B_0} ^0 \\dd B = \\lambda \\pi a^2 b B_0 It doesn't matter how quickly or slowly you tum off the field; the resulting angular velocity of the wheel is the same regardless. (If you find yourself wondering where the angular momentum came from, you're getting ahead of the story! Wait for the next chapter.) Note that it's the electric field that did the rotating. To convince you of this, I deliberately set things up so that the magnetic field is zero at the location of the charge. The experimenter may tell you she never put in any electric field - all she did was switch off the magnetic field. But when she did that, an electric field automatically appeared, and it's this electric field that turned the wheel. I must warn you, now, of a small fraud that tarnishes many applications of Faraday's law: Electromagnetic induction, of course, occurs only when the magnetic fields are changing, and yet we would like to use the apparatus of magnetostatics (Ampere's law, the Biot-Savart law, and the rest) to calculate those magnetic fields. Technically, any result derived in this way is only approximately correct. But in practice the error is usually negligible, unless the field fluctuates extremely rapidly, or you are interested in points very far from the source. Even the case of a wire snipped by a pair of scissors (Prob. 7.18) is static enough for Ampere's law to apply. This regime, in which magnetostatic rules can be used to calculate the magnetic field on the right hand side of Faraday's law, is called quasistatic. Generally speaking, it is only when we come to electromagnetic waves and radiation that we must worry seriously about the breakdown of magnetostatics itself.","title":"Example 7.8"},{"location":"ch7-2/#example-79","text":"An infinitely long straight wire carries a slowly varying current I(t) . Determine the induced electric field, as a function of the distance s from the wire Solution In the quasistatic approximation, the magnetic field is (\\mu_0 I / 2 \\pi s) and it circles around the wire. Like the B -field of a solenoid, E here runs parallel to the axis. For the rectangular \"Amperian loop\" in Fig 7.27, Faraday's law gives: \\begin{align*} \\oint \\vec{E} \\cdot \\dd \\vec{l} & = E (s_0) l - E(s) l \\\\ & = - \\dv{}{t} \\int \\vec{B} \\cdot \\dd \\vec{a} \\\\ & = - \\frac{\\mu_0 l }{2\\pi} \\dv{I}{t} \\int _{s_0} ^s \\frac{1}{s} \\dd s' \\\\ & = - \\frac{\\mu_0 l}{2 \\pi} \\dv{I}{t} ( \\ln \\, s - \\ln \\, s_0 ) \\end{align*} Thus \\vec{E}(s) = \\left( \\frac{\\mu_0}{2\\pi} \\dv{I}{t} \\ln \\, s + K \\right) \\vu{z} \\tagl{7.20} where K is a constant (that is to say, it is independent of s - it might still be a function of t ). The actual value of K depends on the whole history of the function I(t) - we'll see some examples in Chapter 10. Equation 7.20 has the particular implication that E blows up as s goes to infinity. That can't be true... What's gone wrong? Answer: we have overstepped the limits of the quasistatic approximation. As we shall see in Chapter 9, electromagnetic \"news\" travels at the speed of light, and at large distances B depends not on the current now , but on the current as it was at some earlier time (indeed, a whole range of earlier times, since different points on the wire are different distances away). If \\tau is the time it takes I to change substantially, then the quasistatic approximation should hold only for s \\ll c \\tau \\tagl{7.21} and hence Eq. 7.20 simply does not apply, at extremely large s .","title":"Example 7.9"},{"location":"ch7-2/#723-inductance","text":"Suppose you have two loops of wire, at rest (Fig 7.30). If you run a steady current I_1 around loop 1, it produces a magnetic field \\vec{B}_1 . Some of the field lines pass through loop 2; let \\Phi_2 be the flux of \\vec{B}_1 through 2. You might have a tough time actually calculating \\vec{B_1} , but a glance at the Biot-Savart law, \\vec{B}_1 = \\frac{\\mu_0}{4 \\pi} I_1 \\oint \\frac{\\dd \\vec{l}_1 \\cross \\vu{\\gr}}{\\gr^2} reveals one significant fact about this field: It is proportional to the current I_1 . Therefore, so too is the flux through loop 2: \\Phi_2 = \\int \\vec{B}_1 \\cdot \\dd \\vec{a}_2 Thus \\Phi_2 = M_{21} I_1 \\tagl{7.22} where M_{21} is the constant of proportionality; it is known as the mutual inductance of the two loops. There is a cute formula for the mutual inductance, which you can derive by expressing the flux in terms of the vector potential, and invoking Stokes' theorem: \\Phi_2 = \\int \\vec{B}_1 \\cdot \\dd \\vec{a}_2 = \\int (\\curl \\vec{A}_1) \\cdot \\dd \\vec{a}_2 = \\oint \\vec{A}_1 \\cdot \\dd \\vec{l}_2 Now, according to Eq. 5.66, \\vec{A}_1 = \\frac{\\mu_0 I_1}{4 \\pi} \\oint \\frac{\\dd \\vec{l}_1}{\\gr} and hence \\Phi_2 = \\frac{\\mu_0 I_1}{4 \\pi} \\oint \\left( \\oint \\frac{\\dd \\vec{l}_1}{\\gr} \\right) \\cdot \\dd \\vec{l}_2 Evidently M_{21} = \\frac{\\mu_0}{4 \\pi} \\oint \\oint \\frac{\\dd \\vec{l}_1 \\cdot \\dd \\vec{l}_2}{\\gr} \\tagl{7.23} This is the Neumann formula ; it involves a double line integral - one integration around loop 1, the other around loop 2 (Fig 7.31). It's not very useful for practical calculations, but it does reveal two important things about the mutual inductance: M_{21} is a purely geometrical quantity, having to do with the sizes, shapes, and relative positions of the two loops. The integral in Eq. 7.23 is unchanged if we switch the roles of loops 1 and 2; it follows that M_{21} = M_{12} \\tagl{7.24} This is an astonishing conclusion: Whatever the shapes and positions of the loops, the flux through 2 when we run a current I around 1 is identical to the flux through 1 when we send the same current I around 2. We may as well drop the subscripts and call them both M.","title":"7.2.3: Inductance"},{"location":"ch7-2/#example-710","text":"A short solenoid (length l and radius a , with n_1 turns per unit length) lies on the axis of a very long solenoid (radius b , n_2 turns per unit length) as shown in Fig 7.32. Current I flows in the short solenoid. What is the flux through the long solenoid? Solution Since the inner solenoid is short, it has a very complicated field; moreover, it puts a different flux through each turn of the outer solenoid. It would be a miserable task to compute the total flux this way. However, if we exploit the equality of the mutual inductances, the problem becomes very easy. Just look at the reverse situation: run the current I through the outer solenoid, and calculate the flux through the inner one. The field inside the long solenoid is constant B = \\mu_0 n_2 I so the flux through a single loop of the short solenoid is B \\pi a^2 = \\mu_0 n_2 I \\pi a^2 There are n_1 l turns in all, so the total flux through the inner solenoid is \\Phi = \\mu_0 \\pi a^2 n_1 n_2 I This is also the flux a current I in the short solenoid would put through the long one, which is what we set out to find. Incidentally, the mutual inductance, in this case, is M = \\mu_0 \\pi a^2 n_1 n_2 l Suppose now, that you vary the current in loop 1. The flux through loop 2 will vary accordingly, and Faraday's law says this changing flux will induce an emf in loop 2: \\mathcal{E}_2 = - \\dv{\\Phi_2}{t} = - M \\dv{I_1}{t} \\tagl{7.25} (In quoting Eq. 7.22 - which was based on the Biot-Savart law - I am tacitly assuming that the currents change slowly enough for the system to be considered quasistatic.) What a remarkable thing: Every time you change the current in loop 1, and induced current flows in loop 2 - even though there are no wires connecting them! Come to think of it, a changing current not only induces an emf in any nearby loops, it also induces an emf in the source loop itself (Fig 7.33). Once again, the field (and therefore the flux) is proportional to the current \\Phi = L I \\tagl{7.26} The constant of proportionality L is called the self inductance (or simply the inductance ) of the loop. As with M , it depends on the geometry (side and shape ) of the loop. If the current changes, the emf induced in the loop is \\mathcal{E} = - L \\dv{I}{t} \\tagl{7.27} Inductance is measured in henries (H); a henry is a volt-second per ampere.","title":"Example 7.10"},{"location":"ch7-2/#example-711","text":"Find the self inductance of a toroidal coil with rectangular cross-section (inner radius a , outer radius b , height h ), that carries a total of N turns. Solution The magnetic field inside of a toroid is B = \\frac{\\mu_0 N I}{2 \\pi s} The flux through a single turn (Fig 7.34) is \\int \\vec B \\cdot \\dd \\vec a = \\frac{\\mu_0 N I}{2 \\pi} h \\int_a ^b \\frac{1}{s} = \\frac{\\mu_0 N I h}{2 \\pi} \\ln \\frac{b}{a} The total flux is N times this, so the self-inductance (Eq. 7.26) is L = \\frac{\\mu_0 N^2 h}{2 \\pi} \\ln \\left( \\frac{b}{a} \\right) \\tagl{7.28} Inductance (like capacitance) is an intrinsically positive quantity. Lenz's law, which is enforced by the minus sign in Eq. 7.27, dictates that the emf is in such a direction as to oppose any change in current. For this reason, it is called a back emf . Whenever you try to alter the current in a wire, you must fight against this back emf. Inductance plays somewhat the same role in electric currents that mass plays in mechanical systems: The greater L , the harder it is to change the current, just as the larger the mass, the harder it is to change an object's velocity.","title":"Example 7.11"},{"location":"ch7-2/#example-712","text":"Suppose a current I is flowing around a loop, when someone suddenly cuts the wire. The current drops \"instantaneously\" to zero. This generates a whopping back emf, for although I may be small, d I / d t is enormous. (That's why you sometimes draw a spark when you unplug an iron or toaster - electromagnetic induction is desperately trying to keep the current going, even if it has to jump the gap in the circuit.) Nothing so dramatic occurs when you plug in a toaster or iron. In this case induction opposes the sudden increase in current, prescribing instead a smooth and continuous buildup. Suppose, for instance, that a battery (which supplies a constant emf \\mathcal{E}_0 ) is connected to a circuit of resistance R and inductance L (Fig. 7.35). What current flows? Solution The total emf in this circuit is \\mathcal{E}_0 from the battery plus -L (\\dv{I}{t}) from the inductance. Ohm's law says \\mathcal{E}_0 - L \\dv{I}{t} = I R This is a first-order differential equation for I as a function of time. The general solution is I(t) = \\frac{\\mathcal{E}_0}{R} + k e^{-(R/L) t} where k is a constant to be determined by the initial conditions. In particular, if you close the switch at time t = 0 , so I(0) = 0 , then k = - \\mathcal{E}_0 / R , and I(t) = \\frac{\\mathcal{E}_0}{R} \\left( 1 - e^{-(R/L) t} \\right) \\tagl{7.29} This function is plotted in Fig 7.36. Had there been no inductance in the circuit, the current would have jumped immediately to \\mathcal{E}_0 / R . In practice, every circuit has some self-inductance, and the current approaches \\mathcal{E}_0 / R asymptotically. The quantity \\tau = L / R is the time constant for an LR circuit; it tells you how long the current takes to reach a substantial fraction (1 - 1/e) of its final value.","title":"Example 7.12"},{"location":"ch7-2/#724-energy-in-magnetic-fields","text":"It takes a certain amount of energy to start a current flowing in a circuit. I'm not talking about the energy delivered to the resistors and converted into heat - that is irretrievably lost, as far as the circuit is concerned, and can be large or small, depending on how long you let the current run. What I am concerned with, rather, is the work you must do against the back emf to get the current going. This is fixed amount, and it is recoverable: you get it back when the current is turned off. In the meantime, it represents energy latent in the circuit; as we'll see in a moment, it can be regarded as energy stored in the magnetic field. The work done on a unit charge, against the back emf, in one trip around the circuit is - \\mathcal{E} (the minus sign records the fact that this is the work done by you against the emf, not the work done by the emf). The amount of charge per unit time passing down the wire is I. So the total work done per unit time is \\dv{W}{t} = - \\mathcal{E}I = L I \\dv{I}{t} If we start with zero current and build it up to a final value I, the work done (integrating the last equation over time) is W = \\frac{1}{2} L I^2 \\tagl{7.30} So, this is the energy stored in an inductor, or in any loop that has an inductance L . It does not depend on how long we take to crank up the current, only on the geometry of the loop (in the form of L ) and the final current I . This is only really sensible for a system of conducting loops, but we can be a bit more general. We can express W by recalling that the flux \\Phi through a loop (which is LI ) is \\Phi = \\int \\vec{B} \\cdot \\dd \\vec{a} = \\int (\\curl \\vec{A}) \\cdot \\dd \\vec{a} = \\oint \\vec{A} \\cdot \\dd \\vec{l} where the line integral is around the perimeter of the loop. So, we have LI = \\oint \\vec{A} \\cdot \\dd \\vec{l} and therefore W = \\frac{1}{2} I \\oint \\vec{A} \\cdot \\dd \\vec{l} = \\frac{1}{2} \\oint (\\vec{A} \\cdot \\vec{I}) \\dd l \\tagl{7.31} We can pretty obviously generalize this to volume currents W = \\frac{1}{2} \\int _V (\\vec{A} \\cdot \\vec{J}) \\dd \\tau \\tagl{7.32} But we can do one better, expressing W entirely in terms of the magnetic field: \\curl \\vec{B} = \\mu_0 \\vec{J} lets us eliminate the current density from the picture W = \\frac{1}{2 \\mu_0} \\int \\vec{A} \\cdot (\\curl \\vec{B}) \\dd \\tau \\tagl{7.33} Integration by parts gets us to slap the derivative from B to A \\div (\\vec{A} \\cross \\vec{B}) = \\vec{B} \\cdot (\\curl \\vec{A}) - \\vec{A} \\cdot (\\curl \\vec{B}) so \\vec{A} \\cdot (\\curl \\vec{B}) = \\vec{B} \\cdot \\vec{B} - \\div (\\vec{A} \\cross \\vec{B} Consequently W = \\frac{1}{2\\mu_0} \\left( \\int B^2 \\dd \\tau - \\int \\div (\\vec{A} \\cross \\vec{B} \\dd \\tau \\right) \\\\ = \\frac{1}{2\\mu_0} \\left( \\int _V B^2 \\dd \\tau - \\oint_S (\\vec{A} \\cross \\vec{B} ) \\cdot \\dd \\vec{a} \\right) \\tagl{7.34} Now, the integration in Eq. 7.32 is to be taken over the entire volume occupied by the current. But any region larger than this will do just as well, for \\vec{J} is zero out there anyway. In Eq. 7.34, the larger the region we pick the greater is the contribution from the volume integral, and therefore the smaller is that of the surface integral (this makes sense: as the surface gets farther from the current, both A and B decrease). In particular, if we agree to integrate over all space, then the surface integral goes to zero, and we are left with W = \\frac{1}{2 \\mu_0} \\int _{\\text{all space}} B^2 \\dd \\tau \\tagl{7.35} In view of this result, we say the energy is \"stored in the magnetic field,\" in the amount (B^2 / 2 \\mu_0) per unit volume. This is a nice way to think of it, though someone looking at Eq. 7.32 might prefer to say that the energy is stored in the current distribution, in the amount \\frac{1}{2} (\\vec{A} \\cdot \\vec{J}) per unit volume. The distinction is one of bookkeeping; the important quantity is the total energy W , and we need not worry about where (if anywhere) the energy is \"located.\" You might find it strange that it takes energy to set up a magnetic field - after all, magnetic fields themselves do no work. The point is that producing a magnetic field, where previously there was none, requires changing the field, and a changing B-field, according to Faraday, induces an electric field. The latter, of course, can do work. In the beginning, there is no \\vec{E} , and at the end there is no \\vec{E} ; but in between, while \\vec{B} is building up, there is an \\vec{E} , and it is against this that the work is done. (You see why I could not calculate the energy stored in a magnetostatic field back in Chapter 5.) In the light of this, it is extraordinary how similar the magnetic energy formulas are to their electrostatic counterparts:","title":"7.2.4: Energy in Magnetic Fields"},{"location":"ch7-2/#example-713","text":"A long coaxial cable carries current I (the current flows down the surface of the inner cylinder, radius a , and back along the outer cylinder, radius b ) as shown in Fig 7.40. Find the magnetic energy stored in a section of length l Solution Ampere's law will tell us that B between the surfaces is \\vec{B} = \\frac{\\mu_0 I}{2 \\pi s } \\vu{\\phi} and outside the cable, the field is zero. Eq. 7.35 gives us the volume energy density \\frac{1}{2 \\mu_0} \\left( \\frac{\\mu_0 I}{2 \\pi s} \\right)^2 = \\frac{\\mu_0 I^2}{8 \\pi^2 s^2} The energy in a shell of length l , radius s and thickness \\dd s is \\left( \\frac{\\mu_0 I^2}{8 \\pi ^2 s^2} \\right)2 \\pi l s \\, \\dd s = \\frac{\\mu_0 I^2 l}{4 \\pi} \\left( \\frac{\\dd s}{s} \\right) Integrating from a to b , we have W = \\frac{\\mu_0 I^2 l}{4 \\pi} \\ln \\left( \\frac{b}{a} \\right) Incidentally, this suggests a very simple way to calculate the self-inductance of the cable. According to Eq. 7.30, the energy can also be written as \\frac{1}{2} L I^2 . Comparing the two expressions, L = \\frac{\\mu_0 l}{2 \\pi} \\ln \\left( \\frac{b}{a} \\right) This method of calculating the self-inductance is especially useful when the current is not confined to a single path, but spreads over some surface or volume, so that different parts of the current enclose different amounts of flux. In such cases, it can be very tricky to get the inductance directly from Eq. 7.26, and it is best to let 7.30 define L","title":"Example 7.13"},{"location":"ch7-3/","text":"7.3: Maxwell's Equations 7.3.1: Electrodynamics Before Maxwell So far, we have encountered the following laws, specifying the divergence and curl of electric and magnetic fields \\begin{align*} (\\text{i}) & \\quad \\div \\vec{E} = \\frac{1}{\\epsilon_0 } \\rho \\quad \\text{(Gauss's law)} \\\\ (\\text{ii}) & \\quad \\div \\vec{B} = 0 \\quad \\text{(Ng's Law)} \\\\ (\\text{iii}) & \\quad \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} \\quad \\text{(Faraday's Law}) \\\\ (\\text{iv}) & \\quad \\curl \\vec{B} = \\mu_0 \\vec{J} \\quad \\text{(Ampere's Law)} \\end{align*} These equations represent the state of electromagnetic theory in the mid-nineteenth century, when Maxwell began his work. They were not written in so compact a form, in those days, but their physical content was familiar. Now, it happens that there is a fatal inconsistency in these formulas. It has to do with the old rule that divergence of curl is always zero. If you apply the divergence to number (iii), everything works out: \\div (\\curl \\vec{E}) = \\div \\left( - \\pdv{\\vec{B}}{t} \\right) = - \\pdv{}{t} (\\div \\vec{B}) The left side is zero because divergence of curl is zero; the right side is zero by virtue of equation (ii). But when you do the same thing to number (iv), you get into trouble: \\div (\\curl \\vec{B}) = \\mu_0 (\\div \\vec{J}) \\tagl{7.36} the left side must be zero, but the right side, in general, is not. For steady currents, the divergence of J is zero, but when we go beyond magnetostatics Ampere's law cannot be right. There's another way to see that Ampere's law is bound to fail for non-steady currents. Suppose we're in the process of charging up a capacitor (Fig. 7.43). In integral form, Ampere's law reads \\oint \\vec{B} \\cdot \\dd \\vec{l} = \\mu_0 I_{enc} I want to apply it to the Amperian loop shown in the diagram. How do I deter- mine I_{enc} ? Well, it's the total current passing through the loop, or, more precisely, the current piercing a surface that has the loop for its boundary. In this case, the simplest surface lies in the plane of the loop - the wire punctures this surface, so I_{enc} = I . Fine - but what if I draw instead the balloon-shaped surface in Fig. 7.43? No current passes through this surface, and I conclude that I_{enc} = 0 ! We never had this problem in magnetostatics because the conflict arises only when charge is piling up somewhere (in this case, on the capacitor plates). But for nonsteady currents (such as this one) \"the current enclosed by the loop\" is an ill-defined notion; it depends entirely on what surface you use. (If this seems pedantic to you - \"obviously one should use the plane surface\" - remember that the Amperian loop could be some contorted shape that doesn't even lie in a plane.) Of course, we had no right to expect Ampere's law to hold outside of magnetostatics; after all, we derived it from the Biot-Savart law. However, in Maxwell's time there was no experimental reason to doubt that Ampere's law was of wider validity. The flaw was a purely theoretical one, and Maxwell fixed it by purely theoretical arguments. 7.3.2: How Maxwell Fixed Ampere's Law The problem is on the right side of Eq. 7.36, which should be zero, but isn't. Applying the continuity equation (5.29) and Gauss's law, the offending term can be rewritten: \\div \\vec{J} = - \\pdv{\\rho}{t} = - \\pdv{}{t} ( \\epsilon_0 \\div \\vec{E}) = - \\div \\left( \\epsilon_0 \\pdv{\\vec{E}}{t} \\right) If we were to combine \\epsilon_0 (\\partial \\vec{E} / \\partial t) with \\vec{J} , in Ampere's law, it would be just right to kill off the extra divergence: \\curl \\vec{B} = \\mu_0 \\vec{J} + \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} \\tagl{7.37} (Maxwell himself had other reasons for wanting to add this quantity to Ampere's law. To him, the rescue of the continuity equation was a happy dividend rather than a primary motive. But today we recognize this argument as a far more compelling one than Maxwell's, which was based on a now-discredited model of the ether.) Such a modification changes nothing, as far as magnetostatics is concerned: when \\vec{E} is constant, we still have \\curl \\vec{B} = \\mu_0 \\vec{J} . In fact, Maxwell's term is hard to detect in ordinary electromagnetic experiments, where it must compete for attention with \\vec{J} - that's why Faraday and the others never discovered it in the laboratory. However, it plays a crucial role in the propagation of electromagnetic waves, as we'll see in Chapter 9. Apart from curing the defect in Ampere's law, Maxwell's term has a certain aesthetic appeal: Just as a changing magnetic field induces an electric field (Faraday's law), so \\textbf{A changing electric field induces a magnetic field} Of course, theoretical convenience and aesthetic consistency are only suggestive - there might, after all, be other ways to doctor up Ampere's law. The real confirmation of Maxwell's theory came in 1888 with Hertz's experiments on electromagnetic waves. Maxwell called his extra term the displacement current : \\vec{J}_d = \\epsilon_0 \\pdv{\\vec{E}}{t} \\tagl{7.38} (It's a misleading name; \\epsilon_0 (\\partial \\vec{E} / \\partial t) has nothing to do with current, except that it adds to \\vec{J} in Ampere's law.) Let's see now how displacement current resolves the paradox of the charging capacitor (Fig. 7.43). If the capacitor plates are very close together (I didn't draw them that way, but the calculation is simpler if you assume this), then the electric field between them is E = \\frac{1}{\\epsilon_0} \\sigma = \\frac{1}{\\epsilon_0 } \\frac{Q}{A} where Q is the charge on the plate and A is its area. Thus, between the plates \\pdv{E}{t} = \\frac{1}{\\epsilon_0 A} \\dv{Q}{t} = \\frac{1}{\\epsilon_0 A} I Now, Eq. 7.37 reads, in integral form \\oint \\vec{B} \\cdot \\dd \\vec{l} = \\mu_0 I_{enc} + \\mu_0 \\epsilon_0 \\int \\left( \\pdv{\\vec{E}}{t} \\right) \\cdot \\dd \\vec{a} \\tagl{7.39} If we choose the flat surface, then E = 0 and I_{enc} = I . If, on the other hand, we use the balloon-shaped surface, then I_{enc} = 0 , but \\int (\\partial \\vec{E} / \\partial t) \\cdot \\dd \\vec{a} = I / \\epsilon_0 . So we get the same answer for either surface, though in the first case it comes from the conduction current, and in the second from the displacement current. Example 7.14 Imagine two concentric metal spherical shells (Fig. 7.44). The inner one (radius a ) carries a charge Q(t) , and the outer one (radius b ) an opposite charge -Q(t) . The space between them is filled with Ohmic material of conductivity \\sigma , so a radial current flows \\vec J = \\sigma \\vec E = \\sigma \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q}{r^2} \\vu{r}; \\quad I = - \\dot{Q} = \\int \\vec J \\cdot \\dd \\vec a = \\frac{\\sigma Q}{\\epsilon_0} This configuration is spherically symmetrical, so the magnetic field has to be zero (the only direction it could possibly point is radial, and \\div \\vec B = 0 \\rightarrow \\oint \\vec B \\cdot \\dd \\vec a = B(4 \\pi r^2) = 0 , so B = 0 ). What? I thought currents produce magnetic fields! Isn't that what Biot-Savart and Ampere taught us? How can there be a \\vec J with no accompanying \\vec B ? Solution This is not a static configuration! Q, \\vec E, \\vec J are all functions of time; Ampere and Biot-Savart do not apply. The displacement current J_d = \\epsilon_0 \\pdv{\\vec E}{t} = \\frac{1}{4 \\pi} \\frac{\\dot{Q}}{r^2} \\vu{r} = - \\sigma \\frac{Q}{4 \\pi \\epsilon_0 r^2} \\vu{r} exactly cancels the conduction current (in Eq. 7.37), and the magnetic field (determined by \\div \\vec{B} = 0 and \\curl \\vec{B} = 0 is indeed zero. 7.3.3 Maxwell's Equations In the last section we put the finishing touches on Maxwell's equations: Maxwell's Equations: \\begin{align*} (\\text{i}) & \\quad \\div \\vec{E} = \\frac{1}{\\epsilon_0 } \\rho \\quad \\text{(Gauss's law)} \\\\ (\\text{ii}) & \\quad \\div \\vec{B} = 0 \\quad \\text{(Ng's Law)} \\\\ (\\text{iii}) & \\quad \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} \\quad \\text{(Faraday's Law}) \\\\ (\\text{iv}) & \\quad \\curl \\vec{B} = \\mu_0 \\vec{J} + \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} \\quad \\text{(Ampere's Law)} \\end{align*} \\tagl{7.40} Together, with the force law, \\vec{F} q (\\vec{E} + \\vec{v} \\cross \\vec{B}) \\tagl{7.41} they summarize the entire theoretical content of classical electrodynamics (save for some special properties of matter, which we encountered in Chapters 4 and 6). Even the continuity equation, \\div \\vec{J} = - \\pdv{\\rho}{t} \\tagl{7.42} which is the mathematical expression of conservation of charge, can be derived from Maxwell's equations by applying the divergence to number (iv). I have written Maxwell's equations in the traditional way, which emphasizes that they specify the divergence and curl of \\vec{E} and \\vec{B} . In this form, they reinforce the notion that electric fields can be produced either by charges ( \\rho ) or by changing magnetic fields ( \\partial \\vec{B} / \\partial t ), and magnetic fields can be produced either by currents ( \\vec{J} ) or by changing electric fields ( \\partial \\vec{E} / \\partial t ). Actually, this is misleading, because \\partial \\vec{B} / \\partial t and \\partial \\vec{E} / \\partial t are themselves due to charges and currents. I think it is logically preferable to write \\div \\vec{E} = \\frac{1}{\\epsilon_0} \\rho \\div \\vec{B} = 0 \\curl \\vec{E} + \\pdv{\\vec{B}}{t} = 0 \\curl \\vec{B} - \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} = \\mu_0 \\vec{J} with the fields ( \\vec{E} and \\vec{B} ) on the left and the sources ( \\rho and \\vec{J} ) on the right. This notation emphasizes that all electromagnetic fields are ultimately attributable to charges and currents. Maxwell's equations tell you how charges produce fields; reciprocally, the force law tells you how fields affect charges. 7.3.4: Magnetic Charge There is a pleasing symmetry to Maxwell's equations; it is particularly striking in free space, where \\rho and \\vec J vanish \\div \\vec E = 0 \\qquad \\curl \\vec E = - \\pdv{\\vec B}{t} \\\\ \\div \\vec B = 0 \\qquad \\curl \\vec B = - \\mu_0 \\epsilon_0 \\pdv{\\vec E}{t} If you replace \\vec E by \\vec B and \\vec B by - \\mu_0 \\epsilon_0 \\vec E , the first pair of equations turns into the second, and vice versa. This symmetry between \\vec E and \\vec B is spoiled, though, by the charge term in Gauss's law and the current term in Ampere's law. You can't help wondering why the corresponding quantities are \"missing\" from \\div \\vec B = 0 and \\curl \\vec E = - \\partial \\vec B / \\partial t . What if we had \\begin{align*} (\\text{i}) & \\quad \\div \\vec{E} = \\frac{1}{\\epsilon_0 } \\rho_e \\\\ (\\text{ii}) & \\quad \\div \\vec{B} = \\mu_0 \\rho_m \\\\ (\\text{iii}) & \\quad \\curl \\vec{E} = - \\mu_0 \\vec{J}_m - \\pdv{\\vec{B}}{t} \\\\ (\\text{iv}) & \\quad \\curl \\vec{B} = \\mu_0 \\vec{J} + \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} \\end{align*} \\tagl{7.44} Then \\rho_m would represent the density of magnetic \"charge\", and \\rho_e the density of electric charge; \\vec J_m would be the current of magnetic charge, and \\vec J_e the current of electric charge. Both charges would be conserved: \\div \\vec{J}_m = - \\pdv{\\rho_m}{t} \\quad \\div \\vec{J}_e = - \\pdv{\\rho_e}{t} \\tagl{7.45} The former follows by application of the divergence to (iii), the latter by taking the divergence of (iv). In a sense, Maxwell's equations beg for magnetic charge to exist - it would fit in so nicely. And yet, in spite of a diligent search, no one has ever found any. As far as we know, \\rho_m is zero everywhere, and so is \\vec J_m ; \\vec B is not on equal footing with \\vec E : there exist stationary sources for \\vec E (electric charges) but none for \\vec B . (This is reflected in the fact that magnetic multipole expansions have no monopole term, and magnetic dipoles consist of current loops, not separated north and south \"poles.\") In quantum electrodynamics, by the way, it's a more than merely aesthetic shame that magnetic charge does not seem to exist: Dirac showed that the existence of magnetic charge would explain why electric charge is quantized. 7.3.5: Maxwell's Equations in Matter Maxwell's equations in the form 7.40 are complete and correct as they stand. However, when you are working with materials that are subject to electric and magnetic polarization there is a more convenient way to write them. For inside polarized matter there will be accumulations of \"bound\" charge and current, over which you exert no direct control. It would be nice to reformulate Maxwell's equations so as to make explicit reference only to the \"free\" charges and currents. We have already learned, from the static case, that an electric polarization \\vec{P} produces a bound charge density \\rho_b = - \\div \\vec{P} \\tagl{7.47} (Eq. 4.12). Likewise, a magnetic polarization (or \"magnetization\") \\vec{M} results in a bound current \\vec{J}_b = \\curl \\vec{M} \\tagl{7.48} (Eq. 6.13). There's just one new feature to consider in the nonstatic case: Any change in the electric polarization involves a flow of (bound) charge (call it \\vec{J}_p ), which must be included in the total current. For suppose we examine a tiny chunk of polarized material (Fig. 7.47). The polarization introduces a charge density \\sigma_b = P at one end and - \\sigma_b at the other (Eq. 4.11). If P now increases a bit, the charge on each end increases accordingly, giving a net current \\dd I = \\pdv{\\sigma_b}{t} \\dd a_{\\perp} = \\pdv{P}{t} \\dd a_{\\perp} The current density, therefore, is \\vec{J}_p = \\pdv{\\vec{P}}{t} \\tagl{7.49} This polarization current has nothing to do with the bound current \\vec{J}_b . The latter is associated with magnetization of the material and involves the spin and orbital motion of electrons; \\vec{J}_p by contrast, is the result of the linear motion of charge when the electric polarization changes. If P points to the right, and is increasing, then each plus charge moves a bit to the right and each minus charge to the left; the cumulative effect is the polarization current \\vec{J}_p . We ought to check that Eq. 7.49 is consistent with the continuity equation: \\div \\vec{J}_p = \\div \\pdv{\\vec{P}}{t} = \\pdv{}{t} ( \\div \\vec{P}) = - \\pdv{\\rho_b}{t} Yes: the continuity equation is satisfied: in fact \\vec{J}_p is essential to ensure the conservation of bound charge. (Incidentally, a changing magnetization does not lead to any analogous accumulation of charge or current. The bound current \\vec{J}_b = \\curl \\vec{M} varies in response to \\vec{M} , to be sure, but that's about it.) In view of all this, the total charge density can be separated into two parts: \\rho = \\rho_f + \\rho_b = \\rho_f - \\div \\vec{P} \\tagl{7.50} and the current density into three parts \\vec{J} = \\vec{J}_f + \\vec{J}_b + \\vec{J}_p = \\vec{J}_f + \\curl \\vec{M} + \\pdv{\\vec{P}}{t} \\tagl{7.51} Gauss's law can now be written as \\div \\vec{E} = \\frac{1}{\\epsilon_0} (\\rho _f - \\div \\vec{P}) or \\div \\vec{D} = \\rho_f \\tagl{7.52} where, as in the static case \\vec{D} = \\epsilon_0 \\vec{E} + \\vec{P} \\tagl{7.53} Meanwhile, Ampere's law (with Maxwell's term) becomes \\curl \\vec{B} = \\mu_0 \\left( \\vec{J}_f + \\curl \\vec{M} + \\pdv{\\vec{P}}{t} \\right) + \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} or \\curl \\vec{H} = \\vec{J}_f + \\pdv{\\vec{D}}{t} \\tagl{7.54} where, as before \\vec{H} \\equiv \\frac{1}{\\mu_0} \\vec{B} - \\vec{M} \\tagl{7.55} Faraday's law and \\div \\vec{B} = 0 are not affected by our separation of charge and current into free and bound parts, since they do not involve \\rho or \\vec{J} . In terms of free charges and currents, then, Maxwell's equations read Maxwell's Equations in Matter: \\begin{align*} (\\text{i}) & \\quad \\div \\vec{D} = \\rho_f \\quad \\text{(Gauss's law)} \\\\ (\\text{ii}) & \\quad \\div \\vec{B} = 0 \\quad \\text{(Ng's Law)} \\\\ (\\text{iii}) & \\quad \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} \\quad \\text{(Faraday's Law}) \\\\ (\\text{iv}) & \\quad \\curl \\vec{H} = \\vec{J}_f + \\pdv{\\vec{D}}{t} \\quad \\text{(Ampere's Law)} \\end{align*} \\tagl{7.56} Some people regard these as the \"true\" Maxwell's equations, but please understand that they are in no way more \"general\" than Eq. 7.40; they simply reflect a convenient division of charge and current into free and nonfree parts. And they have the disadvantage of hybrid notation, since they contain both \\vec{E} and \\vec{D} , both \\vec{B} and \\vec{H} . They must be supplemented, therefore, by appropriate constitutive relations , giving \\vec{D} and \\vec{H} in terms of \\vec{E} and \\vec{B} . These depend on the nature of the material; for linear media \\vec{P} = \\epsilon_0 \\chi_e \\vec{E} \\qquad \\text{ and } \\qquad \\vec{M} = \\chi_m \\vec{H} \\tagl{7.57} so \\vec{D} = \\epsilon \\vec{E} \\qquad \\text{ and } \\qquad \\vec{H} = \\frac{1}{\\mu} \\vec{B} \\tagl{7.58} where \\epsilon = \\epsilon_0 (1 + \\chi_e) and \\mu = \\mu_0 (1 + \\chi_m) . Incidentally, you'll remember that \\vec{D} is called the electric \"displacement\"; that's why the second term in the Ampere/Maxwell equation came to be called the displacement current . In this context \\vec{J}_d \\equiv \\pdv{\\vec{D}}{t} \\tagl{7.59} 7.3.6: Boundary Conditions In general, the fields \\vec{E}, \\vec{B}, \\vec{D}, and \\vec{H} will be discontinuous at a boundary between two different media, or at a surface that carries a surface charge density \\sigma or a current density \\vec{K} . The explicity form of these discontinuities can be deduced from Maxwell's equations in their integral form \\begin{align*} (1) \\quad & \\oint_S \\vec{D} \\cdot \\dd \\vec{a} = Q_{f, enc} \\\\ (2) \\quad & \\oint _S \\vec{B} \\cdot \\dd \\vec{a} = 0 \\\\ (3) \\quad & \\oint _P \\vec{E} \\cdot \\dd \\vec{l} = - \\dv{}{t} \\int _S \\vec{B} \\cdot \\dd \\vec{a} \\\\ (4) \\quad & \\oint _P \\vec{H} \\cdot \\dd \\vec{l} = I_{f, enc} + \\dv{}{t} \\int_S \\vec{D} \\cdot \\dd \\vec a \\end{align*} Applying (1) to a tiny, wafer-thin Gaussian pillbox extending just slightly into the material on either side of the boundary (Fig 7.48), we obtain \\vec{D}_1 \\cdot \\vec{a} - \\vec{D}_2 \\cdot \\vec a = \\sigma_f a (The positive direction for \\vec a is from 2 toward 1. The edge of the wafer contributes nothing in the limit as the thickness goes to zero; nor does any volume charge density.) Thus, the component of \\vec D that is perpendicular to the interface is discontinuous in the amount D_1 ^\\perp - D_2 ^\\perp = \\sigma_f \\tagl{7.60} Identical reasoning, applied to equation (2) yields B_1 ^\\perp - B_2 ^\\perp = 0 \\tagl{7.61} Turning to (3), a very thin Amperian loop straddling the surface gives \\vec{E_1} \\cdot \\vec{l} - \\vec{E_2} \\cdot \\vec{l} = - \\dv{}{t} \\int_S \\vec{B} \\cdot \\dd \\vec a But in the limit as the width of the loop goes to zero, the flux vanishes. (I have already dropped the contribution of the two ends to \\oint \\vec{E} \\cdot \\dd \\vec l , on the same grounds) Therefore, \\vec{E}_1 ^\\parallel - \\vec{E}_2 ^\\parallel = 0 \\tagl{7.62} That is, the components of \\vec E parallel to the interface are continuous across the boundary. By the same token, (4) implies \\vec{H}_1 \\cdot \\vec l - \\vec{H}_2 \\cdot \\vec l = I_{f, enc} where I_{f, enc} is the free current passing through the Amperian loop. No volume current density will contribute (in the limit of infinitesimal width), but a surface current can. In fact, if \\vu{n} is a unit vector perpendicular to the interface (pointing from 2 toward 1), so that (\\vu{n} \\cross \\vec l ) is normal to the Amperian loop (Fig 7.49), the I_{f, enc} = \\vec{K_f} \\cdot (\\vu n \\cdot \\vec l) = (\\vec{K_f} \\cross \\vu n) \\cdot \\vec l and hence \\vec{H}_1 ^\\parallel - \\vec{H}_2 ^\\parallel = \\vec{K}_f \\cross \\vu n \\tagl{7.63} So the parallel components of \\vec H are discontinuous by an amount proportional to the free surface current density. Equations 7.60-63 are the general boundary conditions for electrodynamics. In the case of linear media, they can be expressed in terms of \\vec E and \\vec B alone \\begin{align*} (1) \\quad & \\epsilon_1 E_1 ^\\perp - \\epsilon_2 E_2 ^\\perp = \\sigma_f \\\\ (2) \\quad & \\vec{B}_1 ^\\perp - \\vec{B}_2 ^\\perp = 0 \\\\ (3) \\quad & \\vec{E}_1 ^\\parallel - \\vec{E}_2 ^\\parallel = 0 \\\\ (4) \\quad & \\frac{1}{\\mu_1} \\vec{B}_1 ^\\parallel - \\frac{1}{\\mu_2} \\vec{B}_2 ^\\parallel = \\vec{K}_f \\cross \\vu{n} \\end{align*} \\tagl{7.64} In particular, if there is no free charge or free current at the interface, then \\begin{align*} (1) \\quad & \\epsilon_1 E_1 ^\\perp - \\epsilon_2 E_2 ^\\perp = 0 \\\\ (2) \\quad & \\vec{B}_1 ^\\perp - \\vec{B}_2 ^\\perp = 0 \\\\ (3) \\quad & \\vec{E}_1 ^\\parallel - \\vec{E}_2 ^\\parallel = 0 \\\\ (4) \\quad & \\frac{1}{\\mu_1} \\vec{B}_1 ^\\parallel - \\frac{1}{\\mu_2} \\vec{B}_2 ^\\parallel = 0 \\end{align*} \\tagl{7.65} These equations provide the basis for the theory of reflection and refraction.","title":"7.3 - Maxwell's Equations"},{"location":"ch7-3/#73-maxwells-equations","text":"","title":"7.3: Maxwell's Equations"},{"location":"ch7-3/#731-electrodynamics-before-maxwell","text":"So far, we have encountered the following laws, specifying the divergence and curl of electric and magnetic fields \\begin{align*} (\\text{i}) & \\quad \\div \\vec{E} = \\frac{1}{\\epsilon_0 } \\rho \\quad \\text{(Gauss's law)} \\\\ (\\text{ii}) & \\quad \\div \\vec{B} = 0 \\quad \\text{(Ng's Law)} \\\\ (\\text{iii}) & \\quad \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} \\quad \\text{(Faraday's Law}) \\\\ (\\text{iv}) & \\quad \\curl \\vec{B} = \\mu_0 \\vec{J} \\quad \\text{(Ampere's Law)} \\end{align*} These equations represent the state of electromagnetic theory in the mid-nineteenth century, when Maxwell began his work. They were not written in so compact a form, in those days, but their physical content was familiar. Now, it happens that there is a fatal inconsistency in these formulas. It has to do with the old rule that divergence of curl is always zero. If you apply the divergence to number (iii), everything works out: \\div (\\curl \\vec{E}) = \\div \\left( - \\pdv{\\vec{B}}{t} \\right) = - \\pdv{}{t} (\\div \\vec{B}) The left side is zero because divergence of curl is zero; the right side is zero by virtue of equation (ii). But when you do the same thing to number (iv), you get into trouble: \\div (\\curl \\vec{B}) = \\mu_0 (\\div \\vec{J}) \\tagl{7.36} the left side must be zero, but the right side, in general, is not. For steady currents, the divergence of J is zero, but when we go beyond magnetostatics Ampere's law cannot be right. There's another way to see that Ampere's law is bound to fail for non-steady currents. Suppose we're in the process of charging up a capacitor (Fig. 7.43). In integral form, Ampere's law reads \\oint \\vec{B} \\cdot \\dd \\vec{l} = \\mu_0 I_{enc} I want to apply it to the Amperian loop shown in the diagram. How do I deter- mine I_{enc} ? Well, it's the total current passing through the loop, or, more precisely, the current piercing a surface that has the loop for its boundary. In this case, the simplest surface lies in the plane of the loop - the wire punctures this surface, so I_{enc} = I . Fine - but what if I draw instead the balloon-shaped surface in Fig. 7.43? No current passes through this surface, and I conclude that I_{enc} = 0 ! We never had this problem in magnetostatics because the conflict arises only when charge is piling up somewhere (in this case, on the capacitor plates). But for nonsteady currents (such as this one) \"the current enclosed by the loop\" is an ill-defined notion; it depends entirely on what surface you use. (If this seems pedantic to you - \"obviously one should use the plane surface\" - remember that the Amperian loop could be some contorted shape that doesn't even lie in a plane.) Of course, we had no right to expect Ampere's law to hold outside of magnetostatics; after all, we derived it from the Biot-Savart law. However, in Maxwell's time there was no experimental reason to doubt that Ampere's law was of wider validity. The flaw was a purely theoretical one, and Maxwell fixed it by purely theoretical arguments.","title":"7.3.1: Electrodynamics Before Maxwell"},{"location":"ch7-3/#732-how-maxwell-fixed-amperes-law","text":"The problem is on the right side of Eq. 7.36, which should be zero, but isn't. Applying the continuity equation (5.29) and Gauss's law, the offending term can be rewritten: \\div \\vec{J} = - \\pdv{\\rho}{t} = - \\pdv{}{t} ( \\epsilon_0 \\div \\vec{E}) = - \\div \\left( \\epsilon_0 \\pdv{\\vec{E}}{t} \\right) If we were to combine \\epsilon_0 (\\partial \\vec{E} / \\partial t) with \\vec{J} , in Ampere's law, it would be just right to kill off the extra divergence: \\curl \\vec{B} = \\mu_0 \\vec{J} + \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} \\tagl{7.37} (Maxwell himself had other reasons for wanting to add this quantity to Ampere's law. To him, the rescue of the continuity equation was a happy dividend rather than a primary motive. But today we recognize this argument as a far more compelling one than Maxwell's, which was based on a now-discredited model of the ether.) Such a modification changes nothing, as far as magnetostatics is concerned: when \\vec{E} is constant, we still have \\curl \\vec{B} = \\mu_0 \\vec{J} . In fact, Maxwell's term is hard to detect in ordinary electromagnetic experiments, where it must compete for attention with \\vec{J} - that's why Faraday and the others never discovered it in the laboratory. However, it plays a crucial role in the propagation of electromagnetic waves, as we'll see in Chapter 9. Apart from curing the defect in Ampere's law, Maxwell's term has a certain aesthetic appeal: Just as a changing magnetic field induces an electric field (Faraday's law), so \\textbf{A changing electric field induces a magnetic field} Of course, theoretical convenience and aesthetic consistency are only suggestive - there might, after all, be other ways to doctor up Ampere's law. The real confirmation of Maxwell's theory came in 1888 with Hertz's experiments on electromagnetic waves. Maxwell called his extra term the displacement current : \\vec{J}_d = \\epsilon_0 \\pdv{\\vec{E}}{t} \\tagl{7.38} (It's a misleading name; \\epsilon_0 (\\partial \\vec{E} / \\partial t) has nothing to do with current, except that it adds to \\vec{J} in Ampere's law.) Let's see now how displacement current resolves the paradox of the charging capacitor (Fig. 7.43). If the capacitor plates are very close together (I didn't draw them that way, but the calculation is simpler if you assume this), then the electric field between them is E = \\frac{1}{\\epsilon_0} \\sigma = \\frac{1}{\\epsilon_0 } \\frac{Q}{A} where Q is the charge on the plate and A is its area. Thus, between the plates \\pdv{E}{t} = \\frac{1}{\\epsilon_0 A} \\dv{Q}{t} = \\frac{1}{\\epsilon_0 A} I Now, Eq. 7.37 reads, in integral form \\oint \\vec{B} \\cdot \\dd \\vec{l} = \\mu_0 I_{enc} + \\mu_0 \\epsilon_0 \\int \\left( \\pdv{\\vec{E}}{t} \\right) \\cdot \\dd \\vec{a} \\tagl{7.39} If we choose the flat surface, then E = 0 and I_{enc} = I . If, on the other hand, we use the balloon-shaped surface, then I_{enc} = 0 , but \\int (\\partial \\vec{E} / \\partial t) \\cdot \\dd \\vec{a} = I / \\epsilon_0 . So we get the same answer for either surface, though in the first case it comes from the conduction current, and in the second from the displacement current.","title":"7.3.2: How Maxwell Fixed Ampere's Law"},{"location":"ch7-3/#example-714","text":"Imagine two concentric metal spherical shells (Fig. 7.44). The inner one (radius a ) carries a charge Q(t) , and the outer one (radius b ) an opposite charge -Q(t) . The space between them is filled with Ohmic material of conductivity \\sigma , so a radial current flows \\vec J = \\sigma \\vec E = \\sigma \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q}{r^2} \\vu{r}; \\quad I = - \\dot{Q} = \\int \\vec J \\cdot \\dd \\vec a = \\frac{\\sigma Q}{\\epsilon_0} This configuration is spherically symmetrical, so the magnetic field has to be zero (the only direction it could possibly point is radial, and \\div \\vec B = 0 \\rightarrow \\oint \\vec B \\cdot \\dd \\vec a = B(4 \\pi r^2) = 0 , so B = 0 ). What? I thought currents produce magnetic fields! Isn't that what Biot-Savart and Ampere taught us? How can there be a \\vec J with no accompanying \\vec B ? Solution This is not a static configuration! Q, \\vec E, \\vec J are all functions of time; Ampere and Biot-Savart do not apply. The displacement current J_d = \\epsilon_0 \\pdv{\\vec E}{t} = \\frac{1}{4 \\pi} \\frac{\\dot{Q}}{r^2} \\vu{r} = - \\sigma \\frac{Q}{4 \\pi \\epsilon_0 r^2} \\vu{r} exactly cancels the conduction current (in Eq. 7.37), and the magnetic field (determined by \\div \\vec{B} = 0 and \\curl \\vec{B} = 0 is indeed zero.","title":"Example 7.14"},{"location":"ch7-3/#733-maxwells-equations","text":"In the last section we put the finishing touches on Maxwell's equations: Maxwell's Equations: \\begin{align*} (\\text{i}) & \\quad \\div \\vec{E} = \\frac{1}{\\epsilon_0 } \\rho \\quad \\text{(Gauss's law)} \\\\ (\\text{ii}) & \\quad \\div \\vec{B} = 0 \\quad \\text{(Ng's Law)} \\\\ (\\text{iii}) & \\quad \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} \\quad \\text{(Faraday's Law}) \\\\ (\\text{iv}) & \\quad \\curl \\vec{B} = \\mu_0 \\vec{J} + \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} \\quad \\text{(Ampere's Law)} \\end{align*} \\tagl{7.40} Together, with the force law, \\vec{F} q (\\vec{E} + \\vec{v} \\cross \\vec{B}) \\tagl{7.41} they summarize the entire theoretical content of classical electrodynamics (save for some special properties of matter, which we encountered in Chapters 4 and 6). Even the continuity equation, \\div \\vec{J} = - \\pdv{\\rho}{t} \\tagl{7.42} which is the mathematical expression of conservation of charge, can be derived from Maxwell's equations by applying the divergence to number (iv). I have written Maxwell's equations in the traditional way, which emphasizes that they specify the divergence and curl of \\vec{E} and \\vec{B} . In this form, they reinforce the notion that electric fields can be produced either by charges ( \\rho ) or by changing magnetic fields ( \\partial \\vec{B} / \\partial t ), and magnetic fields can be produced either by currents ( \\vec{J} ) or by changing electric fields ( \\partial \\vec{E} / \\partial t ). Actually, this is misleading, because \\partial \\vec{B} / \\partial t and \\partial \\vec{E} / \\partial t are themselves due to charges and currents. I think it is logically preferable to write \\div \\vec{E} = \\frac{1}{\\epsilon_0} \\rho \\div \\vec{B} = 0 \\curl \\vec{E} + \\pdv{\\vec{B}}{t} = 0 \\curl \\vec{B} - \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} = \\mu_0 \\vec{J} with the fields ( \\vec{E} and \\vec{B} ) on the left and the sources ( \\rho and \\vec{J} ) on the right. This notation emphasizes that all electromagnetic fields are ultimately attributable to charges and currents. Maxwell's equations tell you how charges produce fields; reciprocally, the force law tells you how fields affect charges.","title":"7.3.3 Maxwell's Equations"},{"location":"ch7-3/#734-magnetic-charge","text":"There is a pleasing symmetry to Maxwell's equations; it is particularly striking in free space, where \\rho and \\vec J vanish \\div \\vec E = 0 \\qquad \\curl \\vec E = - \\pdv{\\vec B}{t} \\\\ \\div \\vec B = 0 \\qquad \\curl \\vec B = - \\mu_0 \\epsilon_0 \\pdv{\\vec E}{t} If you replace \\vec E by \\vec B and \\vec B by - \\mu_0 \\epsilon_0 \\vec E , the first pair of equations turns into the second, and vice versa. This symmetry between \\vec E and \\vec B is spoiled, though, by the charge term in Gauss's law and the current term in Ampere's law. You can't help wondering why the corresponding quantities are \"missing\" from \\div \\vec B = 0 and \\curl \\vec E = - \\partial \\vec B / \\partial t . What if we had \\begin{align*} (\\text{i}) & \\quad \\div \\vec{E} = \\frac{1}{\\epsilon_0 } \\rho_e \\\\ (\\text{ii}) & \\quad \\div \\vec{B} = \\mu_0 \\rho_m \\\\ (\\text{iii}) & \\quad \\curl \\vec{E} = - \\mu_0 \\vec{J}_m - \\pdv{\\vec{B}}{t} \\\\ (\\text{iv}) & \\quad \\curl \\vec{B} = \\mu_0 \\vec{J} + \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} \\end{align*} \\tagl{7.44} Then \\rho_m would represent the density of magnetic \"charge\", and \\rho_e the density of electric charge; \\vec J_m would be the current of magnetic charge, and \\vec J_e the current of electric charge. Both charges would be conserved: \\div \\vec{J}_m = - \\pdv{\\rho_m}{t} \\quad \\div \\vec{J}_e = - \\pdv{\\rho_e}{t} \\tagl{7.45} The former follows by application of the divergence to (iii), the latter by taking the divergence of (iv). In a sense, Maxwell's equations beg for magnetic charge to exist - it would fit in so nicely. And yet, in spite of a diligent search, no one has ever found any. As far as we know, \\rho_m is zero everywhere, and so is \\vec J_m ; \\vec B is not on equal footing with \\vec E : there exist stationary sources for \\vec E (electric charges) but none for \\vec B . (This is reflected in the fact that magnetic multipole expansions have no monopole term, and magnetic dipoles consist of current loops, not separated north and south \"poles.\") In quantum electrodynamics, by the way, it's a more than merely aesthetic shame that magnetic charge does not seem to exist: Dirac showed that the existence of magnetic charge would explain why electric charge is quantized.","title":"7.3.4: Magnetic Charge"},{"location":"ch7-3/#735-maxwells-equations-in-matter","text":"Maxwell's equations in the form 7.40 are complete and correct as they stand. However, when you are working with materials that are subject to electric and magnetic polarization there is a more convenient way to write them. For inside polarized matter there will be accumulations of \"bound\" charge and current, over which you exert no direct control. It would be nice to reformulate Maxwell's equations so as to make explicit reference only to the \"free\" charges and currents. We have already learned, from the static case, that an electric polarization \\vec{P} produces a bound charge density \\rho_b = - \\div \\vec{P} \\tagl{7.47} (Eq. 4.12). Likewise, a magnetic polarization (or \"magnetization\") \\vec{M} results in a bound current \\vec{J}_b = \\curl \\vec{M} \\tagl{7.48} (Eq. 6.13). There's just one new feature to consider in the nonstatic case: Any change in the electric polarization involves a flow of (bound) charge (call it \\vec{J}_p ), which must be included in the total current. For suppose we examine a tiny chunk of polarized material (Fig. 7.47). The polarization introduces a charge density \\sigma_b = P at one end and - \\sigma_b at the other (Eq. 4.11). If P now increases a bit, the charge on each end increases accordingly, giving a net current \\dd I = \\pdv{\\sigma_b}{t} \\dd a_{\\perp} = \\pdv{P}{t} \\dd a_{\\perp} The current density, therefore, is \\vec{J}_p = \\pdv{\\vec{P}}{t} \\tagl{7.49} This polarization current has nothing to do with the bound current \\vec{J}_b . The latter is associated with magnetization of the material and involves the spin and orbital motion of electrons; \\vec{J}_p by contrast, is the result of the linear motion of charge when the electric polarization changes. If P points to the right, and is increasing, then each plus charge moves a bit to the right and each minus charge to the left; the cumulative effect is the polarization current \\vec{J}_p . We ought to check that Eq. 7.49 is consistent with the continuity equation: \\div \\vec{J}_p = \\div \\pdv{\\vec{P}}{t} = \\pdv{}{t} ( \\div \\vec{P}) = - \\pdv{\\rho_b}{t} Yes: the continuity equation is satisfied: in fact \\vec{J}_p is essential to ensure the conservation of bound charge. (Incidentally, a changing magnetization does not lead to any analogous accumulation of charge or current. The bound current \\vec{J}_b = \\curl \\vec{M} varies in response to \\vec{M} , to be sure, but that's about it.) In view of all this, the total charge density can be separated into two parts: \\rho = \\rho_f + \\rho_b = \\rho_f - \\div \\vec{P} \\tagl{7.50} and the current density into three parts \\vec{J} = \\vec{J}_f + \\vec{J}_b + \\vec{J}_p = \\vec{J}_f + \\curl \\vec{M} + \\pdv{\\vec{P}}{t} \\tagl{7.51} Gauss's law can now be written as \\div \\vec{E} = \\frac{1}{\\epsilon_0} (\\rho _f - \\div \\vec{P}) or \\div \\vec{D} = \\rho_f \\tagl{7.52} where, as in the static case \\vec{D} = \\epsilon_0 \\vec{E} + \\vec{P} \\tagl{7.53} Meanwhile, Ampere's law (with Maxwell's term) becomes \\curl \\vec{B} = \\mu_0 \\left( \\vec{J}_f + \\curl \\vec{M} + \\pdv{\\vec{P}}{t} \\right) + \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} or \\curl \\vec{H} = \\vec{J}_f + \\pdv{\\vec{D}}{t} \\tagl{7.54} where, as before \\vec{H} \\equiv \\frac{1}{\\mu_0} \\vec{B} - \\vec{M} \\tagl{7.55} Faraday's law and \\div \\vec{B} = 0 are not affected by our separation of charge and current into free and bound parts, since they do not involve \\rho or \\vec{J} . In terms of free charges and currents, then, Maxwell's equations read Maxwell's Equations in Matter: \\begin{align*} (\\text{i}) & \\quad \\div \\vec{D} = \\rho_f \\quad \\text{(Gauss's law)} \\\\ (\\text{ii}) & \\quad \\div \\vec{B} = 0 \\quad \\text{(Ng's Law)} \\\\ (\\text{iii}) & \\quad \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} \\quad \\text{(Faraday's Law}) \\\\ (\\text{iv}) & \\quad \\curl \\vec{H} = \\vec{J}_f + \\pdv{\\vec{D}}{t} \\quad \\text{(Ampere's Law)} \\end{align*} \\tagl{7.56} Some people regard these as the \"true\" Maxwell's equations, but please understand that they are in no way more \"general\" than Eq. 7.40; they simply reflect a convenient division of charge and current into free and nonfree parts. And they have the disadvantage of hybrid notation, since they contain both \\vec{E} and \\vec{D} , both \\vec{B} and \\vec{H} . They must be supplemented, therefore, by appropriate constitutive relations , giving \\vec{D} and \\vec{H} in terms of \\vec{E} and \\vec{B} . These depend on the nature of the material; for linear media \\vec{P} = \\epsilon_0 \\chi_e \\vec{E} \\qquad \\text{ and } \\qquad \\vec{M} = \\chi_m \\vec{H} \\tagl{7.57} so \\vec{D} = \\epsilon \\vec{E} \\qquad \\text{ and } \\qquad \\vec{H} = \\frac{1}{\\mu} \\vec{B} \\tagl{7.58} where \\epsilon = \\epsilon_0 (1 + \\chi_e) and \\mu = \\mu_0 (1 + \\chi_m) . Incidentally, you'll remember that \\vec{D} is called the electric \"displacement\"; that's why the second term in the Ampere/Maxwell equation came to be called the displacement current . In this context \\vec{J}_d \\equiv \\pdv{\\vec{D}}{t} \\tagl{7.59}","title":"7.3.5: Maxwell's Equations in Matter"},{"location":"ch7-3/#736-boundary-conditions","text":"In general, the fields \\vec{E}, \\vec{B}, \\vec{D}, and \\vec{H} will be discontinuous at a boundary between two different media, or at a surface that carries a surface charge density \\sigma or a current density \\vec{K} . The explicity form of these discontinuities can be deduced from Maxwell's equations in their integral form \\begin{align*} (1) \\quad & \\oint_S \\vec{D} \\cdot \\dd \\vec{a} = Q_{f, enc} \\\\ (2) \\quad & \\oint _S \\vec{B} \\cdot \\dd \\vec{a} = 0 \\\\ (3) \\quad & \\oint _P \\vec{E} \\cdot \\dd \\vec{l} = - \\dv{}{t} \\int _S \\vec{B} \\cdot \\dd \\vec{a} \\\\ (4) \\quad & \\oint _P \\vec{H} \\cdot \\dd \\vec{l} = I_{f, enc} + \\dv{}{t} \\int_S \\vec{D} \\cdot \\dd \\vec a \\end{align*} Applying (1) to a tiny, wafer-thin Gaussian pillbox extending just slightly into the material on either side of the boundary (Fig 7.48), we obtain \\vec{D}_1 \\cdot \\vec{a} - \\vec{D}_2 \\cdot \\vec a = \\sigma_f a (The positive direction for \\vec a is from 2 toward 1. The edge of the wafer contributes nothing in the limit as the thickness goes to zero; nor does any volume charge density.) Thus, the component of \\vec D that is perpendicular to the interface is discontinuous in the amount D_1 ^\\perp - D_2 ^\\perp = \\sigma_f \\tagl{7.60} Identical reasoning, applied to equation (2) yields B_1 ^\\perp - B_2 ^\\perp = 0 \\tagl{7.61} Turning to (3), a very thin Amperian loop straddling the surface gives \\vec{E_1} \\cdot \\vec{l} - \\vec{E_2} \\cdot \\vec{l} = - \\dv{}{t} \\int_S \\vec{B} \\cdot \\dd \\vec a But in the limit as the width of the loop goes to zero, the flux vanishes. (I have already dropped the contribution of the two ends to \\oint \\vec{E} \\cdot \\dd \\vec l , on the same grounds) Therefore, \\vec{E}_1 ^\\parallel - \\vec{E}_2 ^\\parallel = 0 \\tagl{7.62} That is, the components of \\vec E parallel to the interface are continuous across the boundary. By the same token, (4) implies \\vec{H}_1 \\cdot \\vec l - \\vec{H}_2 \\cdot \\vec l = I_{f, enc} where I_{f, enc} is the free current passing through the Amperian loop. No volume current density will contribute (in the limit of infinitesimal width), but a surface current can. In fact, if \\vu{n} is a unit vector perpendicular to the interface (pointing from 2 toward 1), so that (\\vu{n} \\cross \\vec l ) is normal to the Amperian loop (Fig 7.49), the I_{f, enc} = \\vec{K_f} \\cdot (\\vu n \\cdot \\vec l) = (\\vec{K_f} \\cross \\vu n) \\cdot \\vec l and hence \\vec{H}_1 ^\\parallel - \\vec{H}_2 ^\\parallel = \\vec{K}_f \\cross \\vu n \\tagl{7.63} So the parallel components of \\vec H are discontinuous by an amount proportional to the free surface current density. Equations 7.60-63 are the general boundary conditions for electrodynamics. In the case of linear media, they can be expressed in terms of \\vec E and \\vec B alone \\begin{align*} (1) \\quad & \\epsilon_1 E_1 ^\\perp - \\epsilon_2 E_2 ^\\perp = \\sigma_f \\\\ (2) \\quad & \\vec{B}_1 ^\\perp - \\vec{B}_2 ^\\perp = 0 \\\\ (3) \\quad & \\vec{E}_1 ^\\parallel - \\vec{E}_2 ^\\parallel = 0 \\\\ (4) \\quad & \\frac{1}{\\mu_1} \\vec{B}_1 ^\\parallel - \\frac{1}{\\mu_2} \\vec{B}_2 ^\\parallel = \\vec{K}_f \\cross \\vu{n} \\end{align*} \\tagl{7.64} In particular, if there is no free charge or free current at the interface, then \\begin{align*} (1) \\quad & \\epsilon_1 E_1 ^\\perp - \\epsilon_2 E_2 ^\\perp = 0 \\\\ (2) \\quad & \\vec{B}_1 ^\\perp - \\vec{B}_2 ^\\perp = 0 \\\\ (3) \\quad & \\vec{E}_1 ^\\parallel - \\vec{E}_2 ^\\parallel = 0 \\\\ (4) \\quad & \\frac{1}{\\mu_1} \\vec{B}_1 ^\\parallel - \\frac{1}{\\mu_2} \\vec{B}_2 ^\\parallel = 0 \\end{align*} \\tagl{7.65} These equations provide the basis for the theory of reflection and refraction.","title":"7.3.6: Boundary Conditions"},{"location":"ch8-0/","text":"8.0: Phys 544 Introduction Author Notes From here on out, the text will be based on the lecture content, not on the Griffiths textbook. The class will follow the 4th edition pretty closely, but there will definitely be some differences. Hope I don't get too much wrong! Course Structure We've got some real homework this time! 33% Homework, more or less weekly 33% Midterm. 1 hour, closed-book 33% Final + presentation. Topic should be related to EM radiation applications. It'll be a ~10 minute presentation and a 4-page paper. For homework, either email it by Thursday 5pm, or preferably just hand it in on Wednesday evening during class.","title":"8.0 - Phys544 Introduction"},{"location":"ch8-0/#80-phys-544-introduction","text":"","title":"8.0: Phys 544 Introduction"},{"location":"ch8-0/#author-notes","text":"From here on out, the text will be based on the lecture content, not on the Griffiths textbook. The class will follow the 4th edition pretty closely, but there will definitely be some differences. Hope I don't get too much wrong!","title":"Author Notes"},{"location":"ch8-0/#course-structure","text":"We've got some real homework this time! 33% Homework, more or less weekly 33% Midterm. 1 hour, closed-book 33% Final + presentation. Topic should be related to EM radiation applications. It'll be a ~10 minute presentation and a 4-page paper. For homework, either email it by Thursday 5pm, or preferably just hand it in on Wednesday evening during class.","title":"Course Structure"},{"location":"ch8-1/","text":"8.1: Charge and Energy As far as particles go, we're familiar with the momentum and energy of charges. With fields, we have a similar situation. The fields determine what the particles do, and together with the energy/momentum of the particles themselves, the energy/momentum of the fields form a set of conservation laws Conservation of Charge We know that charge is conserved. You can't create or destroy charge by itself - you must compensate by destroying or creating some other charge. It's captured in the equation of continuity, which relates charge to the divergence of current. In laymen's terms, it says \"if a charge was here, it must still be here or it must flow away.\" You can relate the amount of charge that's missing from a given region of space to the amount of charge that flows away from that element of space. This is a statement of conservation of local charge, not just global charge, which is a much stronger statement. Say we have some charge distribution \\rho(\\vec{r}, t) over a region S , and we have some current defined over the boundary \\vec{J}(\\vec{r}, t) . The relation between \\rho and \\vec{J} gives the equation of continuity. The change of charge within the entire volume must be compensated for by an amount of charge flowing in/out of the system: \\dv{Q}{t} = \\dv{}{t} \\int_{V} \\rho \\dd \\tau = - \\oint \\vec{J} \\cdot \\dd a We get a negative sign because by convention the normal to the surface points outward. We can take the time integral inside, and use Gauss's theorem (divergence theorem) on the right-side \\int_V \\pdv{\\rho}{t} \\dd \\tau = - \\int_V (\\div \\vec{J}) \\dd \\tau Since we chose an arbitrary volume, it must be the case that the integrands are equal, over all space. \\pdv{\\rho}{t} = - \\div \\vec{J} \\tagl{8.1} Let's check that this is consistent with our Maxwell equations. Ampere law: \\curl \\vec{B} = \\mu_0 \\vec{J} + \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} Take divergence of both sides \\div( \\curl \\vec{B} ) = \\mu_0 \\div( \\vec{J} ) + \\mu_0 \\epsilon_0 \\div \\left( \\pdv{\\vec{E}}{t} \\right) \\\\ 0 = \\mu_0 \\div( \\vec{J} ) + \\mu_0 \\epsilon_0 \\pdv{}{t} \\left( \\div \\vec{E} \\right) \\\\ 0 = \\mu_0 \\div( \\vec{J} ) + \\mu_0 \\epsilon_0 \\pdv{\\rho}{t} \\frac{1}{\\epsilon_0} \\\\ \\rightarrow \\pdv{\\rho}{t} + \\div \\vec{J} = 0 So we're back to the continuity equation! Conservation of Energy and Poynting's Theorem The energy conservation we'll find is a combination of the energy stored in / done on the charges and the energy stored in the field. The new concept is the Poynting vector, describing the spatial flow of energy. The Maxwell's equations tell us how to get from distributions of charges and currents to fields \\vec{E} and \\vec{B} . The given distributions must satisfy the continuity relation, of course. Let's take a look at the amount of work that's done on the source charges, and use the result to connect to \"where\" the energy is, in some volume. \\vec{F} = q [ \\vec{E} + \\vec{v} \\cross \\vec{B} ] \\dd W = \\vec{F} \\cdot \\dd \\vec{l} = q ( \\vec{E} + \\vec{v} \\cross \\vec{B} ) \\cdot \\vec{v} \\dd t We want to generalize to a charge distribution \\dd q = \\rho \\dd \\tau \\dv{W}{t} = \\int_V \\vec{E} \\cdot [ (\\rho \\dd \\tau) \\vec{v} ] = \\int_V (\\vec{E} \\cdot \\vec{J}) \\dd \\tau Can we get an expression for \\dd W in terms of the fields only? Let's try and eliminate \\vec{F} in terms of \\vec{E} and \\vec{B} using Maxwell's equations. \\vec{J} = \\frac{\\curl \\vec{B}}{\\mu_0} - \\epsilon_0 \\pdv{\\vec{E}}{t} \\dv{W}{t} = \\int_V \\left( \\frac{\\vec{E} \\cdot (\\curl \\vec{B}) }{\\mu_0} - \\epsilon_0 \\vec{E} \\cdot \\pdv{\\vec{E}}{t} \\right) \\dd \\tau Using vector identity \\curl (\\vec{A} \\cross \\vec{B}) = \\vec{B} \\cdot (\\curl \\vec{A}) - \\vec{A} \\cdot (\\curl \\vec{B}) \\dv{W}{t} = \\int_V \\left( \\frac{1}{\\mu_0} \\left( \\vec{B} \\cdot (\\curl \\vec{E}) - \\div (\\vec{E} \\cross \\vec{B}) \\right) - \\epsilon_0 \\vec{E} \\cdot \\pdv{\\vec{E}}{t} \\right) \\dd \\tau \\\\ = \\int_V \\left( - \\frac{\\vec{B}}{\\mu_0} \\cdot \\pdv{\\vec{B}}{t} - \\div \\vec{S} - \\epsilon_0 \\vec{E} \\cdot \\pdv{\\vec{E}}{t} \\right) \\dd \\tau where \\vec{S} is the so-called Poynting vector defined as \\vec{S} = \\frac{\\vec{E} \\cross \\vec{B}}{\\mu_0} Let's take a look at the quantity \\dv{}{t} (\\vec{E} ^2) \\dv{}{t} \\left( \\vec{E}^2 \\right) = \\vec{E} \\cdot \\pdv{\\vec{E}}{t} + \\pdv{\\vec{E}}{t} \\cdot \\vec E \\quad \\rightarrow \\quad \\vec E \\cdot \\pdv{\\vec E}{t} = \\frac{1}{2} \\dv{}{t} \\left( \\vec E ^2 \\right) \\dv{W}{t} = \\int_V \\left( - \\frac{1}{2 \\mu_0} \\pdv{}{t} \\left( \\vec{B} ^2 \\right) - \\frac{\\epsilon_0}{2} \\pdv{}{t} \\left( \\vec E ^2 \\right) - \\div \\vec{S} \\right) \\dd \\tau \\\\ = - \\dv{}{t} \\left[ \\int_V \\frac{\\vec B ^2}{2 \\mu_0} \\dd \\tau + \\int_V \\frac{\\epsilon_0 \\vec E ^2}{2} \\dd \\tau \\right] - \\int_V \\div \\vec{S} \\dd \\tau The integrands in the middle are just the energy densities of the electric and magnetic fields \\frac{\\vec B ^2}{2 \\mu_0} = u_m \\qquad \\frac{\\epsilon_0 \\vec E ^2}{2} = u_e \\qquad u_{em} \\equiv u_e + u_m = u \\dv{W}{t} = - \\dv{}{t} \\int_V u_{em} \\dd \\tau - \\oint _S \\vec{S} \\cdot \\dd \\vec a \\tagl{8.2} So what does this tell us? The left hand side reads \"the change in energy in a region V \". The first part of the r.h.s is the change in the local energy stored in the E and B fields, and the second part is an acknowledgment that our region is part of a larger space, and energy may be flowing in/out of the region. The flow of energy is therefore given by the Poynting vector, so we call \\eqref{8.1} \"Poynting's Theorem\" In a charge-free region: 0 = - \\dv{}{t} \\int_V u_{em} \\dd \\tau - \\int_V ( \\div \\vec S ) \\cdot \\dd \\tau \\\\ 0 = - \\int_V \\pdv{u_{em}}{t} \\dd \\tau - \\int_V ( \\div \\vec S) \\dd \\tau Since this is true for any arbitrary V , we have \\rightarrow \\pdv{u_{em}}{t} + \\div \\vec{S} = 0 \\quad \\text{(in charge-free region)} Let's look at an example of the application of this statement of conservation of energy. Problem 8.2 Consider the charging capacitor from problem 7.34. (a) Find the electric and magnetic fields in the gap, as functions of the distance s from the axis at time t (assume the charge is zero at t = 0). (b) Find the energy density u_{em} and the Poynting vector \\vec S in the gap. Note especially the direction of \\vec S . Check that \\eqref{8.2} is satisfied. (c) Determine the total energy in the gap, as a function of time. Calculate the total power flowing into the gap, by integrating the Poynting vector over the appropriate surface. Check that the power input is equal to the rate of increase of energy in the gap. (a) From the ch. 7 problems, we know \\vec B = \\frac{\\mu_0 I}{2 \\pi a^2} s \\hat{\\phi} Treating the gap as a parallel-plate capacitor, the electric field in the gap is \\vec E = \\frac{\\sigma}{\\epsilon_0} \\hat{z} = \\frac{I t}{\\epsilon_0 \\pi a^2} \\hat{z} (b) u_{em} = \\frac{\\vec B ^2}{2 \\mu_0} + \\frac{\\epsilon_0 ^2 \\vec E ^2}{2} \\\\ = \\frac{1}{2} \\epsilon_0 \\frac{ I^2 t^2}{\\epsilon_0 \\pi^2 a^4} + \\frac{\\mu_0 ^2 I^2}{2 \\mu_0 4 \\pi ^2 a^4} s^2 \\\\ u_{em} = \\frac{1}{2} \\frac{I^2}{\\pi ^2 a^4} \\left[ \\frac{t^2}{\\epsilon_0} + \\frac{\\mu_0 s^2}{4} \\right] \\vec S = \\frac{1}{\\mu_0} (\\vec E \\cross \\vec B) First, let's check what the direction of \\vec S is. We know that \\vec B is azimuthal and \\vec E is axial, so \\vec S must point radially inward. Working through the expression, we get \\vec S = - \\frac{1}{\\epsilon_0} \\frac{I^2 s t}{2 \\pi^2 a^4} \\hat{s} Checking Poynting's theorem... \\pdv{u_{em}}{t} = \\frac{I^2 t}{\\pi^2 a^4 \\epsilon_0} \\div \\vec S = \\frac{1}{s} \\pdv{}{s} \\left( s S_s \\right) = \\frac{I^2 t}{\\pi^2 a^4 \\epsilon_0} \\quad \\checkmark (c) The power flowing into the gap: we add a negative sign, since conventionally the normal direction is outwards, and the poynting vector points inwards. - \\frac{1}{\\mu_0} \\int (\\vec E \\cross \\vec B) \\cdot \\dd \\vec a = + \\frac{1}{\\epsilon_0} \\frac{I^2 s t}{2 \\pi^2 a^4} \\cross 2 \\pi a w \\\\ = \\frac{1}{\\epsilon_0} \\frac{I^2 t w}{\\pi a^2} The rate of increase of energy in the gap would be the time integral of the volume integral of the energy density u_{em} in the gap. Again, the area of the cylinder is \\pi a^2 w \\dv{}{t} \\left[ \\int u_{em} \\dd \\tau \\right] = \\dv{}{t} \\frac{I^2}{2 \\pi^2 a^4} \\frac{t^2}{\\epsilon_0} \\cdot \\pi a^2 w \\\\ = \\frac{I^2 t}{\\epsilon_0 \\pi a^2 } = - \\oint \\vec S \\cdot \\dd \\vec a \\quad \\checkmark","title":"8.1 - Charge and Energy"},{"location":"ch8-1/#81-charge-and-energy","text":"As far as particles go, we're familiar with the momentum and energy of charges. With fields, we have a similar situation. The fields determine what the particles do, and together with the energy/momentum of the particles themselves, the energy/momentum of the fields form a set of conservation laws","title":"8.1: Charge and Energy"},{"location":"ch8-1/#conservation-of-charge","text":"We know that charge is conserved. You can't create or destroy charge by itself - you must compensate by destroying or creating some other charge. It's captured in the equation of continuity, which relates charge to the divergence of current. In laymen's terms, it says \"if a charge was here, it must still be here or it must flow away.\" You can relate the amount of charge that's missing from a given region of space to the amount of charge that flows away from that element of space. This is a statement of conservation of local charge, not just global charge, which is a much stronger statement. Say we have some charge distribution \\rho(\\vec{r}, t) over a region S , and we have some current defined over the boundary \\vec{J}(\\vec{r}, t) . The relation between \\rho and \\vec{J} gives the equation of continuity. The change of charge within the entire volume must be compensated for by an amount of charge flowing in/out of the system: \\dv{Q}{t} = \\dv{}{t} \\int_{V} \\rho \\dd \\tau = - \\oint \\vec{J} \\cdot \\dd a We get a negative sign because by convention the normal to the surface points outward. We can take the time integral inside, and use Gauss's theorem (divergence theorem) on the right-side \\int_V \\pdv{\\rho}{t} \\dd \\tau = - \\int_V (\\div \\vec{J}) \\dd \\tau Since we chose an arbitrary volume, it must be the case that the integrands are equal, over all space. \\pdv{\\rho}{t} = - \\div \\vec{J} \\tagl{8.1} Let's check that this is consistent with our Maxwell equations. Ampere law: \\curl \\vec{B} = \\mu_0 \\vec{J} + \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} Take divergence of both sides \\div( \\curl \\vec{B} ) = \\mu_0 \\div( \\vec{J} ) + \\mu_0 \\epsilon_0 \\div \\left( \\pdv{\\vec{E}}{t} \\right) \\\\ 0 = \\mu_0 \\div( \\vec{J} ) + \\mu_0 \\epsilon_0 \\pdv{}{t} \\left( \\div \\vec{E} \\right) \\\\ 0 = \\mu_0 \\div( \\vec{J} ) + \\mu_0 \\epsilon_0 \\pdv{\\rho}{t} \\frac{1}{\\epsilon_0} \\\\ \\rightarrow \\pdv{\\rho}{t} + \\div \\vec{J} = 0 So we're back to the continuity equation!","title":"Conservation of Charge"},{"location":"ch8-1/#conservation-of-energy-and-poyntings-theorem","text":"The energy conservation we'll find is a combination of the energy stored in / done on the charges and the energy stored in the field. The new concept is the Poynting vector, describing the spatial flow of energy. The Maxwell's equations tell us how to get from distributions of charges and currents to fields \\vec{E} and \\vec{B} . The given distributions must satisfy the continuity relation, of course. Let's take a look at the amount of work that's done on the source charges, and use the result to connect to \"where\" the energy is, in some volume. \\vec{F} = q [ \\vec{E} + \\vec{v} \\cross \\vec{B} ] \\dd W = \\vec{F} \\cdot \\dd \\vec{l} = q ( \\vec{E} + \\vec{v} \\cross \\vec{B} ) \\cdot \\vec{v} \\dd t We want to generalize to a charge distribution \\dd q = \\rho \\dd \\tau \\dv{W}{t} = \\int_V \\vec{E} \\cdot [ (\\rho \\dd \\tau) \\vec{v} ] = \\int_V (\\vec{E} \\cdot \\vec{J}) \\dd \\tau Can we get an expression for \\dd W in terms of the fields only? Let's try and eliminate \\vec{F} in terms of \\vec{E} and \\vec{B} using Maxwell's equations. \\vec{J} = \\frac{\\curl \\vec{B}}{\\mu_0} - \\epsilon_0 \\pdv{\\vec{E}}{t} \\dv{W}{t} = \\int_V \\left( \\frac{\\vec{E} \\cdot (\\curl \\vec{B}) }{\\mu_0} - \\epsilon_0 \\vec{E} \\cdot \\pdv{\\vec{E}}{t} \\right) \\dd \\tau Using vector identity \\curl (\\vec{A} \\cross \\vec{B}) = \\vec{B} \\cdot (\\curl \\vec{A}) - \\vec{A} \\cdot (\\curl \\vec{B}) \\dv{W}{t} = \\int_V \\left( \\frac{1}{\\mu_0} \\left( \\vec{B} \\cdot (\\curl \\vec{E}) - \\div (\\vec{E} \\cross \\vec{B}) \\right) - \\epsilon_0 \\vec{E} \\cdot \\pdv{\\vec{E}}{t} \\right) \\dd \\tau \\\\ = \\int_V \\left( - \\frac{\\vec{B}}{\\mu_0} \\cdot \\pdv{\\vec{B}}{t} - \\div \\vec{S} - \\epsilon_0 \\vec{E} \\cdot \\pdv{\\vec{E}}{t} \\right) \\dd \\tau where \\vec{S} is the so-called Poynting vector defined as \\vec{S} = \\frac{\\vec{E} \\cross \\vec{B}}{\\mu_0} Let's take a look at the quantity \\dv{}{t} (\\vec{E} ^2) \\dv{}{t} \\left( \\vec{E}^2 \\right) = \\vec{E} \\cdot \\pdv{\\vec{E}}{t} + \\pdv{\\vec{E}}{t} \\cdot \\vec E \\quad \\rightarrow \\quad \\vec E \\cdot \\pdv{\\vec E}{t} = \\frac{1}{2} \\dv{}{t} \\left( \\vec E ^2 \\right) \\dv{W}{t} = \\int_V \\left( - \\frac{1}{2 \\mu_0} \\pdv{}{t} \\left( \\vec{B} ^2 \\right) - \\frac{\\epsilon_0}{2} \\pdv{}{t} \\left( \\vec E ^2 \\right) - \\div \\vec{S} \\right) \\dd \\tau \\\\ = - \\dv{}{t} \\left[ \\int_V \\frac{\\vec B ^2}{2 \\mu_0} \\dd \\tau + \\int_V \\frac{\\epsilon_0 \\vec E ^2}{2} \\dd \\tau \\right] - \\int_V \\div \\vec{S} \\dd \\tau The integrands in the middle are just the energy densities of the electric and magnetic fields \\frac{\\vec B ^2}{2 \\mu_0} = u_m \\qquad \\frac{\\epsilon_0 \\vec E ^2}{2} = u_e \\qquad u_{em} \\equiv u_e + u_m = u \\dv{W}{t} = - \\dv{}{t} \\int_V u_{em} \\dd \\tau - \\oint _S \\vec{S} \\cdot \\dd \\vec a \\tagl{8.2} So what does this tell us? The left hand side reads \"the change in energy in a region V \". The first part of the r.h.s is the change in the local energy stored in the E and B fields, and the second part is an acknowledgment that our region is part of a larger space, and energy may be flowing in/out of the region. The flow of energy is therefore given by the Poynting vector, so we call \\eqref{8.1} \"Poynting's Theorem\" In a charge-free region: 0 = - \\dv{}{t} \\int_V u_{em} \\dd \\tau - \\int_V ( \\div \\vec S ) \\cdot \\dd \\tau \\\\ 0 = - \\int_V \\pdv{u_{em}}{t} \\dd \\tau - \\int_V ( \\div \\vec S) \\dd \\tau Since this is true for any arbitrary V , we have \\rightarrow \\pdv{u_{em}}{t} + \\div \\vec{S} = 0 \\quad \\text{(in charge-free region)} Let's look at an example of the application of this statement of conservation of energy.","title":"Conservation of Energy and Poynting's Theorem"},{"location":"ch8-1/#problem-82","text":"Consider the charging capacitor from problem 7.34. (a) Find the electric and magnetic fields in the gap, as functions of the distance s from the axis at time t (assume the charge is zero at t = 0). (b) Find the energy density u_{em} and the Poynting vector \\vec S in the gap. Note especially the direction of \\vec S . Check that \\eqref{8.2} is satisfied. (c) Determine the total energy in the gap, as a function of time. Calculate the total power flowing into the gap, by integrating the Poynting vector over the appropriate surface. Check that the power input is equal to the rate of increase of energy in the gap. (a) From the ch. 7 problems, we know \\vec B = \\frac{\\mu_0 I}{2 \\pi a^2} s \\hat{\\phi} Treating the gap as a parallel-plate capacitor, the electric field in the gap is \\vec E = \\frac{\\sigma}{\\epsilon_0} \\hat{z} = \\frac{I t}{\\epsilon_0 \\pi a^2} \\hat{z} (b) u_{em} = \\frac{\\vec B ^2}{2 \\mu_0} + \\frac{\\epsilon_0 ^2 \\vec E ^2}{2} \\\\ = \\frac{1}{2} \\epsilon_0 \\frac{ I^2 t^2}{\\epsilon_0 \\pi^2 a^4} + \\frac{\\mu_0 ^2 I^2}{2 \\mu_0 4 \\pi ^2 a^4} s^2 \\\\ u_{em} = \\frac{1}{2} \\frac{I^2}{\\pi ^2 a^4} \\left[ \\frac{t^2}{\\epsilon_0} + \\frac{\\mu_0 s^2}{4} \\right] \\vec S = \\frac{1}{\\mu_0} (\\vec E \\cross \\vec B) First, let's check what the direction of \\vec S is. We know that \\vec B is azimuthal and \\vec E is axial, so \\vec S must point radially inward. Working through the expression, we get \\vec S = - \\frac{1}{\\epsilon_0} \\frac{I^2 s t}{2 \\pi^2 a^4} \\hat{s} Checking Poynting's theorem... \\pdv{u_{em}}{t} = \\frac{I^2 t}{\\pi^2 a^4 \\epsilon_0} \\div \\vec S = \\frac{1}{s} \\pdv{}{s} \\left( s S_s \\right) = \\frac{I^2 t}{\\pi^2 a^4 \\epsilon_0} \\quad \\checkmark (c) The power flowing into the gap: we add a negative sign, since conventionally the normal direction is outwards, and the poynting vector points inwards. - \\frac{1}{\\mu_0} \\int (\\vec E \\cross \\vec B) \\cdot \\dd \\vec a = + \\frac{1}{\\epsilon_0} \\frac{I^2 s t}{2 \\pi^2 a^4} \\cross 2 \\pi a w \\\\ = \\frac{1}{\\epsilon_0} \\frac{I^2 t w}{\\pi a^2} The rate of increase of energy in the gap would be the time integral of the volume integral of the energy density u_{em} in the gap. Again, the area of the cylinder is \\pi a^2 w \\dv{}{t} \\left[ \\int u_{em} \\dd \\tau \\right] = \\dv{}{t} \\frac{I^2}{2 \\pi^2 a^4} \\frac{t^2}{\\epsilon_0} \\cdot \\pi a^2 w \\\\ = \\frac{I^2 t}{\\epsilon_0 \\pi a^2 } = - \\oint \\vec S \\cdot \\dd \\vec a \\quad \\checkmark","title":"Problem 8.2"},{"location":"ch8-2/","text":"8.2: Momentum In this chapter we talk about: Electromagnetic momentum Maxwell stress tensor Conservation of electromagnetic momentum Angular momentum in EM fields 8.2.1: Electromagnetic Momentum As it turns out, if you disregard the momentum associated with electromagnetic fields, Newton's laws appear not to work out! Consider a basic system in cartesian coordinates of two moving point charges: What happens between the two charges? Well, the magnetic field of q_1 points into the page at q_2 , so the magnetic force on q_2 is to the right, and the magnetic field of q_2 is out of the page at q_1 , so the magnetic force on q_1 is upward. The net electric force between the two charges is repulsive and opposite, but the magnetic forces aren't, so the electromagnetic force on q_1 on q_2 is equal but not opposite to the force of q_2 on q_1 , in violation of Newton's third law! We've got a problem, and we're going to solve it by invoking the momentum of the EM field. 8.2.2 The Maxwell Stress Tensor The way to recover conservation of momentum proceeds the same way we recovered the conservation of energy via the Poynting vector. Starting with the basic Coulomb/Lorentz laws, we'll write down an expression for the electromagnetic force on charges in a volume. We're going to integrate that over all space, which can have any distribution of charge, and relate that expression for force to an expression which only involves the field. In the interest of brevity, we'll skip around a bit and leave the full derivations for the real textbook. Suppose we have a volume V containing some distribution of charge, current, and electromagnetic fields. The total force on that volume is \\vec{F} = \\int_V ( \\vec E + \\vec v \\cross \\vec B) \\rho \\dd \\tau \\qquad (\\dd q = \\rho \\dd \\tau) \\\\ = \\int _V (\\rho \\vec E + \\vec J \\cross \\vec B ) \\dd \\tau \\qquad (\\vec J = \\rho \\vec v) Again, the goal is to replace anything that looks like a source in favor of fields, using Maxwell's equations. It's handy to define the force per unit volume f : \\vec f \\equiv \\rho \\vec E + \\vec J \\cross \\vec B \\rho = \\epsilon_0 ( \\div \\vec E ) \\quad \\text{(Gauss' Law)} \\\\ \\vec J = \\frac{1}{\\mu_0} \\curl \\vec B - \\epsilon_0 \\pdv{\\vec E}{t} \\rightarrow \\vec f = \\epsilon_0 ( \\div \\vec E) \\vec E + \\left( \\frac{\\curl \\vec B}{\\mu_0} - \\epsilon_0 \\pdv{\\vec E}{t} \\right) \\cross \\vec B Skipping through a few steps, we cut to the chase. Similar to the derivation of the Poynting theorem, also using the other two Maxwell equations we haven't yet, we get \\vec f = \\div \\overline{\\vec T} - \\epsilon_0 \\mu_0 \\pdv{\\vec S}{t} \\overline{\\vec T} \\equiv \\begin{pmatrix} T_{xx} & T_{xy} & T_{xz} \\\\ T_{yx} & T_{yy} & T_{yz} \\\\ T_{zx} & T_{zy} & T_{zz} \\end{pmatrix} \\\\ T_{ij} \\equiv \\epsilon_0 \\left( E_i E_j - \\frac{1}{2} \\delta_{ij} E^2 \\right) + \\frac{1}{\\mu_0} \\left(B_i B_j - \\frac{1}{2} \\delta_{ij} B^2 \\right) where \\vec S is the Poynting vector and \\overline{\\vec T} is the so-called \"Maxwell stress tensor.\" To keep in mind what kind of units we're talking about here, \\vec f has units force per unit volume, and the divergence will strip one spatial dimension, so the Maxwell stress tensor will have units of stress (force per unit area). The tensor has diagonal \"pressure\" terms and off-diagonal \"shear\" terms. For \"pressure\" forces, the force and area are in the same direction, and in the \"shear\" case the force and area are orthogonal. The divergence term \\div \\overline{\\vec T} is itself a vector \\div \\overline{\\vec T} = \\left( \\vu{i} \\pdv{}{x} + \\vu j \\pdv{}{y} + \\vu k \\pdv{}{z} \\right) \\cdot \\overline{\\vec T} \\\\ = \\epsilon_0 \\left[ (\\div \\vec E) E_j + (\\vec E \\cdot \\grad) E_j - \\frac{1}{2} \\grad _j E^2 \\right] \\\\ + \\frac{1}{\\mu_0} \\left[ ( \\div \\vec B) B_j + (\\vec B \\cdot \\grad) B_j - \\frac{1}{2} \\grad _j B^2 \\right] As we do the volume integral to go from \\vec f to \\vec F \\vec F = \\int _V \\left(\\div\\overline{\\vec T} - \\epsilon_0 \\mu_0 \\pdv{\\vec S}{t} \\right) \\dd \\tau \\\\ = \\oint \\overline{\\vec{T}} \\cdot \\dd \\vec a - \\epsilon_0 \\mu_0 \\pdv{}{t} \\int \\vec{S} \\dd \\tau 8.2.3 Conservation of Electromagnetic Momentum \\vec F = \\dv{\\vec{p}_{mech}}{t} = - \\epsilon_0 \\mu_0 \\dv{}{t} \\int_V \\vec S \\dd \\tau + \\oint \\overline{\\vec T} \\cdot \\dd \\vec a The first term on the right is related to the momentum stored in the electromagnetic field. The second term is the rate at which momentum flows across the surface, and we describe the left-hand-side as the rate of change of the momentum of charges within the volume. We identify another useful term as the first integrand on the right: \\vec g \\equiv \\epsilon_0 \\mu_0 \\vec S = \\epsilon_0 (\\vec E \\cross B) \\quad \\text{(momentum density in EM fields)} which is the momentum density within the fields. Just as a note, the signs here are swapped from the Poynting theorem - the Maxwell stress tensor is defined such that momentum flowing into the region corresponds with increasing \\overline{\\vec T} , and vice-versa, opposite the case we had with \\vec S . In a charge-free region, - \\dv{}{t} \\int_V \\vec g \\dd \\tau + \\oint_S \\overline{\\vec T} \\cdot \\dd \\vec a = 0 and since the above is true for all regions V , we have our familiar continuity-type equation - \\pdv{\\vec g}{t} + \\div \\overline{\\vec T} = 0 Example: Problem 8.7 Consider an infinite parallel-plate capacitor, with the lower plate (at z = -d/2 ) carrying surface charge density -\\sigma , and the upper plate (at z = +d/2 ) carrying charge density + \\sigma . (a) Determine all elements of the stress tensor, in the region between the plates. Lucky for us, the magnetic field between the plates is \\vec B = 0 The electric field is very simple \\vec E = \\frac{\\sigma}{\\epsilon_0} ( - \\vu z) So we can already tell that the off-diagonal terms will be zero, since they all contain two factors of E_i E_j , one of which will be zero. The B terms on the diagonal will be zero, and \\overline{\\vec T} = \\frac{\\sigma^2}{2 \\epsilon_0} \\begin{pmatrix} -1 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} (b) Use \\vec F = \\oint_S \\overline{\\vec T} \\cdot \\dd \\vec a at the boundary to determine the electromagnetic force per unit area on the top plate. The force on the top plate will be \\vec F / A = \\frac{1}{A} \\oint _S \\overline{\\vec T} \\cdot \\dd \\vec a \\\\ = \\frac{1}{A} \\frac{\\sigma ^2}{2 \\epsilon_0} \\begin{pmatrix} -1 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ -A \\end{pmatrix} \\\\ = - \\frac{\\sigma^2}{2 \\epsilon_0} \\vu z (c) What is the electromagnetic momentum per unit area, per unit time, crossing the xy plane (or any other plane parallel to that one, between the plates)? This is just to show us that the momentum flow through the interior of the capacitor is the same as whatever force is pulling the top and bottom plates apart. - T_{zz} = - \\sigma^2 / 2 \\epsilon_0 is the momentum in the z direction crossing a surface perpendicular to z, per unit area, per unit time. 8.2.4 Angular Momentum in EM Fields As a reminder, we associate with the electromagnetic fields an energy density u_{em} = \\frac{1}{2} \\epsilon_0 E^2 + \\frac{\\mu_0}{2} B^2 and a momentum density \\vec g = \\epsilon_0 ( \\vec E \\cross \\vec B) For that matter, we define angular momentum in the normal fashion \\vec l = \\vec r \\cross \\vec g = \\epsilon_0 [ \\vec r \\cross ( \\vec E \\cross \\vec B) ] where the presence of \\vec r means it's defined about some point or axis. Example 8.4 Imagine a very long solenoid with radius R , n turns per unit length, and current I . Coaxial with the solenoid are two long cylindrical (non-conducting) shells of length l - one inside the solenoid at radius a carries a charge +Q distributed uniformly over its surface; the other outside the solenoid at radius b carries charge -Q . When the current in the solenoid is gradually reduced to nothing, the cylinders begin to rotate. Question: where does the angular momentum come from? We'll start by writing down the initial angular momentum of the system, then see what happens when we start to change the current. It is assumed that the solenoid is very long compared with the charged cylinders, and that the length of each charged cylinder is much much greater than its radius. The initial \\vec E is given by looking at a Gaussian cylinder between the two charged cylinders E 2 \\pi s l = \\frac{Q}{\\epsilon_0} \\rightarrow \\vec E = \\frac{Q}{2 \\pi \\epsilon_0 l} \\frac{1}{s} \\vu s \\qquad a < s < b The initial \\vec B is just that of the solenoid, namely \\vec B = \\begin{cases} \\mu_0 n I \\vu z & \\qquad 0 < s < R \\\\ 0, & \\qquad s > R \\end{cases} The linear momentum density we defined earlier is \\vec g = \\epsilon_0 \\mu_0 \\vec S = \\epsilon_0 ( \\vec E \\cross \\vec B) = \\frac{Q n I \\mu_0 ^2}{2 \\pi s} (- \\vu \\phi) And the angular momentum density with respect to the cylindrical axis is \\vec l = \\vec s \\cross \\vec g And it will point in the - \\vu z direction \\vec L = \\int _V \\vec l \\dd \\tau = \\int_V \\vec s \\cross \\epsilon_0 \\frac{Q}{2 \\pi \\epsilon_0 l} \\frac{1}{s} \\vu s \\cross \\mu_0 I n \\vu z \\dd \\tau = - \\frac{\\mu_0 n I Q }{2 \\pi l} \\int _V \\vu s \\cross \\vu \\phi \\dd \\tau = - \\frac{\\mu_0 n I Q}{2 \\pi l} \\vu z \\pi (R^2 - a^2 ) l = - \\frac{\\mu_n n I Q}{2} (R^2 - a^2) \\vu z So that's the initial angular momentum. Of course, nothing is moving, so this is just the momentum stored in the EM fields. Assuming nothing like friction complicates the situation, any angular momentum lost by the EM fields will be transferred to the cylinders as rotation. As we crank down the current, we know that a time variation in the magnetic field implies an induced EMF \\mathcal{E} = - \\dv{\\Phi}{t} . \\vec E experienced at radii a and b are now different. Use Faraday E_a 2 \\pi a = - \\mu_0 n \\dv{I}{t} \\pi a^2 \\rightarrow \\vec E_a = - \\frac{\\mu_0 n a}{2} \\dv{I}{t} \\vu \\phi This field is in the direction to torque cylinder +Q \\vec{\\Gamma_a} = \\vec s \\cross (Q \\vec{E_a}) = - \\frac{\\mu_0 n a}{2} \\dv{I}{t} Q a \\vu \\phi = - \\frac{\\mu_0 n a^2}{2} Q \\dv{I}{t} \\vu z Integrating torque over time, we get the changed angular momentum of the cylinder \\vec{L_a} = \\int_0 ^t \\tau \\dd t = - \\frac{\\mu_0 n a^2}{2} Q \\int_0 ^\\infty \\dv{I}{t} \\dd t \\vu z = - \\frac{\\mu_0 n a^2}{2} Q \\vu z \\int_0 ^\\infty \\dd I = \\frac{\\mu_0 n I}{2} a^2 \\vu z What happens to the outer cylinder? Something very similar, but when we calculated from Faraday's law our loop radius is b , and the area of flux is \\pi R^2 \\vec E_b = - \\frac{\\mu_0 n R^2}{2a} \\dv{I}{t} \\vu \\phi \\vec{F_b} = - Q \\vec{E}_b \\vec {\\Gamma _b} = b \\vu s \\cross \\vec{F_b} \\rightarrow \\vec{L}_b = \\frac{\\mu_0 n R^2 Q}{2} \\int_0 ^\\infty \\dv{I}{t} \\dd t \\vu z = - \\frac{\\mu_0 n R^2 Q I}{2} \\vu z So the total angular momentum once the current is finally turned down is \\vec{L_a} + \\vec{L_b} = - \\frac{\\mu_0 n I Q}{2} (R^2 - a^2) \\vu z which is exactly the angular momentum that was stored in the fields, so we've successfully conserved angular momentum :)","title":"8.2 - Momentum"},{"location":"ch8-2/#82-momentum","text":"In this chapter we talk about: Electromagnetic momentum Maxwell stress tensor Conservation of electromagnetic momentum Angular momentum in EM fields","title":"8.2: Momentum"},{"location":"ch8-2/#821-electromagnetic-momentum","text":"As it turns out, if you disregard the momentum associated with electromagnetic fields, Newton's laws appear not to work out! Consider a basic system in cartesian coordinates of two moving point charges: What happens between the two charges? Well, the magnetic field of q_1 points into the page at q_2 , so the magnetic force on q_2 is to the right, and the magnetic field of q_2 is out of the page at q_1 , so the magnetic force on q_1 is upward. The net electric force between the two charges is repulsive and opposite, but the magnetic forces aren't, so the electromagnetic force on q_1 on q_2 is equal but not opposite to the force of q_2 on q_1 , in violation of Newton's third law! We've got a problem, and we're going to solve it by invoking the momentum of the EM field.","title":"8.2.1: Electromagnetic Momentum"},{"location":"ch8-2/#822-the-maxwell-stress-tensor","text":"The way to recover conservation of momentum proceeds the same way we recovered the conservation of energy via the Poynting vector. Starting with the basic Coulomb/Lorentz laws, we'll write down an expression for the electromagnetic force on charges in a volume. We're going to integrate that over all space, which can have any distribution of charge, and relate that expression for force to an expression which only involves the field. In the interest of brevity, we'll skip around a bit and leave the full derivations for the real textbook. Suppose we have a volume V containing some distribution of charge, current, and electromagnetic fields. The total force on that volume is \\vec{F} = \\int_V ( \\vec E + \\vec v \\cross \\vec B) \\rho \\dd \\tau \\qquad (\\dd q = \\rho \\dd \\tau) \\\\ = \\int _V (\\rho \\vec E + \\vec J \\cross \\vec B ) \\dd \\tau \\qquad (\\vec J = \\rho \\vec v) Again, the goal is to replace anything that looks like a source in favor of fields, using Maxwell's equations. It's handy to define the force per unit volume f : \\vec f \\equiv \\rho \\vec E + \\vec J \\cross \\vec B \\rho = \\epsilon_0 ( \\div \\vec E ) \\quad \\text{(Gauss' Law)} \\\\ \\vec J = \\frac{1}{\\mu_0} \\curl \\vec B - \\epsilon_0 \\pdv{\\vec E}{t} \\rightarrow \\vec f = \\epsilon_0 ( \\div \\vec E) \\vec E + \\left( \\frac{\\curl \\vec B}{\\mu_0} - \\epsilon_0 \\pdv{\\vec E}{t} \\right) \\cross \\vec B Skipping through a few steps, we cut to the chase. Similar to the derivation of the Poynting theorem, also using the other two Maxwell equations we haven't yet, we get \\vec f = \\div \\overline{\\vec T} - \\epsilon_0 \\mu_0 \\pdv{\\vec S}{t} \\overline{\\vec T} \\equiv \\begin{pmatrix} T_{xx} & T_{xy} & T_{xz} \\\\ T_{yx} & T_{yy} & T_{yz} \\\\ T_{zx} & T_{zy} & T_{zz} \\end{pmatrix} \\\\ T_{ij} \\equiv \\epsilon_0 \\left( E_i E_j - \\frac{1}{2} \\delta_{ij} E^2 \\right) + \\frac{1}{\\mu_0} \\left(B_i B_j - \\frac{1}{2} \\delta_{ij} B^2 \\right) where \\vec S is the Poynting vector and \\overline{\\vec T} is the so-called \"Maxwell stress tensor.\" To keep in mind what kind of units we're talking about here, \\vec f has units force per unit volume, and the divergence will strip one spatial dimension, so the Maxwell stress tensor will have units of stress (force per unit area). The tensor has diagonal \"pressure\" terms and off-diagonal \"shear\" terms. For \"pressure\" forces, the force and area are in the same direction, and in the \"shear\" case the force and area are orthogonal. The divergence term \\div \\overline{\\vec T} is itself a vector \\div \\overline{\\vec T} = \\left( \\vu{i} \\pdv{}{x} + \\vu j \\pdv{}{y} + \\vu k \\pdv{}{z} \\right) \\cdot \\overline{\\vec T} \\\\ = \\epsilon_0 \\left[ (\\div \\vec E) E_j + (\\vec E \\cdot \\grad) E_j - \\frac{1}{2} \\grad _j E^2 \\right] \\\\ + \\frac{1}{\\mu_0} \\left[ ( \\div \\vec B) B_j + (\\vec B \\cdot \\grad) B_j - \\frac{1}{2} \\grad _j B^2 \\right] As we do the volume integral to go from \\vec f to \\vec F \\vec F = \\int _V \\left(\\div\\overline{\\vec T} - \\epsilon_0 \\mu_0 \\pdv{\\vec S}{t} \\right) \\dd \\tau \\\\ = \\oint \\overline{\\vec{T}} \\cdot \\dd \\vec a - \\epsilon_0 \\mu_0 \\pdv{}{t} \\int \\vec{S} \\dd \\tau","title":"8.2.2 The Maxwell Stress Tensor"},{"location":"ch8-2/#823-conservation-of-electromagnetic-momentum","text":"\\vec F = \\dv{\\vec{p}_{mech}}{t} = - \\epsilon_0 \\mu_0 \\dv{}{t} \\int_V \\vec S \\dd \\tau + \\oint \\overline{\\vec T} \\cdot \\dd \\vec a The first term on the right is related to the momentum stored in the electromagnetic field. The second term is the rate at which momentum flows across the surface, and we describe the left-hand-side as the rate of change of the momentum of charges within the volume. We identify another useful term as the first integrand on the right: \\vec g \\equiv \\epsilon_0 \\mu_0 \\vec S = \\epsilon_0 (\\vec E \\cross B) \\quad \\text{(momentum density in EM fields)} which is the momentum density within the fields. Just as a note, the signs here are swapped from the Poynting theorem - the Maxwell stress tensor is defined such that momentum flowing into the region corresponds with increasing \\overline{\\vec T} , and vice-versa, opposite the case we had with \\vec S . In a charge-free region, - \\dv{}{t} \\int_V \\vec g \\dd \\tau + \\oint_S \\overline{\\vec T} \\cdot \\dd \\vec a = 0 and since the above is true for all regions V , we have our familiar continuity-type equation - \\pdv{\\vec g}{t} + \\div \\overline{\\vec T} = 0","title":"8.2.3 Conservation of Electromagnetic Momentum"},{"location":"ch8-2/#example-problem-87","text":"Consider an infinite parallel-plate capacitor, with the lower plate (at z = -d/2 ) carrying surface charge density -\\sigma , and the upper plate (at z = +d/2 ) carrying charge density + \\sigma . (a) Determine all elements of the stress tensor, in the region between the plates. Lucky for us, the magnetic field between the plates is \\vec B = 0 The electric field is very simple \\vec E = \\frac{\\sigma}{\\epsilon_0} ( - \\vu z) So we can already tell that the off-diagonal terms will be zero, since they all contain two factors of E_i E_j , one of which will be zero. The B terms on the diagonal will be zero, and \\overline{\\vec T} = \\frac{\\sigma^2}{2 \\epsilon_0} \\begin{pmatrix} -1 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} (b) Use \\vec F = \\oint_S \\overline{\\vec T} \\cdot \\dd \\vec a at the boundary to determine the electromagnetic force per unit area on the top plate. The force on the top plate will be \\vec F / A = \\frac{1}{A} \\oint _S \\overline{\\vec T} \\cdot \\dd \\vec a \\\\ = \\frac{1}{A} \\frac{\\sigma ^2}{2 \\epsilon_0} \\begin{pmatrix} -1 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ -A \\end{pmatrix} \\\\ = - \\frac{\\sigma^2}{2 \\epsilon_0} \\vu z (c) What is the electromagnetic momentum per unit area, per unit time, crossing the xy plane (or any other plane parallel to that one, between the plates)? This is just to show us that the momentum flow through the interior of the capacitor is the same as whatever force is pulling the top and bottom plates apart. - T_{zz} = - \\sigma^2 / 2 \\epsilon_0 is the momentum in the z direction crossing a surface perpendicular to z, per unit area, per unit time.","title":"Example: Problem 8.7"},{"location":"ch8-2/#824-angular-momentum-in-em-fields","text":"As a reminder, we associate with the electromagnetic fields an energy density u_{em} = \\frac{1}{2} \\epsilon_0 E^2 + \\frac{\\mu_0}{2} B^2 and a momentum density \\vec g = \\epsilon_0 ( \\vec E \\cross \\vec B) For that matter, we define angular momentum in the normal fashion \\vec l = \\vec r \\cross \\vec g = \\epsilon_0 [ \\vec r \\cross ( \\vec E \\cross \\vec B) ] where the presence of \\vec r means it's defined about some point or axis.","title":"8.2.4 Angular Momentum in EM Fields"},{"location":"ch8-2/#example-84","text":"Imagine a very long solenoid with radius R , n turns per unit length, and current I . Coaxial with the solenoid are two long cylindrical (non-conducting) shells of length l - one inside the solenoid at radius a carries a charge +Q distributed uniformly over its surface; the other outside the solenoid at radius b carries charge -Q . When the current in the solenoid is gradually reduced to nothing, the cylinders begin to rotate. Question: where does the angular momentum come from? We'll start by writing down the initial angular momentum of the system, then see what happens when we start to change the current. It is assumed that the solenoid is very long compared with the charged cylinders, and that the length of each charged cylinder is much much greater than its radius. The initial \\vec E is given by looking at a Gaussian cylinder between the two charged cylinders E 2 \\pi s l = \\frac{Q}{\\epsilon_0} \\rightarrow \\vec E = \\frac{Q}{2 \\pi \\epsilon_0 l} \\frac{1}{s} \\vu s \\qquad a < s < b The initial \\vec B is just that of the solenoid, namely \\vec B = \\begin{cases} \\mu_0 n I \\vu z & \\qquad 0 < s < R \\\\ 0, & \\qquad s > R \\end{cases} The linear momentum density we defined earlier is \\vec g = \\epsilon_0 \\mu_0 \\vec S = \\epsilon_0 ( \\vec E \\cross \\vec B) = \\frac{Q n I \\mu_0 ^2}{2 \\pi s} (- \\vu \\phi) And the angular momentum density with respect to the cylindrical axis is \\vec l = \\vec s \\cross \\vec g And it will point in the - \\vu z direction \\vec L = \\int _V \\vec l \\dd \\tau = \\int_V \\vec s \\cross \\epsilon_0 \\frac{Q}{2 \\pi \\epsilon_0 l} \\frac{1}{s} \\vu s \\cross \\mu_0 I n \\vu z \\dd \\tau = - \\frac{\\mu_0 n I Q }{2 \\pi l} \\int _V \\vu s \\cross \\vu \\phi \\dd \\tau = - \\frac{\\mu_0 n I Q}{2 \\pi l} \\vu z \\pi (R^2 - a^2 ) l = - \\frac{\\mu_n n I Q}{2} (R^2 - a^2) \\vu z So that's the initial angular momentum. Of course, nothing is moving, so this is just the momentum stored in the EM fields. Assuming nothing like friction complicates the situation, any angular momentum lost by the EM fields will be transferred to the cylinders as rotation. As we crank down the current, we know that a time variation in the magnetic field implies an induced EMF \\mathcal{E} = - \\dv{\\Phi}{t} . \\vec E experienced at radii a and b are now different. Use Faraday E_a 2 \\pi a = - \\mu_0 n \\dv{I}{t} \\pi a^2 \\rightarrow \\vec E_a = - \\frac{\\mu_0 n a}{2} \\dv{I}{t} \\vu \\phi This field is in the direction to torque cylinder +Q \\vec{\\Gamma_a} = \\vec s \\cross (Q \\vec{E_a}) = - \\frac{\\mu_0 n a}{2} \\dv{I}{t} Q a \\vu \\phi = - \\frac{\\mu_0 n a^2}{2} Q \\dv{I}{t} \\vu z Integrating torque over time, we get the changed angular momentum of the cylinder \\vec{L_a} = \\int_0 ^t \\tau \\dd t = - \\frac{\\mu_0 n a^2}{2} Q \\int_0 ^\\infty \\dv{I}{t} \\dd t \\vu z = - \\frac{\\mu_0 n a^2}{2} Q \\vu z \\int_0 ^\\infty \\dd I = \\frac{\\mu_0 n I}{2} a^2 \\vu z What happens to the outer cylinder? Something very similar, but when we calculated from Faraday's law our loop radius is b , and the area of flux is \\pi R^2 \\vec E_b = - \\frac{\\mu_0 n R^2}{2a} \\dv{I}{t} \\vu \\phi \\vec{F_b} = - Q \\vec{E}_b \\vec {\\Gamma _b} = b \\vu s \\cross \\vec{F_b} \\rightarrow \\vec{L}_b = \\frac{\\mu_0 n R^2 Q}{2} \\int_0 ^\\infty \\dv{I}{t} \\dd t \\vu z = - \\frac{\\mu_0 n R^2 Q I}{2} \\vu z So the total angular momentum once the current is finally turned down is \\vec{L_a} + \\vec{L_b} = - \\frac{\\mu_0 n I Q}{2} (R^2 - a^2) \\vu z which is exactly the angular momentum that was stored in the fields, so we've successfully conserved angular momentum :)","title":"Example 8.4"},{"location":"ch9-1/","text":"9.1: Electromagnetic Waves in One Dimension 9.1.1 THE WAVE EQUATION Before we start writing down how electromagnetic energy propagates in space as radiation, we'll dust off our mathematical background on waves. Thanks to Fourier analysis, we know that sinusoidal waves form a complete basis for all periodic functions, so we restrict our investigation to sinusoidal variations. Suppose some quantity \" f \" is propagating with speed v in the +z direction, and we focus on sinusoidal \"disturbances.\" At t = 0 , f = f_m \\cos k z \\qquad k = \\frac{2 \\pi}{\\lambda} And at any time t = t f = f_m \\cos [ k ( z - vt) ] = a_m \\cos ( kz - \\omega t) \\qquad \\omega \\equiv v k And if we look at a particular position z = 0 (or other) and watch f as a function of time, we'll see the same sinusoidal behavior f(z = 0, t) = f_m ( \\omega t) Generalizing to the 3-D case, instead of a wavenumber we have a wave vector \\vec k = k_x \\vu x + k_y \\vu y + k_z \\vu z f = f_m \\cos (\\vec k \\cdot \\vec r - \\omega t + \\delta) Any wave that looks like this satisfies the wave equation \\frac{\\partial ^2 a}{\\partial z^2} = \\frac{1}{v} \\frac{\\partial ^2 a}{\\partial t^2} \\qquad v = \\frac{\\omega}{k} \\quad \\text{(in 1-D)}","title":"9.1 - Waves in One Dimension"},{"location":"ch9-1/#91-electromagnetic-waves-in-one-dimension","text":"","title":"9.1: Electromagnetic Waves in One Dimension"},{"location":"ch9-1/#911-the-wave-equation","text":"Before we start writing down how electromagnetic energy propagates in space as radiation, we'll dust off our mathematical background on waves. Thanks to Fourier analysis, we know that sinusoidal waves form a complete basis for all periodic functions, so we restrict our investigation to sinusoidal variations. Suppose some quantity \" f \" is propagating with speed v in the +z direction, and we focus on sinusoidal \"disturbances.\" At t = 0 , f = f_m \\cos k z \\qquad k = \\frac{2 \\pi}{\\lambda} And at any time t = t f = f_m \\cos [ k ( z - vt) ] = a_m \\cos ( kz - \\omega t) \\qquad \\omega \\equiv v k And if we look at a particular position z = 0 (or other) and watch f as a function of time, we'll see the same sinusoidal behavior f(z = 0, t) = f_m ( \\omega t) Generalizing to the 3-D case, instead of a wavenumber we have a wave vector \\vec k = k_x \\vu x + k_y \\vu y + k_z \\vu z f = f_m \\cos (\\vec k \\cdot \\vec r - \\omega t + \\delta) Any wave that looks like this satisfies the wave equation \\frac{\\partial ^2 a}{\\partial z^2} = \\frac{1}{v} \\frac{\\partial ^2 a}{\\partial t^2} \\qquad v = \\frac{\\omega}{k} \\quad \\text{(in 1-D)}","title":"9.1.1 THE WAVE EQUATION"},{"location":"ch9-2/","text":"9.2.1 Wave Equation for E and B Recall Maxwell's equations \\begin{align*} (\\text{i}) & \\quad \\div \\vec{D} = \\rho_f \\quad \\text{(Gauss's law)} \\\\ (\\text{ii}) & \\quad \\div \\vec{B} = 0 \\quad \\text{(Ng's Law)} \\\\ (\\text{iii}) & \\quad \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} \\quad \\text{(Faraday's Law}) \\\\ (\\text{iv}) & \\quad \\curl \\vec{H} = \\vec{J}_f + \\pdv{\\vec{D}}{t} \\quad \\text{(Ampere's Law)} \\end{align*} To get to a wave equation from these, we start with a curl of curl and use a standard vector identity \\curl (\\curl \\vec E) = \\grad ( \\div \\vec E) - \\grad ^2 \\vec E Use Faraday's law on the left hand side (and move the spatial derivative through the temporal one), and on the right hand side use Gauss' law to re-write the divergence \\rightarrow - \\pdv{}{t} ( \\curl \\vec B) = \\frac{1}{\\epsilon_0} \\grad \\rho - \\grad ^2 \\vec E \\rightarrow - \\pdv{}{t} \\left( \\mu_0 \\vec J + \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} \\right) = - \\mu_0 \\pdv{\\vec J}{t} - \\mu_0 \\epsilon_0 \\frac{\\partial ^2 \\vec E}{\\partial t ^2} = \\frac{\\grad \\rho}{\\epsilon_0} - \\grad ^2 \\vec E \\rightarrow \\grad ^2 \\vec E - \\mu_0 \\epsilon_0 \\frac{\\partial ^2 \\vec E}{\\partial t^2} = \\frac{1}{\\epsilon_0} \\grad \\rho + \\mu_0 \\pdv{\\vec{J}}{t} \\quad \\text{\"non-homogeneous\" wave equation for E} In a vacuum, \\rho = 0 and \\vec J = 0 so \\grad ^2 \\vec E - \\mu_0 \\epsilon_0 \\frac{\\partial ^2 \\vec E}{\\partial t^2} = 0 which is just a standard 3D wave equation. We can identify the speed of propagation based on the constant of proportionality v = \\frac{1}{\\sqrt{\\mu_0 \\epsilon_0}} = c We could also have started with \\curl (\\curl \\vec B) to obtain \\grad ^2 \\vec B - \\mu_0 \\epsilon_0 \\frac{\\partial ^2 \\vec B}{\\partial t^2} = - \\mu_0 (\\curl \\vec J) \\quad \\text{\"non-homogeneous\" wave equation for B} So, in vacuum you get the exact same wave equation \\grad ^2 \\vec B - \\mu_0 \\epsilon_0 \\frac{\\partial ^2 \\vec B}{\\partial t^2} = 0 So, both \\vec E and \\vec B must satisfy these wave equations. We know that the wave equations admit certain sets of solutions, but that's not the entire story. We'll see additional constraints on solutions to \\vec E and \\vec B due to the fact that the waves need to satisfy all of the Maxwell equations, so \\vec E and \\vec B are very intimately linked. 9.2.2: Monochromatic Plane Waves Consider monochromatic sine waves (plane waves) in a single direction, so that all variation happens in the z-direction. Again, using the superposition principle we'll be able to build up more complicated solutions. \\vec E = \\vec E_0 \\cos (k z - \\omega t + \\delta) \\qquad \\vec B = \\vec B_0 \\cos (k z - \\omega t + \\delta) \\vec E_0 = E_{0, x} \\vu x + E_{0, y} \\vu y + E_{0, z} \\vu z Let's apply Gauss' law (in vacuum) \\div \\vec E = 0 \\div (\\vec E_0 \\cos (k z - \\omega t + \\delta) ) = 0 \\rightarrow \\pdv{E_{0, x}}{x} \\cos (k z - \\omega t + \\delta ) + \\pdv{E_{0, y}}{y} \\cos (kz - \\omega t + \\delta) + \\pdv{E_{0, z}}{z} \\cos (kz - \\omega t + \\delta) + E_{0,z}(- k \\sin(kt - \\omega t + \\delta)) = 0 We've decided that there is no variation in the x- and y-directions, so only the final term survives, and must be equal to zero E_{0, z} (- k \\sin(kt - \\omega t + \\delta)) = 0 \\quad \\rightarrow \\quad E_{0,z} = 0 Which is to say that EM plane waves are \"transverse\" waves. Let's also use Faraday's law to relate \\vec E and \\vec B : \\curl \\vec E = - \\pdv{B}{t} \\curl \\vec E = \\begin{pmatrix} \\vu x & \\vu y & \\vu z \\\\ \\pdv{}{x} & \\pdv{}{y} & \\pdv{}{z} \\\\ E_{0, x} \\cos (k z - \\omega t) & E_{0, y} \\cos (k z - \\omega t) & 0 \\end{pmatrix} = \\vu x ( E_{0, y} k \\sin(kz - \\omega t)) - \\vu y (E_{0, x} k \\sin(k z - \\omega t)) - \\pdv{B}{t} = - \\vu x \\pdv{}{t} B_{0, x} \\cos (kz - \\omega t) - \\vu y \\pdv{}{t}B_{0, y} \\cos (kz - \\omega t) = - \\vu x B_{0, x} \\omega \\sin (k z - \\omega t) - \\vu y B_{0, y} \\omega \\sin (kz - \\omega t) Matching up components and canceling the sine functions, \\rightarrow k E_{0, y} = - \\omega B_{0, x} \\qquad k E_{0, x} = \\omega B_{0, y} So every time you have an \\vec E field, you will have a \\vec B field in an orthogonal direction - they are mutually orthogonal - and they are in phase, since the proportionality factors are real . 9.2.3: Energy and Momentum in Electromagnetic Waves To repeat, for monochromatic plane waves propagating in the z-direction, \\vec E = \\vec{E_0} \\cos ( k z - \\omega t + \\delta) = E_0 \\cos (k z - \\omega t + \\delta) \\vu x \\vec B = B_0 \\cos (kz - \\omega t + \\delta) \\vu y and E_0 / B_0 = c What does the energy density due to these fields look like? u = \\frac{1}{2} \\epsilon_0 E^2 + \\frac{1}{2\\mu_0} B^2 = \\epsilon_0 E^2 = \\epsilon_0 E_0 \\cos ^2 (kz - \\omega t + \\delta) The electric and magnetic contributions are equal. The resulting Poynting vector is \\vec S = \\frac{1}{\\mu_0} \\vec E \\cross \\vec B = \\frac{E_0 B_0 }{\\mu_0} \\cos ^2 (kz - \\omega t + \\delta ) \\vu z = \\frac{E_0 ^2}{\\mu_0 c} \\cos ^2 (kz - \\omega t + \\delta) \\vu z = c \\epsilon_0 E_0 ^2 \\cos ^2 (kz - \\omega t + \\delta) \\vu z \\vec S = c u \\vu z So the Poynting vector points in the direction of propagation, and it has amplitude c u . What about the momentum density? \\vec g = \\mu_0 \\epsilon_0 \\vec S = \\epsilon_0 (\\vec E \\cross \\vec B) = \\frac{1}{c^2} cu \\vu z = \\frac{u}{c} \\vu z The rate of oscillation of these waves is typically very high, so we are mostly interested in the average of the oscillatory behavior over a period. Recall that the time-average of \\cos ^2 over a cycle is \\frac{1}{2} , so \\langle u \\rangle = \\frac{1}{2} \\epsilon_0 E_0 ^2 \\langle \\vec S \\rangle = \\frac{1}{2} c \\epsilon_0 E_0 ^2 \\vu z \\langle \\vec g \\rangle = \\frac{1}{2c} \\epsilon_0 E_0 ^2 \\vu z Another useful quantity we usually throw around is the Root Mean Square (RMS) value of the field \\sqrt{\\langle E_0 ^2 \\cos ^2 (kz - \\omega t + \\delta) \\rangle } = \\sqrt{\\frac{E_0^2}{2}} = \\frac{E_0}{\\sqrt{2}} \\approx 0.7 E_0 The \"intensity\" of the electromagnetic wave is defined as its power per unit area, or energy per unit area per unit time I = \\frac{\\text{power}}{\\text{area}} = \\frac{\\text{energy}}{\\text{area}\\cdot\\text{time}} = \\langle | \\vec S | \\rangle = \\langle | cu \\vu z | \\rangle = c \\langle u \\rangle = \\frac{1}{2} c \\epsilon_0 E_0 ^2 If the light hits the surface of a perfect absorber, it will transfer its momentum to the surface. In a time \\Delta t the momentum transfer will be \\Delta \\vec p = \\langle \\vec g \\rangle A c \\Delta t so the radiation pressure (average force per unit area) is P = \\frac{1}{A} \\frac{\\Delta p}{\\Delta t} = \\frac{1}{2} \\epsilon_0 E_0 ^2 = \\frac{I}{c} Of course, when falling on a perfect reflector , the radiation pressure is twice as big, since the resulting momentum of the reflected light switches direction instead of being absorbed. Example Problem 9.10 The intensity of sunlight hitting the earth is about 1300 W/m^2 . If sunlight strikes a perfect absorber, what pressure does it exert? How about a perfect reflector? What fraction of atmospheric pressure does this amount to? The radiation pressure for a perfect absorber is P = \\frac{I}{c} = \\frac{1300}{3 \\cdot 10^8} \\approx 4.3 \\cdot 10^{-6} N/m^2 The atmospheric pressure on earth's surface is about 10^5 N , so \\frac{\\text{Radiation pressure}}{\\text{Atmos pressure}} \\approx 10^{-11} The resulting radiation pressure is tiny compared with the normal pressures we're used to, but in space where atmospheric pressure is absent the result can be significant, and laser beams on individual atoms with tiny masses can slow and trap individual particles via radiation pressure.","title":"9.2 - Electromagnetic Waves in Vacuum"},{"location":"ch9-2/#921-wave-equation-for-e-and-b","text":"Recall Maxwell's equations \\begin{align*} (\\text{i}) & \\quad \\div \\vec{D} = \\rho_f \\quad \\text{(Gauss's law)} \\\\ (\\text{ii}) & \\quad \\div \\vec{B} = 0 \\quad \\text{(Ng's Law)} \\\\ (\\text{iii}) & \\quad \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} \\quad \\text{(Faraday's Law}) \\\\ (\\text{iv}) & \\quad \\curl \\vec{H} = \\vec{J}_f + \\pdv{\\vec{D}}{t} \\quad \\text{(Ampere's Law)} \\end{align*} To get to a wave equation from these, we start with a curl of curl and use a standard vector identity \\curl (\\curl \\vec E) = \\grad ( \\div \\vec E) - \\grad ^2 \\vec E Use Faraday's law on the left hand side (and move the spatial derivative through the temporal one), and on the right hand side use Gauss' law to re-write the divergence \\rightarrow - \\pdv{}{t} ( \\curl \\vec B) = \\frac{1}{\\epsilon_0} \\grad \\rho - \\grad ^2 \\vec E \\rightarrow - \\pdv{}{t} \\left( \\mu_0 \\vec J + \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} \\right) = - \\mu_0 \\pdv{\\vec J}{t} - \\mu_0 \\epsilon_0 \\frac{\\partial ^2 \\vec E}{\\partial t ^2} = \\frac{\\grad \\rho}{\\epsilon_0} - \\grad ^2 \\vec E \\rightarrow \\grad ^2 \\vec E - \\mu_0 \\epsilon_0 \\frac{\\partial ^2 \\vec E}{\\partial t^2} = \\frac{1}{\\epsilon_0} \\grad \\rho + \\mu_0 \\pdv{\\vec{J}}{t} \\quad \\text{\"non-homogeneous\" wave equation for E} In a vacuum, \\rho = 0 and \\vec J = 0 so \\grad ^2 \\vec E - \\mu_0 \\epsilon_0 \\frac{\\partial ^2 \\vec E}{\\partial t^2} = 0 which is just a standard 3D wave equation. We can identify the speed of propagation based on the constant of proportionality v = \\frac{1}{\\sqrt{\\mu_0 \\epsilon_0}} = c We could also have started with \\curl (\\curl \\vec B) to obtain \\grad ^2 \\vec B - \\mu_0 \\epsilon_0 \\frac{\\partial ^2 \\vec B}{\\partial t^2} = - \\mu_0 (\\curl \\vec J) \\quad \\text{\"non-homogeneous\" wave equation for B} So, in vacuum you get the exact same wave equation \\grad ^2 \\vec B - \\mu_0 \\epsilon_0 \\frac{\\partial ^2 \\vec B}{\\partial t^2} = 0 So, both \\vec E and \\vec B must satisfy these wave equations. We know that the wave equations admit certain sets of solutions, but that's not the entire story. We'll see additional constraints on solutions to \\vec E and \\vec B due to the fact that the waves need to satisfy all of the Maxwell equations, so \\vec E and \\vec B are very intimately linked.","title":"9.2.1 Wave Equation for E and B"},{"location":"ch9-2/#922-monochromatic-plane-waves","text":"Consider monochromatic sine waves (plane waves) in a single direction, so that all variation happens in the z-direction. Again, using the superposition principle we'll be able to build up more complicated solutions. \\vec E = \\vec E_0 \\cos (k z - \\omega t + \\delta) \\qquad \\vec B = \\vec B_0 \\cos (k z - \\omega t + \\delta) \\vec E_0 = E_{0, x} \\vu x + E_{0, y} \\vu y + E_{0, z} \\vu z Let's apply Gauss' law (in vacuum) \\div \\vec E = 0 \\div (\\vec E_0 \\cos (k z - \\omega t + \\delta) ) = 0 \\rightarrow \\pdv{E_{0, x}}{x} \\cos (k z - \\omega t + \\delta ) + \\pdv{E_{0, y}}{y} \\cos (kz - \\omega t + \\delta) + \\pdv{E_{0, z}}{z} \\cos (kz - \\omega t + \\delta) + E_{0,z}(- k \\sin(kt - \\omega t + \\delta)) = 0 We've decided that there is no variation in the x- and y-directions, so only the final term survives, and must be equal to zero E_{0, z} (- k \\sin(kt - \\omega t + \\delta)) = 0 \\quad \\rightarrow \\quad E_{0,z} = 0 Which is to say that EM plane waves are \"transverse\" waves. Let's also use Faraday's law to relate \\vec E and \\vec B : \\curl \\vec E = - \\pdv{B}{t} \\curl \\vec E = \\begin{pmatrix} \\vu x & \\vu y & \\vu z \\\\ \\pdv{}{x} & \\pdv{}{y} & \\pdv{}{z} \\\\ E_{0, x} \\cos (k z - \\omega t) & E_{0, y} \\cos (k z - \\omega t) & 0 \\end{pmatrix} = \\vu x ( E_{0, y} k \\sin(kz - \\omega t)) - \\vu y (E_{0, x} k \\sin(k z - \\omega t)) - \\pdv{B}{t} = - \\vu x \\pdv{}{t} B_{0, x} \\cos (kz - \\omega t) - \\vu y \\pdv{}{t}B_{0, y} \\cos (kz - \\omega t) = - \\vu x B_{0, x} \\omega \\sin (k z - \\omega t) - \\vu y B_{0, y} \\omega \\sin (kz - \\omega t) Matching up components and canceling the sine functions, \\rightarrow k E_{0, y} = - \\omega B_{0, x} \\qquad k E_{0, x} = \\omega B_{0, y} So every time you have an \\vec E field, you will have a \\vec B field in an orthogonal direction - they are mutually orthogonal - and they are in phase, since the proportionality factors are real .","title":"9.2.2: Monochromatic Plane Waves"},{"location":"ch9-2/#923-energy-and-momentum-in-electromagnetic-waves","text":"To repeat, for monochromatic plane waves propagating in the z-direction, \\vec E = \\vec{E_0} \\cos ( k z - \\omega t + \\delta) = E_0 \\cos (k z - \\omega t + \\delta) \\vu x \\vec B = B_0 \\cos (kz - \\omega t + \\delta) \\vu y and E_0 / B_0 = c What does the energy density due to these fields look like? u = \\frac{1}{2} \\epsilon_0 E^2 + \\frac{1}{2\\mu_0} B^2 = \\epsilon_0 E^2 = \\epsilon_0 E_0 \\cos ^2 (kz - \\omega t + \\delta) The electric and magnetic contributions are equal. The resulting Poynting vector is \\vec S = \\frac{1}{\\mu_0} \\vec E \\cross \\vec B = \\frac{E_0 B_0 }{\\mu_0} \\cos ^2 (kz - \\omega t + \\delta ) \\vu z = \\frac{E_0 ^2}{\\mu_0 c} \\cos ^2 (kz - \\omega t + \\delta) \\vu z = c \\epsilon_0 E_0 ^2 \\cos ^2 (kz - \\omega t + \\delta) \\vu z \\vec S = c u \\vu z So the Poynting vector points in the direction of propagation, and it has amplitude c u . What about the momentum density? \\vec g = \\mu_0 \\epsilon_0 \\vec S = \\epsilon_0 (\\vec E \\cross \\vec B) = \\frac{1}{c^2} cu \\vu z = \\frac{u}{c} \\vu z The rate of oscillation of these waves is typically very high, so we are mostly interested in the average of the oscillatory behavior over a period. Recall that the time-average of \\cos ^2 over a cycle is \\frac{1}{2} , so \\langle u \\rangle = \\frac{1}{2} \\epsilon_0 E_0 ^2 \\langle \\vec S \\rangle = \\frac{1}{2} c \\epsilon_0 E_0 ^2 \\vu z \\langle \\vec g \\rangle = \\frac{1}{2c} \\epsilon_0 E_0 ^2 \\vu z Another useful quantity we usually throw around is the Root Mean Square (RMS) value of the field \\sqrt{\\langle E_0 ^2 \\cos ^2 (kz - \\omega t + \\delta) \\rangle } = \\sqrt{\\frac{E_0^2}{2}} = \\frac{E_0}{\\sqrt{2}} \\approx 0.7 E_0 The \"intensity\" of the electromagnetic wave is defined as its power per unit area, or energy per unit area per unit time I = \\frac{\\text{power}}{\\text{area}} = \\frac{\\text{energy}}{\\text{area}\\cdot\\text{time}} = \\langle | \\vec S | \\rangle = \\langle | cu \\vu z | \\rangle = c \\langle u \\rangle = \\frac{1}{2} c \\epsilon_0 E_0 ^2 If the light hits the surface of a perfect absorber, it will transfer its momentum to the surface. In a time \\Delta t the momentum transfer will be \\Delta \\vec p = \\langle \\vec g \\rangle A c \\Delta t so the radiation pressure (average force per unit area) is P = \\frac{1}{A} \\frac{\\Delta p}{\\Delta t} = \\frac{1}{2} \\epsilon_0 E_0 ^2 = \\frac{I}{c} Of course, when falling on a perfect reflector , the radiation pressure is twice as big, since the resulting momentum of the reflected light switches direction instead of being absorbed.","title":"9.2.3: Energy and Momentum in Electromagnetic Waves"},{"location":"ch9-2/#example-problem-910","text":"The intensity of sunlight hitting the earth is about 1300 W/m^2 . If sunlight strikes a perfect absorber, what pressure does it exert? How about a perfect reflector? What fraction of atmospheric pressure does this amount to? The radiation pressure for a perfect absorber is P = \\frac{I}{c} = \\frac{1300}{3 \\cdot 10^8} \\approx 4.3 \\cdot 10^{-6} N/m^2 The atmospheric pressure on earth's surface is about 10^5 N , so \\frac{\\text{Radiation pressure}}{\\text{Atmos pressure}} \\approx 10^{-11} The resulting radiation pressure is tiny compared with the normal pressures we're used to, but in space where atmospheric pressure is absent the result can be significant, and laser beams on individual atoms with tiny masses can slow and trap individual particles via radiation pressure.","title":"Example Problem 9.10"},{"location":"ch9-3/","text":"9.3: Electromagnetic Waves in Matter 9.3.1: Propagation in Linear Media and Non-conductors Maxwell's equations in linear media are \\begin{align*} (\\text{i}) & \\quad \\div \\vec{D} = \\rho_f \\quad \\text{(Gauss's law)} \\\\ (\\text{ii}) & \\quad \\div \\vec{B} = 0 \\quad \\text{(Ng's Law)} \\\\ (\\text{iii}) & \\quad \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} \\quad \\text{(Faraday's Law}) \\\\ (\\text{iv}) & \\quad \\curl \\vec{H} = \\vec{J}_f + \\pdv{\\vec{D}}{t} \\quad \\text{(Ampere's Law)} \\end{align*} In linear media, our constitutive relations are \\vec D = \\epsilon_0 \\vec E + \\vec P = \\epsilon \\vec E and \\vec H = \\frac{\\vec B}{\\mu_0} - \\vec M = \\frac{\\vec B}{\\mu} In charge-free region, the Maxwell equations in linear media look a lot like those in vacuum \\rho_f = 0 \\qquad \\vec J_f = 0 \\rightarrow \\begin{align*} (\\text{i}) & \\quad \\div \\vec E = 0 \\\\ (\\text{ii}) & \\div \\vec B = 0 \\\\ (\\text{iii}) & \\curl \\vec E = - \\pdv{\\vec B}{t} \\\\ (\\text{iv}) & \\curl \\vec B = \\mu \\epsilon \\pdv{\\vec E}{t} \\end{align*} So that's nearly identical, except where we had \\epsilon_0 now we have \\epsilon , and where we had \\mu_0 now we have \\mu , so we can make these same substitutions in the solutions. The resulting wave equations in linear matter are \\nabla ^2 \\vec E = \\frac{1}{v^2} \\frac{\\partial ^2 \\vec E}{\\partial t^2} \\qquad \\nabla ^2 \\vec B = \\frac{1}{v^2} \\frac{\\partial ^2 \\vec B}{\\partial t^2} where now the speed is v = \\frac{1}{\\sqrt{\\mu \\epsilon}} = \\text{speed of EM wave in linear medium} = \\frac{c}{n} where n \\equiv \\sqrt{\\frac{\\mu}{\\mu_0} \\frac{\\epsilon}{\\epsilon_0}} = \\text{ index of refraction } 9.3.2: Reflection and Transmission at Normal Incidence On of the most simple interesting situations that can arise when the index of refraction changes is when light crosses a sudden interface, i.e., what happens when light passes from one transparent medium into another? As in the case of waves on a string, we expect to get a reflected wave and a transmitted wave. Suppose we have waves incident on the boundary (in the x-y plane) between two media, call the media \"1\" and \"2\" with indices of refraction n_1 and n_2 . The z-axis is normal to the boundary. Let's write our incident wave \\vec{E_I} in so-called \"phasor notation\" (just complex exponential notation) \\vec{E_I} (\\vec r, t) = \\vec{E_{0, I}} e^{i(\\vec{k_I} \\cdot \\vec r - \\omega t)} where the actual wave itself is the real part of the complex exponential. We define the magnetic field in the same way \\vec{B_I} (\\vec r, t) = \\vec{B_{0, I}} e^{i(\\vec{k_I} \\cdot \\vec r - \\omega t)} = \\frac{1}{v_1} ( \\vec{k_I} \\cross \\vec{E_I}) We write down similar expressions \\vec{E_{R}}, \\quad \\vec{B_{R}} for the reflected wave, and \\vec{E_{T}}, \\quad \\vec{B_{T}} for the transmitted wave. At the z = 0 plane \\vec{E_{0,I}} e^{i(\\vec{k_I} \\cdot \\vec r - \\omega t)} + \\vec{E_{0,R}} e^{i(\\vec{k_T} \\cdot \\vec r - \\omega t)} = \\vec{E_{0,T}} e^{i(\\vec{k_T} \\cdot \\vec r - \\omega t)} That's true for all x, y on the interface and for all time, so this immediately implies that \\omega has to be the same for each of the waves (we already implicitly assumed this in the notation). So the \\omega t terms drop out of all three terms, and we can focus on the wavenumbers. It has to be the case that \\vec{k_I} \\cdot \\vec r = \\vec{k_R} \\cdot \\vec r = \\vec{k_T} \\cdot \\vec r \\rightarrow k_{I, x}x + k_{I, y} y = k_{R, x}x + k_{R, y} y = k_{T, x}x + k_{T, y} y where so far there is no restriction on k_z . We can simply orient our x-z axes such that \\vec{k_I} lies in the x-z plane. This means that \\vec{k_R} and \\vec{k_T} will also lie in the plane, and k_{I, x} = k_{I} \\sin \\theta_I = k_R \\sin \\theta_R \\quad \\rightarrow \\quad \\sin \\theta_I = \\sin \\theta_R \\rightarrow \\quad \\theta_I = \\theta_R k_{I} \\sin \\theta_I = k_T \\sin \\theta_T \\quad \\rightarrow \\frac{n_1}{n_2} \\sin \\theta_I = \\sin \\theta_T \\rightarrow \\frac{\\sin \\theta_T}{\\sin \\theta_I} = \\frac{n_1}{n_2} 9.3.3 Reflection and Transmission at Oblique Incidence In the more general case where the incoming wave hits the boundary at some angle \\theta_I . Suppose that a monochromatic plane wave \\vec{E_I}(\\vec r, t) = \\vec{E_0}_I e^{i(\\vec{k_I} \\cdot \\vec r - \\omega t)}, \\qquad \\vec{B_I}(\\vec r, t) = \\frac{1}{v_1} (\\vu{k}_I \\cross \\vec{E_I}) approaches from the left. We'll get a reflected wave \\vec{E_R}(\\vec r, t) = \\vec{E_0}_R e^{i(\\vec{k_R} \\cdot \\vec r - \\omega t)}, \\qquad \\vec{B_R}(\\vec r, t) = \\frac{1}{v_1} (\\vu{k}_R \\cross \\vec{E_T}) and a transmitted wave \\vec{E_T}(\\vec r, t) = \\vec{E_0}_T e^{i(\\vec{k_T} \\cdot \\vec r - \\omega t)}, \\qquad \\vec{B_T}(\\vec r, t) = \\frac{1}{v_1} (\\vu{k}_T \\cross \\vec{E_T}) All three waves have the same frequency \\omega . The three wave numbers are related by k_I v_1 = k_R v_1 = k_T v_2 = \\omega \\qquad \\rightarrow \\qquad k_I = k_R = \\frac{v_2}{v_1} k_T = \\frac{n_1}{n_2} k_T The combined fields in medium 1, \\vec{E_I} + \\vec{E_R} and \\vec{B_I} + \\vec{B_R} must be joined to the fields in medium 2 using the boundary conditions we get from Maxwell's equations. All of the boundary conditions share the generic structure ()e^{i(\\vec{k}_I \\cdot \\vec r - \\omega t)} + ()e^{i(\\vec{k}_R \\cdot \\vec r - \\omega t)} = ()e^{i(\\vec{k}_T \\cdot \\vec r - \\omega t)} \\qquad \\text{ at } z = 0 For now the important thing to notice is that the x, y, and t dependence is confined to the exponents. Because the boundary conditions must hold at all points on the plane, and for all times, these exponential factors must be equal at the boundary. \\vec{k_I} \\cdot r = \\vec{k_R} \\cdot r = \\vec{k_T} \\cdot r or x(k_I)_x + y(k_I)_y = x(k_R)_x + y(k_R)_y = x(k_T)_x + y(k_T)_y for all x and y, which can only be true if all both components are separately equal. So we may as well orient our axes so that \\vec{k_I} lies in the x-z plane - our boundary condition ensures that if we do that, \\vec{k_R} and \\vec{k_T} will also lie in the plane. The incident, reflected, and transmitted wave vectors form a plane (called the plane of incidence), which also includes the normal to the surface. We can also say that k_I \\sin \\theta_I = k_R \\sin \\theta_R = k_T \\sin \\theta_T where \\theta_I is the angle of incidence, \\theta_R is the angle of reflection, and \\theta_T is the angle of transmission, or more commonly the \"angle of refraction,\" all of them measured with respect to the normal. The angle of incidence is equal to the angle of refraction \\theta_I = \\theta_R And as for the transmitted angle The law of refraction: \\frac{\\sin \\theta_T}{\\sin \\theta_I} = \\frac{n_1}{n_2} So our exponential factors are dealt with, and we can move on to the Maxwell boundary conditions (i) \\quad \\epsilon_0 \\left( \\vec{E}_{0,I} + \\vec{E}_{0,R} \\right)_z = \\epsilon_2 \\left( \\vec{E}_{0,T} \\right) _z \\\\ (ii) \\quad \\left( \\vec{B}_{0,I} + \\vec{B}_{0,R} \\right) _z = \\left( \\vec{B}_{0,T} \\right)_z \\\\ (iii) \\quad \\left( \\vec{E}_{0,I} + \\vec{E}_{0,R} \\right)_{x,y} = \\left( \\vec{E}_{0,T} \\right)_{x,y} \\\\ (iv) \\quad \\frac{1}{\\mu_1} \\left( \\vec{B}_{0,I} + \\vec{B}_{0,R} \\right) _{x,y} = \\frac{1}{\\mu_2} \\left( \\vec{B}_{0,T} \\right)_{x,y} where \\vec{B}_0 = \\frac{1}{v} \\vu k \\cross \\vec{E}_0 in each case. If we now suppose the plane-polarized case, in which the polarization of the incident light is parallel to the plane of incidence, it follows that the reflected and transmitted waves are also polarized in this plane. Then (i) reads \\epsilon_i \\left( - \\vec{E}_{0,I} \\sin \\theta_I + \\vec{E}_{0,R} \\sin \\theta_R \\right) = \\epsilon_2 \\left( - \\vec{E}_{0,T} \\sin \\theta_T \\right) and (iv) says \\frac{1}{\\mu_1 v_1} \\left( \\vec{E}_{0,I} - \\vec{E}_{0,R} \\right) = \\frac{1}{\\mu_2 v_2} \\vec{E}_{0,T} We can reduce these down to \\vec{E}_{0,I} - \\vec{E}_{0,R} = \\beta \\vec{E}_{0,T} \\qquad \\text{ and } \\qquad \\vec{E}_{0,I} + \\vec{E}_{0,R} = \\alpha \\vec{E}_{0,T} where \\beta is defined as \\beta \\equiv \\frac{\\mu_1 v_1}{\\mu_2 v_2} = \\frac{\\mu_1 n_2}{\\mu_2 n_1} and \\alpha is \\alpha \\equiv \\frac{\\cos \\theta_T}{\\cos \\theta_I} Solving for the reflected and transmitted amplitudes, we obtain \\vec{E}_{0,R} = \\left( \\frac{\\alpha - \\beta}{\\alpha + \\beta} \\right) \\vec{E}_{0,I} \\qquad \\vec{E}_{0,T} = \\left( \\frac{2}{\\alpha + \\beta } \\right) \\vec{E}_{0,I} These are the Fresnel's equations for the case of polarization in the plane of incidence. Note that the transmitted wave is always in phase with the incident one; the reflected wave is either in phase (\"right side up\") if \\alpha > \\beta , or \\pi out of phase (\"upside down\") if \\alpha < \\beta Of note is the interesting incident angle \\theta_B where the reflected wave is completely extinguished. That happens when \\alpha = \\beta , or \\sin^2 \\theta_B = \\frac{1 - \\beta^2}{(n_1 / n_2)^2 - \\beta^2} In a typical case \\mu_1 \\approx \\mu_2 and \\beta \\approx n_2 / n_1 so \\sin^2 \\theta_B \\approx \\beta^2 / (1 + \\beta^2) or \\tan \\theta_B \\approx \\frac{n_2}{n_1} For s-polarized fields (i.e., the electric field is polarized perpendicular to the plane of incidence), \\vec{E}_{0,R} = \\left( \\frac{1 - \\alpha \\beta}{1 + \\alpha \\beta} \\right) \\vec{E}_{0,I} \\qquad \\vec{E}_{0,T} = \\frac{2}{1 + \\alpha \\beta} \\vec{E}_{0,I} To finish things up, let's look at the intensity of the reflected and transmitted waves, since that's what we're generally measuring directly. The intensity depends on the electric field magnitude and the index of refraction I = \\frac{1}{2} c \\epsilon_0 E_{0} ^2 \\qquad \\text{(in vacuum)} = \\frac{1}{2} v \\epsilon E_{0} ^2 = \\frac{1}{2} c n \\epsilon_0 E_0 ^2 \\qquad \\text{(in linear media)} We get reflection and transmission coefficients defined as: R = \\frac{I_R}{I_I} \\qquad T = \\frac{I_T}{I_I} Plugging in our previous expressions for the incoming and reflected fields, R = \\frac{\\frac{1}{2} c n_1 \\epsilon_0}{\\frac{1}{2} c n_1 \\epsilon_0} \\left( \\frac{1 - \\beta}{1 + \\beta} \\right) ^2 = \\left( \\frac{n_1 - n_2}{n_1 + n_2} \\right) and the transmitted field, T = \\frac{\\frac{1}{2} c n_2 \\epsilon_0}{\\frac{1}{2} c n_1 \\epsilon_0} \\left( \\frac{2}{1 + \\frac{n_2}{n_1} } \\right) ^2 = \\frac{n_2}{n_1} \\frac{4 n_1 ^2}{(n_1 + n_2)^2} = \\frac{4 n_1 n_2}{(n_1 + n_2)^2} We should check that R + T = 1 , since we haven't got any absorption in our scenario: R + T = \\left( \\frac{n_1 - n_2}{n_1 + n_2} \\right)^2 + \\frac{4 n_1 n_2}{(n_1 + n_2)^2} = 1","title":"9.3 - Electromagnetic Waves in Matter"},{"location":"ch9-3/#93-electromagnetic-waves-in-matter","text":"","title":"9.3: Electromagnetic Waves in Matter"},{"location":"ch9-3/#931-propagation-in-linear-media-and-non-conductors","text":"Maxwell's equations in linear media are \\begin{align*} (\\text{i}) & \\quad \\div \\vec{D} = \\rho_f \\quad \\text{(Gauss's law)} \\\\ (\\text{ii}) & \\quad \\div \\vec{B} = 0 \\quad \\text{(Ng's Law)} \\\\ (\\text{iii}) & \\quad \\curl \\vec{E} = - \\pdv{\\vec{B}}{t} \\quad \\text{(Faraday's Law}) \\\\ (\\text{iv}) & \\quad \\curl \\vec{H} = \\vec{J}_f + \\pdv{\\vec{D}}{t} \\quad \\text{(Ampere's Law)} \\end{align*} In linear media, our constitutive relations are \\vec D = \\epsilon_0 \\vec E + \\vec P = \\epsilon \\vec E and \\vec H = \\frac{\\vec B}{\\mu_0} - \\vec M = \\frac{\\vec B}{\\mu} In charge-free region, the Maxwell equations in linear media look a lot like those in vacuum \\rho_f = 0 \\qquad \\vec J_f = 0 \\rightarrow \\begin{align*} (\\text{i}) & \\quad \\div \\vec E = 0 \\\\ (\\text{ii}) & \\div \\vec B = 0 \\\\ (\\text{iii}) & \\curl \\vec E = - \\pdv{\\vec B}{t} \\\\ (\\text{iv}) & \\curl \\vec B = \\mu \\epsilon \\pdv{\\vec E}{t} \\end{align*} So that's nearly identical, except where we had \\epsilon_0 now we have \\epsilon , and where we had \\mu_0 now we have \\mu , so we can make these same substitutions in the solutions. The resulting wave equations in linear matter are \\nabla ^2 \\vec E = \\frac{1}{v^2} \\frac{\\partial ^2 \\vec E}{\\partial t^2} \\qquad \\nabla ^2 \\vec B = \\frac{1}{v^2} \\frac{\\partial ^2 \\vec B}{\\partial t^2} where now the speed is v = \\frac{1}{\\sqrt{\\mu \\epsilon}} = \\text{speed of EM wave in linear medium} = \\frac{c}{n} where n \\equiv \\sqrt{\\frac{\\mu}{\\mu_0} \\frac{\\epsilon}{\\epsilon_0}} = \\text{ index of refraction }","title":"9.3.1: Propagation in Linear Media and Non-conductors"},{"location":"ch9-3/#932-reflection-and-transmission-at-normal-incidence","text":"On of the most simple interesting situations that can arise when the index of refraction changes is when light crosses a sudden interface, i.e., what happens when light passes from one transparent medium into another? As in the case of waves on a string, we expect to get a reflected wave and a transmitted wave. Suppose we have waves incident on the boundary (in the x-y plane) between two media, call the media \"1\" and \"2\" with indices of refraction n_1 and n_2 . The z-axis is normal to the boundary. Let's write our incident wave \\vec{E_I} in so-called \"phasor notation\" (just complex exponential notation) \\vec{E_I} (\\vec r, t) = \\vec{E_{0, I}} e^{i(\\vec{k_I} \\cdot \\vec r - \\omega t)} where the actual wave itself is the real part of the complex exponential. We define the magnetic field in the same way \\vec{B_I} (\\vec r, t) = \\vec{B_{0, I}} e^{i(\\vec{k_I} \\cdot \\vec r - \\omega t)} = \\frac{1}{v_1} ( \\vec{k_I} \\cross \\vec{E_I}) We write down similar expressions \\vec{E_{R}}, \\quad \\vec{B_{R}} for the reflected wave, and \\vec{E_{T}}, \\quad \\vec{B_{T}} for the transmitted wave. At the z = 0 plane \\vec{E_{0,I}} e^{i(\\vec{k_I} \\cdot \\vec r - \\omega t)} + \\vec{E_{0,R}} e^{i(\\vec{k_T} \\cdot \\vec r - \\omega t)} = \\vec{E_{0,T}} e^{i(\\vec{k_T} \\cdot \\vec r - \\omega t)} That's true for all x, y on the interface and for all time, so this immediately implies that \\omega has to be the same for each of the waves (we already implicitly assumed this in the notation). So the \\omega t terms drop out of all three terms, and we can focus on the wavenumbers. It has to be the case that \\vec{k_I} \\cdot \\vec r = \\vec{k_R} \\cdot \\vec r = \\vec{k_T} \\cdot \\vec r \\rightarrow k_{I, x}x + k_{I, y} y = k_{R, x}x + k_{R, y} y = k_{T, x}x + k_{T, y} y where so far there is no restriction on k_z . We can simply orient our x-z axes such that \\vec{k_I} lies in the x-z plane. This means that \\vec{k_R} and \\vec{k_T} will also lie in the plane, and k_{I, x} = k_{I} \\sin \\theta_I = k_R \\sin \\theta_R \\quad \\rightarrow \\quad \\sin \\theta_I = \\sin \\theta_R \\rightarrow \\quad \\theta_I = \\theta_R k_{I} \\sin \\theta_I = k_T \\sin \\theta_T \\quad \\rightarrow \\frac{n_1}{n_2} \\sin \\theta_I = \\sin \\theta_T \\rightarrow \\frac{\\sin \\theta_T}{\\sin \\theta_I} = \\frac{n_1}{n_2}","title":"9.3.2: Reflection and Transmission at Normal Incidence"},{"location":"ch9-3/#933-reflection-and-transmission-at-oblique-incidence","text":"In the more general case where the incoming wave hits the boundary at some angle \\theta_I . Suppose that a monochromatic plane wave \\vec{E_I}(\\vec r, t) = \\vec{E_0}_I e^{i(\\vec{k_I} \\cdot \\vec r - \\omega t)}, \\qquad \\vec{B_I}(\\vec r, t) = \\frac{1}{v_1} (\\vu{k}_I \\cross \\vec{E_I}) approaches from the left. We'll get a reflected wave \\vec{E_R}(\\vec r, t) = \\vec{E_0}_R e^{i(\\vec{k_R} \\cdot \\vec r - \\omega t)}, \\qquad \\vec{B_R}(\\vec r, t) = \\frac{1}{v_1} (\\vu{k}_R \\cross \\vec{E_T}) and a transmitted wave \\vec{E_T}(\\vec r, t) = \\vec{E_0}_T e^{i(\\vec{k_T} \\cdot \\vec r - \\omega t)}, \\qquad \\vec{B_T}(\\vec r, t) = \\frac{1}{v_1} (\\vu{k}_T \\cross \\vec{E_T}) All three waves have the same frequency \\omega . The three wave numbers are related by k_I v_1 = k_R v_1 = k_T v_2 = \\omega \\qquad \\rightarrow \\qquad k_I = k_R = \\frac{v_2}{v_1} k_T = \\frac{n_1}{n_2} k_T The combined fields in medium 1, \\vec{E_I} + \\vec{E_R} and \\vec{B_I} + \\vec{B_R} must be joined to the fields in medium 2 using the boundary conditions we get from Maxwell's equations. All of the boundary conditions share the generic structure ()e^{i(\\vec{k}_I \\cdot \\vec r - \\omega t)} + ()e^{i(\\vec{k}_R \\cdot \\vec r - \\omega t)} = ()e^{i(\\vec{k}_T \\cdot \\vec r - \\omega t)} \\qquad \\text{ at } z = 0 For now the important thing to notice is that the x, y, and t dependence is confined to the exponents. Because the boundary conditions must hold at all points on the plane, and for all times, these exponential factors must be equal at the boundary. \\vec{k_I} \\cdot r = \\vec{k_R} \\cdot r = \\vec{k_T} \\cdot r or x(k_I)_x + y(k_I)_y = x(k_R)_x + y(k_R)_y = x(k_T)_x + y(k_T)_y for all x and y, which can only be true if all both components are separately equal. So we may as well orient our axes so that \\vec{k_I} lies in the x-z plane - our boundary condition ensures that if we do that, \\vec{k_R} and \\vec{k_T} will also lie in the plane. The incident, reflected, and transmitted wave vectors form a plane (called the plane of incidence), which also includes the normal to the surface. We can also say that k_I \\sin \\theta_I = k_R \\sin \\theta_R = k_T \\sin \\theta_T where \\theta_I is the angle of incidence, \\theta_R is the angle of reflection, and \\theta_T is the angle of transmission, or more commonly the \"angle of refraction,\" all of them measured with respect to the normal. The angle of incidence is equal to the angle of refraction \\theta_I = \\theta_R And as for the transmitted angle The law of refraction: \\frac{\\sin \\theta_T}{\\sin \\theta_I} = \\frac{n_1}{n_2} So our exponential factors are dealt with, and we can move on to the Maxwell boundary conditions (i) \\quad \\epsilon_0 \\left( \\vec{E}_{0,I} + \\vec{E}_{0,R} \\right)_z = \\epsilon_2 \\left( \\vec{E}_{0,T} \\right) _z \\\\ (ii) \\quad \\left( \\vec{B}_{0,I} + \\vec{B}_{0,R} \\right) _z = \\left( \\vec{B}_{0,T} \\right)_z \\\\ (iii) \\quad \\left( \\vec{E}_{0,I} + \\vec{E}_{0,R} \\right)_{x,y} = \\left( \\vec{E}_{0,T} \\right)_{x,y} \\\\ (iv) \\quad \\frac{1}{\\mu_1} \\left( \\vec{B}_{0,I} + \\vec{B}_{0,R} \\right) _{x,y} = \\frac{1}{\\mu_2} \\left( \\vec{B}_{0,T} \\right)_{x,y} where \\vec{B}_0 = \\frac{1}{v} \\vu k \\cross \\vec{E}_0 in each case. If we now suppose the plane-polarized case, in which the polarization of the incident light is parallel to the plane of incidence, it follows that the reflected and transmitted waves are also polarized in this plane. Then (i) reads \\epsilon_i \\left( - \\vec{E}_{0,I} \\sin \\theta_I + \\vec{E}_{0,R} \\sin \\theta_R \\right) = \\epsilon_2 \\left( - \\vec{E}_{0,T} \\sin \\theta_T \\right) and (iv) says \\frac{1}{\\mu_1 v_1} \\left( \\vec{E}_{0,I} - \\vec{E}_{0,R} \\right) = \\frac{1}{\\mu_2 v_2} \\vec{E}_{0,T} We can reduce these down to \\vec{E}_{0,I} - \\vec{E}_{0,R} = \\beta \\vec{E}_{0,T} \\qquad \\text{ and } \\qquad \\vec{E}_{0,I} + \\vec{E}_{0,R} = \\alpha \\vec{E}_{0,T} where \\beta is defined as \\beta \\equiv \\frac{\\mu_1 v_1}{\\mu_2 v_2} = \\frac{\\mu_1 n_2}{\\mu_2 n_1} and \\alpha is \\alpha \\equiv \\frac{\\cos \\theta_T}{\\cos \\theta_I} Solving for the reflected and transmitted amplitudes, we obtain \\vec{E}_{0,R} = \\left( \\frac{\\alpha - \\beta}{\\alpha + \\beta} \\right) \\vec{E}_{0,I} \\qquad \\vec{E}_{0,T} = \\left( \\frac{2}{\\alpha + \\beta } \\right) \\vec{E}_{0,I} These are the Fresnel's equations for the case of polarization in the plane of incidence. Note that the transmitted wave is always in phase with the incident one; the reflected wave is either in phase (\"right side up\") if \\alpha > \\beta , or \\pi out of phase (\"upside down\") if \\alpha < \\beta Of note is the interesting incident angle \\theta_B where the reflected wave is completely extinguished. That happens when \\alpha = \\beta , or \\sin^2 \\theta_B = \\frac{1 - \\beta^2}{(n_1 / n_2)^2 - \\beta^2} In a typical case \\mu_1 \\approx \\mu_2 and \\beta \\approx n_2 / n_1 so \\sin^2 \\theta_B \\approx \\beta^2 / (1 + \\beta^2) or \\tan \\theta_B \\approx \\frac{n_2}{n_1} For s-polarized fields (i.e., the electric field is polarized perpendicular to the plane of incidence), \\vec{E}_{0,R} = \\left( \\frac{1 - \\alpha \\beta}{1 + \\alpha \\beta} \\right) \\vec{E}_{0,I} \\qquad \\vec{E}_{0,T} = \\frac{2}{1 + \\alpha \\beta} \\vec{E}_{0,I} To finish things up, let's look at the intensity of the reflected and transmitted waves, since that's what we're generally measuring directly. The intensity depends on the electric field magnitude and the index of refraction I = \\frac{1}{2} c \\epsilon_0 E_{0} ^2 \\qquad \\text{(in vacuum)} = \\frac{1}{2} v \\epsilon E_{0} ^2 = \\frac{1}{2} c n \\epsilon_0 E_0 ^2 \\qquad \\text{(in linear media)} We get reflection and transmission coefficients defined as: R = \\frac{I_R}{I_I} \\qquad T = \\frac{I_T}{I_I} Plugging in our previous expressions for the incoming and reflected fields, R = \\frac{\\frac{1}{2} c n_1 \\epsilon_0}{\\frac{1}{2} c n_1 \\epsilon_0} \\left( \\frac{1 - \\beta}{1 + \\beta} \\right) ^2 = \\left( \\frac{n_1 - n_2}{n_1 + n_2} \\right) and the transmitted field, T = \\frac{\\frac{1}{2} c n_2 \\epsilon_0}{\\frac{1}{2} c n_1 \\epsilon_0} \\left( \\frac{2}{1 + \\frac{n_2}{n_1} } \\right) ^2 = \\frac{n_2}{n_1} \\frac{4 n_1 ^2}{(n_1 + n_2)^2} = \\frac{4 n_1 n_2}{(n_1 + n_2)^2} We should check that R + T = 1 , since we haven't got any absorption in our scenario: R + T = \\left( \\frac{n_1 - n_2}{n_1 + n_2} \\right)^2 + \\frac{4 n_1 n_2}{(n_1 + n_2)^2} = 1","title":"9.3.3 Reflection and Transmission at Oblique Incidence"},{"location":"ch9-4/","text":"9.4.1 Electromagnetic Waves in Conductors","title":"9.4 - Absorption and Dispersion"},{"location":"ch9-4/#941-electromagnetic-waves-in-conductors","text":"","title":"9.4.1 Electromagnetic Waves in Conductors"},{"location":"problems-ch3/","text":"Problem 3.24 Solution Since we are in cylindrical coordinates, we will write Laplace's equation in cylindrical coordinates (s, \\phi, z) : \\laplacian V = \\frac{1}{s} \\pdv{}{s} \\left( s \\pdv{V}{s} \\right) + \\frac{1}{s^2} \\frac{\\partial ^2 V}{\\partial \\phi ^2} = 0 We'll try the method of separation of variables on s and \\phi by searching for solutions which are products of the form V(s, \\phi) = S(s) + \\Phi(\\phi) \\frac{1}{s} \\Phi \\dv{}{s} \\left( s \\dv{S}{s} \\right) + \\frac{1}{s^2} S \\frac{d^2 \\Phi}{d\\phi ^2} = 0 to separate the variables, we need to divide by V and multiply by s^2 \\frac{s}{S} \\dv{}{s} \\left( s \\dv{S}{s} \\right) + \\frac{1}{\\Phi} \\frac{d^2 \\Phi}{d \\phi ^2} = 0 We define f(s) = \\frac{s}{S} \\dv{}{s} \\left( s \\dv{S}{s} \\right) and g(\\phi) = \\frac{1}{\\Phi} \\frac{d^2 \\Phi}{d \\phi ^2} Since we have separated our independent variables and the sum is equal to zero, they must both be constant f(s) = C_1 \\qquad g(\\phi) = C_2 \\qquad C_1 + C_2 = 0 Cylindrical symmetry implies that \\text{ when } \\phi \\rightarrow \\phi + 2 \\pi : \\qquad \\Phi(\\phi + 2 \\pi) = \\Phi(\\phi) So C_2 must be the positive one, since we know that will give us the periodic solutions to Laplace's equation. We write our constant as k^2 so \\frac{d^2 \\Phi}{d \\phi ^2} = - k^2 \\Phi \\\\ \\rightarrow \\Phi(\\phi) = A \\cos (k \\phi) + B \\sin (k \\phi), \\quad k = 0, 1, 2, 3, \\ldots Back to the S part, we need a solution to s \\dv{}{s} \\left( s \\dv{S}{s} \\right) = k^2 S A convenient solution would be a power function, S(s) = s^n if we choose the power n appropriately \\begin{align*} s \\dv{}{s} \\left( s \\dv{s^n}{s} \\right) & = s \\dv{}{s} \\left( s n s^{n-1} \\right) \\\\ & = s \\dv{}{s} (n s^n) \\\\ & = s n^2 s^{n-1} \\\\ & = n^2 s^n \\\\ & = k^2 S = k^2 s^n \\\\ & \\rightarrow n = \\pm k \\end{align*} So, our general solution for S is S(s) = C s^k + D s^{-k} And our general solution will be an infinite series over k . But we have to now be careful, because previously we've expressed our general solution in terms of strictly non-zero k , but here we have k = 0 , which gives us a constant solution k = 0: \\qquad S(s) = C s^0 + D s^0 = \\text{const.} But we should get two solutions for a second-order ordinary differential equation. If we go back to the differential equation for S, s \\dv{}{s} \\left( s \\dv{S}{s} \\right) = k^2 S \\\\ \\rightarrow s \\dv{S}{s} = \\text{ const. } = C \\\\ \\rightarrow \\dv{S}{s} = \\frac{c}{s} \\\\ \\rightarrow \\dd S = C \\frac{ds}{s} \\\\ S(s) = C \\ln s + D This gives us our second solution for S for k = 0 . Now what about for \\Phi ? Looking at the k = 0 case for the \\Phi ODE, \\frac{d^2 \\Phi}{d \\phi ^2} = - k^2 \\Phi = 0 \\quad \\text{ for } k = 0 \\\\ \\frac{d \\Phi}{d \\phi} = \\text{ const. } = B \\\\ \\rightarrow \\Phi(\\phi) = B \\phi + A But this doesn't meet our periodicity requirement! This isn't a physically acceptable solution. For k = 0, \\Phi = B is the only 'physically acceptable' solution (we discard B \\phi + A out of hand.) Finally, our general solution looks like V(s, \\phi) = a_0 + b_0 \\ln s + \\sum_{k=1} ^\\infty \\left[ s^k (a_k \\cos k \\phi + b_k \\sin k \\phi) + s^{-k} (a_k \\cos k \\phi + b_k \\sin k \\phi) \\right] We've only been asked for the general solution in cylindrical coordinates (from which we can tell that our solution is independent of a ), and we must be given boundary conditions in order to solve for the constants a_k, b_k . Problem 3.27 A sphere of radius R, centered at the origin, carries charge density \\rho(r, \\theta) = k \\frac{R}{r^2} (R - 2r) \\sin \\theta where k is a constant, and r, \\theta are the usual spherical coordinates. Find the approximate potential for points on the z axis, far from the sphere. We are asked for the approximate potential for points on the z-axis far from the charge distribution, so we'll calculate the terms of our potential from Eq 3.95, and stop when we find the first non-zero term, replacing \\theta for \\alpha and z for r as we go. V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{n=0} ^\\infty \\frac{1}{r^{(n+1)}} \\int (r') P_n(\\cos \\alpha) \\rho(\\vec{r'}) \\dd{\\tau'} Let's start with the monopole term. The integral we have to calculate is simply the charge density integrated over the charge distribution \\int \\rho(r) \\dd \\tau = k R \\int_0 ^R \\int _0 ^{\\pi} \\int_{0} ^{2 \\pi} \\frac{1}{r^2} (R - 2r) \\sin \\theta (r^2 \\sin \\theta ) \\dd r \\dd \\theta \\dd \\phi \\\\ \\int _{0} ^R (R - 2r) \\dd r = \\left.(R r - r^2)\\right|_{0} ^R = 0 So the monopole term comes out to zero. Next, we try calculating the dipole term: \\int r \\cos \\theta \\rho(r) \\dd \\tau = k R \\iiint r \\cos \\theta \\frac{1}{r^2} (R - 2r) \\sin \\theta (r^2 \\sin \\theta) \\dd r \\dd \\theta \\dd \\phi The \\theta integral will come out to \\int_0 ^{\\pi} \\sin ^2 \\cos \\theta \\dd \\theta = \\int _0 ^\\pi \\sin ^2 \\theta \\dd (\\sin \\theta) = \\left. \\frac{1}{3} \\sin ^3 \\theta \\right|_0 ^\\pi = 0 Well dangit, we still don't have the first non-zero term! On to the quadrupole term: \\begin{align*} & \\int r^2 \\left( \\frac{3}{2} \\cos ^2 \\theta - \\frac{1}{2} \\right) \\rho \\dd \\tau \\\\ = & \\iiint r^2 \\left( \\frac{3}{2} \\cos ^2 \\theta - \\frac{1}{2} \\right) \\frac{kR}{r^2} (R - 2r) \\sin \\theta r^2 \\sin \\theta \\dd r \\dd \\theta \\dd \\phi \\\\ = & \\frac{1}{2} kR \\iiint r^2 (3 \\cos ^2 \\theta - 1)(R - 2r) \\sin ^2 \\theta \\dd r \\dd \\theta \\dd \\phi \\end{align*} Thankfully we don't have any cross-terms, so we can do the integrals separately. The integral in r is \\int_0 ^R r^2 (R - 2r) \\dd r = - \\frac{R^4}{6} The integral in \\theta is \\int_0 ^\\pi (3 \\cos ^2 \\theta - 1) \\sin ^2 \\theta \\dd \\theta = \\int _0 ^\\pi \\left[ 3 (1 - \\sin ^2 \\theta) - 1 \\right] \\sin ^2 \\theta \\dd \\theta = - \\frac{\\pi}{8} And we just get a 2 \\pi from the \\phi integral, so converting our r to z in our coordinate system, the whole quadrupole potential is V(\\vec{r}) \\approx \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{z^3} \\frac{1}{2} k R \\left( - \\frac{R^4}{6} \\right) \\left( - \\frac{\\pi}{8} \\right) (2 \\pi) \\\\ = \\frac{1}{4 \\pi \\epsilon_0} \\frac{k \\pi ^2 R ^5}{48 z^3} \\quad \\text{(Quadrupole)} Problem 3.31 In Ex. 3.9, we derived the exact potential for a spherical shell of radius R , which carries a surface charge \\sigma = k \\cos \\theta . a) Calculate the dipole moment of this charge distribution. b) Find the approximate potential, at points far from the sphere, and compare the exact answer (Eq 3.87). What can you conclude about the higher multipoles? By the symmetry of the problem, p is going to be in the z-direction: \\vec{p} = p \\vu{z}; \\, p = \\int z \\rho \\dd \\tau \\rightarrow \\int z \\sigma \\dd a . \\begin{align*} p & = \\int (R \\cos \\theta)(k \\cos \\theta) R^3 \\sin \\theta \\dd \\theta \\dd \\phi \\\\ & = 2 \\pi R^3 k \\int _0 ^\\pi \\cos ^2 \\theta \\sin \\theta \\dd \\theta \\\\ & = 2 \\pi R^3 k \\left. \\left( - \\frac{\\cos ^3 \\theta}{3} \\right) \\right|_0 ^\\pi \\\\ & = \\frac{2}{3} \\pi R^3 k [ 1 - (-1) ] \\\\ & = \\frac{4 \\pi R^3 k}{3} \\end{align*} \\tag{a} \\vec{p} = \\frac{4 \\pi R^3 k}{3} \\vu{z} The associated dipole potential is just V_{dip} \\approx \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vu{r} \\cdot \\vec{p}}{r^2} = \\frac{k R^3}{3 \\epsilon_0} \\frac{\\cos \\theta}{r^2} Problem 3.33 Show that the electric field of a 'pure' dipole can be written in the coordinate-free form \\vec{E_{dip}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^3} [3 (\\vec{p} \\cdot \\vu{r}) \\vu{r} - \\vec{p} ] We still assume the dipole is pointing in the z-direction and start with spherical coordinates, and then move to a coordinate-free system \\vec{p} = p \\vu{z} \\vec{p} = p_r \\vu{r} + p_\\theta \\vu{\\theta} + p_{\\phi} \\vu{\\phi} Since p is in the z-direction, we can safely say p_\\phi = 0 p_r = \\vec{p} \\cdot \\vu{r} = p \\cos \\theta \\\\ p_\\theta = \\vec{p} \\cdot \\vu{\\theta} = - p \\sin \\theta \\\\ \\vec{p} = p \\cos \\theta \\vu{r} - p \\sin \\theta \\vu{\\theta} So we can directly check this expression against the expression we got as Eqn 3.103 (\\vec{E_{dip}}(r, \\theta) = \\frac{p}{4 \\pi \\epsilon_0 r^3} (2 \\cos \\theta \\vu{r} + \\sin \\theta \\vu{\\theta} ) : \\begin{align*} 3 ( \\vec{p} \\cdot \\vu{r}) \\vu{r} - \\vec{p} = & 3 p \\cos \\theta \\vu{r} - p \\cos \\theta \\vu{r} + p \\sin \\theta \\vu{\\theta} \\\\ & = 2 p \\cos \\theta \\vu{r} + p \\sin \\theta \\vu{\\theta} \\\\ \\rightarrow \\vec{E_{dip}} & = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^3} [3 (\\vec{p} \\cdot \\vu{r}) \\vu{r} - \\vec{p} ] \\end{align*} So it all checks out. Problem 3.34 Three point charges are located as shown in Fig 3.38, each a distance a from the origin. Find the approximate electric field at points far from the origin. Express your answer in spherical coordinates, and include the two lowest orders in the multipole expansion. We'll get to the electric field by writing down the multipole expansion of the potential, and then using the approximate potential to get the electric field. The total charge is -q, so the monopole term will be V_{mon} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r} (-q) The dipole moment is given by \\begin{align*} \\vec{p} & = \\sum_{i=1} ^3 q_i \\vec{r_i} \\\\ & = (-q) a \\vu{y} + (-q) a (-\\vu{y}) + q a \\vu{z} \\\\ & = qa \\vu{z} \\end{align*} The dipole term in the multipole expansion of V is then \\begin{align*} V_{dip} & = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} \\\\ & = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q a \\vu{z} \\cdot \\vu{r}}{r^2} \\\\ & = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q a \\cos \\theta}{r^2} \\end{align*} V(r, \\theta) \\approx \\frac{q}{4 \\pi \\epsilon_0 } \\left( - \\frac{1}{r} + \\frac{a \\cos \\theta}{r^2} \\right) The field is given by \\vec{E} = - \\grad V \\approx \\frac{q}{4 \\pi \\epsilon_0} \\left( - \\frac{1}{r^2} \\vu{r} + \\frac{2 a \\cos \\theta \\vu{r}}{r^3} \\vu{r} + \\frac{a}{r^3} \\sin \\theta \\vu{\\theta} \\right)","title":"Problems"},{"location":"problems-ch3/#problem-324","text":"Solution Since we are in cylindrical coordinates, we will write Laplace's equation in cylindrical coordinates (s, \\phi, z) : \\laplacian V = \\frac{1}{s} \\pdv{}{s} \\left( s \\pdv{V}{s} \\right) + \\frac{1}{s^2} \\frac{\\partial ^2 V}{\\partial \\phi ^2} = 0 We'll try the method of separation of variables on s and \\phi by searching for solutions which are products of the form V(s, \\phi) = S(s) + \\Phi(\\phi) \\frac{1}{s} \\Phi \\dv{}{s} \\left( s \\dv{S}{s} \\right) + \\frac{1}{s^2} S \\frac{d^2 \\Phi}{d\\phi ^2} = 0 to separate the variables, we need to divide by V and multiply by s^2 \\frac{s}{S} \\dv{}{s} \\left( s \\dv{S}{s} \\right) + \\frac{1}{\\Phi} \\frac{d^2 \\Phi}{d \\phi ^2} = 0 We define f(s) = \\frac{s}{S} \\dv{}{s} \\left( s \\dv{S}{s} \\right) and g(\\phi) = \\frac{1}{\\Phi} \\frac{d^2 \\Phi}{d \\phi ^2} Since we have separated our independent variables and the sum is equal to zero, they must both be constant f(s) = C_1 \\qquad g(\\phi) = C_2 \\qquad C_1 + C_2 = 0 Cylindrical symmetry implies that \\text{ when } \\phi \\rightarrow \\phi + 2 \\pi : \\qquad \\Phi(\\phi + 2 \\pi) = \\Phi(\\phi) So C_2 must be the positive one, since we know that will give us the periodic solutions to Laplace's equation. We write our constant as k^2 so \\frac{d^2 \\Phi}{d \\phi ^2} = - k^2 \\Phi \\\\ \\rightarrow \\Phi(\\phi) = A \\cos (k \\phi) + B \\sin (k \\phi), \\quad k = 0, 1, 2, 3, \\ldots Back to the S part, we need a solution to s \\dv{}{s} \\left( s \\dv{S}{s} \\right) = k^2 S A convenient solution would be a power function, S(s) = s^n if we choose the power n appropriately \\begin{align*} s \\dv{}{s} \\left( s \\dv{s^n}{s} \\right) & = s \\dv{}{s} \\left( s n s^{n-1} \\right) \\\\ & = s \\dv{}{s} (n s^n) \\\\ & = s n^2 s^{n-1} \\\\ & = n^2 s^n \\\\ & = k^2 S = k^2 s^n \\\\ & \\rightarrow n = \\pm k \\end{align*} So, our general solution for S is S(s) = C s^k + D s^{-k} And our general solution will be an infinite series over k . But we have to now be careful, because previously we've expressed our general solution in terms of strictly non-zero k , but here we have k = 0 , which gives us a constant solution k = 0: \\qquad S(s) = C s^0 + D s^0 = \\text{const.} But we should get two solutions for a second-order ordinary differential equation. If we go back to the differential equation for S, s \\dv{}{s} \\left( s \\dv{S}{s} \\right) = k^2 S \\\\ \\rightarrow s \\dv{S}{s} = \\text{ const. } = C \\\\ \\rightarrow \\dv{S}{s} = \\frac{c}{s} \\\\ \\rightarrow \\dd S = C \\frac{ds}{s} \\\\ S(s) = C \\ln s + D This gives us our second solution for S for k = 0 . Now what about for \\Phi ? Looking at the k = 0 case for the \\Phi ODE, \\frac{d^2 \\Phi}{d \\phi ^2} = - k^2 \\Phi = 0 \\quad \\text{ for } k = 0 \\\\ \\frac{d \\Phi}{d \\phi} = \\text{ const. } = B \\\\ \\rightarrow \\Phi(\\phi) = B \\phi + A But this doesn't meet our periodicity requirement! This isn't a physically acceptable solution. For k = 0, \\Phi = B is the only 'physically acceptable' solution (we discard B \\phi + A out of hand.) Finally, our general solution looks like V(s, \\phi) = a_0 + b_0 \\ln s + \\sum_{k=1} ^\\infty \\left[ s^k (a_k \\cos k \\phi + b_k \\sin k \\phi) + s^{-k} (a_k \\cos k \\phi + b_k \\sin k \\phi) \\right] We've only been asked for the general solution in cylindrical coordinates (from which we can tell that our solution is independent of a ), and we must be given boundary conditions in order to solve for the constants a_k, b_k .","title":"Problem 3.24"},{"location":"problems-ch3/#problem-327","text":"A sphere of radius R, centered at the origin, carries charge density \\rho(r, \\theta) = k \\frac{R}{r^2} (R - 2r) \\sin \\theta where k is a constant, and r, \\theta are the usual spherical coordinates. Find the approximate potential for points on the z axis, far from the sphere. We are asked for the approximate potential for points on the z-axis far from the charge distribution, so we'll calculate the terms of our potential from Eq 3.95, and stop when we find the first non-zero term, replacing \\theta for \\alpha and z for r as we go. V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{n=0} ^\\infty \\frac{1}{r^{(n+1)}} \\int (r') P_n(\\cos \\alpha) \\rho(\\vec{r'}) \\dd{\\tau'} Let's start with the monopole term. The integral we have to calculate is simply the charge density integrated over the charge distribution \\int \\rho(r) \\dd \\tau = k R \\int_0 ^R \\int _0 ^{\\pi} \\int_{0} ^{2 \\pi} \\frac{1}{r^2} (R - 2r) \\sin \\theta (r^2 \\sin \\theta ) \\dd r \\dd \\theta \\dd \\phi \\\\ \\int _{0} ^R (R - 2r) \\dd r = \\left.(R r - r^2)\\right|_{0} ^R = 0 So the monopole term comes out to zero. Next, we try calculating the dipole term: \\int r \\cos \\theta \\rho(r) \\dd \\tau = k R \\iiint r \\cos \\theta \\frac{1}{r^2} (R - 2r) \\sin \\theta (r^2 \\sin \\theta) \\dd r \\dd \\theta \\dd \\phi The \\theta integral will come out to \\int_0 ^{\\pi} \\sin ^2 \\cos \\theta \\dd \\theta = \\int _0 ^\\pi \\sin ^2 \\theta \\dd (\\sin \\theta) = \\left. \\frac{1}{3} \\sin ^3 \\theta \\right|_0 ^\\pi = 0 Well dangit, we still don't have the first non-zero term! On to the quadrupole term: \\begin{align*} & \\int r^2 \\left( \\frac{3}{2} \\cos ^2 \\theta - \\frac{1}{2} \\right) \\rho \\dd \\tau \\\\ = & \\iiint r^2 \\left( \\frac{3}{2} \\cos ^2 \\theta - \\frac{1}{2} \\right) \\frac{kR}{r^2} (R - 2r) \\sin \\theta r^2 \\sin \\theta \\dd r \\dd \\theta \\dd \\phi \\\\ = & \\frac{1}{2} kR \\iiint r^2 (3 \\cos ^2 \\theta - 1)(R - 2r) \\sin ^2 \\theta \\dd r \\dd \\theta \\dd \\phi \\end{align*} Thankfully we don't have any cross-terms, so we can do the integrals separately. The integral in r is \\int_0 ^R r^2 (R - 2r) \\dd r = - \\frac{R^4}{6} The integral in \\theta is \\int_0 ^\\pi (3 \\cos ^2 \\theta - 1) \\sin ^2 \\theta \\dd \\theta = \\int _0 ^\\pi \\left[ 3 (1 - \\sin ^2 \\theta) - 1 \\right] \\sin ^2 \\theta \\dd \\theta = - \\frac{\\pi}{8} And we just get a 2 \\pi from the \\phi integral, so converting our r to z in our coordinate system, the whole quadrupole potential is V(\\vec{r}) \\approx \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{z^3} \\frac{1}{2} k R \\left( - \\frac{R^4}{6} \\right) \\left( - \\frac{\\pi}{8} \\right) (2 \\pi) \\\\ = \\frac{1}{4 \\pi \\epsilon_0} \\frac{k \\pi ^2 R ^5}{48 z^3} \\quad \\text{(Quadrupole)}","title":"Problem 3.27"},{"location":"problems-ch3/#problem-331","text":"In Ex. 3.9, we derived the exact potential for a spherical shell of radius R , which carries a surface charge \\sigma = k \\cos \\theta . a) Calculate the dipole moment of this charge distribution. b) Find the approximate potential, at points far from the sphere, and compare the exact answer (Eq 3.87). What can you conclude about the higher multipoles? By the symmetry of the problem, p is going to be in the z-direction: \\vec{p} = p \\vu{z}; \\, p = \\int z \\rho \\dd \\tau \\rightarrow \\int z \\sigma \\dd a . \\begin{align*} p & = \\int (R \\cos \\theta)(k \\cos \\theta) R^3 \\sin \\theta \\dd \\theta \\dd \\phi \\\\ & = 2 \\pi R^3 k \\int _0 ^\\pi \\cos ^2 \\theta \\sin \\theta \\dd \\theta \\\\ & = 2 \\pi R^3 k \\left. \\left( - \\frac{\\cos ^3 \\theta}{3} \\right) \\right|_0 ^\\pi \\\\ & = \\frac{2}{3} \\pi R^3 k [ 1 - (-1) ] \\\\ & = \\frac{4 \\pi R^3 k}{3} \\end{align*} \\tag{a} \\vec{p} = \\frac{4 \\pi R^3 k}{3} \\vu{z} The associated dipole potential is just V_{dip} \\approx \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vu{r} \\cdot \\vec{p}}{r^2} = \\frac{k R^3}{3 \\epsilon_0} \\frac{\\cos \\theta}{r^2}","title":"Problem 3.31"},{"location":"problems-ch3/#problem-333","text":"Show that the electric field of a 'pure' dipole can be written in the coordinate-free form \\vec{E_{dip}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^3} [3 (\\vec{p} \\cdot \\vu{r}) \\vu{r} - \\vec{p} ] We still assume the dipole is pointing in the z-direction and start with spherical coordinates, and then move to a coordinate-free system \\vec{p} = p \\vu{z} \\vec{p} = p_r \\vu{r} + p_\\theta \\vu{\\theta} + p_{\\phi} \\vu{\\phi} Since p is in the z-direction, we can safely say p_\\phi = 0 p_r = \\vec{p} \\cdot \\vu{r} = p \\cos \\theta \\\\ p_\\theta = \\vec{p} \\cdot \\vu{\\theta} = - p \\sin \\theta \\\\ \\vec{p} = p \\cos \\theta \\vu{r} - p \\sin \\theta \\vu{\\theta} So we can directly check this expression against the expression we got as Eqn 3.103 (\\vec{E_{dip}}(r, \\theta) = \\frac{p}{4 \\pi \\epsilon_0 r^3} (2 \\cos \\theta \\vu{r} + \\sin \\theta \\vu{\\theta} ) : \\begin{align*} 3 ( \\vec{p} \\cdot \\vu{r}) \\vu{r} - \\vec{p} = & 3 p \\cos \\theta \\vu{r} - p \\cos \\theta \\vu{r} + p \\sin \\theta \\vu{\\theta} \\\\ & = 2 p \\cos \\theta \\vu{r} + p \\sin \\theta \\vu{\\theta} \\\\ \\rightarrow \\vec{E_{dip}} & = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^3} [3 (\\vec{p} \\cdot \\vu{r}) \\vu{r} - \\vec{p} ] \\end{align*} So it all checks out.","title":"Problem 3.33"},{"location":"problems-ch3/#problem-334","text":"Three point charges are located as shown in Fig 3.38, each a distance a from the origin. Find the approximate electric field at points far from the origin. Express your answer in spherical coordinates, and include the two lowest orders in the multipole expansion. We'll get to the electric field by writing down the multipole expansion of the potential, and then using the approximate potential to get the electric field. The total charge is -q, so the monopole term will be V_{mon} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r} (-q) The dipole moment is given by \\begin{align*} \\vec{p} & = \\sum_{i=1} ^3 q_i \\vec{r_i} \\\\ & = (-q) a \\vu{y} + (-q) a (-\\vu{y}) + q a \\vu{z} \\\\ & = qa \\vu{z} \\end{align*} The dipole term in the multipole expansion of V is then \\begin{align*} V_{dip} & = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\vu{r}}{r^2} \\\\ & = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q a \\vu{z} \\cdot \\vu{r}}{r^2} \\\\ & = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q a \\cos \\theta}{r^2} \\end{align*} V(r, \\theta) \\approx \\frac{q}{4 \\pi \\epsilon_0 } \\left( - \\frac{1}{r} + \\frac{a \\cos \\theta}{r^2} \\right) The field is given by \\vec{E} = - \\grad V \\approx \\frac{q}{4 \\pi \\epsilon_0} \\left( - \\frac{1}{r^2} \\vu{r} + \\frac{2 a \\cos \\theta \\vu{r}}{r^3} \\vu{r} + \\frac{a}{r^3} \\sin \\theta \\vu{\\theta} \\right)","title":"Problem 3.34"},{"location":"problems-ch5/","text":"Problem 5.5 A current I flows down a wire of radius a. (a) If it is uniformly distributed over the surface, what is the surface current density K? (b) If it is distributed in such a way that the volume current density is inversely proportional to the distance from the axis, what is J(s)? K is the current per unit width \\perp to the direction of the flow. K = \\frac{I}{2 \\pi a} Suppose instead the current is distributed somehow throughout the volume of the wire such that the current density is inversely proportional to the distance from the axis. Then j = \\frac{\\text{current}}{\\text{unit area } \\perp \\text{ flow}} = \\frac{d I}{da_{\\perp}} We suppose that j has the form j = \\frac{\\text{const.}}{s} = \\frac{c}{s} \\begin{align*} I & = \\int j \\dd a_{\\perp} \\\\ & = \\int_{0} ^a \\int_{0} ^{2 \\pi} \\frac{c}{s} s \\dd s \\dd \\phi \\\\ & = 2 \\pi c a \\end{align*} so c = \\frac{I}{2 \\pi a} and j = \\frac{I}{2 \\pi a s} Problem 5.11 Find the magnetic field at point P on the axis of a tightly wound solenoid (helical coil) consisting of n turns per unit length wrapped around a cylindrical tube of radius a and carrying current I (Fig 5.25). Express your answer in terms of \\theta_0 and \\theta_2 (it's easiest that way). Consider the turns to be essentially circular and use the result of Ex 5.6. What is the field on the axis of an infinite solenoid (infinite in both directions)? If I have n turns per unit length, then I have n \\dd z turns along a length \\dd z (using the natural cylindrical coordinates of the problem). The total current of the resulting loop is I n \\dd z . From Ex 5.6, we know the magnetic field due to a circular loop is \\dd \\vec{B}(z) = \\frac{ \\mu_0 n I \\dd z}{2} \\frac{a^2}{(a^2 + z^2 )^{3/2}} \\vu{\\phi} From the geometry of Fig 5.25, \\tan \\theta = \\frac{a}{z} \\quad \\rightarrow \\quad z = \\frac{a}{\\tan \\theta} \\dd z = - \\frac{a}{\\sin ^2 \\theta} \\dd \\theta (a^2 + z^2)^{3/2} = \\left( a^2 + \\frac{a^2}{\\tan ^2 \\theta} \\right)^{3/2} = \\left( \\frac{a}{\\sin \\theta} \\right)^3 \\begin{align*} B(z) & = \\frac{\\mu_0 n I}{2} \\left( - \\frac{a}{\\sin ^2 \\theta} \\dd \\theta \\right)\\frac{a^2}{(a^3 / \\sin ^3 \\theta)} \\\\ & = - \\frac{\\mu_0 n I}{2} \\sin \\theta \\dd \\theta \\end{align*} \\begin{align*} B(z) & = - \\frac{\\mu_0 n I}{2} \\int _{\\theta_1} ^{\\theta_2} \\sin \\theta \\dd \\theta \\\\ & = \\frac{\\mu_0 n I}{2} (\\cos \\theta_2 - \\cos \\theta_1) \\end{align*} For an infinite solenoid, we get B(z) = \\frac{\\mu_0 n I}{2} (\\cos(0) - \\cos \\theta_1) Problem 5.23 Find the magnetic vector potential of a finite segment of straight wire carrying a current I. [Put the wire on the z axis, from z_1 to z_2 , and use Eq. 5.66.] Check that your answer is consistent with Eq. 5.37. We will get our vector potential using Eq 5.66, as suggested \\begin{align*} \\vec{A} & = \\frac{\\mu_0 }{4 \\pi} \\int \\frac{I \\vu{z}}{\\gr} \\\\ & = \\frac{\\mu_0 I}{4 \\pi} \\vu{z} \\int _{z_1} ^{z_2} \\frac{dz}{\\sqrt{z^2 + s^2}} \\\\ & = \\left. \\frac{\\mu_0 I}{4 \\pi} \\vu{z} \\left[ \\ln \\left( z + \\sqrt{z^2 + s^2} \\right) \\right] \\right|_{z_1} ^{z_2} \\\\ & = \\frac{\\mu_0 I}{4 \\pi} \\ln \\left( \\frac{z_2 + \\sqrt{(z_2)^2 + s^2}}{z_1 + \\sqrt{(z_1)^2 + s^2}} \\right) \\vu{z} \\end{align*} To get the magnetic field, we need to take the curl of A . We can easily tell from the symmetry of the problem that the field will be \"circumferential\" (in the \\vu{\\phi} direction): \\begin{align*} \\vec{B} & = \\curl \\vec{A} = - \\pdv{A}{s} \\vu{\\phi} \\\\ & = - \\frac{\\mu_0 I}{4 \\pi} \\left( \\frac{1}{z_2 + \\sqrt{(z_2)^2 + s^2}} \\frac{s}{\\sqrt{(z_2)^2 + s^2}} - \\frac{1}{z_1 + \\sqrt{(z_1)^2 + s^2}} \\frac{s}{\\sqrt{(z_1)^2 + s^2}} \\right) \\vu{\\phi} \\\\ & = - \\frac{\\mu_0 I s}{4 \\pi} \\left( \\frac{z_2 - \\sqrt{z_2 ^2 + s^2}}{z_2 ^2 - (z_2 ^2 + s^2)} \\frac{1}{\\sqrt{z_2 ^2 + s^2}} - \\frac{z_1 - \\sqrt{z_1 ^2 + s^2}}{z_1 ^2 - (z_1 ^2 + s^2)} \\frac{1}{\\sqrt{z_1 ^2 + s^2}} \\right) \\vu{\\phi} \\\\ & = - \\frac{\\mu_0 I s}{4 \\pi} \\left( - \\frac{1}{s^2} \\right) \\left( \\frac{z_2}{\\sqrt{z_2 ^2 + s^2}} - 1 - \\frac{z_1}{\\sqrt{z_1 ^2 + s^2}} + 1 \\right) \\vu{\\phi} \\\\ & = \\frac{\\mu_0 I}{4 \\pi s} \\left( \\frac{z_2}{\\sqrt{(z_2) ^2 + s^2}} - \\frac{z_1}{\\sqrt{z_1 ^2 + s^2}} \\right) \\vu{\\phi} \\end{align*} or, in terms of the angles made between r and the axis of the wire, \\sin \\theta_1 = \\frac{z_1}{\\sqrt{z_1 ^2 + s^2}} \\quad \\text{ and } \\quad \\sin \\theta_2 = \\frac{z_2}{\\sqrt{z_2 ^2 + s^2}} \\vec{B} = \\frac{\\mu_0 I}{4 \\pi s} (\\sin \\theta_2 - \\sin \\theta_1) \\vu{\\phi} which is just what we got back in Eq. 5.37. Problem 5.26 (a) By whatever means you can think of (short of looking it up), find the vector potential a distance s from an infinite straight wire carrying a current I . Check that \\div \\vec{A} = 0 and \\curl \\vec{A} = \\vec{B} . (b) Find the magnetic potential inside the wire, if it has radius R and the current is uniformly distributed. (a) As we said, because the current distribution is infinite, we cannot use Eq. 5.65 to get A . So let's use some symmetry. A must be parallel (or antiparallel) to I , and is a function of only s (the distance from the wire). In cylindrical coordinates, then, \\vec{A} = A(s) \\vu{z} . We already calculated the magnetic field of an infinite straight wire via Biot-Savart: \\vec{B} = \\frac{\\mu_0 I}{2 \\pi s} \\vu{\\phi} We can work backwards to get A from B in this case. \\vec{B} = \\curl \\vec{A} = - \\pdv{A}{s} \\vu{\\phi} = \\frac{\\mu_0 I}{2 \\pi s} \\vu{\\phi} Therefore \\pdv{A}{s} = -\\frac{\\mu_0 I}{2 \\pi s} \\quad \\rightarrow \\quad \\vec{A}(r) = - \\frac{\\mu_0 I}{2 \\pi} \\ln (s / a) \\vu{z} There is an arbitrary constant a here which doesn't actually affect our gauge at all: \\div \\vec{A} = \\pdv{A_z}{z} = 0 \\curl \\vec{A} = - \\pdv{A_z}{s} \\vu{\\phi} = \\frac{\\mu_0 I}{2 \\pi s} \\vu{\\phi} = \\vec{B} (b) Ampere's law in this case says \\oint \\vec{B} \\cdot \\dd \\vec{l} = B 2 \\pi s = \\mu_0 I_{enc} = \\mu_0 J \\pi s^2 = \\mu_0 \\frac{I}{\\pi R^2} \\pi s^2 = \\frac{\\mu_0 I s^2}{R^2} so, inside the wire, \\vec{B} = \\frac{\\mu_0 I s}{2\\pi R^2} \\vu{\\phi} From the definition of A , \\pdv{A}{s} = - \\frac{\\mu_0 I}{2 \\pi} \\frac{s}{R^2} \\rightarrow \\vec{A} = -\\frac{\\mu_0 I}{2 \\pi R^2} \\int_{b} ^s s \\, \\dd s = - \\frac{\\mu_0 I}{4 \\pi R^2} (s^2 - b^2) \\vu{z} Here, again, b is arbitrary, except that A must be continuous at R (we know that A is continuous!) - \\frac{\\mu_0 I}{2 \\pi } \\ln (R / a) = - \\frac{\\mu_0 I}{4 \\pi R^2} (R^2 - b^2) which means that we have to pick a and b such that 2 \\ln (R / b) = 1 - (b / R)^2 One such combination of a and b is a = b = R . Then \\vec{A} = \\begin{cases} - \\frac{\\mu_0 I}{4 \\pi R^2} (s^2 - R^2) \\vu{z} & \\quad \\text{ for } s \\leq R \\\\ - \\frac{\\mu_0 I}{2 \\pi} \\ln(s / R) \\vu{z}& \\quad \\text{ for } s \\geq R \\end{cases} Problem 5.37 (a) A phonograph record of radius R, carrying a uniform surface charge sigma is rotating at constant angular velocity \\omega . Find its magnetic dipole moment. (b) Find the magnetic dipole moment of the spinning spherical shell in Example 5.11. Show that for points r > R the potential is that of a perfect dipole. (a) We get the monopole moment by integrating over the disk of the record. For a ring at radius r , m = I \\pi r^2 . In this case, I \\rightarrow \\sigma v \\dd r = \\sigma \\omega r \\dd r so m = \\int _0 ^R \\pi r^2 \\sigma \\omega r \\dd \\r = \\pi \\sigma \\omega R^4 / 4 (b) To get the magnetic dipole moment of our sphere, we need to integrate over the surface of the sphere: The total charge on the shaded ring is \\dd q = \\sigma (2 \\pi R \\sin \\theta) R \\dd \\theta . The time to make one revolution is \\dd t = 2 \\pi \\omega , so the current in the ring is I = \\frac{dq}{dt} = \\sigma \\omega R^2 \\sin \\theta \\dd \\theta The area of the ring is \\pi (R \\sin \\theta)^2 , so the magnetic moment of the ring is \\dd m = (\\sigma \\omega R^2 \\sin \\theta \\dd \\theta) \\pi R^2 \\sin ^2 \\theta and the total dipole moment is m = \\sigma \\omega \\pi R^4 \\int_0 ^\\pi \\sin ^3 \\theta \\dd \\theta = (4 / 3) \\sigma \\omega \\pi R^4 and we know that m points in the \\vu{z} direction (right-hand-rule), so \\vec{m} = \\frac{4 \\pi}{3} \\sigma \\omega R^4 \\vu{z} The dipole term in the multipole expansion for A is therefore \\vec{A}_{dip} = \\frac{\\mu_0}{4 \\pi} \\frac{4 \\pi}{3} \\sigma \\omega R^4 \\frac{\\sin \\theta}{r^2} \\vu{\\phi} = \\frac{\\mu_0 \\sigma \\omega R^4}{3} \\frac{\\sin \\theta}{r^2} \\vu{\\phi} This is actually the exact vector potential we calculated (Eq. 5.69); evidently a spinning sphere produces a perfect dipole field, with no higher multipole contributions.","title":"Problems"},{"location":"problems-ch5/#problem-55","text":"A current I flows down a wire of radius a. (a) If it is uniformly distributed over the surface, what is the surface current density K? (b) If it is distributed in such a way that the volume current density is inversely proportional to the distance from the axis, what is J(s)? K is the current per unit width \\perp to the direction of the flow. K = \\frac{I}{2 \\pi a} Suppose instead the current is distributed somehow throughout the volume of the wire such that the current density is inversely proportional to the distance from the axis. Then j = \\frac{\\text{current}}{\\text{unit area } \\perp \\text{ flow}} = \\frac{d I}{da_{\\perp}} We suppose that j has the form j = \\frac{\\text{const.}}{s} = \\frac{c}{s} \\begin{align*} I & = \\int j \\dd a_{\\perp} \\\\ & = \\int_{0} ^a \\int_{0} ^{2 \\pi} \\frac{c}{s} s \\dd s \\dd \\phi \\\\ & = 2 \\pi c a \\end{align*} so c = \\frac{I}{2 \\pi a} and j = \\frac{I}{2 \\pi a s}","title":"Problem 5.5"},{"location":"problems-ch5/#problem-511","text":"Find the magnetic field at point P on the axis of a tightly wound solenoid (helical coil) consisting of n turns per unit length wrapped around a cylindrical tube of radius a and carrying current I (Fig 5.25). Express your answer in terms of \\theta_0 and \\theta_2 (it's easiest that way). Consider the turns to be essentially circular and use the result of Ex 5.6. What is the field on the axis of an infinite solenoid (infinite in both directions)? If I have n turns per unit length, then I have n \\dd z turns along a length \\dd z (using the natural cylindrical coordinates of the problem). The total current of the resulting loop is I n \\dd z . From Ex 5.6, we know the magnetic field due to a circular loop is \\dd \\vec{B}(z) = \\frac{ \\mu_0 n I \\dd z}{2} \\frac{a^2}{(a^2 + z^2 )^{3/2}} \\vu{\\phi} From the geometry of Fig 5.25, \\tan \\theta = \\frac{a}{z} \\quad \\rightarrow \\quad z = \\frac{a}{\\tan \\theta} \\dd z = - \\frac{a}{\\sin ^2 \\theta} \\dd \\theta (a^2 + z^2)^{3/2} = \\left( a^2 + \\frac{a^2}{\\tan ^2 \\theta} \\right)^{3/2} = \\left( \\frac{a}{\\sin \\theta} \\right)^3 \\begin{align*} B(z) & = \\frac{\\mu_0 n I}{2} \\left( - \\frac{a}{\\sin ^2 \\theta} \\dd \\theta \\right)\\frac{a^2}{(a^3 / \\sin ^3 \\theta)} \\\\ & = - \\frac{\\mu_0 n I}{2} \\sin \\theta \\dd \\theta \\end{align*} \\begin{align*} B(z) & = - \\frac{\\mu_0 n I}{2} \\int _{\\theta_1} ^{\\theta_2} \\sin \\theta \\dd \\theta \\\\ & = \\frac{\\mu_0 n I}{2} (\\cos \\theta_2 - \\cos \\theta_1) \\end{align*} For an infinite solenoid, we get B(z) = \\frac{\\mu_0 n I}{2} (\\cos(0) - \\cos \\theta_1)","title":"Problem 5.11"},{"location":"problems-ch5/#problem-523","text":"Find the magnetic vector potential of a finite segment of straight wire carrying a current I. [Put the wire on the z axis, from z_1 to z_2 , and use Eq. 5.66.] Check that your answer is consistent with Eq. 5.37. We will get our vector potential using Eq 5.66, as suggested \\begin{align*} \\vec{A} & = \\frac{\\mu_0 }{4 \\pi} \\int \\frac{I \\vu{z}}{\\gr} \\\\ & = \\frac{\\mu_0 I}{4 \\pi} \\vu{z} \\int _{z_1} ^{z_2} \\frac{dz}{\\sqrt{z^2 + s^2}} \\\\ & = \\left. \\frac{\\mu_0 I}{4 \\pi} \\vu{z} \\left[ \\ln \\left( z + \\sqrt{z^2 + s^2} \\right) \\right] \\right|_{z_1} ^{z_2} \\\\ & = \\frac{\\mu_0 I}{4 \\pi} \\ln \\left( \\frac{z_2 + \\sqrt{(z_2)^2 + s^2}}{z_1 + \\sqrt{(z_1)^2 + s^2}} \\right) \\vu{z} \\end{align*} To get the magnetic field, we need to take the curl of A . We can easily tell from the symmetry of the problem that the field will be \"circumferential\" (in the \\vu{\\phi} direction): \\begin{align*} \\vec{B} & = \\curl \\vec{A} = - \\pdv{A}{s} \\vu{\\phi} \\\\ & = - \\frac{\\mu_0 I}{4 \\pi} \\left( \\frac{1}{z_2 + \\sqrt{(z_2)^2 + s^2}} \\frac{s}{\\sqrt{(z_2)^2 + s^2}} - \\frac{1}{z_1 + \\sqrt{(z_1)^2 + s^2}} \\frac{s}{\\sqrt{(z_1)^2 + s^2}} \\right) \\vu{\\phi} \\\\ & = - \\frac{\\mu_0 I s}{4 \\pi} \\left( \\frac{z_2 - \\sqrt{z_2 ^2 + s^2}}{z_2 ^2 - (z_2 ^2 + s^2)} \\frac{1}{\\sqrt{z_2 ^2 + s^2}} - \\frac{z_1 - \\sqrt{z_1 ^2 + s^2}}{z_1 ^2 - (z_1 ^2 + s^2)} \\frac{1}{\\sqrt{z_1 ^2 + s^2}} \\right) \\vu{\\phi} \\\\ & = - \\frac{\\mu_0 I s}{4 \\pi} \\left( - \\frac{1}{s^2} \\right) \\left( \\frac{z_2}{\\sqrt{z_2 ^2 + s^2}} - 1 - \\frac{z_1}{\\sqrt{z_1 ^2 + s^2}} + 1 \\right) \\vu{\\phi} \\\\ & = \\frac{\\mu_0 I}{4 \\pi s} \\left( \\frac{z_2}{\\sqrt{(z_2) ^2 + s^2}} - \\frac{z_1}{\\sqrt{z_1 ^2 + s^2}} \\right) \\vu{\\phi} \\end{align*} or, in terms of the angles made between r and the axis of the wire, \\sin \\theta_1 = \\frac{z_1}{\\sqrt{z_1 ^2 + s^2}} \\quad \\text{ and } \\quad \\sin \\theta_2 = \\frac{z_2}{\\sqrt{z_2 ^2 + s^2}} \\vec{B} = \\frac{\\mu_0 I}{4 \\pi s} (\\sin \\theta_2 - \\sin \\theta_1) \\vu{\\phi} which is just what we got back in Eq. 5.37.","title":"Problem 5.23"},{"location":"problems-ch5/#problem-526","text":"(a) By whatever means you can think of (short of looking it up), find the vector potential a distance s from an infinite straight wire carrying a current I . Check that \\div \\vec{A} = 0 and \\curl \\vec{A} = \\vec{B} . (b) Find the magnetic potential inside the wire, if it has radius R and the current is uniformly distributed. (a) As we said, because the current distribution is infinite, we cannot use Eq. 5.65 to get A . So let's use some symmetry. A must be parallel (or antiparallel) to I , and is a function of only s (the distance from the wire). In cylindrical coordinates, then, \\vec{A} = A(s) \\vu{z} . We already calculated the magnetic field of an infinite straight wire via Biot-Savart: \\vec{B} = \\frac{\\mu_0 I}{2 \\pi s} \\vu{\\phi} We can work backwards to get A from B in this case. \\vec{B} = \\curl \\vec{A} = - \\pdv{A}{s} \\vu{\\phi} = \\frac{\\mu_0 I}{2 \\pi s} \\vu{\\phi} Therefore \\pdv{A}{s} = -\\frac{\\mu_0 I}{2 \\pi s} \\quad \\rightarrow \\quad \\vec{A}(r) = - \\frac{\\mu_0 I}{2 \\pi} \\ln (s / a) \\vu{z} There is an arbitrary constant a here which doesn't actually affect our gauge at all: \\div \\vec{A} = \\pdv{A_z}{z} = 0 \\curl \\vec{A} = - \\pdv{A_z}{s} \\vu{\\phi} = \\frac{\\mu_0 I}{2 \\pi s} \\vu{\\phi} = \\vec{B} (b) Ampere's law in this case says \\oint \\vec{B} \\cdot \\dd \\vec{l} = B 2 \\pi s = \\mu_0 I_{enc} = \\mu_0 J \\pi s^2 = \\mu_0 \\frac{I}{\\pi R^2} \\pi s^2 = \\frac{\\mu_0 I s^2}{R^2} so, inside the wire, \\vec{B} = \\frac{\\mu_0 I s}{2\\pi R^2} \\vu{\\phi} From the definition of A , \\pdv{A}{s} = - \\frac{\\mu_0 I}{2 \\pi} \\frac{s}{R^2} \\rightarrow \\vec{A} = -\\frac{\\mu_0 I}{2 \\pi R^2} \\int_{b} ^s s \\, \\dd s = - \\frac{\\mu_0 I}{4 \\pi R^2} (s^2 - b^2) \\vu{z} Here, again, b is arbitrary, except that A must be continuous at R (we know that A is continuous!) - \\frac{\\mu_0 I}{2 \\pi } \\ln (R / a) = - \\frac{\\mu_0 I}{4 \\pi R^2} (R^2 - b^2) which means that we have to pick a and b such that 2 \\ln (R / b) = 1 - (b / R)^2 One such combination of a and b is a = b = R . Then \\vec{A} = \\begin{cases} - \\frac{\\mu_0 I}{4 \\pi R^2} (s^2 - R^2) \\vu{z} & \\quad \\text{ for } s \\leq R \\\\ - \\frac{\\mu_0 I}{2 \\pi} \\ln(s / R) \\vu{z}& \\quad \\text{ for } s \\geq R \\end{cases}","title":"Problem 5.26"},{"location":"problems-ch5/#problem-537","text":"(a) A phonograph record of radius R, carrying a uniform surface charge sigma is rotating at constant angular velocity \\omega . Find its magnetic dipole moment. (b) Find the magnetic dipole moment of the spinning spherical shell in Example 5.11. Show that for points r > R the potential is that of a perfect dipole. (a) We get the monopole moment by integrating over the disk of the record. For a ring at radius r , m = I \\pi r^2 . In this case, I \\rightarrow \\sigma v \\dd r = \\sigma \\omega r \\dd r so m = \\int _0 ^R \\pi r^2 \\sigma \\omega r \\dd \\r = \\pi \\sigma \\omega R^4 / 4 (b) To get the magnetic dipole moment of our sphere, we need to integrate over the surface of the sphere: The total charge on the shaded ring is \\dd q = \\sigma (2 \\pi R \\sin \\theta) R \\dd \\theta . The time to make one revolution is \\dd t = 2 \\pi \\omega , so the current in the ring is I = \\frac{dq}{dt} = \\sigma \\omega R^2 \\sin \\theta \\dd \\theta The area of the ring is \\pi (R \\sin \\theta)^2 , so the magnetic moment of the ring is \\dd m = (\\sigma \\omega R^2 \\sin \\theta \\dd \\theta) \\pi R^2 \\sin ^2 \\theta and the total dipole moment is m = \\sigma \\omega \\pi R^4 \\int_0 ^\\pi \\sin ^3 \\theta \\dd \\theta = (4 / 3) \\sigma \\omega \\pi R^4 and we know that m points in the \\vu{z} direction (right-hand-rule), so \\vec{m} = \\frac{4 \\pi}{3} \\sigma \\omega R^4 \\vu{z} The dipole term in the multipole expansion for A is therefore \\vec{A}_{dip} = \\frac{\\mu_0}{4 \\pi} \\frac{4 \\pi}{3} \\sigma \\omega R^4 \\frac{\\sin \\theta}{r^2} \\vu{\\phi} = \\frac{\\mu_0 \\sigma \\omega R^4}{3} \\frac{\\sin \\theta}{r^2} \\vu{\\phi} This is actually the exact vector potential we calculated (Eq. 5.69); evidently a spinning sphere produces a perfect dipole field, with no higher multipole contributions.","title":"Problem 5.37"},{"location":"problems-ch7/","text":"Problem 7.7 A metal bar of mass m slides frictionlessly on two parallel conducting rails a distance l apart. A resistor R is connected across the rails, and a uniform magnetic field B pointing into the page fills the region. (a) If the bar moves to the right at speed v , what is the current in the resistor? (b) What is the magnetic force on the bar? In what direction? (c) If the bar starts out with speed v_0 at time t = 0 , and is left to slide, what is its speed at a later time t? (d) The initial kinetic energy of the bar was, of course, 1/2 m v_0 ^2 . Check that the energy delivered to the resistor is exactly 1/2 m v_0 ^2 (a) To get the current through the resistor, calculate the flux through the loop: \\Phi_B = B l x \\\\ emf = - \\pdv{\\Phi}{t} = - Blv We can always use good old Ohm's law I = V / R = -Blv / R The induced magnetic flux opposes the change in flux, so the current flows down through the resistor. (b) Force on the bar? Just use Lorentz force law F = I \\int \\dd \\vec{l} \\cross \\vec{B} = - \\frac{B^2 l^2 v}{R} The direction opposes x (force is to the left). (c) v(t = 0) = v_0 \\\\ F = m \\dv{v}{t} = - \\frac{B^2 l^2}{R} v \\\\ \\frac{\\dd v}{v} = - \\frac{B^2 l^2 }{m R } \\dd t \\\\ \\int_{v_0} ^v \\frac{\\dd v}{v} = - \\frac{B^2 l^2 }{m R } \\int_0 ^t \\dd t \\\\ \\ln \\frac{v}{v_0} = - \\frac{B^2 l^2 }{m R } t \\\\ v = v_0 e^{-\\frac{B^2 l^2 }{m R } t} (d) Power dissipated in a resistor is P = I^2 R So energy delivered is \\int_0 ^\\infty I^2 R \\dd t = \\frac{B^2 l^2 v^2}{R^2} R \\dd t \\\\ = \\int_0 ^\\infty \\frac{B^2 l^2}{R} v_0 ^2 e^{-2\\frac{B^2 l^2 }{m R } t} \\dd t \\\\ = - \\frac{B^2 l^2}{R} v_0 ^2 \\frac{m R}{2 B^2 l^2} [ 0 - 1 ] \\\\ = \\frac{1}{2} m v_0 ^2 Hooray! Problem 7.34 A fat wire, radius a , carries constant current I , uniformly distributed over its cross section. A narrow gap in the wire of width w << a forms a parallel-plate capacitor, as shown. Find the magnetic field in the gap, at a distance s < a from the axis. Within the wire, you can draw an Amperian loop to find B within the wire with \\int B \\cdot \\dd l = \\mu_0 I_{enc} Within the gap, we need the Ampere's correction term \\curl \\vec{B} = \\mu_0 \\vec{J} + \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} \\\\ \\oint B \\cdot \\dd l = \\mu_0 \\epsilon_0 \\int \\pdv{\\vec{E}}{t} \\cdot \\dd l Current is flowing to the end of the wire with nowhere to go, so it must build up there. We've got a parallel plate capacitor within the gap B(s) \\cdot 2 \\pi s = \\mu_0 \\epsilon_0 \\dv{}{t} \\int \\frac{\\sigma (t) }{\\epsilon_0} \\dd a B(s) = \\frac{\\mu_0}{2} s \\pdv{\\sigma}{t} = \\frac{\\mu_0 s}{2} \\frac{I}{\\pi a^2} where \\pdv{\\sigma}{t} = \\dv{A}{t} \\frac{1}{\\pi a^2} B(s) = \\frac{\\mu_0 I}{2 \\pi a^2} s \\hat{\\phi}","title":"Problems"},{"location":"problems-ch7/#problem-77","text":"A metal bar of mass m slides frictionlessly on two parallel conducting rails a distance l apart. A resistor R is connected across the rails, and a uniform magnetic field B pointing into the page fills the region. (a) If the bar moves to the right at speed v , what is the current in the resistor? (b) What is the magnetic force on the bar? In what direction? (c) If the bar starts out with speed v_0 at time t = 0 , and is left to slide, what is its speed at a later time t? (d) The initial kinetic energy of the bar was, of course, 1/2 m v_0 ^2 . Check that the energy delivered to the resistor is exactly 1/2 m v_0 ^2 (a) To get the current through the resistor, calculate the flux through the loop: \\Phi_B = B l x \\\\ emf = - \\pdv{\\Phi}{t} = - Blv We can always use good old Ohm's law I = V / R = -Blv / R The induced magnetic flux opposes the change in flux, so the current flows down through the resistor. (b) Force on the bar? Just use Lorentz force law F = I \\int \\dd \\vec{l} \\cross \\vec{B} = - \\frac{B^2 l^2 v}{R} The direction opposes x (force is to the left). (c) v(t = 0) = v_0 \\\\ F = m \\dv{v}{t} = - \\frac{B^2 l^2}{R} v \\\\ \\frac{\\dd v}{v} = - \\frac{B^2 l^2 }{m R } \\dd t \\\\ \\int_{v_0} ^v \\frac{\\dd v}{v} = - \\frac{B^2 l^2 }{m R } \\int_0 ^t \\dd t \\\\ \\ln \\frac{v}{v_0} = - \\frac{B^2 l^2 }{m R } t \\\\ v = v_0 e^{-\\frac{B^2 l^2 }{m R } t} (d) Power dissipated in a resistor is P = I^2 R So energy delivered is \\int_0 ^\\infty I^2 R \\dd t = \\frac{B^2 l^2 v^2}{R^2} R \\dd t \\\\ = \\int_0 ^\\infty \\frac{B^2 l^2}{R} v_0 ^2 e^{-2\\frac{B^2 l^2 }{m R } t} \\dd t \\\\ = - \\frac{B^2 l^2}{R} v_0 ^2 \\frac{m R}{2 B^2 l^2} [ 0 - 1 ] \\\\ = \\frac{1}{2} m v_0 ^2 Hooray!","title":"Problem 7.7"},{"location":"problems-ch7/#problem-734","text":"A fat wire, radius a , carries constant current I , uniformly distributed over its cross section. A narrow gap in the wire of width w << a forms a parallel-plate capacitor, as shown. Find the magnetic field in the gap, at a distance s < a from the axis. Within the wire, you can draw an Amperian loop to find B within the wire with \\int B \\cdot \\dd l = \\mu_0 I_{enc} Within the gap, we need the Ampere's correction term \\curl \\vec{B} = \\mu_0 \\vec{J} + \\mu_0 \\epsilon_0 \\pdv{\\vec{E}}{t} \\\\ \\oint B \\cdot \\dd l = \\mu_0 \\epsilon_0 \\int \\pdv{\\vec{E}}{t} \\cdot \\dd l Current is flowing to the end of the wire with nowhere to go, so it must build up there. We've got a parallel plate capacitor within the gap B(s) \\cdot 2 \\pi s = \\mu_0 \\epsilon_0 \\dv{}{t} \\int \\frac{\\sigma (t) }{\\epsilon_0} \\dd a B(s) = \\frac{\\mu_0}{2} s \\pdv{\\sigma}{t} = \\frac{\\mu_0 s}{2} \\frac{I}{\\pi a^2} where \\pdv{\\sigma}{t} = \\dv{A}{t} \\frac{1}{\\pi a^2} B(s) = \\frac{\\mu_0 I}{2 \\pi a^2} s \\hat{\\phi}","title":"Problem 7.34"}]}