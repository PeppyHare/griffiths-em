{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Griffiths - Introduction to Electrodynamics https://peppyhare.github.io/griffiths-em/ This is basically just a web-friendly version of David Griffiths' Introduction to Electrodynamics, 4th Ed. . These are my class notes for the University of Washington's PHYS 543. This is mostly an exercise for myself in learning about Mkdocs, MathJax, and physics! Click around in the sidebar to find a chapter to read, or follow the links at the bottom of the page to read in order! Don't forget to try out the interface on mobile, it's very slick ;) Internals All content is written in Markdown and rendered to a static site using MkDocs . The theme for the site is Material for MkDocs . I use python-markdown-math to turn any LaTeX in my source into full-blown MathJax to be rendered in the browser (and in a mobile-friendly format!).","title":"Home"},{"location":"#griffiths-introduction-to-electrodynamics","text":"","title":"Griffiths - Introduction to Electrodynamics"},{"location":"#httpspeppyharegithubiogriffiths-em","text":"This is basically just a web-friendly version of David Griffiths' Introduction to Electrodynamics, 4th Ed. . These are my class notes for the University of Washington's PHYS 543. This is mostly an exercise for myself in learning about Mkdocs, MathJax, and physics! Click around in the sidebar to find a chapter to read, or follow the links at the bottom of the page to read in order! Don't forget to try out the interface on mobile, it's very slick ;)","title":"https://peppyhare.github.io/griffiths-em/"},{"location":"#internals","text":"All content is written in Markdown and rendered to a static site using MkDocs . The theme for the site is Material for MkDocs . I use python-markdown-math to turn any LaTeX in my source into full-blown MathJax to be rendered in the browser (and in a mobile-friendly format!).","title":"Internals"},{"location":"ch1-1/","text":"1.1 Vector Algebra 1.1.1 Vector Operations If you walk 4 miles due north and then 3 miles due east (Fig. 1.1), you will have gone a total of 7 miles, but you're not 7 miles from where you set out-you're only 5. We need an arithmetic to describe quantities like this, which evidently do not add in the ordinary way. The reason they don't, of course, is that displacements (straight line segments going from one point to another) have direction as well as magnitude (length), and it is essential to take both into account when you combine them. Such objects are called vectors: velocity, acceleration, force and momentum are other examples. By contrast, quantities that have magnitude but no direction are called scalars: examples include mass, charge, density, and temperature. I shall use boldface ( \\vec{A} , \\vec{B} , and so on) for vectors and ordinary type for scalars. The magnitude of a vector \\vec{A} is written |\\vec{A}| or, more simply, A . In diagrams, vectors are denoted by arrows: the length of the arrow is proportional to the magnitude of the vector, and the arrowhead indicates its direction. Minus \\vec{A} ( - \\vec{A} ) is a vector with the same magnitude as A but of opposite direction (Fig. 1.2). Note that vectors have magnitude and direction but not location: a displacement of 4 miles due north from Washington is represented by the same vector as a displacement 4 miles north from Baltimore (neglecting, of course, the curvature of the earth). On a diagram, therefore, you can slide the arrow around at will, as long as you don't change its length or direction. We define four vector operations: addition and three kinds of multiplication. (i) Addition of two vectors. . Place the tail of \\vec{B} at the head of \\vec{A} ; the sum, \\vec{A} + \\vec{B} , is the vector from the tail of \\vec{A} to the head of \\vec{B} (Fig 1.3). This rule generalizes the obvious procedure for combining two displacements. Addition is commutative : \\vec{A} + \\vec{B} = \\vec{B} + \\vec{A} 3 miles east followed by 4 miles north gets you to the same place as 4 miles north followed by 3 miles east. Addition is also associative: (\\vec{A} + \\vec{B}) + \\vec{C} = \\vec{A} + (\\vec{B} + \\vec{C}) To subtract a vector, add its opposite (Fig. 1.4): \\vec{A} - \\vec{B} = \\vec{A} + (- \\vec{B}) (ii) Multiplication by a scalar. Multiplication of a vector by a positive scalar a multiplies the magnitude but leaves the direction unchanged (Fig. 1.5). (If a is negative, the direction is reversed.) Scalar multiplication is distributive: a(\\vec{A} + \\vec{B}) = a \\vec{A} + a \\vec{B} (iii) Dot product of two vectors. The dot product of two vectors is defined by \\vec{A} \\cdot \\vec{B} = A B \\cos \\theta \\tag{1.1} where \\theta is the angle they form when placed tail-to-tail (Fig. 1.6). Note that \\vec{A} \\cdot \\vec{B} is itself a scalar (hence the alternative name scalar product ). The dot product is commutative, \\vec{A} \\cdot \\vec{B} = \\vec{B} \\cdot \\vec{A} and distributive \\vec{A} \\cdot (\\vec{B} + \\vec{C}) = \\vec{A} \\cdot \\vec{B} + \\vec{A} \\cdot \\vec{C} \\tag{1.2} Geometrically, \\vec{A} \\cdot \\vec{B} is the product of A times the projection of B along A (or the product of B times the projection of A along B). If the two vectors are parallel, then \\vec{A} \\cdot \\vec{B} = AB . In particular, for any vector A \\vec{A} \\cdot \\vec{A} = A^2 \\tag{1.3} If A and B are perpendicular, then \\vec{A} \\cdot \\vec{B} = 0 Example 1.1 Let \\vec{C} = \\vec{A} - \\vec{B} (Fig 1.7), and calculate the dot product of \\vec{C} with itself. Solution \\vec{C} \\cdot \\vec{C} = ( \\vec{A} - \\vec{B} ) \\cdot (\\vec{A} - \\vec{B}) = \\vec{A} \\cdot \\vec{A} - \\vec{A} \\cdot \\vec{B} - \\vec{B} \\cdot \\vec{A} + \\vec{B} \\cdot \\vec{B} or C^2 = A^2 + B^2 - 2AB\\cos \\theta This is the law of cosines. (iv) Cross product of two vectors. The cross product of two vectors is defined by \\vec{A} \\cross \\vec{B} = AB \\sin \\theta \\hat{n} \\tag{1.4} where \\hat{n} is a unit vector (vector of magnitude 1) pointing perpendicular to the plane of A and B. (I shall use a hat \\hat{} to denote unit vectors.) Of course, there are two directions perpendicular to any plane: \"in\" and \"out.\" The ambiguity is resolved by the right-hand rule: let your fingers point in the direction of the first vector and curl around (via the smaller angle) toward the second; then your thumb indicates the direction of \\hat{n} . (In Fig. 1.8, \\vec{A} \\cross \\vec{B} points into the page; \\vec{B} \\cross \\vec{A} points out of the page.) Note that \\vec{A} \\cross \\vec{B} is itself a vector (hence the alternative name vector product). The cross product is distributive \\vec{A} \\cross ( \\vec{B} + \\vec{C}) = ( \\vec{A} \\cross \\vec{B}) + (\\vec{A} \\cross \\vec{C}) but not commutative . In fact, (\\vec{B} \\cross \\vec{A}) = - (\\vec{A} \\cross \\vec{B}) Geometrically, | \\vec{A} \\cross \\vec{B} | is the area of the parallelogram generated by \\vec{A} and \\vec{B} (Fig 1.8). If two vectors are parallel, their cross product is zero. In particular, \\vec{A} \\cross \\vec{A} = 0 for any vector A. 1.1.2: Vector Algebra: Component Form In the previous section, I defined the four vector operations (addition, scalar multiplication, dot product, and cross product) in \"abstract\" form-that is, without reference to any particular coordinate system. In practice, it is often easier to set up Cartesian coordinates x, y, z and work with vector components. Let \\hat{x} , \\hat{y} , and \\hat{z} be unit vectors parallel to the x, y, and z axes, respectively (Fig. 1.9(a)). An arbitrary vector A can be expanded in terms ofthese basis vectors (Fig. 1.9(b)): \\vec{A} = A_x \\hat{x} + A_y \\hat{y} + A_z \\hat{z} The numbers A_x , A_y , and A_z are the \"components\" of A; geometrically, they are the projections of A along the three coordinate axes ( A_x = \\vec{A} \\cdot \\hat{x}, A_y = \\vec{A} \\cdot \\hat{y}, A_z = \\vec{A} \\cdot \\hat{z} ). We can now reformulate each of the four vector operations as a rule for manipulating components: \\vec{A} + \\vec{B} = (A_x \\hat{x} + A_y \\hat{y} + A_z \\hat{z}) + (B_x \\hat{x} + B_y \\hat{y} + B_z \\hat{z}) \\\\ = (A_x + B_x) \\hat{x} + (A_y + B_y) \\hat{y} + (A_z + B_z) \\hat{z} \\tag{1.7} Rule (i): To add vectors, add like components. a\\vec{A} = (a A_x) \\hat{x} + (a A_y) \\hat{y} + (a A_z)\\hat{z} \\tag{1.8} Rule (ii): To multiply by a scalar, multiply each component. Because \\hat{x}, \\hat{y} , and \\hat{z} are mutually perpendicular unit vectors \\hat{x} \\cdot \\hat{x} = \\hat{y} \\cdot \\hat{y} = \\hat{z} \\cdot \\hat{z} = 1; \\qquad \\hat{x} \\cdot \\hat{y} = \\hat{x} \\cdot \\hat{z} = \\hat{y} \\cdot \\hat{z} = 0 \\tag{1.9} Accordingly, \\vec{A} \\cdot \\vec{B} = (A_x \\hat{x} + A_y \\hat{y} + A_z \\hat{z}) \\cdot (B_x \\hat{x} + B_y \\hat{y} + B_z \\hat{z}) \\\\ = A_x B_x + A_y B_y + A_z B_z \\tag{1.10} Rule (iii): To calculate the dot product, multiply like components and add. In particular, \\vec{A} \\cdot \\vec{A} = A_x ^2 + A_y ^2 + A_z ^2 so A = \\sqrt{A_x ^2 + A_y ^2 + A_z ^2} \\tag{1.11} Similarly, \\begin{align} \\hat{x} \\cross \\hat{x} & = & \\hat{y} \\cross \\hat{y} & = & \\hat{z} \\cross \\hat{z} = 0 \\\\ \\hat{x} \\cross \\hat{y} & = & - \\hat{y} \\cross \\hat{x} & = & \\hat{z} \\\\ \\hat{y} \\cross \\hat{z} & = & - \\hat{z} \\cross \\hat{y} & = & \\hat{x} \\\\ \\hat{z} \\cross \\hat{x} & = & - \\hat{x} \\cross \\hat{z} & = & \\hat{y} \\end{align} Therefore, \\vec{A} \\cross \\vec{B} = (A_x \\hat{x} + A_y \\hat{y} + A_z \\hat{z}) \\cross (B_x \\hat{x} + B_y \\hat{y} + B_z \\hat{z}) \\\\ = (A_y B_z - A_z B_y) \\hat{x} + (A_z B_x - A_x B_z)\\hat{y} + (A_x B_y - A_y B_x) \\hat{z} \\tag{1.13} This cumbersome expression can be written more neatly as a determinant: \\vec{A} \\cross \\vec{B} = \\begin{vmatrix} \\hat{x} & \\hat{y} & \\hat{z} \\\\ A_x & A_y & A_z \\\\ B_x & B_y & B_z \\end{vmatrix} Rule (iv): To calculate the cross product, form the determinant whose first row is \\hat{x}, \\hat{y}, \\hat{z} , whose second row is A, and whose third row is B.","title":"1.1 - Vector Algebra"},{"location":"ch1-1/#11-vector-algebra","text":"","title":"1.1 Vector Algebra"},{"location":"ch1-1/#111-vector-operations","text":"If you walk 4 miles due north and then 3 miles due east (Fig. 1.1), you will have gone a total of 7 miles, but you're not 7 miles from where you set out-you're only 5. We need an arithmetic to describe quantities like this, which evidently do not add in the ordinary way. The reason they don't, of course, is that displacements (straight line segments going from one point to another) have direction as well as magnitude (length), and it is essential to take both into account when you combine them. Such objects are called vectors: velocity, acceleration, force and momentum are other examples. By contrast, quantities that have magnitude but no direction are called scalars: examples include mass, charge, density, and temperature. I shall use boldface ( \\vec{A} , \\vec{B} , and so on) for vectors and ordinary type for scalars. The magnitude of a vector \\vec{A} is written |\\vec{A}| or, more simply, A . In diagrams, vectors are denoted by arrows: the length of the arrow is proportional to the magnitude of the vector, and the arrowhead indicates its direction. Minus \\vec{A} ( - \\vec{A} ) is a vector with the same magnitude as A but of opposite direction (Fig. 1.2). Note that vectors have magnitude and direction but not location: a displacement of 4 miles due north from Washington is represented by the same vector as a displacement 4 miles north from Baltimore (neglecting, of course, the curvature of the earth). On a diagram, therefore, you can slide the arrow around at will, as long as you don't change its length or direction. We define four vector operations: addition and three kinds of multiplication. (i) Addition of two vectors. . Place the tail of \\vec{B} at the head of \\vec{A} ; the sum, \\vec{A} + \\vec{B} , is the vector from the tail of \\vec{A} to the head of \\vec{B} (Fig 1.3). This rule generalizes the obvious procedure for combining two displacements. Addition is commutative : \\vec{A} + \\vec{B} = \\vec{B} + \\vec{A} 3 miles east followed by 4 miles north gets you to the same place as 4 miles north followed by 3 miles east. Addition is also associative: (\\vec{A} + \\vec{B}) + \\vec{C} = \\vec{A} + (\\vec{B} + \\vec{C}) To subtract a vector, add its opposite (Fig. 1.4): \\vec{A} - \\vec{B} = \\vec{A} + (- \\vec{B}) (ii) Multiplication by a scalar. Multiplication of a vector by a positive scalar a multiplies the magnitude but leaves the direction unchanged (Fig. 1.5). (If a is negative, the direction is reversed.) Scalar multiplication is distributive: a(\\vec{A} + \\vec{B}) = a \\vec{A} + a \\vec{B} (iii) Dot product of two vectors. The dot product of two vectors is defined by \\vec{A} \\cdot \\vec{B} = A B \\cos \\theta \\tag{1.1} where \\theta is the angle they form when placed tail-to-tail (Fig. 1.6). Note that \\vec{A} \\cdot \\vec{B} is itself a scalar (hence the alternative name scalar product ). The dot product is commutative, \\vec{A} \\cdot \\vec{B} = \\vec{B} \\cdot \\vec{A} and distributive \\vec{A} \\cdot (\\vec{B} + \\vec{C}) = \\vec{A} \\cdot \\vec{B} + \\vec{A} \\cdot \\vec{C} \\tag{1.2} Geometrically, \\vec{A} \\cdot \\vec{B} is the product of A times the projection of B along A (or the product of B times the projection of A along B). If the two vectors are parallel, then \\vec{A} \\cdot \\vec{B} = AB . In particular, for any vector A \\vec{A} \\cdot \\vec{A} = A^2 \\tag{1.3} If A and B are perpendicular, then \\vec{A} \\cdot \\vec{B} = 0","title":"1.1.1 Vector Operations"},{"location":"ch1-1/#example-11","text":"Let \\vec{C} = \\vec{A} - \\vec{B} (Fig 1.7), and calculate the dot product of \\vec{C} with itself. Solution \\vec{C} \\cdot \\vec{C} = ( \\vec{A} - \\vec{B} ) \\cdot (\\vec{A} - \\vec{B}) = \\vec{A} \\cdot \\vec{A} - \\vec{A} \\cdot \\vec{B} - \\vec{B} \\cdot \\vec{A} + \\vec{B} \\cdot \\vec{B} or C^2 = A^2 + B^2 - 2AB\\cos \\theta This is the law of cosines. (iv) Cross product of two vectors. The cross product of two vectors is defined by \\vec{A} \\cross \\vec{B} = AB \\sin \\theta \\hat{n} \\tag{1.4} where \\hat{n} is a unit vector (vector of magnitude 1) pointing perpendicular to the plane of A and B. (I shall use a hat \\hat{} to denote unit vectors.) Of course, there are two directions perpendicular to any plane: \"in\" and \"out.\" The ambiguity is resolved by the right-hand rule: let your fingers point in the direction of the first vector and curl around (via the smaller angle) toward the second; then your thumb indicates the direction of \\hat{n} . (In Fig. 1.8, \\vec{A} \\cross \\vec{B} points into the page; \\vec{B} \\cross \\vec{A} points out of the page.) Note that \\vec{A} \\cross \\vec{B} is itself a vector (hence the alternative name vector product). The cross product is distributive \\vec{A} \\cross ( \\vec{B} + \\vec{C}) = ( \\vec{A} \\cross \\vec{B}) + (\\vec{A} \\cross \\vec{C}) but not commutative . In fact, (\\vec{B} \\cross \\vec{A}) = - (\\vec{A} \\cross \\vec{B}) Geometrically, | \\vec{A} \\cross \\vec{B} | is the area of the parallelogram generated by \\vec{A} and \\vec{B} (Fig 1.8). If two vectors are parallel, their cross product is zero. In particular, \\vec{A} \\cross \\vec{A} = 0 for any vector A.","title":"Example 1.1"},{"location":"ch1-1/#112-vector-algebra-component-form","text":"In the previous section, I defined the four vector operations (addition, scalar multiplication, dot product, and cross product) in \"abstract\" form-that is, without reference to any particular coordinate system. In practice, it is often easier to set up Cartesian coordinates x, y, z and work with vector components. Let \\hat{x} , \\hat{y} , and \\hat{z} be unit vectors parallel to the x, y, and z axes, respectively (Fig. 1.9(a)). An arbitrary vector A can be expanded in terms ofthese basis vectors (Fig. 1.9(b)): \\vec{A} = A_x \\hat{x} + A_y \\hat{y} + A_z \\hat{z} The numbers A_x , A_y , and A_z are the \"components\" of A; geometrically, they are the projections of A along the three coordinate axes ( A_x = \\vec{A} \\cdot \\hat{x}, A_y = \\vec{A} \\cdot \\hat{y}, A_z = \\vec{A} \\cdot \\hat{z} ). We can now reformulate each of the four vector operations as a rule for manipulating components: \\vec{A} + \\vec{B} = (A_x \\hat{x} + A_y \\hat{y} + A_z \\hat{z}) + (B_x \\hat{x} + B_y \\hat{y} + B_z \\hat{z}) \\\\ = (A_x + B_x) \\hat{x} + (A_y + B_y) \\hat{y} + (A_z + B_z) \\hat{z} \\tag{1.7} Rule (i): To add vectors, add like components. a\\vec{A} = (a A_x) \\hat{x} + (a A_y) \\hat{y} + (a A_z)\\hat{z} \\tag{1.8} Rule (ii): To multiply by a scalar, multiply each component. Because \\hat{x}, \\hat{y} , and \\hat{z} are mutually perpendicular unit vectors \\hat{x} \\cdot \\hat{x} = \\hat{y} \\cdot \\hat{y} = \\hat{z} \\cdot \\hat{z} = 1; \\qquad \\hat{x} \\cdot \\hat{y} = \\hat{x} \\cdot \\hat{z} = \\hat{y} \\cdot \\hat{z} = 0 \\tag{1.9} Accordingly, \\vec{A} \\cdot \\vec{B} = (A_x \\hat{x} + A_y \\hat{y} + A_z \\hat{z}) \\cdot (B_x \\hat{x} + B_y \\hat{y} + B_z \\hat{z}) \\\\ = A_x B_x + A_y B_y + A_z B_z \\tag{1.10} Rule (iii): To calculate the dot product, multiply like components and add. In particular, \\vec{A} \\cdot \\vec{A} = A_x ^2 + A_y ^2 + A_z ^2 so A = \\sqrt{A_x ^2 + A_y ^2 + A_z ^2} \\tag{1.11} Similarly, \\begin{align} \\hat{x} \\cross \\hat{x} & = & \\hat{y} \\cross \\hat{y} & = & \\hat{z} \\cross \\hat{z} = 0 \\\\ \\hat{x} \\cross \\hat{y} & = & - \\hat{y} \\cross \\hat{x} & = & \\hat{z} \\\\ \\hat{y} \\cross \\hat{z} & = & - \\hat{z} \\cross \\hat{y} & = & \\hat{x} \\\\ \\hat{z} \\cross \\hat{x} & = & - \\hat{x} \\cross \\hat{z} & = & \\hat{y} \\end{align} Therefore, \\vec{A} \\cross \\vec{B} = (A_x \\hat{x} + A_y \\hat{y} + A_z \\hat{z}) \\cross (B_x \\hat{x} + B_y \\hat{y} + B_z \\hat{z}) \\\\ = (A_y B_z - A_z B_y) \\hat{x} + (A_z B_x - A_x B_z)\\hat{y} + (A_x B_y - A_y B_x) \\hat{z} \\tag{1.13} This cumbersome expression can be written more neatly as a determinant: \\vec{A} \\cross \\vec{B} = \\begin{vmatrix} \\hat{x} & \\hat{y} & \\hat{z} \\\\ A_x & A_y & A_z \\\\ B_x & B_y & B_z \\end{vmatrix} Rule (iv): To calculate the cross product, form the determinant whose first row is \\hat{x}, \\hat{y}, \\hat{z} , whose second row is A, and whose third row is B.","title":"1.1.2: Vector Algebra: Component Form"},{"location":"ch2-1/","text":"2.1: The Electric Field 2.1.1: Introduction The fundamental problem electrodynamics hopes to solve is this (Fig 2.1): We have some electric charges q_1, q_2, q_3, \\ldots (call them source charges ); what force do they exert on another charge, Q (call it the test charge )? The positions of the source charges are given (as functions of time); the trajectory of the test particle is to be calculated. In general, both the source charges and the test charge are in motion. The solution to this problem is facilitated by the principle of superposition, which states that the interaction between any two charges is completely unaffected by the presence of others. This means that to determine the force on Q, we can first compute the force \\vec{F_1} , due to q_1 alone (ignoring all the others); then we compute the force \\vec{F_2} , due to q_2 alone, and so in. Finally, we take the vector sum of all these individual forces: \\vec{F} = \\vec{F_1} + \\vec{F_2} + \\vec{F_3} + \\ldots Thus, if we can find the force on Q due to a single source charge q , we are, in principle, done (the rest is just a question of repeating the same operation over and over, and adding it all up) The principle of superposition may seem \"obvious\" to you, but it did not have to be so simple: if the electromagnetic force were proportional to the square of the total source charge, for instance, the principle of superposition would not hold, since (q_1 + q_2)^2 \\neq q_1 ^2 + q_2 ^2 (there would be \"cross terms\" to consider). Superposition is not a logical necessity, but an experimental fact. Well, at first sight this looks very easy: Why don't I just write down the formula for the force on Q due to q, and be done with it? I could , and in Chapter 10 I shall, but you would be shocked to see it at this stage, for not only does the force on Q depend on the separation distance \\gr between the charges (Fig 2.2), it also depends on both their velocities and on the acceleration of q . Moreover, it is not the position, velocity, and acceleration of q right now that matter: electromagnetic \"news\" travels at the speed of light, so what concerns Q is the position, velocity, and acceleration q had at some earlier time, when the message left. Therefore, in spite of the fact that the basic question (\"What is the force on Q due to q?\") is easy to state, it does not pay to confront it head on; rather, we shall go at it by stages. In the meantime, the theory we develop will allow for the solution of more subtle electromagnetic problems that do not present themselves in quite this simple format. To begin with, we shall consider the special case of electrostatics in which all the source charges are stationary (though the test charge may be moving). 2.1.2: Coulomb's Law What is the force on a test charge Q due to a single point charge q, that is at rest a distance \\gr away? The answer (based on experiments) is given by Coulomb's Law : \\vec{F} = \\frac{1}{4 \\pi \\epsilon_0}\\frac{q Q}{\\gr^2} \\hat{\\vec{\\gr}} \\label{2.1} \\tag{2.1} The constant \\epsilon_0 is called (ludicrously) the permittivity of free space . In SI units, where force is in newtons (N), distance in meters (m), and charge in coulombs (C), \\epsilon_0 = 8.85 \\times 10^{-12} \\frac{C^2}{N \\cdot m ^2} In words, the force is proportional to the product of the charges and inversely proportional to the square of the separation distance. As always (Sect 1.1.4), \\vec{\\gr} is the separation vector from \\vec{r'} (the location of q) to \\vec{r} (the location of Q): \\vec{\\gr} = \\vec{r} - \\vec{r}' \\gr is its magnitude, and \\hat{\\gr} is its direction. The force points along the line from q to Q; it is repulsive if q and Q have the same sign, and attractive if their signs are opposite. Coulomb's law and the principle of superposition constitute the physical input for electrostatics - the rest, except for some special properties of matter, is mathematical elaboration of these fundamental rules. 2.1.3: The Electric Field If we have several point charges q_1, q_2, \\ldots , q_n at distances \\gr_1 \\gr_2 \\ldots, \\gr_n from Q , the total force on Q is evidently \\begin{align} \\vec{F} & = & \\vec{F_1} + \\vec{F_2} + \\ldots \\\\ & = & \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q_1 Q}{\\gr_1 ^2} \\hat{\\gr}_1 + \\frac{q_2 Q}{\\gr_2 ^2} \\hat{\\gr}_2 + \\ldots \\right) \\\\ & = & \\frac{Q}{4 \\pi \\epsilon _0} \\left( \\frac{q_1}{\\gr ^2 _1} \\hat{\\gr_1} + \\frac{q_2}{\\gr _2 ^2}\\hat{\\gr_2} + \\ldots \\right) \\end{align} or \\vec{F} = Q \\vec{E} \\label{2.3} \\tag{2.3} where \\vec{E}(\\vec{r}) \\equiv \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i = 1}^n \\frac{q_i}{\\gr_{i}^2} \\hat{\\gr_i} \\label{2.4} \\tag{2.4} E is called the electric field of the source charges. Notice that it is a function of position ( r ), because the separation vectors \\gr_i depend on the location of the field point P (Fig 2.3). But it makes no reference to the test charge Q. The electric field is a vector quantity that varies from point to point and is determined by the configuration of source charges; physically, \\vec{E}(\\vec{r}) is the force per unit charge that would be exerted on a test charge, if you were to place one at P. What exactly is an electric field? I have deliberately begun with what you might call the \"minimal\" interpretation of E , as an intermediate step in the calculation of electric forces. But I encourage you to think of the field as a \"real\" physical entity, filling the space around electric charges. Maxwell himself came to believe that electric and magnetic fields are stresses and strains in an invisible primordial jellylike \"ether.\" Special relativity has forced us to abandon the notion of either, and with it Maxwell's mechanical interpretation of electromagnetic fields. (It is even possible, although cumbersome, to formulate classical electrodynamics as an \"action-at-a-distance\" theory, and dispense with the field concept altogether.) I can't tell you, then, what a field is -- only how to calculate it and what it can do for you once you've got it. Example 2.1 Find the electric field a distance z above the midpoint between two equal charges (q), a distance d apart (Fig. 2.4a) Solution Let \\vec{E_1} be the field of the left charge alone, and \\vec{E_2} that of the right charge alone (Fig. 2.4b). Adding them (vectorially), the horizontal components cancel and the vertical components conspire E_z = 2 \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{\\gr ^2} \\cos \\theta Here \\gr = \\sqrt{z^2 + (d/2)^2} and \\cos \\theta = z / \\gr , so \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{2qz}{\\left[ z^2 + (d/2)^2 \\right]^{3/2}} \\hat{z} Check : When z \\gg d you're so far away that it just looks like a single charge 2q , so the field should reduce to \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{2q}{z^2} \\hat{z} . And it does, just set d \\rightarrow 0 in the formula). 2.1.4: Continuous Charge Distributions Our definition of the electric field (Eq. \\eqref{2.4} ) assumes that the source of the field is a collection of discrete point charges q_i . If, instead, the charge is distributed continuously over some region, the sum becomes an integral (Fig 2.5a): \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{1}{\\gr ^2} \\hat{\\gr} \\dd{q} If the charge is spread out along a line (Fig. 2.5b), with charge-per-unit-length \\lambda then \\dd{q} = \\lambda \\dd{l}' (where \\dd{l}' ) is an element of length along the line); if the charge is smeared out over a surface (Fig. 2.5c) with charge-per-unit-area \\sigma , then \\dd{q} = \\sigma \\dd{a}' (where \\dd{a'} ) is an element of area on the surface); and if the charge fills a volume (Fig 2.5d), with charge-per-unit-volume \\rho , then \\dd{q} = \\rho\\dd{\\tau'} (where \\dd{\\tau'} is an element of volume): dq \\rightarrow \\lambda \\dd{l'} \\sim \\sigma \\dd{a'} \\sim \\rho \\dd{\\tau'} Thus the electric field of a line charge is \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\lambda(\\vec{r'})}{\\gr ^2} \\hat{\\gr} \\dd{l'} for a surface charge, \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma(\\vec{r'})}{\\gr ^2} \\hat{\\gr} \\dd{a'} and for a volume charge, \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r'})}{\\gr ^2} \\hat{\\gr} \\dd{\\tau'} \\label{2.8} \\tag{2.8} Equation \\eqref{2.8} itself is often referred to as \"Coulomb's law,\" because it is such a short step from the original, and because a volume charge is in a sense the most general and realistic case. Please note carefully the meaning of \\gr in these formulas. Originally, in \\eqref{2.4} , \\gr_i stood for the vector from the source charge q_i to the field point r . Correspondingly, in Eq.s 9-11, \\gr is the vector from \\dd{q} to the field point \\vec{r} . Warning: the unit vector \\hat{\\gr} is not constant: its direction depends on the source point \\vec{r'} , and hence it cannot be taken outside the integrals (9-11). In practice, you must work with Cartesian components ( \\hat{x}, \\hat{y}, \\hat{z} are constant, and do come out) , even if you use curvilinear coordinates to perform the integration. Example 2.2 Find the electric field a distance z above the midpoint of a straight line segment of length 2L that carries a uniform line charge \\lambda (Fig. 2.6). TODO!","title":"2.1 - The Electric Field"},{"location":"ch2-1/#21-the-electric-field","text":"","title":"2.1: The Electric Field"},{"location":"ch2-1/#211-introduction","text":"The fundamental problem electrodynamics hopes to solve is this (Fig 2.1): We have some electric charges q_1, q_2, q_3, \\ldots (call them source charges ); what force do they exert on another charge, Q (call it the test charge )? The positions of the source charges are given (as functions of time); the trajectory of the test particle is to be calculated. In general, both the source charges and the test charge are in motion. The solution to this problem is facilitated by the principle of superposition, which states that the interaction between any two charges is completely unaffected by the presence of others. This means that to determine the force on Q, we can first compute the force \\vec{F_1} , due to q_1 alone (ignoring all the others); then we compute the force \\vec{F_2} , due to q_2 alone, and so in. Finally, we take the vector sum of all these individual forces: \\vec{F} = \\vec{F_1} + \\vec{F_2} + \\vec{F_3} + \\ldots Thus, if we can find the force on Q due to a single source charge q , we are, in principle, done (the rest is just a question of repeating the same operation over and over, and adding it all up) The principle of superposition may seem \"obvious\" to you, but it did not have to be so simple: if the electromagnetic force were proportional to the square of the total source charge, for instance, the principle of superposition would not hold, since (q_1 + q_2)^2 \\neq q_1 ^2 + q_2 ^2 (there would be \"cross terms\" to consider). Superposition is not a logical necessity, but an experimental fact. Well, at first sight this looks very easy: Why don't I just write down the formula for the force on Q due to q, and be done with it? I could , and in Chapter 10 I shall, but you would be shocked to see it at this stage, for not only does the force on Q depend on the separation distance \\gr between the charges (Fig 2.2), it also depends on both their velocities and on the acceleration of q . Moreover, it is not the position, velocity, and acceleration of q right now that matter: electromagnetic \"news\" travels at the speed of light, so what concerns Q is the position, velocity, and acceleration q had at some earlier time, when the message left. Therefore, in spite of the fact that the basic question (\"What is the force on Q due to q?\") is easy to state, it does not pay to confront it head on; rather, we shall go at it by stages. In the meantime, the theory we develop will allow for the solution of more subtle electromagnetic problems that do not present themselves in quite this simple format. To begin with, we shall consider the special case of electrostatics in which all the source charges are stationary (though the test charge may be moving).","title":"2.1.1: Introduction"},{"location":"ch2-1/#212-coulombs-law","text":"What is the force on a test charge Q due to a single point charge q, that is at rest a distance \\gr away? The answer (based on experiments) is given by Coulomb's Law : \\vec{F} = \\frac{1}{4 \\pi \\epsilon_0}\\frac{q Q}{\\gr^2} \\hat{\\vec{\\gr}} \\label{2.1} \\tag{2.1} The constant \\epsilon_0 is called (ludicrously) the permittivity of free space . In SI units, where force is in newtons (N), distance in meters (m), and charge in coulombs (C), \\epsilon_0 = 8.85 \\times 10^{-12} \\frac{C^2}{N \\cdot m ^2} In words, the force is proportional to the product of the charges and inversely proportional to the square of the separation distance. As always (Sect 1.1.4), \\vec{\\gr} is the separation vector from \\vec{r'} (the location of q) to \\vec{r} (the location of Q): \\vec{\\gr} = \\vec{r} - \\vec{r}' \\gr is its magnitude, and \\hat{\\gr} is its direction. The force points along the line from q to Q; it is repulsive if q and Q have the same sign, and attractive if their signs are opposite. Coulomb's law and the principle of superposition constitute the physical input for electrostatics - the rest, except for some special properties of matter, is mathematical elaboration of these fundamental rules.","title":"2.1.2: Coulomb's Law"},{"location":"ch2-1/#213-the-electric-field","text":"If we have several point charges q_1, q_2, \\ldots , q_n at distances \\gr_1 \\gr_2 \\ldots, \\gr_n from Q , the total force on Q is evidently \\begin{align} \\vec{F} & = & \\vec{F_1} + \\vec{F_2} + \\ldots \\\\ & = & \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q_1 Q}{\\gr_1 ^2} \\hat{\\gr}_1 + \\frac{q_2 Q}{\\gr_2 ^2} \\hat{\\gr}_2 + \\ldots \\right) \\\\ & = & \\frac{Q}{4 \\pi \\epsilon _0} \\left( \\frac{q_1}{\\gr ^2 _1} \\hat{\\gr_1} + \\frac{q_2}{\\gr _2 ^2}\\hat{\\gr_2} + \\ldots \\right) \\end{align} or \\vec{F} = Q \\vec{E} \\label{2.3} \\tag{2.3} where \\vec{E}(\\vec{r}) \\equiv \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i = 1}^n \\frac{q_i}{\\gr_{i}^2} \\hat{\\gr_i} \\label{2.4} \\tag{2.4} E is called the electric field of the source charges. Notice that it is a function of position ( r ), because the separation vectors \\gr_i depend on the location of the field point P (Fig 2.3). But it makes no reference to the test charge Q. The electric field is a vector quantity that varies from point to point and is determined by the configuration of source charges; physically, \\vec{E}(\\vec{r}) is the force per unit charge that would be exerted on a test charge, if you were to place one at P. What exactly is an electric field? I have deliberately begun with what you might call the \"minimal\" interpretation of E , as an intermediate step in the calculation of electric forces. But I encourage you to think of the field as a \"real\" physical entity, filling the space around electric charges. Maxwell himself came to believe that electric and magnetic fields are stresses and strains in an invisible primordial jellylike \"ether.\" Special relativity has forced us to abandon the notion of either, and with it Maxwell's mechanical interpretation of electromagnetic fields. (It is even possible, although cumbersome, to formulate classical electrodynamics as an \"action-at-a-distance\" theory, and dispense with the field concept altogether.) I can't tell you, then, what a field is -- only how to calculate it and what it can do for you once you've got it.","title":"2.1.3: The Electric Field"},{"location":"ch2-1/#example-21","text":"Find the electric field a distance z above the midpoint between two equal charges (q), a distance d apart (Fig. 2.4a) Solution Let \\vec{E_1} be the field of the left charge alone, and \\vec{E_2} that of the right charge alone (Fig. 2.4b). Adding them (vectorially), the horizontal components cancel and the vertical components conspire E_z = 2 \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{\\gr ^2} \\cos \\theta Here \\gr = \\sqrt{z^2 + (d/2)^2} and \\cos \\theta = z / \\gr , so \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{2qz}{\\left[ z^2 + (d/2)^2 \\right]^{3/2}} \\hat{z} Check : When z \\gg d you're so far away that it just looks like a single charge 2q , so the field should reduce to \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{2q}{z^2} \\hat{z} . And it does, just set d \\rightarrow 0 in the formula).","title":"Example 2.1"},{"location":"ch2-1/#214-continuous-charge-distributions","text":"Our definition of the electric field (Eq. \\eqref{2.4} ) assumes that the source of the field is a collection of discrete point charges q_i . If, instead, the charge is distributed continuously over some region, the sum becomes an integral (Fig 2.5a): \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{1}{\\gr ^2} \\hat{\\gr} \\dd{q} If the charge is spread out along a line (Fig. 2.5b), with charge-per-unit-length \\lambda then \\dd{q} = \\lambda \\dd{l}' (where \\dd{l}' ) is an element of length along the line); if the charge is smeared out over a surface (Fig. 2.5c) with charge-per-unit-area \\sigma , then \\dd{q} = \\sigma \\dd{a}' (where \\dd{a'} ) is an element of area on the surface); and if the charge fills a volume (Fig 2.5d), with charge-per-unit-volume \\rho , then \\dd{q} = \\rho\\dd{\\tau'} (where \\dd{\\tau'} is an element of volume): dq \\rightarrow \\lambda \\dd{l'} \\sim \\sigma \\dd{a'} \\sim \\rho \\dd{\\tau'} Thus the electric field of a line charge is \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\lambda(\\vec{r'})}{\\gr ^2} \\hat{\\gr} \\dd{l'} for a surface charge, \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma(\\vec{r'})}{\\gr ^2} \\hat{\\gr} \\dd{a'} and for a volume charge, \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r'})}{\\gr ^2} \\hat{\\gr} \\dd{\\tau'} \\label{2.8} \\tag{2.8} Equation \\eqref{2.8} itself is often referred to as \"Coulomb's law,\" because it is such a short step from the original, and because a volume charge is in a sense the most general and realistic case. Please note carefully the meaning of \\gr in these formulas. Originally, in \\eqref{2.4} , \\gr_i stood for the vector from the source charge q_i to the field point r . Correspondingly, in Eq.s 9-11, \\gr is the vector from \\dd{q} to the field point \\vec{r} . Warning: the unit vector \\hat{\\gr} is not constant: its direction depends on the source point \\vec{r'} , and hence it cannot be taken outside the integrals (9-11). In practice, you must work with Cartesian components ( \\hat{x}, \\hat{y}, \\hat{z} are constant, and do come out) , even if you use curvilinear coordinates to perform the integration.","title":"2.1.4: Continuous Charge Distributions"},{"location":"ch2-1/#example-22","text":"Find the electric field a distance z above the midpoint of a straight line segment of length 2L that carries a uniform line charge \\lambda (Fig. 2.6). TODO!","title":"Example 2.2"},{"location":"ch2-2/","text":"2.2: Divergence and Curl of Electrostatic Fields 2.2.1 Field Lines, Flux, and Gauss' Law In principle, we are done with the subject of electrostatics. Eq. 2.8 tells us how to compute the field of a charge distribution, and Eq. 2.3 tells us what the force on a charge Q placed in this field will be. Unfortunately, as you may have discovered, the integrals involved in computing E can be formidable, even for reasonably simple charge distributions. Much of the rest of electrostatics is devoted to assembling a bag of tools and tricks for avoiding these integrals. It all begins with the divergence and curl of E . I shall calculate the divergence of E directly from Eq. 2.8 in section 2.2.2, but first I want to show you a more qualitative, and perhaps more illuminating, intuitive approach. Let's begin with the simplest possible case: a single point charge q , situated at the origin: \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\hat{\\vec{r}} \\tag{2.10} \\label{2.10} To get a \"feel\" for this field, I might sketch a few representative vectors, as in Fig. 2.12a. Because the field falls off like 1/r^2 , the vectors get shorter as you go farther away from the origin; they always point radially outward. But there is a nicer way to represent this field, and that's to connect up the arrows, to form field lines (Fig. 2.12b). You might think that I have thereby thrown away information about the strength of the field, which was contained in the length of the arrows. But actually I have not. The magnitude of the field is indicated by the density of the field lines: it's strong near the center where the field lines are close together, and weak farther out, where they are relatively far apart. In truth, the field-line diagram is deceptive, when I draw it on a two-dimensional surface, for the density of lines passing through a circle of radius r is the total number divided by the circumference ( n / 2 \\pi r ), which goes like (1/r) , not (1/r^2) . But if you imagine the model in three dimensions (a pincushion with needles sticking out in all directions), then the density of lines is the total number divided by the area of the sphere (n/4 \\pi r^2) , which does go like (1/r^2) . Such diagrams are also convenient for representing more complicated fields. Of course, the number of lines you draw depends on how lazy you are (and how sharp your pencil is), though you ought to include enough to get an accurate sense of the field, and you must be consistent: if q gets 8 lines, then 2q deserves 16. And you must space them fairly - they emanate from a point charge symmetrically in all directions. Field lines begin on positive charges and end on negative ones; they cannot simply terminate in midair, though they may extend out to infinity. Moreover, field lines can never cross - at the intersection the field would have two different directions at once! With all this in mind, it is easy to sketch the field of any simple configuration of point charges: Begin by drawing the lines in the neighborhood of each charge, and then connect them up or extend them to infinity (Figs. 2.13 and 2.14) In this model, the flux of E through a surface S, \\Phi_E \\equiv \\int _S \\vec{E} \\cdot \\dd{\\vec{a}} \\label{2.11} \\tag{2.11} is a measure of the \"number of lines\" passing through S. I put this in quotes because of course we can only draw a representative sample of field lines - the total number would be infinite. But for a given sampling rate the flux is proportional to the number of lines drawn, because the field strength, remember, is proportional to the density of field lines (the number per unit area), and hence \\vec{E} \\cdot \\dd{\\vec{a}} is proportional to the number of lines passing through the infinitesimal area \\dd{\\vec{a}} . (The dot product picks out the component of \\dd{\\vec{a}} along the direction of E , as indicated in Fig 2.15. It is the area in the plane perpendicular to E that we have in mind when we say that the density of field lines is the number per unit area). This suggests that the flux through any closed surface is a measure of the total charge inside. For the field lines that originate on a positive charge must either pass out through the surface or else terminate on a negative charge inside (Fig 2.16a). On the other hand, a charge outside the surface will contribute nothing to the total flux, since its field lines pass in one side and out the other (Fig 2.16b). This is the essence of Gauss's law. Now let's make it quantitative. In the case of a point charge q at the origin, the flux of E through a spherical surface or radius r is \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\int \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{r^2} \\hat{r} \\right) \\cdot \\left( r^2 \\sin \\theta \\dd{\\theta} \\dd{\\phi} \\hat{r} \\right) = \\frac{1}{\\epsilon_0} q \\label{2.12} \\tag{2.12} Notice that the radius of the sphere cancels out, for while the surface area goes up as r^2 , the field goes down as 1/r^2 , so the product is constant. In terms of the field-line picture, this makes good sense, since the same number of field lines pass through any sphere centered at the origin, regardless of its size. In fact, it didn't have to be a sphere - any closed surface, whatever its shape, would be pierced by the same number of field lines. Evidently, the flux through any surface enclosing the charge is q / \\epsilon_0 . Now suppose that instead of a single charge at the origin, we have a bunch of charges scattered about. According to the principle of superposition, the total field is the (vector) sum of all the individual fields: \\vec{E} = \\sum _{i = 1} ^\\nu \\vec{E}_i The flux through a surface that encloses them all is \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = \\sum _{i = 1}^n \\left( \\oint \\vec{E_i} \\cdot \\dd{\\vec{a}} \\right) = \\sum_{i = 1}^n \\left( \\frac{1}{\\epsilon_0} q_i \\right) For any closed surface, then \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} \\label{2.13} \\tag{2.13} where Q_{enc} is the total charge enclosed within the surface. This is the quantitative statement of Gauss's law. Although it contains no information that was not already present in Coulomb's law plus the principle of superposition, it is of almost magical power, as you will see in Sect. 2.2.3. Notice that it all hinges on the 1/r^2 character of Coulomb's law; without that, the crucial cancellation of the r 's in \\eqref{2.12} would not take place, and the total flux of E would depend on the surface chosen, not merely on the total charge enclosed. Other 1/r^2 forces (I am thinking particularly of Newton's law of universal gravitation) will obey \"Gauss's laws\" of their own, and the applications we develop here carry over directly. As it stands, Gauss's law is an integral equation, but we can easily turn it into a differential one by applying the divergence theorem: \\oint_{S} \\vec{E} \\cdot \\dd{\\vec{a}} = \\int_{\\mathscr{V}} (\\div{\\vec{E}}) \\dd{\\tau} Rewriting Q_{enc} in terms of the charge density \\rho we have Q_{enc} = \\int_{\\mathscr{V}} \\rho \\dd{\\tau} So Gauss's law becomes \\int_{\\mathscr{V}} (\\div{\\vec{E}}) \\dd{\\tau} = \\int_{\\mathscr{V}} \\left( \\frac{\\rho}{\\epsilon_0} \\dd{\\tau} \\right) And since this holds for any volume, the integrands must be equal: \\nabla \\cdot \\vec{E} = \\frac{1}{\\epsilon_0} \\rho \\label{2.14} \\tag{2.14} Equation \\eqref{2.14} carries the same message as \\eqref{2.13} ; it is Gauss's law in differential form . The differential version is tidier, but the integral form has the advantage that it accommodates point, line, and surface charges more naturally. 2.2.2: The Divergence of E Let's go back now, and calculate the divergence of \\vec{E} directly from Eq. 2.8: \\vec{E}(\\vec{r}) = \\frac{1}{4\\pi\\epsilon_0} \\int_{\\text{all space}} \\frac{\\hat{\\gr}}{\\gr ^2} \\rho(\\vec{r}') \\dd{\\tau'} \\label{2.15} (Originally the integration was over the volume occupied by the charge, but I may as well extend it to all space, since \\rho = 0 in the exterior region anyway.) Noting that the r-dependence is contained in \\gr = r - r' , we have \\div{\\vec{E}} = \\frac{1}{4\\pi\\epsilon_0} \\int \\vec{\\nabla} \\cdot \\left( \\frac{\\hat{\\gr}}{\\gr^2} \\right) \\rho(\\vec{r'}) \\dd{\\tau'} We calculated this divergence in Section 1.5: \\div{\\left( \\frac{\\hat{\\gr}}{\\gr^2} \\right)} = 4 \\pi \\delta ^3(\\gr) Thus \\div{\\vec{E}} = \\frac{1}{4\\pi\\epsilon_0} \\int 4 \\pi \\delta^3(\\vec{r} - \\vec{r'}) \\rho(\\vec{r'}) \\dd{\\tau'} = \\frac{1}{\\epsilon_0} \\rho(\\vec{r}) \\label{2.16} \\tag{2.16} which is Gauss's law in differential form \\eqref{2.14} . To recover the integral form \\eqref{2.13} we run the previous argument in reverse - integrate over a volume and apply the divergence theorem: \\int_{\\mathscr{V}} \\div{\\vec{E}} \\dd{\\tau} = \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} \\int_{\\mathscr{V}} \\rho \\dd{\\tau} = \\frac{1}{\\epsilon_0} Q_{enc} 2.2.3: Applications of Gauss's Law I must interrupt the theoretical development at this point to show you the extraordinary power of Gauss's law, in integral form. When symmetry permits, it affords by far the quickest and easiest way of computing electric fields. I'll illustrate the method with a series of examples. Example 2.3 Find the field outside a uniformly charged solid sphere of radius R and total charge q Solution Imagine a spherical surface at radius r > R (Fig. 2.18). This is called a Gaussian surface in the trade. Gauss's law says that \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} and in this case Q_{enc} = q . At first glance this doesn't seem to get us very far, because the quantity we want (E) is buried inside the surface integral. Luckily, symmetry allows us to extract E from under the integral sign: E certainly points radially outward, as does \\dd{\\vec{a}} , so we can drop the dot product \\int_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\int_{\\mathscr{S}} | \\vec{E} | da and the magnitude of E is constant over the Gaussian surface, so it comes outside the integral: \\int_{S} | E | da = |E| \\int_{S} da = E 4 \\pi r^2 Thus |\\vec{E}|4\\pi r^2 = \\frac{1}{\\epsilon_0} q or \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\hat{r} Notice a remarkable feature of this result: the field outside the sphere is exactly the same as it would have been if all the charge had been concentrated at the center. Gauss's law is always true , but not always useful . If \\rho had not been uniform (or at any rate, not spherically symmetrical), or if I had chosen some other shape for my Gaussian surface, it would have still been true that the flux of \\vec{E} is q / \\epsilon_0 , but \\vec{E} would not have pointed in the same direction as \\dd{\\vec{a}} , and its magnitude would not have been constant over the surface, and without that I cannot get |\\vec{E}| outside the integral. Symmetry is crucial to this application of Gauss's law. As far as I know, there are only three kinds of symmetry that work: Spherical symmetry. Make your Gaussian survace a concentric sphere. Cylindrical symmetry. Make your Gaussian surface a coaxial cylinder. Plane symmetry. Use a Gaussian \"pillbox\" that straddles the surface. Although 2 and 3 technically require infinitely long cylinders, and planes extending to infinity, we shall often use them to get approximate answers for \"long\" cylinders or \"large\" planes, at points far from the edges. Example 2.4 A long cylinder (Fig 2.21) carries a charge density that is proportional to the distance from the axis: \\rho = ks for some constant k . Find the electric field inside this cylinder. Solution : Draw a Gaussian cylinder of length l and radius s. For this surface, Gauss's law states \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} The enclosed charge is \\begin{align} Q_{enc} & = & \\int \\rho \\dd{\\tau} \\\\ & = & \\int(ks')(s' \\dd{s'} \\dd{\\phi} \\dd{z}) \\\\ & = & 2 \\pi k l \\int_{0}^{s} s'^2 \\dd{s'} \\\\ & = & \\frac{2}{3} \\pi k l s^3 \\end{align} (I used the volume element appropriate to cylindrical coordinates, and integrated \\phi from 0 to 2\\pi , \\dd{z} from 0 to l . I put a prime on the integration variable s' to distinguish it from the radius s of the Gaussian surface.) Now, symmetry dictates that \\vec{E} must point radially outward, so for the curved portion of the Gaussian cylinder we have: \\int \\vec{E} \\cdot \\dd{\\vec{a}} = \\int | \\vec{E}| da = | \\vec{E}| \\int da = |\\vec{E} 2 \\pi s l while the two ends contribute nothing (here \\vec{E} is perpendicular to \\dd{\\vec{a}} ). Thus, |\\vec{E} | 2 \\pi s l = \\frac{1}{\\epsilon_0} \\frac{2}{3} \\pi k l s^3 or, finally, \\vec{E} = \\frac{1}{3\\epsilon_0} k s^2 \\hat{s} Example 2.5 An infinite plane carries a uniform surface charge \\sigma . Find its electric field. Solution Draw a Gaussian pillbox, extending equal distances above and below the plane (Fig. 2.22). Apply Gauss's law to this surface: \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} In this case, Q = \\sigma A , where A is the area of the lid of the pillbox. By symmetry, \\vec{E} points away from the plane (upward for points above, downward for points below). So the top and bottom surfaces yield \\int \\vec{E} \\cdot \\dd{\\vec{a}} = 2 A |\\vec{E}|, whereas the sides contribute nothing. Thus 2 A | \\vec{E} | = \\frac{1}{\\epsilon_0} \\sigma A or \\vec{E} = \\frac{\\sigma}{2 \\epsilon_0} \\hat{n} where \\hat{n} is a unit vector pointing away from the surface. In Prob 2.6, you obtained this same result by a much more laborious method. It seems surprising, at first, that the field of an infinite plane is independent of how fara away you are . What about the 1/r^2 in Coulomb's law? The point is that as you move farther and farther away from the plane, more and more charge comes into your \"field of view,\" and this compensates for the diminishing influence of any particular piece. The electric field of a sphere falls off like 1/r^2 ; the electric field of an infinite line falls off like 1/r ; and the electric field of an infinite plane does not fall off at all (you cannot escape from an infinite plane). Although the direct use of Gauss's law to compute fields is limited to cases of spherical, cylindrical, and planar symmetry, we can put together combinations of objects posessing such symmetry, even though the arrangement as a whole is not symmetrical. For example, invoking the principle of superposition, we could find the field in the vicinity of two uniformly charged parallel cylinders, or a sphere near an infinite charged plane. Example 2.6 Two infinite parallel planes carry equal but opposite uniform charge densities \\pm \\sigma (Fig 2.23). Find the field in each of the three regions: (i) to the left of both, (ii) between them, (iii) to the right of both. Solution The left plate produces a field (1/2 \\epsilon_0)\\sigma , which points away from it (Fig. 2.24) to the left in region in (i) and to the right in regions (ii) and (iii). The right plate, being negatively charged, produces a field (1/2 \\epsilon_0)\\sigma which points toward it - to the right in regions (i) and (ii) and to the left in region (iii). The two fields cancel in regions (i) and (iii); they conspire in region (ii). Conclusion: The field between the plates is \\sigma / \\epsilon_0 , and points to the right; elsewhere it is zero. 2.2.4: The Curl of E I'll calculate the curl of \\vec{E} as I did the divergence in Sect 2.2.1, by studying first the simplest possible configuration: a point charge at the origin. In this case \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\hat{r} Now, a glance at Fig 2.12 should convince you that the curl of this field has to be zero, but I suppose we ought to come up with something a little more rigorous than that. What if we calculate the line integral of this field from some point \\vec{a} to some other point \\vec{b} (Fig 2.29): \\int_{\\vec{a}}^{\\vec{b}} \\vec{E} \\cdot \\dd{\\vec{l}} In spherical coordinates, \\dd{\\vec{l}} = \\dd{r} \\hat{r} + r \\dd{\\theta} \\hat{\\theta} + r \\sin \\theta \\dd{\\phi} \\hat{\\phi} , so \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\dd{r} Therefore, \\int_{\\vec{a}}^{\\vec{b}} \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{a}^{b} \\frac{q}{r^2} \\dd{r} \\\\ = \\left.\\frac{-1}{4 \\pi \\epsilon_0} \\frac{q}{r} \\right|_{r_a} ^{r_b} \\\\ = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{r_a} - \\frac{q}{r_b} \\right) The integral around a closed path is evidently zero (for then r_a = r_b ): \\oint \\vec{E} \\cdot \\dd\\vec{l} = 0 \\label{2.19} \\tag{2.19} and hence, applying Stokes' theorem \\curl{\\vec{E}} = 0 \\label{2.20} \\tag{2.20} Now, I proved eqs. \\eqref{2.19} and \\eqref{2.20} only for the field of a single point charge at the origin, but these results make no reference to what is, after all, a perfectly arbitrary choice of coordinates; they hold no matter where the charge is located. Moreover, if we have many charges, the principle of superposition states that the total field is a vector sum of their individual fields: \\vec{E} = \\vec{E_1} + \\vec{E_2} + \\ldots so \\curl{\\vec{E}} = \\curl{(\\vec{E_1} + \\vec{E_2} + \\ldots)} = (\\curl{\\vec{E_1}}) + (\\curl{\\vec{E_2}}) + \\ldots = 0 Thus, Eqs. \\eqref{2.19} and \\eqref{2.20} hold for any static charge distribution whatever.","title":"2.2 - Divergence and Curl of Electrostatic Fields"},{"location":"ch2-2/#22-divergence-and-curl-of-electrostatic-fields","text":"","title":"2.2: Divergence and Curl of Electrostatic Fields"},{"location":"ch2-2/#221-field-lines-flux-and-gauss-law","text":"In principle, we are done with the subject of electrostatics. Eq. 2.8 tells us how to compute the field of a charge distribution, and Eq. 2.3 tells us what the force on a charge Q placed in this field will be. Unfortunately, as you may have discovered, the integrals involved in computing E can be formidable, even for reasonably simple charge distributions. Much of the rest of electrostatics is devoted to assembling a bag of tools and tricks for avoiding these integrals. It all begins with the divergence and curl of E . I shall calculate the divergence of E directly from Eq. 2.8 in section 2.2.2, but first I want to show you a more qualitative, and perhaps more illuminating, intuitive approach. Let's begin with the simplest possible case: a single point charge q , situated at the origin: \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\hat{\\vec{r}} \\tag{2.10} \\label{2.10} To get a \"feel\" for this field, I might sketch a few representative vectors, as in Fig. 2.12a. Because the field falls off like 1/r^2 , the vectors get shorter as you go farther away from the origin; they always point radially outward. But there is a nicer way to represent this field, and that's to connect up the arrows, to form field lines (Fig. 2.12b). You might think that I have thereby thrown away information about the strength of the field, which was contained in the length of the arrows. But actually I have not. The magnitude of the field is indicated by the density of the field lines: it's strong near the center where the field lines are close together, and weak farther out, where they are relatively far apart. In truth, the field-line diagram is deceptive, when I draw it on a two-dimensional surface, for the density of lines passing through a circle of radius r is the total number divided by the circumference ( n / 2 \\pi r ), which goes like (1/r) , not (1/r^2) . But if you imagine the model in three dimensions (a pincushion with needles sticking out in all directions), then the density of lines is the total number divided by the area of the sphere (n/4 \\pi r^2) , which does go like (1/r^2) . Such diagrams are also convenient for representing more complicated fields. Of course, the number of lines you draw depends on how lazy you are (and how sharp your pencil is), though you ought to include enough to get an accurate sense of the field, and you must be consistent: if q gets 8 lines, then 2q deserves 16. And you must space them fairly - they emanate from a point charge symmetrically in all directions. Field lines begin on positive charges and end on negative ones; they cannot simply terminate in midair, though they may extend out to infinity. Moreover, field lines can never cross - at the intersection the field would have two different directions at once! With all this in mind, it is easy to sketch the field of any simple configuration of point charges: Begin by drawing the lines in the neighborhood of each charge, and then connect them up or extend them to infinity (Figs. 2.13 and 2.14) In this model, the flux of E through a surface S, \\Phi_E \\equiv \\int _S \\vec{E} \\cdot \\dd{\\vec{a}} \\label{2.11} \\tag{2.11} is a measure of the \"number of lines\" passing through S. I put this in quotes because of course we can only draw a representative sample of field lines - the total number would be infinite. But for a given sampling rate the flux is proportional to the number of lines drawn, because the field strength, remember, is proportional to the density of field lines (the number per unit area), and hence \\vec{E} \\cdot \\dd{\\vec{a}} is proportional to the number of lines passing through the infinitesimal area \\dd{\\vec{a}} . (The dot product picks out the component of \\dd{\\vec{a}} along the direction of E , as indicated in Fig 2.15. It is the area in the plane perpendicular to E that we have in mind when we say that the density of field lines is the number per unit area). This suggests that the flux through any closed surface is a measure of the total charge inside. For the field lines that originate on a positive charge must either pass out through the surface or else terminate on a negative charge inside (Fig 2.16a). On the other hand, a charge outside the surface will contribute nothing to the total flux, since its field lines pass in one side and out the other (Fig 2.16b). This is the essence of Gauss's law. Now let's make it quantitative. In the case of a point charge q at the origin, the flux of E through a spherical surface or radius r is \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\int \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{r^2} \\hat{r} \\right) \\cdot \\left( r^2 \\sin \\theta \\dd{\\theta} \\dd{\\phi} \\hat{r} \\right) = \\frac{1}{\\epsilon_0} q \\label{2.12} \\tag{2.12} Notice that the radius of the sphere cancels out, for while the surface area goes up as r^2 , the field goes down as 1/r^2 , so the product is constant. In terms of the field-line picture, this makes good sense, since the same number of field lines pass through any sphere centered at the origin, regardless of its size. In fact, it didn't have to be a sphere - any closed surface, whatever its shape, would be pierced by the same number of field lines. Evidently, the flux through any surface enclosing the charge is q / \\epsilon_0 . Now suppose that instead of a single charge at the origin, we have a bunch of charges scattered about. According to the principle of superposition, the total field is the (vector) sum of all the individual fields: \\vec{E} = \\sum _{i = 1} ^\\nu \\vec{E}_i The flux through a surface that encloses them all is \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = \\sum _{i = 1}^n \\left( \\oint \\vec{E_i} \\cdot \\dd{\\vec{a}} \\right) = \\sum_{i = 1}^n \\left( \\frac{1}{\\epsilon_0} q_i \\right) For any closed surface, then \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} \\label{2.13} \\tag{2.13} where Q_{enc} is the total charge enclosed within the surface. This is the quantitative statement of Gauss's law. Although it contains no information that was not already present in Coulomb's law plus the principle of superposition, it is of almost magical power, as you will see in Sect. 2.2.3. Notice that it all hinges on the 1/r^2 character of Coulomb's law; without that, the crucial cancellation of the r 's in \\eqref{2.12} would not take place, and the total flux of E would depend on the surface chosen, not merely on the total charge enclosed. Other 1/r^2 forces (I am thinking particularly of Newton's law of universal gravitation) will obey \"Gauss's laws\" of their own, and the applications we develop here carry over directly. As it stands, Gauss's law is an integral equation, but we can easily turn it into a differential one by applying the divergence theorem: \\oint_{S} \\vec{E} \\cdot \\dd{\\vec{a}} = \\int_{\\mathscr{V}} (\\div{\\vec{E}}) \\dd{\\tau} Rewriting Q_{enc} in terms of the charge density \\rho we have Q_{enc} = \\int_{\\mathscr{V}} \\rho \\dd{\\tau} So Gauss's law becomes \\int_{\\mathscr{V}} (\\div{\\vec{E}}) \\dd{\\tau} = \\int_{\\mathscr{V}} \\left( \\frac{\\rho}{\\epsilon_0} \\dd{\\tau} \\right) And since this holds for any volume, the integrands must be equal: \\nabla \\cdot \\vec{E} = \\frac{1}{\\epsilon_0} \\rho \\label{2.14} \\tag{2.14} Equation \\eqref{2.14} carries the same message as \\eqref{2.13} ; it is Gauss's law in differential form . The differential version is tidier, but the integral form has the advantage that it accommodates point, line, and surface charges more naturally.","title":"2.2.1 Field Lines, Flux, and Gauss' Law"},{"location":"ch2-2/#222-the-divergence-of-e","text":"Let's go back now, and calculate the divergence of \\vec{E} directly from Eq. 2.8: \\vec{E}(\\vec{r}) = \\frac{1}{4\\pi\\epsilon_0} \\int_{\\text{all space}} \\frac{\\hat{\\gr}}{\\gr ^2} \\rho(\\vec{r}') \\dd{\\tau'} \\label{2.15} (Originally the integration was over the volume occupied by the charge, but I may as well extend it to all space, since \\rho = 0 in the exterior region anyway.) Noting that the r-dependence is contained in \\gr = r - r' , we have \\div{\\vec{E}} = \\frac{1}{4\\pi\\epsilon_0} \\int \\vec{\\nabla} \\cdot \\left( \\frac{\\hat{\\gr}}{\\gr^2} \\right) \\rho(\\vec{r'}) \\dd{\\tau'} We calculated this divergence in Section 1.5: \\div{\\left( \\frac{\\hat{\\gr}}{\\gr^2} \\right)} = 4 \\pi \\delta ^3(\\gr) Thus \\div{\\vec{E}} = \\frac{1}{4\\pi\\epsilon_0} \\int 4 \\pi \\delta^3(\\vec{r} - \\vec{r'}) \\rho(\\vec{r'}) \\dd{\\tau'} = \\frac{1}{\\epsilon_0} \\rho(\\vec{r}) \\label{2.16} \\tag{2.16} which is Gauss's law in differential form \\eqref{2.14} . To recover the integral form \\eqref{2.13} we run the previous argument in reverse - integrate over a volume and apply the divergence theorem: \\int_{\\mathscr{V}} \\div{\\vec{E}} \\dd{\\tau} = \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} \\int_{\\mathscr{V}} \\rho \\dd{\\tau} = \\frac{1}{\\epsilon_0} Q_{enc}","title":"2.2.2: The Divergence of E"},{"location":"ch2-2/#223-applications-of-gausss-law","text":"I must interrupt the theoretical development at this point to show you the extraordinary power of Gauss's law, in integral form. When symmetry permits, it affords by far the quickest and easiest way of computing electric fields. I'll illustrate the method with a series of examples.","title":"2.2.3: Applications of Gauss's Law"},{"location":"ch2-2/#example-23","text":"Find the field outside a uniformly charged solid sphere of radius R and total charge q Solution Imagine a spherical surface at radius r > R (Fig. 2.18). This is called a Gaussian surface in the trade. Gauss's law says that \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} and in this case Q_{enc} = q . At first glance this doesn't seem to get us very far, because the quantity we want (E) is buried inside the surface integral. Luckily, symmetry allows us to extract E from under the integral sign: E certainly points radially outward, as does \\dd{\\vec{a}} , so we can drop the dot product \\int_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\int_{\\mathscr{S}} | \\vec{E} | da and the magnitude of E is constant over the Gaussian surface, so it comes outside the integral: \\int_{S} | E | da = |E| \\int_{S} da = E 4 \\pi r^2 Thus |\\vec{E}|4\\pi r^2 = \\frac{1}{\\epsilon_0} q or \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\hat{r} Notice a remarkable feature of this result: the field outside the sphere is exactly the same as it would have been if all the charge had been concentrated at the center. Gauss's law is always true , but not always useful . If \\rho had not been uniform (or at any rate, not spherically symmetrical), or if I had chosen some other shape for my Gaussian surface, it would have still been true that the flux of \\vec{E} is q / \\epsilon_0 , but \\vec{E} would not have pointed in the same direction as \\dd{\\vec{a}} , and its magnitude would not have been constant over the surface, and without that I cannot get |\\vec{E}| outside the integral. Symmetry is crucial to this application of Gauss's law. As far as I know, there are only three kinds of symmetry that work: Spherical symmetry. Make your Gaussian survace a concentric sphere. Cylindrical symmetry. Make your Gaussian surface a coaxial cylinder. Plane symmetry. Use a Gaussian \"pillbox\" that straddles the surface. Although 2 and 3 technically require infinitely long cylinders, and planes extending to infinity, we shall often use them to get approximate answers for \"long\" cylinders or \"large\" planes, at points far from the edges.","title":"Example 2.3"},{"location":"ch2-2/#example-24","text":"A long cylinder (Fig 2.21) carries a charge density that is proportional to the distance from the axis: \\rho = ks for some constant k . Find the electric field inside this cylinder. Solution : Draw a Gaussian cylinder of length l and radius s. For this surface, Gauss's law states \\oint_{\\mathscr{S}} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} The enclosed charge is \\begin{align} Q_{enc} & = & \\int \\rho \\dd{\\tau} \\\\ & = & \\int(ks')(s' \\dd{s'} \\dd{\\phi} \\dd{z}) \\\\ & = & 2 \\pi k l \\int_{0}^{s} s'^2 \\dd{s'} \\\\ & = & \\frac{2}{3} \\pi k l s^3 \\end{align} (I used the volume element appropriate to cylindrical coordinates, and integrated \\phi from 0 to 2\\pi , \\dd{z} from 0 to l . I put a prime on the integration variable s' to distinguish it from the radius s of the Gaussian surface.) Now, symmetry dictates that \\vec{E} must point radially outward, so for the curved portion of the Gaussian cylinder we have: \\int \\vec{E} \\cdot \\dd{\\vec{a}} = \\int | \\vec{E}| da = | \\vec{E}| \\int da = |\\vec{E} 2 \\pi s l while the two ends contribute nothing (here \\vec{E} is perpendicular to \\dd{\\vec{a}} ). Thus, |\\vec{E} | 2 \\pi s l = \\frac{1}{\\epsilon_0} \\frac{2}{3} \\pi k l s^3 or, finally, \\vec{E} = \\frac{1}{3\\epsilon_0} k s^2 \\hat{s}","title":"Example 2.4"},{"location":"ch2-2/#example-25","text":"An infinite plane carries a uniform surface charge \\sigma . Find its electric field. Solution Draw a Gaussian pillbox, extending equal distances above and below the plane (Fig. 2.22). Apply Gauss's law to this surface: \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} In this case, Q = \\sigma A , where A is the area of the lid of the pillbox. By symmetry, \\vec{E} points away from the plane (upward for points above, downward for points below). So the top and bottom surfaces yield \\int \\vec{E} \\cdot \\dd{\\vec{a}} = 2 A |\\vec{E}|, whereas the sides contribute nothing. Thus 2 A | \\vec{E} | = \\frac{1}{\\epsilon_0} \\sigma A or \\vec{E} = \\frac{\\sigma}{2 \\epsilon_0} \\hat{n} where \\hat{n} is a unit vector pointing away from the surface. In Prob 2.6, you obtained this same result by a much more laborious method. It seems surprising, at first, that the field of an infinite plane is independent of how fara away you are . What about the 1/r^2 in Coulomb's law? The point is that as you move farther and farther away from the plane, more and more charge comes into your \"field of view,\" and this compensates for the diminishing influence of any particular piece. The electric field of a sphere falls off like 1/r^2 ; the electric field of an infinite line falls off like 1/r ; and the electric field of an infinite plane does not fall off at all (you cannot escape from an infinite plane). Although the direct use of Gauss's law to compute fields is limited to cases of spherical, cylindrical, and planar symmetry, we can put together combinations of objects posessing such symmetry, even though the arrangement as a whole is not symmetrical. For example, invoking the principle of superposition, we could find the field in the vicinity of two uniformly charged parallel cylinders, or a sphere near an infinite charged plane.","title":"Example 2.5"},{"location":"ch2-2/#example-26","text":"Two infinite parallel planes carry equal but opposite uniform charge densities \\pm \\sigma (Fig 2.23). Find the field in each of the three regions: (i) to the left of both, (ii) between them, (iii) to the right of both. Solution The left plate produces a field (1/2 \\epsilon_0)\\sigma , which points away from it (Fig. 2.24) to the left in region in (i) and to the right in regions (ii) and (iii). The right plate, being negatively charged, produces a field (1/2 \\epsilon_0)\\sigma which points toward it - to the right in regions (i) and (ii) and to the left in region (iii). The two fields cancel in regions (i) and (iii); they conspire in region (ii). Conclusion: The field between the plates is \\sigma / \\epsilon_0 , and points to the right; elsewhere it is zero.","title":"Example 2.6"},{"location":"ch2-2/#224-the-curl-of-e","text":"I'll calculate the curl of \\vec{E} as I did the divergence in Sect 2.2.1, by studying first the simplest possible configuration: a point charge at the origin. In this case \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\hat{r} Now, a glance at Fig 2.12 should convince you that the curl of this field has to be zero, but I suppose we ought to come up with something a little more rigorous than that. What if we calculate the line integral of this field from some point \\vec{a} to some other point \\vec{b} (Fig 2.29): \\int_{\\vec{a}}^{\\vec{b}} \\vec{E} \\cdot \\dd{\\vec{l}} In spherical coordinates, \\dd{\\vec{l}} = \\dd{r} \\hat{r} + r \\dd{\\theta} \\hat{\\theta} + r \\sin \\theta \\dd{\\phi} \\hat{\\phi} , so \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\dd{r} Therefore, \\int_{\\vec{a}}^{\\vec{b}} \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{a}^{b} \\frac{q}{r^2} \\dd{r} \\\\ = \\left.\\frac{-1}{4 \\pi \\epsilon_0} \\frac{q}{r} \\right|_{r_a} ^{r_b} \\\\ = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{r_a} - \\frac{q}{r_b} \\right) The integral around a closed path is evidently zero (for then r_a = r_b ): \\oint \\vec{E} \\cdot \\dd\\vec{l} = 0 \\label{2.19} \\tag{2.19} and hence, applying Stokes' theorem \\curl{\\vec{E}} = 0 \\label{2.20} \\tag{2.20} Now, I proved eqs. \\eqref{2.19} and \\eqref{2.20} only for the field of a single point charge at the origin, but these results make no reference to what is, after all, a perfectly arbitrary choice of coordinates; they hold no matter where the charge is located. Moreover, if we have many charges, the principle of superposition states that the total field is a vector sum of their individual fields: \\vec{E} = \\vec{E_1} + \\vec{E_2} + \\ldots so \\curl{\\vec{E}} = \\curl{(\\vec{E_1} + \\vec{E_2} + \\ldots)} = (\\curl{\\vec{E_1}}) + (\\curl{\\vec{E_2}}) + \\ldots = 0 Thus, Eqs. \\eqref{2.19} and \\eqref{2.20} hold for any static charge distribution whatever.","title":"2.2.4: The Curl of E"},{"location":"ch2-3/","text":"2.3: Electric Potential 2.3.1: Introduction to Potential The electric field E is not just any old vector function. It is a very special kind of vector function: one whose curl id zero. \\vec{E} = y \\hat{x} , for example, could not possibly be an electrostatic field; no set of charges, regardless of their sizes and positions, could ever produce such a field. We're going to exploit this special property of electric fields to reduce a vector problem (finding E ) to a much simpler scalar problem. The first theorem in Sect 1.6.2 asserts that any vector whose curl is zero is equal to the gradient of some scalar. What I'm going to do now amounts to a proof of that claim, in the context of electrostatics. Because \\nabla \\cross \\vec{E} = 0 , the line integral of E around any closed loop is zero (that follows from Stokes' theorem). Because \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = 0 , the line integral of E from point a to point b is the same for all paths (otherwise you could go out along path (i) and return along path (ii) - Fig 2.30 - and obtain \\oint \\vec{E} \\cdot \\dd{\\vec{l}} \\neq 0 ). Because the line integral is independent of path, we can define a function V(\\vec{r}) \\equiv - \\int _{O} ^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\label{2.21} \\tag{2.21} Here O is some standard reference point on which we have agreed beforehand; V then depends only on the point \\vec{r} . It is called the electric potential . The potential difference between two points a and b is \\begin{align} V(\\vec{b}) - V(\\vec{a}) & = & -\\int_{O}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} + \\int_{O}^{\\vec{a}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ & = & -\\int_{O}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} - \\int_{\\vec{a}}^{O} \\vec{E}\\cdot \\dd{\\vec{l}} \\\\ & = & - \\int_{\\vec{a}} ^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} \\end{align} \\label{2.22} \\tag{2.22} Now, the fundamental theorem for gradients states that V(\\vec{b}) - V(\\vec{a}) = \\int_{\\vec{a}} ^{\\vec{b}} (\\grad{V}) \\cdot \\dd{\\vec{l}} so \\int_{\\vec{a}}^{\\vec{b}} (\\grad{V})\\cdot \\dd{\\vec{l}} = - \\int_{\\vec{a}}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} Since, finally, this is true for any points a and b , the integrands must be equal: \\vec{E} = - \\grad{V} \\label{2.23} \\tag{2.23} Equation \\eqref{2.23} is the differential version of \\eqref{2.21} ; it says that the electric field is the gradient of a scalar potential, which is what we set out to prove. Notice the subtle but crucial role played by path independence (or, equivalently, the fact that \\nabla \\times \\vec{E} = 0 ) in this argument. If the line integral of E depended on the path taken, then the \"definition\" of V \\eqref{2.21} would be nonsense. It simply would not define a function, since changing the path would alter the value of V(\\vec{r}) . By the way, don't let the minus sign in \\eqref{2.23} distract you; it carries over from \\eqref{2.21} and is largely a matter of convention. 2.3.2: Comments on Potential The name . The word \"potential\" is a hideous misnomer because it inevitably reminds you of potential energy . This is particularly insidious, because there is a connection between \"potential\" and \"potential energy,\" as you will see in Sect 2.4. I'm sorry that it is impossible to escape this word. The best I can do is to insist once and for all that \"potential\" and \"potential energy\" are completely different terms and should, by all rights, have different names. Incidentially, a surface over which the potential is constant is called an equipotential . Advantage of the potential formulation . If you know V, you can easily get E - just take the gradient: \\vec{E} =- \\grad{V} . This is quite extraordinary when you stop to think about it, for E is a vector quantity (three components), but V is a scalar (one component). How can one function possibly contain all the information that three independent functions carry? The answer is that the three components of E are not really as independent as they look; in fact, they are explicitly interrelated by the very condition we started with, \\nabla \\times \\vec{E} = 0 . In terms of components, \\pdv{E_x}{y} = \\pdv{E_y}{x}, \\qquad \\pdv{E_z}{y} = \\pdv{E_y}{z}, \\qquad \\pdv{E_x}{z} = \\pdv{E_z}{x} This brings us back to my observation at the beginning of Sect 2.3.1: E is a very special kind of vector.What the potential formulation does is to exploit this feature to maximum advantage, reducing a vector problem to a scalar one, in which there is no need to fuss with components. The reference point \\mathscr{O} . There is an essential ambiguity in the definition of potential, since the choice of reference point \\mathscr{O} was arbitrary. Changing reference points amounts to adding a constant K to the potential: V'(r) = -\\int_{\\mathscr{O}'}^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = - \\int_{\\mathscr{O}'} ^{\\mathscr{O}} \\vec{E} \\cdot \\dd{\\vec{l}} - \\int_{\\mathscr{O}}^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = K + V(\\vec{r}) where K is the line integral of E from the old reference point \\mathscr{O} to the new one \\mathscr{O}' . Of course, adding a constant to V will not affect the potential difference between two points, since the K's cancel out. Nor does the ambiguity affect the gradient of V: \\grad{V'} = \\grad{V} since the derivative of a constant is zero. That's why all such V's, differing only in their choice of reference point, correspond to the same field E Potential as such carries no real physical significance, for at any given point we can adjust its value at will by suitable relocation of \\mathscr{O} . In this sense, it is rather like altitude: if I ask you how high Denver is, you will probably tell me its height above sea level, because that is a convenient and traditional reference point. But we could as well agree to measure altitude above Washington, DC, or Greenwich, or wherever. That would add (or rather, subtract) a fixed amount from all our sea-level readings, but it wouldn't change anything about the real world. The only quantity of interest is the difference in altitude between two points, and that is the same whatever your reference level. Having said this, however, there is a \"natural\" spot to use for \\mathscr{O} in electrostatics - analogous to sea level for altitude - and that is a point infinitely far from the charge. Ordinarily, then, we s\"set the zero of potential at infinity.\" (Since V(\\mathscr{O}) = 0 , choosing a reference point is equivalent to selecting a place where V is to be zero.) But I must warn you that there is one special circumstance in which this convention fails: when the charge distribution itself extends to infinity. The symptom of trouble, in such cases, is that the potential blows up. For instance, the field of a uniformly charged plane is (\\sigma / 2 \\epsilon_0) \\hat{n} , as we found in Ex 2.5; if we naively put \\mathscr{O} = \\infty , then the potential at height z above the plane becomes V(z) = - \\int_{\\infty}^{z}\\frac{1}{2\\epsilon_0} \\sigma \\dd{z} = - \\frac{1}{2\\epsilon_0} \\sigma(z - \\infty) The remedy is simply to choose some other reference point (in this example you might use a point on the plane). Notice that the difficulty occurs only in textbook problems; in \"real life\" there is no such thing as a charge distribution that goes on forever, and we can always use infinity as our reference point. Potential obeys the superposition principle . The original superposition principle pertains to the force on a test charge Q. It says that the total force on Q is the vector sum of the forces attributable to the source charges individually: \\vec{F} = \\vec{F_1} + \\vec{F_2} + \\ldots Dividing through by Q, we see that the electric field, too, obeys the superposition principle: \\vec{E} = \\vec{E_1} + \\vec{E_2} + \\ldots \\label{2.38} Integrating from the common reference point to \\vec{r} , it follows that the potential also satisfies such a principle: V = V_1 + V_2 + \\ldots That is, the potential at any given point is the sum of the potentials due to all the source charges separately. Only this time it is an ordinary sum, not a vector sum, which makes it a lot easier to work with. Units of Potential . In our units, force is measured in newtons and charge in coulombs, so electric fields are in newtons per coulomb. Accordingly, potential is newton-meters per coulomb, or joules per coulomb. A joule per coulomb is a volt . 2.3.3: Poisson's Equation and Laplace's Equation We found in Sect 2.3.1 that the electric field can be written as the gradient of a scalar potential \\vec{E} = - \\grad{V} The question arises, what do the divergence and curl of E , \\div{\\vec{E}} = \\frac{\\rho}{\\epsilon_0} \\qquad \\text{ and } \\qquad \\curl{\\vec{E}} = 0 look like, in terms of V? Well, \\div{\\vec{E}} = \\div(-\\grad{V}) = -\\laplacian{V} , so, apart from that persistent minus sign, the divergence of E is the Laplacian of V. Gauss's law, then, says \\laplacian{V} = -\\frac{\\rho}{\\epsilon_0} \\label{2.24} This is known as Poisson's equation . In regions where there is no charge, so \\rho = 0 , Poisson's equation reduces to Laplace's equation, \\laplacian{V} = 0 \\label{2.25} We'll explore this equation more fully in Chapter 3. So much for Gauss's law. What about the curl law? This says that \\curl{\\vec{E}} = \\curl(-\\grad{V}) = 0 But that's no condition on V - curl of gradient is always zero. Of course, we used the curl law to show that E could be expressed as the gradient of a scalar, so it's not really surprising that this works out: \\curl{\\vec{E}} = 0 permits our definition of V; in return, \\vec{E} = - \\grad{V} guarantees \\curl{\\vec{E}} = 0 . It only takes one differential equation (Poisson's) to determine V, because V is a scalar. For \\vec{E} we needed two, the divergence and the curl. 2.3.4: The potential of a Localized Charge Distribution I defined V in terms of \\vec{E} \\eqref{2.21} . Ordinarily, though, it's E that we're looking for (if we already knew E , there wouldn't be much point in calculating V). The idea is that it might be easier to get V first, and then calculate E by taking the gradient. Typically, then, we know where the charge is (that is, we know \\rho ), and we want to find V. Now, Poisson's equation relates V and \\rho , but unfortunately it's \"the wrong way round\": it would give us \\rho if we knew V, whereas we want V, knowing \\rho . What we must do, then, is \"invert\" Poisson's equation. That's the program for this section, although I shall do it by roundabout means, beginning, as always, with a point charge at the origin. The electric field is \\vec{E} = (1 / 4 \\pi \\epsilon_0)(1 / r^2) \\hat{r} , and \\dd{\\vec{l}} = \\dd{r} \\hat{r} , and \\dd{\\vec{l}} = \\dd{r} \\hat{r} + r \\dd{\\theta} \\hat{\\theta} + r \\sin \\theta \\dd{\\theta} \\hat{\\phi} , so \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\dd{r} Setting the reference point at infinity, the potential of a point charge q at the origin is V(r) = - \\int_{\\mathscr{O}} ^r \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = \\frac{-1}{4 \\pi \\epsilon_0} \\int_{\\infty}^r \\frac{q}{r' ^2} \\dd{r'} \\\\ = \\left.\\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r'} \\right| ^r _{\\infty} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r} (You see here the advantage of using infinity for the reference point: it kills the lower limit on the integral.) Notice the sign of V; presumably the conventional minus sign in the definition was chosen in order to make the potential of a positive charge come out positive. It is useful to remember that regions of positive charge are potential \"hills,\" and electric field points \"downhill\" from plus toward minus. In general, the potential of a point charge q is V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{\\gr} where \\gr , as always, is the distance from q to \\vec{r} (Fig 2.32). Invoking the superposition principle, then, the potential of a collection of charges is V(r) = \\frac{1}{4\\pi \\epsilon_0} \\sum_{i=1} ^n \\frac{q_i}{\\gr _i} or, for a continuous distribution, V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r}')}{\\gr} \\dd{\\tau'} \\label{2.29} \\tag{2.29} This is the equation we were looking for, telling us how to compute V when we know \\rho ; it is, if you like, the \"solution\" to Poisson's equation, for a localized charge distribution. Compare \\eqref{2.29} with the corresponding formula for the electric field in terms of \\rho : \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r'})}{\\gr ^2} \\hat{\\gr} \\dd{\\tau'} The main point is that the pesky unit vector \\hat{\\gr} is gone, so there is no need to fuss with components. The potentials of line and surface charges are V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\lambda(\\vec{r'})}{\\gr} \\dd{l'} \\qquad \\text{ and } \\qquad V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma(\\vec{r'})}{\\gr} \\dd{a'} I should warn you that everything in this section is predicated on the assumption that the reference point is at infinity. This is hardly apparent in \\eqref{2.29} , but remember that we got the equation from the potential of a point charge at the origin, (1/4 \\pi \\epsilon_0) (q / r) , which is valid only when \\mathscr{O} = \\infty . If you try to apply these formulas to one of those artificial problems in which the charge itself extends to infinity, the integral will diverge. 2.3.5: Boundary Conditions In the typical electrostatic problem you are given a source charge distribution \\rho , and you want to find the electric field \\vec{E} it produces. Unless the symmetry of the problem allows a solution by Gauss's law, it is generally to your advantage to calculate the potential first, as an intermediate step. These are the three fundamental quantities of electrostatics: \\rho , \\vec{E} , and V . We have, in the course of our discussion, derived all six formulas interrelating them. These equations are neatly summarized in Fig. 2.35. We began with just two experimental observations: (1) the principle of superposition - a broad general rule applying to all electromagnetic forces, and (2) Coulomb's law - the fundamental law of electrostatics. From these, all else followed. You may have noticed, in studying the exercises in this chapter, that the electric field always undergoes a discontinuity when you cross a surface charge \\sigma . In fact, it is a simple matter to find the amount by which E changes at such a boundary. Suppose we draw a wafer-thin Gaussian pillbox, extending just barely over the edge in each direction (Fig. 2.36). Gauss's law says that \\oint _{S} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} = \\frac{1}{\\epsilon_0} \\sigma A where A is the area of the pillbox lid. If \\sigma varies from point to point or the surface is curved, we can simply pick A to be extremely small. Now, the sides of the pillbox contribute nothing to the flux, in the limit as the thickness \\epsilon goes to zero, so we are left with E_{above}^{\\perp} - E_{below} ^{\\perp} = \\frac{1}{\\epsilon_0} \\sigma \\label{2.31} \\tag{2.31} where E_{above}^{\\perp} denotes the component of \\vec{E} that is perpendicular to the surface immediately above, and E_{below} ^{\\perp} is the same, only just below the surface. For consistency, let \"upward\" be the positive direction for both. Conclusion: the normal component of \\vec{E} is discontinuous by an amount \\sigma / \\epsilon_0 at any boundary. In particular, where there is no surface charge, \\vec{E}^{\\perp} is continuous, as for instance at the surface of a uniformly charged solid sphere. The tangential component of \\vec{E} , by contrast, is always continuous. For if we apply Eq. 2.19, \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = 0 to the thin rectangular loop of Fig 2.37, the ends give nothing (as \\epsilon \\rightarrow 0 ), and the sides give (E_{above} ^{\\parallel} l - E_{below} ^{\\parallel} l) , so \\vec{E}_{above} ^{\\parallel} = \\vec{E}_{below} ^{\\parallel} \\label{2.32} \\tag{2.32} where \\vec{E}^{\\parallel} stands for the components of \\vec{E} parallel to the surface. The boundary conditions on \\vec{E} (Eqs. \\eqref{2.31} and \\eqref{2.32} ) can be combined into a single formula: \\vec{E}_{above} - \\vec{E}_{below} = \\frac{\\sigma}{\\epsilon_0} \\hat{n} \\label{2.33} where \\hat{n} is a unit vector perpendicular to the surface, pointing from \"below\" to \"above.\" The potential, meanwhile, is continuous across any boundary (Fig 2.38), since V_{above} - V_{below} = -\\int_{a}^{b} \\vec{E} \\cdot \\dd{\\vec{l}} as the path length shrinks to zero, so too does the integral V_{above} = V_{below} \\label{2.34} \\tag{2.34} However, the gradient of V inherits the discontinuity in \\vec{E} , since \\vec{E} - \\grad{V} , so \\grad{V}_{above} - \\grad{V}_{below} = - \\frac{\\sigma}{\\epsilon_0} \\hat{n} or more conveniently \\pdv{V_{above}}{n} - \\pdv{V_{below}}{n} = - \\frac{1}{\\epsilon_0} \\sigma \\label{2.36} \\tag{2.36} where \\pdv{V}{n} = \\grad{V} \\cdot \\hat{n} denotes the normal derivative of V (that is, the rate of change in the direction perpendicular to the surface). Please note that these boundary conditions relate the fields and potentials just above and just below the surface. For example, the derivatives in \\eqref{2.36} are the limiting values as we approach the surface from either side.","title":"2.3 - Electric Potential"},{"location":"ch2-3/#23-electric-potential","text":"","title":"2.3: Electric Potential"},{"location":"ch2-3/#231-introduction-to-potential","text":"The electric field E is not just any old vector function. It is a very special kind of vector function: one whose curl id zero. \\vec{E} = y \\hat{x} , for example, could not possibly be an electrostatic field; no set of charges, regardless of their sizes and positions, could ever produce such a field. We're going to exploit this special property of electric fields to reduce a vector problem (finding E ) to a much simpler scalar problem. The first theorem in Sect 1.6.2 asserts that any vector whose curl is zero is equal to the gradient of some scalar. What I'm going to do now amounts to a proof of that claim, in the context of electrostatics. Because \\nabla \\cross \\vec{E} = 0 , the line integral of E around any closed loop is zero (that follows from Stokes' theorem). Because \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = 0 , the line integral of E from point a to point b is the same for all paths (otherwise you could go out along path (i) and return along path (ii) - Fig 2.30 - and obtain \\oint \\vec{E} \\cdot \\dd{\\vec{l}} \\neq 0 ). Because the line integral is independent of path, we can define a function V(\\vec{r}) \\equiv - \\int _{O} ^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\label{2.21} \\tag{2.21} Here O is some standard reference point on which we have agreed beforehand; V then depends only on the point \\vec{r} . It is called the electric potential . The potential difference between two points a and b is \\begin{align} V(\\vec{b}) - V(\\vec{a}) & = & -\\int_{O}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} + \\int_{O}^{\\vec{a}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ & = & -\\int_{O}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} - \\int_{\\vec{a}}^{O} \\vec{E}\\cdot \\dd{\\vec{l}} \\\\ & = & - \\int_{\\vec{a}} ^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} \\end{align} \\label{2.22} \\tag{2.22} Now, the fundamental theorem for gradients states that V(\\vec{b}) - V(\\vec{a}) = \\int_{\\vec{a}} ^{\\vec{b}} (\\grad{V}) \\cdot \\dd{\\vec{l}} so \\int_{\\vec{a}}^{\\vec{b}} (\\grad{V})\\cdot \\dd{\\vec{l}} = - \\int_{\\vec{a}}^{\\vec{b}} \\vec{E}\\cdot \\dd{\\vec{l}} Since, finally, this is true for any points a and b , the integrands must be equal: \\vec{E} = - \\grad{V} \\label{2.23} \\tag{2.23} Equation \\eqref{2.23} is the differential version of \\eqref{2.21} ; it says that the electric field is the gradient of a scalar potential, which is what we set out to prove. Notice the subtle but crucial role played by path independence (or, equivalently, the fact that \\nabla \\times \\vec{E} = 0 ) in this argument. If the line integral of E depended on the path taken, then the \"definition\" of V \\eqref{2.21} would be nonsense. It simply would not define a function, since changing the path would alter the value of V(\\vec{r}) . By the way, don't let the minus sign in \\eqref{2.23} distract you; it carries over from \\eqref{2.21} and is largely a matter of convention.","title":"2.3.1: Introduction to Potential"},{"location":"ch2-3/#232-comments-on-potential","text":"The name . The word \"potential\" is a hideous misnomer because it inevitably reminds you of potential energy . This is particularly insidious, because there is a connection between \"potential\" and \"potential energy,\" as you will see in Sect 2.4. I'm sorry that it is impossible to escape this word. The best I can do is to insist once and for all that \"potential\" and \"potential energy\" are completely different terms and should, by all rights, have different names. Incidentially, a surface over which the potential is constant is called an equipotential . Advantage of the potential formulation . If you know V, you can easily get E - just take the gradient: \\vec{E} =- \\grad{V} . This is quite extraordinary when you stop to think about it, for E is a vector quantity (three components), but V is a scalar (one component). How can one function possibly contain all the information that three independent functions carry? The answer is that the three components of E are not really as independent as they look; in fact, they are explicitly interrelated by the very condition we started with, \\nabla \\times \\vec{E} = 0 . In terms of components, \\pdv{E_x}{y} = \\pdv{E_y}{x}, \\qquad \\pdv{E_z}{y} = \\pdv{E_y}{z}, \\qquad \\pdv{E_x}{z} = \\pdv{E_z}{x} This brings us back to my observation at the beginning of Sect 2.3.1: E is a very special kind of vector.What the potential formulation does is to exploit this feature to maximum advantage, reducing a vector problem to a scalar one, in which there is no need to fuss with components. The reference point \\mathscr{O} . There is an essential ambiguity in the definition of potential, since the choice of reference point \\mathscr{O} was arbitrary. Changing reference points amounts to adding a constant K to the potential: V'(r) = -\\int_{\\mathscr{O}'}^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = - \\int_{\\mathscr{O}'} ^{\\mathscr{O}} \\vec{E} \\cdot \\dd{\\vec{l}} - \\int_{\\mathscr{O}}^{\\vec{r}} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = K + V(\\vec{r}) where K is the line integral of E from the old reference point \\mathscr{O} to the new one \\mathscr{O}' . Of course, adding a constant to V will not affect the potential difference between two points, since the K's cancel out. Nor does the ambiguity affect the gradient of V: \\grad{V'} = \\grad{V} since the derivative of a constant is zero. That's why all such V's, differing only in their choice of reference point, correspond to the same field E Potential as such carries no real physical significance, for at any given point we can adjust its value at will by suitable relocation of \\mathscr{O} . In this sense, it is rather like altitude: if I ask you how high Denver is, you will probably tell me its height above sea level, because that is a convenient and traditional reference point. But we could as well agree to measure altitude above Washington, DC, or Greenwich, or wherever. That would add (or rather, subtract) a fixed amount from all our sea-level readings, but it wouldn't change anything about the real world. The only quantity of interest is the difference in altitude between two points, and that is the same whatever your reference level. Having said this, however, there is a \"natural\" spot to use for \\mathscr{O} in electrostatics - analogous to sea level for altitude - and that is a point infinitely far from the charge. Ordinarily, then, we s\"set the zero of potential at infinity.\" (Since V(\\mathscr{O}) = 0 , choosing a reference point is equivalent to selecting a place where V is to be zero.) But I must warn you that there is one special circumstance in which this convention fails: when the charge distribution itself extends to infinity. The symptom of trouble, in such cases, is that the potential blows up. For instance, the field of a uniformly charged plane is (\\sigma / 2 \\epsilon_0) \\hat{n} , as we found in Ex 2.5; if we naively put \\mathscr{O} = \\infty , then the potential at height z above the plane becomes V(z) = - \\int_{\\infty}^{z}\\frac{1}{2\\epsilon_0} \\sigma \\dd{z} = - \\frac{1}{2\\epsilon_0} \\sigma(z - \\infty) The remedy is simply to choose some other reference point (in this example you might use a point on the plane). Notice that the difficulty occurs only in textbook problems; in \"real life\" there is no such thing as a charge distribution that goes on forever, and we can always use infinity as our reference point. Potential obeys the superposition principle . The original superposition principle pertains to the force on a test charge Q. It says that the total force on Q is the vector sum of the forces attributable to the source charges individually: \\vec{F} = \\vec{F_1} + \\vec{F_2} + \\ldots Dividing through by Q, we see that the electric field, too, obeys the superposition principle: \\vec{E} = \\vec{E_1} + \\vec{E_2} + \\ldots \\label{2.38} Integrating from the common reference point to \\vec{r} , it follows that the potential also satisfies such a principle: V = V_1 + V_2 + \\ldots That is, the potential at any given point is the sum of the potentials due to all the source charges separately. Only this time it is an ordinary sum, not a vector sum, which makes it a lot easier to work with. Units of Potential . In our units, force is measured in newtons and charge in coulombs, so electric fields are in newtons per coulomb. Accordingly, potential is newton-meters per coulomb, or joules per coulomb. A joule per coulomb is a volt .","title":"2.3.2: Comments on Potential"},{"location":"ch2-3/#233-poissons-equation-and-laplaces-equation","text":"We found in Sect 2.3.1 that the electric field can be written as the gradient of a scalar potential \\vec{E} = - \\grad{V} The question arises, what do the divergence and curl of E , \\div{\\vec{E}} = \\frac{\\rho}{\\epsilon_0} \\qquad \\text{ and } \\qquad \\curl{\\vec{E}} = 0 look like, in terms of V? Well, \\div{\\vec{E}} = \\div(-\\grad{V}) = -\\laplacian{V} , so, apart from that persistent minus sign, the divergence of E is the Laplacian of V. Gauss's law, then, says \\laplacian{V} = -\\frac{\\rho}{\\epsilon_0} \\label{2.24} This is known as Poisson's equation . In regions where there is no charge, so \\rho = 0 , Poisson's equation reduces to Laplace's equation, \\laplacian{V} = 0 \\label{2.25} We'll explore this equation more fully in Chapter 3. So much for Gauss's law. What about the curl law? This says that \\curl{\\vec{E}} = \\curl(-\\grad{V}) = 0 But that's no condition on V - curl of gradient is always zero. Of course, we used the curl law to show that E could be expressed as the gradient of a scalar, so it's not really surprising that this works out: \\curl{\\vec{E}} = 0 permits our definition of V; in return, \\vec{E} = - \\grad{V} guarantees \\curl{\\vec{E}} = 0 . It only takes one differential equation (Poisson's) to determine V, because V is a scalar. For \\vec{E} we needed two, the divergence and the curl.","title":"2.3.3: Poisson's Equation and Laplace's Equation"},{"location":"ch2-3/#234-the-potential-of-a-localized-charge-distribution","text":"I defined V in terms of \\vec{E} \\eqref{2.21} . Ordinarily, though, it's E that we're looking for (if we already knew E , there wouldn't be much point in calculating V). The idea is that it might be easier to get V first, and then calculate E by taking the gradient. Typically, then, we know where the charge is (that is, we know \\rho ), and we want to find V. Now, Poisson's equation relates V and \\rho , but unfortunately it's \"the wrong way round\": it would give us \\rho if we knew V, whereas we want V, knowing \\rho . What we must do, then, is \"invert\" Poisson's equation. That's the program for this section, although I shall do it by roundabout means, beginning, as always, with a point charge at the origin. The electric field is \\vec{E} = (1 / 4 \\pi \\epsilon_0)(1 / r^2) \\hat{r} , and \\dd{\\vec{l}} = \\dd{r} \\hat{r} , and \\dd{\\vec{l}} = \\dd{r} \\hat{r} + r \\dd{\\theta} \\hat{\\theta} + r \\sin \\theta \\dd{\\theta} \\hat{\\phi} , so \\vec{E} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\dd{r} Setting the reference point at infinity, the potential of a point charge q at the origin is V(r) = - \\int_{\\mathscr{O}} ^r \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = \\frac{-1}{4 \\pi \\epsilon_0} \\int_{\\infty}^r \\frac{q}{r' ^2} \\dd{r'} \\\\ = \\left.\\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r'} \\right| ^r _{\\infty} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r} (You see here the advantage of using infinity for the reference point: it kills the lower limit on the integral.) Notice the sign of V; presumably the conventional minus sign in the definition was chosen in order to make the potential of a positive charge come out positive. It is useful to remember that regions of positive charge are potential \"hills,\" and electric field points \"downhill\" from plus toward minus. In general, the potential of a point charge q is V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{\\gr} where \\gr , as always, is the distance from q to \\vec{r} (Fig 2.32). Invoking the superposition principle, then, the potential of a collection of charges is V(r) = \\frac{1}{4\\pi \\epsilon_0} \\sum_{i=1} ^n \\frac{q_i}{\\gr _i} or, for a continuous distribution, V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r}')}{\\gr} \\dd{\\tau'} \\label{2.29} \\tag{2.29} This is the equation we were looking for, telling us how to compute V when we know \\rho ; it is, if you like, the \"solution\" to Poisson's equation, for a localized charge distribution. Compare \\eqref{2.29} with the corresponding formula for the electric field in terms of \\rho : \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r'})}{\\gr ^2} \\hat{\\gr} \\dd{\\tau'} The main point is that the pesky unit vector \\hat{\\gr} is gone, so there is no need to fuss with components. The potentials of line and surface charges are V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\lambda(\\vec{r'})}{\\gr} \\dd{l'} \\qquad \\text{ and } \\qquad V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma(\\vec{r'})}{\\gr} \\dd{a'} I should warn you that everything in this section is predicated on the assumption that the reference point is at infinity. This is hardly apparent in \\eqref{2.29} , but remember that we got the equation from the potential of a point charge at the origin, (1/4 \\pi \\epsilon_0) (q / r) , which is valid only when \\mathscr{O} = \\infty . If you try to apply these formulas to one of those artificial problems in which the charge itself extends to infinity, the integral will diverge.","title":"2.3.4: The potential of a Localized Charge Distribution"},{"location":"ch2-3/#235-boundary-conditions","text":"In the typical electrostatic problem you are given a source charge distribution \\rho , and you want to find the electric field \\vec{E} it produces. Unless the symmetry of the problem allows a solution by Gauss's law, it is generally to your advantage to calculate the potential first, as an intermediate step. These are the three fundamental quantities of electrostatics: \\rho , \\vec{E} , and V . We have, in the course of our discussion, derived all six formulas interrelating them. These equations are neatly summarized in Fig. 2.35. We began with just two experimental observations: (1) the principle of superposition - a broad general rule applying to all electromagnetic forces, and (2) Coulomb's law - the fundamental law of electrostatics. From these, all else followed. You may have noticed, in studying the exercises in this chapter, that the electric field always undergoes a discontinuity when you cross a surface charge \\sigma . In fact, it is a simple matter to find the amount by which E changes at such a boundary. Suppose we draw a wafer-thin Gaussian pillbox, extending just barely over the edge in each direction (Fig. 2.36). Gauss's law says that \\oint _{S} \\vec{E} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{enc} = \\frac{1}{\\epsilon_0} \\sigma A where A is the area of the pillbox lid. If \\sigma varies from point to point or the surface is curved, we can simply pick A to be extremely small. Now, the sides of the pillbox contribute nothing to the flux, in the limit as the thickness \\epsilon goes to zero, so we are left with E_{above}^{\\perp} - E_{below} ^{\\perp} = \\frac{1}{\\epsilon_0} \\sigma \\label{2.31} \\tag{2.31} where E_{above}^{\\perp} denotes the component of \\vec{E} that is perpendicular to the surface immediately above, and E_{below} ^{\\perp} is the same, only just below the surface. For consistency, let \"upward\" be the positive direction for both. Conclusion: the normal component of \\vec{E} is discontinuous by an amount \\sigma / \\epsilon_0 at any boundary. In particular, where there is no surface charge, \\vec{E}^{\\perp} is continuous, as for instance at the surface of a uniformly charged solid sphere. The tangential component of \\vec{E} , by contrast, is always continuous. For if we apply Eq. 2.19, \\oint \\vec{E} \\cdot \\dd{\\vec{l}} = 0 to the thin rectangular loop of Fig 2.37, the ends give nothing (as \\epsilon \\rightarrow 0 ), and the sides give (E_{above} ^{\\parallel} l - E_{below} ^{\\parallel} l) , so \\vec{E}_{above} ^{\\parallel} = \\vec{E}_{below} ^{\\parallel} \\label{2.32} \\tag{2.32} where \\vec{E}^{\\parallel} stands for the components of \\vec{E} parallel to the surface. The boundary conditions on \\vec{E} (Eqs. \\eqref{2.31} and \\eqref{2.32} ) can be combined into a single formula: \\vec{E}_{above} - \\vec{E}_{below} = \\frac{\\sigma}{\\epsilon_0} \\hat{n} \\label{2.33} where \\hat{n} is a unit vector perpendicular to the surface, pointing from \"below\" to \"above.\" The potential, meanwhile, is continuous across any boundary (Fig 2.38), since V_{above} - V_{below} = -\\int_{a}^{b} \\vec{E} \\cdot \\dd{\\vec{l}} as the path length shrinks to zero, so too does the integral V_{above} = V_{below} \\label{2.34} \\tag{2.34} However, the gradient of V inherits the discontinuity in \\vec{E} , since \\vec{E} - \\grad{V} , so \\grad{V}_{above} - \\grad{V}_{below} = - \\frac{\\sigma}{\\epsilon_0} \\hat{n} or more conveniently \\pdv{V_{above}}{n} - \\pdv{V_{below}}{n} = - \\frac{1}{\\epsilon_0} \\sigma \\label{2.36} \\tag{2.36} where \\pdv{V}{n} = \\grad{V} \\cdot \\hat{n} denotes the normal derivative of V (that is, the rate of change in the direction perpendicular to the surface). Please note that these boundary conditions relate the fields and potentials just above and just below the surface. For example, the derivatives in \\eqref{2.36} are the limiting values as we approach the surface from either side.","title":"2.3.5: Boundary Conditions"},{"location":"ch2-4/","text":"2.4: Work and Energy in Electrostatics 2.4.1: The Work it Takes to Move a Charge Suppose you have a stationary configuration of source charges, and you want to move a test charge Q from point a to point b (Fig. 2.39). Question : how much work will you have to do? At any point along the path, the electric force on Q is \\vec{F} = Q \\vec{E} ; the force you must exert, in opposition to the electric force, is -Q\\vec{E} . The work you do is therefore W = \\int_{a}^{b} \\vec{F} \\cdot \\dd{\\vec{l}} \\\\ = - Q \\int_{a}^{b} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = - Q[V(b) - V(a)] Notice that the answer is independent of the path you take from a to b; in mechanics, then, we would call the electrostatic force \"conservative.\" Dividing through by Q, we have V(b) - V(a) = \\frac{W}{Q} In words, the potential difference between points a and b is equal to the work per unit charge required to carry a particle from a to b. In particular, if you want to bring Q in from far away and stick it at point r, the work you must do is W = Q[V(\\vec{r}) - V(\\infty)], so if you have set the reference point at infinity, W = Q V(\\vec{r}) \\label{2.39} \\tag{2.39} In this sense, potential is potential energy (the work it takes to create a system) per unit charge (just as the field is force per unit charge). 2.4.2: The Energy of a Point Charge Distribution How much work would it take to assemble an entire collection of point charges? Imagine bringing in the charges, one by one, from far away (Fig 2.40). The first charge q_1 takes no work, since there is no field to fight against. Now bring in q_2 . According to \\eqref{2.39} this will cost you q_2 V_1(\\vec{r}_2) , where V_1 is the potential due to q_1 , and \\vec{r}_2 is the place we're putting q_2 : W_2 = \\frac{1}{4 \\pi \\epsilon_0} q_2 \\left( \\frac{q_1}{\\gr_{12}} \\right) ( \\gr_{12} is the distance between q_1 and q_2 , once they are in position). As you bring in each charge, nail it down in its final location, so it doesn't move when you bring in the next charge. Now bring in q_3 . This requires work q_3 V_{1,2}(\\vec{r}_3) , where V_{1,2} is the potential due to charges q_1 and q_2 , namely (1 / 4 \\pi \\epsilon_0) (q_1 / \\gr_{13} + q_2 / \\gr_{23} ) . Thus W_3 = \\frac{1}{4 \\pi \\epsilon_0} q_3 \\left( \\frac{q_1}{\\gr_{13}} + \\frac{q_2}{\\gr_{23}} \\right) Similarly, the extra work to bring in q_4 will be W_4 = \\frac{1}{4 \\pi \\epsilon_0} q_4 \\left( \\frac{q_1}{\\gr_{14}} + \\frac{q_2}{\\gr_{24}} + \\frac{q_3}{\\gr_{34}} \\right) The total work necessary to assemble the first four charges, then, is W = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q_1 q_2}{\\gr_{12}} + \\frac{q_1 q_3}{\\gr_{13}} + \\frac{q_1 q_4}{\\gr_{14}} + \\frac{q_2 q_3}{\\gr_{23}} + \\frac{q_2 q_4}{\\gr_{24}} + \\frac{q_3 q_4}{\\gr_{34}} \\right) You see the general rule: Take the product of each pair of charges, divide by their separation distance, and add it all up: W = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i = 1} ^{n} \\sum_{j > i} ^n \\frac{q_i q_j}{\\gr_{ij}} The stipulation j > i is to remind you not to count the same pair twice. A nicer way to accomplish this is intentionally to count each pair twice, and then divide by 2: W = \\frac{1}{8 \\pi \\epsilon_0} \\sum_{i = 1} ^n \\sum_{j \\neq i} \\frac{q_i q_j}{\\gr_{ij}} (we must still avoid i = j , of course). Notice that in this form the answer plainly does not depend on the order in which you assemble the charges, since every pair occurs in the sum. Finally, let's pull out the factor q_i : W = \\frac{1}{2} \\sum_{i = 1}^n q_i \\left( \\sum_{j \\neq i} ^n \\frac{1}{4 \\pi \\epsilon_0} \\frac{q_j}{\\gr_{ij}} \\right) The term in parentheses is the potential at point \\vec{r_i} (the position of q_i ) due to all the other charges - all of them, now, not just the ones that were present at some stage during the assembly. Thus, W = \\frac{1}{2} \\sum_{i = 1} ^n q_i V(\\vec{r_i}) \\label{2.42} \\tag{2.42} That's how much work it takes to assemble a configuration of point charges; it's also the amount of work you'd get back if you dismantled the system. In the meantime, it represents energy stored in the configuration (\"potential\" energy, if you insist, though for obvious reasons I prefer to avoid that word in this context). 2.4.3: The Energy of a Continuous Charge Distribution For a volume charge density \\rho , \\eqref{2.42} becomes W = \\frac{1}{2} \\int \\rho V \\dd{\\tau} \\label{2.43} \\tag{2.43} There is a lovely way to write this result, in which \\rho and V are eliminated in favor of \\vec{E} . First, use Gauss's law to express \\rho in terms of \\vec{E} \\rho = \\epsilon_0 \\div{\\vec{E}} \\qquad \\text{so,} \\qquad W = \\frac{\\epsilon_0}{2} \\int (\\div{\\vec{E}}) V \\dd{\\tau} Now, use integration by parts to transfer the derivative from \\vec{E} to V : W = \\frac{\\epsilon_0}{2} \\left[ - \\int \\vec{E} \\cdot (\\grad{V}) \\dd{\\tau} + \\oint V \\vec{E} \\cdot \\dd{\\vec{a}} \\right] But \\grad{V} = - \\vec{E} , so W = \\frac{\\epsilon_0}{2} \\left( \\int_{\\mathscr{V}} E^2 \\dd{\\tau} + \\oint V \\vec{E} \\cdot \\dd{\\vec{a}} \\right) \\label{2.44} \\tag{2.44} But what volume is this we're integrating over? Let's go back to the formula we started with, \\eqref{2.43} . From its derivation, it is clear that we should integrate over the region where the charge is located. But actually, any larger volume would do just as well: The \"extra\" territory we throw in will contribute nothing to the integral, since \\rho = 0 out there. With this in mind, we return to \\eqref{2.44} . What happens here, as we enlarge the volume beyond the minimum necessary to trap all the charge? Well, the integral of E^2 can only increase (the integrand being positive); evidently the surface integral must decrease accordingly to leave the sum intact. (In fact, at large distances from the charge, E goes like 1 / r^2 and V like 1/r , while the surface area grows like r^2 ; roughly speaking, then, the surface integral goes down like 1/r . Please understand: \\eqref{2.44} gives you the correct energy W, whatever volume you use (as long as it encloses all the charge), but the contribution of the volume integral goes up, and that of the surface integral goes down, as you take larger and larger volumes. In particular, why not integrate over all space? Then the surface integral goes to zero, and we are left with W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} \\quad \\text{(all space)} \\label{2.45} \\tag{2.45} Example 2.9 Find the energy of a uniformly charged spherical shell of total charge q and radius R Solution Use \\eqref{2.43} in the version appropriate to surface charges W = \\frac{1}{2} \\sigma V \\dd{a} Now, the potential at the surface of this sphere is (1/4 \\pi \\epsilon_0)q/R (a constant), so W = \\frac{1}{8\\pi \\epsilon_0} \\frac{q}{R} \\int \\sigma \\dd{a} = \\frac{1}{8 \\pi \\epsilon_0} \\frac{q^2}{R} Solution 2 Use \\eqref{2.45} . Inside the sphere, \\vec{E} = 0 ; outside: \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\hat{r} \\quad \\text{so} \\quad E^2 = \\frac{q^2}{(4 \\pi \\epsilon_0)^2 r^4} Therefore, W_{tot} = \\frac{\\epsilon_0}{2 (4 \\pi \\epsilon_0)^2}\\int \\left( \\frac{q^2}{r^4} \\right) (r^2 \\sin \\theta \\dd{r} \\dd{\\theta} \\dd{\\phi}) \\\\ = \\frac{1}{32 \\pi ^2 \\epsilon_0} q^2 4 \\pi \\int_{R} ^{\\infty} \\frac{1}{r^2} \\dd{r} = \\frac{1}{8 \\pi \\epsilon_0} \\frac{q^2}{R} 2.4.4: Comments on Electrostatic Energy A perplexing \"inconsistency\" Equation \\eqref{2.45} clearly implies that the energy of a stationary charge distribution is always positive. On the other hand, \\eqref{2.42} (from which \\eqref{2.45} was in fact derived), can be positive or negative. For instance, according to \\eqref{2.42} the energy of two equal but opposite charges a distance \\gr apart is -(1/4 \\pi \\epsilon_0) (q^2/\\gr) . What's gone wrong? Which equation is correct? The answer is that both are correct, but they speak to slightly different questions. Equation \\eqref{2.42} does not take into account the work necessary to make the point charges in the first place; we started with point charges and simply found the work required to bring them together. This is wise strategy, since \\eqref{2.45} indicates that the energy of a point charge is in fact infinite W = \\frac{\\epsilon_0}{2(4 \\pi \\epsilon_0)^2} \\int \\left( \\frac{q^2}{r^4} \\right) (r^2 \\sin \\theta \\dd{r} \\dd{\\theta} \\dd{\\phi}) = \\frac{q^2}{8 \\pi \\epsilon_0} \\int_{0} ^{\\infty} \\frac{1}{r^2} \\dd{r} = \\infty Equation \\eqref{2.45} is more complete , in the sense that it tells you the total energy stored in a charge configuration, but \\eqref{2.42} is more appropriate when you're dealing with point charges, because we prefer (for good reason!) to leave out that portion of the total energy that is attributable to the fabrication of the point charges themselves. In practice, after all, the point charges (electrons, say) are given to us ready-made; all we do is move them around. Since we did not put them together, and we cannot take them apart, it is immaterial how much work the process would involve. (Still, the infinite energy of a point charge is a recurring source of embarrassment for electromagnetic theory, afflicting the quantum version as well as the classical. We shall return to the problem in Chapter 11). Now, you may wonder where the inconsistency crept into an apparently water-tight derivation. The \"flaw\" lies between \\eqref{2.42} and \\eqref{2.43} : in the former, V(\\vec{r_i}) represents the potential due to all the other charges, but not q_i , whereas in the latter, V(\\vec{r}) is the full potential. For a continuous distribution, there is no distinction, since the amount of charge right at the point \\vec{r} is vanishingly small, and its contribution to the potential is zero. But in the presence of point charges you'd better stick with \\eqref{2.42} . Where is the energy stored? Equations \\eqref{2.43} and \\eqref{2.45} offer two different ways of calculating the same thing. The first is an integral over the charge distribution, the second is an integral over the field. These can involve completely different regions. For instance, in the case of a spherical shell, the charge is confined to the surface, whereas the electric field is everywhere outside this surface. Where is the energy, then? Is it stored in the field, as \\eqref{2.45} seems to suggest, or is it stored in the charge, as \\eqref{2.43} implies? At the present stage this is simply an unanswerable question: I can tell you what the total energy is, and I can provide you with several different ways to compute it, but it is impertinent to worry about where the energy is located. In the context of radiation theory (Chapter 11) it is useful (and in general relativity it is essential) to regard the energy as stored in the field, with a density \\frac{\\epsilon_0}{2} E^2 = \\text{ energy per unit volume} \\label{2.46} \\tag{2.46} But in electrostatics one could just as well say it is stored in the charge, with a density \\frac{1}{2} \\rho V . The difference is purely a matter of bookkeeping. The superposition principle . Because electrostatic energy is quadratic in the fields, it does not obey a superposition principle. The energy of a compound system is not the sum of the energies of its parts considered separately - there are also \"cross terms\": \\begin{align} W_{tot} & = & \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} = \\frac{\\epsilon_0}{2} \\int (\\vec{E_1} + \\vec{E_2})^2 \\dd{\\tau} \\\\ & = & \\frac{\\epsilon_0}{2} \\int (E_1 ^2 + E_2 ^2 + 2 \\vec{E_1} \\cdot \\vec{E_2}) \\dd{\\tau} \\\\ & = & W_1 + W_2 + \\epsilon_0 \\int \\vec{E_1} \\cdot \\vec{E_2} \\dd{\\tau} \\end{align} For example, if you double the charge everywhere, you quadruple the total energy.","title":"2.4 - Work and Energy in Electrostatics"},{"location":"ch2-4/#24-work-and-energy-in-electrostatics","text":"","title":"2.4: Work and Energy in Electrostatics"},{"location":"ch2-4/#241-the-work-it-takes-to-move-a-charge","text":"Suppose you have a stationary configuration of source charges, and you want to move a test charge Q from point a to point b (Fig. 2.39). Question : how much work will you have to do? At any point along the path, the electric force on Q is \\vec{F} = Q \\vec{E} ; the force you must exert, in opposition to the electric force, is -Q\\vec{E} . The work you do is therefore W = \\int_{a}^{b} \\vec{F} \\cdot \\dd{\\vec{l}} \\\\ = - Q \\int_{a}^{b} \\vec{E} \\cdot \\dd{\\vec{l}} \\\\ = - Q[V(b) - V(a)] Notice that the answer is independent of the path you take from a to b; in mechanics, then, we would call the electrostatic force \"conservative.\" Dividing through by Q, we have V(b) - V(a) = \\frac{W}{Q} In words, the potential difference between points a and b is equal to the work per unit charge required to carry a particle from a to b. In particular, if you want to bring Q in from far away and stick it at point r, the work you must do is W = Q[V(\\vec{r}) - V(\\infty)], so if you have set the reference point at infinity, W = Q V(\\vec{r}) \\label{2.39} \\tag{2.39} In this sense, potential is potential energy (the work it takes to create a system) per unit charge (just as the field is force per unit charge).","title":"2.4.1: The Work it Takes to Move a Charge"},{"location":"ch2-4/#242-the-energy-of-a-point-charge-distribution","text":"How much work would it take to assemble an entire collection of point charges? Imagine bringing in the charges, one by one, from far away (Fig 2.40). The first charge q_1 takes no work, since there is no field to fight against. Now bring in q_2 . According to \\eqref{2.39} this will cost you q_2 V_1(\\vec{r}_2) , where V_1 is the potential due to q_1 , and \\vec{r}_2 is the place we're putting q_2 : W_2 = \\frac{1}{4 \\pi \\epsilon_0} q_2 \\left( \\frac{q_1}{\\gr_{12}} \\right) ( \\gr_{12} is the distance between q_1 and q_2 , once they are in position). As you bring in each charge, nail it down in its final location, so it doesn't move when you bring in the next charge. Now bring in q_3 . This requires work q_3 V_{1,2}(\\vec{r}_3) , where V_{1,2} is the potential due to charges q_1 and q_2 , namely (1 / 4 \\pi \\epsilon_0) (q_1 / \\gr_{13} + q_2 / \\gr_{23} ) . Thus W_3 = \\frac{1}{4 \\pi \\epsilon_0} q_3 \\left( \\frac{q_1}{\\gr_{13}} + \\frac{q_2}{\\gr_{23}} \\right) Similarly, the extra work to bring in q_4 will be W_4 = \\frac{1}{4 \\pi \\epsilon_0} q_4 \\left( \\frac{q_1}{\\gr_{14}} + \\frac{q_2}{\\gr_{24}} + \\frac{q_3}{\\gr_{34}} \\right) The total work necessary to assemble the first four charges, then, is W = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q_1 q_2}{\\gr_{12}} + \\frac{q_1 q_3}{\\gr_{13}} + \\frac{q_1 q_4}{\\gr_{14}} + \\frac{q_2 q_3}{\\gr_{23}} + \\frac{q_2 q_4}{\\gr_{24}} + \\frac{q_3 q_4}{\\gr_{34}} \\right) You see the general rule: Take the product of each pair of charges, divide by their separation distance, and add it all up: W = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{i = 1} ^{n} \\sum_{j > i} ^n \\frac{q_i q_j}{\\gr_{ij}} The stipulation j > i is to remind you not to count the same pair twice. A nicer way to accomplish this is intentionally to count each pair twice, and then divide by 2: W = \\frac{1}{8 \\pi \\epsilon_0} \\sum_{i = 1} ^n \\sum_{j \\neq i} \\frac{q_i q_j}{\\gr_{ij}} (we must still avoid i = j , of course). Notice that in this form the answer plainly does not depend on the order in which you assemble the charges, since every pair occurs in the sum. Finally, let's pull out the factor q_i : W = \\frac{1}{2} \\sum_{i = 1}^n q_i \\left( \\sum_{j \\neq i} ^n \\frac{1}{4 \\pi \\epsilon_0} \\frac{q_j}{\\gr_{ij}} \\right) The term in parentheses is the potential at point \\vec{r_i} (the position of q_i ) due to all the other charges - all of them, now, not just the ones that were present at some stage during the assembly. Thus, W = \\frac{1}{2} \\sum_{i = 1} ^n q_i V(\\vec{r_i}) \\label{2.42} \\tag{2.42} That's how much work it takes to assemble a configuration of point charges; it's also the amount of work you'd get back if you dismantled the system. In the meantime, it represents energy stored in the configuration (\"potential\" energy, if you insist, though for obvious reasons I prefer to avoid that word in this context).","title":"2.4.2: The Energy of a Point Charge Distribution"},{"location":"ch2-4/#243-the-energy-of-a-continuous-charge-distribution","text":"For a volume charge density \\rho , \\eqref{2.42} becomes W = \\frac{1}{2} \\int \\rho V \\dd{\\tau} \\label{2.43} \\tag{2.43} There is a lovely way to write this result, in which \\rho and V are eliminated in favor of \\vec{E} . First, use Gauss's law to express \\rho in terms of \\vec{E} \\rho = \\epsilon_0 \\div{\\vec{E}} \\qquad \\text{so,} \\qquad W = \\frac{\\epsilon_0}{2} \\int (\\div{\\vec{E}}) V \\dd{\\tau} Now, use integration by parts to transfer the derivative from \\vec{E} to V : W = \\frac{\\epsilon_0}{2} \\left[ - \\int \\vec{E} \\cdot (\\grad{V}) \\dd{\\tau} + \\oint V \\vec{E} \\cdot \\dd{\\vec{a}} \\right] But \\grad{V} = - \\vec{E} , so W = \\frac{\\epsilon_0}{2} \\left( \\int_{\\mathscr{V}} E^2 \\dd{\\tau} + \\oint V \\vec{E} \\cdot \\dd{\\vec{a}} \\right) \\label{2.44} \\tag{2.44} But what volume is this we're integrating over? Let's go back to the formula we started with, \\eqref{2.43} . From its derivation, it is clear that we should integrate over the region where the charge is located. But actually, any larger volume would do just as well: The \"extra\" territory we throw in will contribute nothing to the integral, since \\rho = 0 out there. With this in mind, we return to \\eqref{2.44} . What happens here, as we enlarge the volume beyond the minimum necessary to trap all the charge? Well, the integral of E^2 can only increase (the integrand being positive); evidently the surface integral must decrease accordingly to leave the sum intact. (In fact, at large distances from the charge, E goes like 1 / r^2 and V like 1/r , while the surface area grows like r^2 ; roughly speaking, then, the surface integral goes down like 1/r . Please understand: \\eqref{2.44} gives you the correct energy W, whatever volume you use (as long as it encloses all the charge), but the contribution of the volume integral goes up, and that of the surface integral goes down, as you take larger and larger volumes. In particular, why not integrate over all space? Then the surface integral goes to zero, and we are left with W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} \\quad \\text{(all space)} \\label{2.45} \\tag{2.45}","title":"2.4.3: The Energy of a Continuous Charge Distribution"},{"location":"ch2-4/#example-29","text":"Find the energy of a uniformly charged spherical shell of total charge q and radius R Solution Use \\eqref{2.43} in the version appropriate to surface charges W = \\frac{1}{2} \\sigma V \\dd{a} Now, the potential at the surface of this sphere is (1/4 \\pi \\epsilon_0)q/R (a constant), so W = \\frac{1}{8\\pi \\epsilon_0} \\frac{q}{R} \\int \\sigma \\dd{a} = \\frac{1}{8 \\pi \\epsilon_0} \\frac{q^2}{R} Solution 2 Use \\eqref{2.45} . Inside the sphere, \\vec{E} = 0 ; outside: \\vec{E} = \\frac{1}{4\\pi \\epsilon_0} \\frac{q}{r^2} \\hat{r} \\quad \\text{so} \\quad E^2 = \\frac{q^2}{(4 \\pi \\epsilon_0)^2 r^4} Therefore, W_{tot} = \\frac{\\epsilon_0}{2 (4 \\pi \\epsilon_0)^2}\\int \\left( \\frac{q^2}{r^4} \\right) (r^2 \\sin \\theta \\dd{r} \\dd{\\theta} \\dd{\\phi}) \\\\ = \\frac{1}{32 \\pi ^2 \\epsilon_0} q^2 4 \\pi \\int_{R} ^{\\infty} \\frac{1}{r^2} \\dd{r} = \\frac{1}{8 \\pi \\epsilon_0} \\frac{q^2}{R}","title":"Example 2.9"},{"location":"ch2-4/#244-comments-on-electrostatic-energy","text":"A perplexing \"inconsistency\" Equation \\eqref{2.45} clearly implies that the energy of a stationary charge distribution is always positive. On the other hand, \\eqref{2.42} (from which \\eqref{2.45} was in fact derived), can be positive or negative. For instance, according to \\eqref{2.42} the energy of two equal but opposite charges a distance \\gr apart is -(1/4 \\pi \\epsilon_0) (q^2/\\gr) . What's gone wrong? Which equation is correct? The answer is that both are correct, but they speak to slightly different questions. Equation \\eqref{2.42} does not take into account the work necessary to make the point charges in the first place; we started with point charges and simply found the work required to bring them together. This is wise strategy, since \\eqref{2.45} indicates that the energy of a point charge is in fact infinite W = \\frac{\\epsilon_0}{2(4 \\pi \\epsilon_0)^2} \\int \\left( \\frac{q^2}{r^4} \\right) (r^2 \\sin \\theta \\dd{r} \\dd{\\theta} \\dd{\\phi}) = \\frac{q^2}{8 \\pi \\epsilon_0} \\int_{0} ^{\\infty} \\frac{1}{r^2} \\dd{r} = \\infty Equation \\eqref{2.45} is more complete , in the sense that it tells you the total energy stored in a charge configuration, but \\eqref{2.42} is more appropriate when you're dealing with point charges, because we prefer (for good reason!) to leave out that portion of the total energy that is attributable to the fabrication of the point charges themselves. In practice, after all, the point charges (electrons, say) are given to us ready-made; all we do is move them around. Since we did not put them together, and we cannot take them apart, it is immaterial how much work the process would involve. (Still, the infinite energy of a point charge is a recurring source of embarrassment for electromagnetic theory, afflicting the quantum version as well as the classical. We shall return to the problem in Chapter 11). Now, you may wonder where the inconsistency crept into an apparently water-tight derivation. The \"flaw\" lies between \\eqref{2.42} and \\eqref{2.43} : in the former, V(\\vec{r_i}) represents the potential due to all the other charges, but not q_i , whereas in the latter, V(\\vec{r}) is the full potential. For a continuous distribution, there is no distinction, since the amount of charge right at the point \\vec{r} is vanishingly small, and its contribution to the potential is zero. But in the presence of point charges you'd better stick with \\eqref{2.42} . Where is the energy stored? Equations \\eqref{2.43} and \\eqref{2.45} offer two different ways of calculating the same thing. The first is an integral over the charge distribution, the second is an integral over the field. These can involve completely different regions. For instance, in the case of a spherical shell, the charge is confined to the surface, whereas the electric field is everywhere outside this surface. Where is the energy, then? Is it stored in the field, as \\eqref{2.45} seems to suggest, or is it stored in the charge, as \\eqref{2.43} implies? At the present stage this is simply an unanswerable question: I can tell you what the total energy is, and I can provide you with several different ways to compute it, but it is impertinent to worry about where the energy is located. In the context of radiation theory (Chapter 11) it is useful (and in general relativity it is essential) to regard the energy as stored in the field, with a density \\frac{\\epsilon_0}{2} E^2 = \\text{ energy per unit volume} \\label{2.46} \\tag{2.46} But in electrostatics one could just as well say it is stored in the charge, with a density \\frac{1}{2} \\rho V . The difference is purely a matter of bookkeeping. The superposition principle . Because electrostatic energy is quadratic in the fields, it does not obey a superposition principle. The energy of a compound system is not the sum of the energies of its parts considered separately - there are also \"cross terms\": \\begin{align} W_{tot} & = & \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} = \\frac{\\epsilon_0}{2} \\int (\\vec{E_1} + \\vec{E_2})^2 \\dd{\\tau} \\\\ & = & \\frac{\\epsilon_0}{2} \\int (E_1 ^2 + E_2 ^2 + 2 \\vec{E_1} \\cdot \\vec{E_2}) \\dd{\\tau} \\\\ & = & W_1 + W_2 + \\epsilon_0 \\int \\vec{E_1} \\cdot \\vec{E_2} \\dd{\\tau} \\end{align} For example, if you double the charge everywhere, you quadruple the total energy.","title":"2.4.4: Comments on Electrostatic Energy"},{"location":"ch2-5/","text":"2.5: Conductors 2.5.1: Basic Properties In an insulator , such as glass or rubber, each electron is on a short leash, attached to a particular atom. In a metallic conductor , by contrast, one or more electrons per atom are free to roam. (In liquid conductors such as salt water, it is ions that do the moving). A perfect conductor would contain an unlimited supply of free charges. In real life there are no perfect conductors, but metals come pretty close, for most purposes. From this definition, the basic electrostatic properties of ideal conductors immediately follow: (i) E = 0 inside a conductor . Why? Because if there were any field, those free charges would move, and it wouldn't be electrostatics any more. Hmm... that's hardly a satisfactory explanation; maybe all it proves is that you can't have electrostatics when conductors are present. We had better examine what happens when you put a conductor into an external electric field \\vec{E_0} (Fig. 2.42). Initially, the field will drive any free positive charges to the right, and negative ones to the left. (In practice, it's the negative charges - electrons - that do the moving, but when they depart, the right side is left with a net positive charge - the stationary nuclei - so it doesn't really matter which charges move; the effect is the same). When they come to the edge of the material, the charges pile up: plus on the right side, minus on the left. Now, these induced charges produce a field of their own, \\vec{E_1} , which, as you can see from the figure, is in the opposite direction to \\vec{E_0} . That's the crucial point, for it means that the field of the induced charges tends to cancel the original field. Charge will continue to flow until this cancellation is complete, and the resultant field inside the conductor is precisely zero. The whole process is practically instantaneous. (ii) \\rho = 0 inside a conductor. This follows from Gauss's law: if E is zero, so also is \\rho . There is still charge around, but exactly as much plus as minus, so the net charge density in the interior is zero. (iii) Any net charge resides on the surface . That's the only place left. (iv) A conductor is an equipotential . For if a and b are any two points within (or at the surface of) a given conductor, V(b) - V(a) = - \\int _{a} ^{b} \\vec{E} \\cdot \\dd{\\vec{l}} = 0 , and hence V(a) = V(b) . (v) E is perpendicular to the surface, just outside a conductor. Otherwise, as in (i), charge will immediately flow around the surface until it kills off the tangential component (Fig. 2.43). (Perpendicular to the surface, charge cannot flow, of course, since it is confined to the conducting object.) I think it is astonishing that the charge on a conductor flows to the surface. Because of their mutual repulsion, the charges naturally spread out as much as possible, but for all of them to go to the surface seems like a waste of the interior space. Surely we could do better, from the point of view of making each charge as possible from its neighbors, to sprinkle some of them throughout the volume. Well, it simply is not so. You do best to put all the charge on the surface, and this is true regardless of the size or shape of the conductor. The problem can also be phrased in terms of energy. Like any other free dynamical system, the charge on a conductor will seek the configuration that minimizes its potential energy. What property (iii) asserts is that the electrostatic energy of a solid object (with specified shape and total charge) is a minimum when that charge is spread over the surface. For instance, the energy of a sphere is (1 / 8 \\pi \\epsilon_0)(q^2 / R) if the charge is uniformly distributed over the surface, as we found in Ex 2.9, but it is greater (3/20 \\pi \\epsilon_0)(q^2 / R) if the charge is uniformly distributed throughout the volume (Prob. 2.34). 2.5.2: Induced Charges If you hold a charge +q near an uncharged conductor (Fig 2.44), the two will attract one another. The reason for this is that q will pull minus charges over to the near side and repel plus charges to the far side (Another way to think of it is that the charge moves around in such a way as to kill off the field of q for points inside the conductor, where the total field must be zero.) Since the negative induced charge is closer to q, there is a net force of attraction. (In chapter 3 we will calculate this force explicitly, for the case of a spherical conductor.) When I speak of the field, charge, or potential \"inside\" a conductor, I mean in the \"meat\" of the conductor. If there is some hollow cavity in the conductor, and within that cavity you put some charge, then the field in the cavity will not be zero. But in a remarkable way the cavity and its contents are electrically isolated from the outside world by the surrounding conductor (Fig. 2.45). No external fields penetrate the conductor; they are canceled at the outer surface by the induced charge there. Similarly, the field due to charges within the cavity is canceled, for all exterior points, by the induced charge on the inner surface. However, the compensating charge left over on the outer surface of the conductor effectively \"communicates\" the presence of q to the outside world. The total charge induced on the cavity wall is equal and opposite to the charge inside, for if we surround the cavity with a Gaussian surface, all points of which are in the conductor (Fig 2.45), \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = 0 , and hence (by Gauss's law) the net enclosed charge must be zero. But Q_{enc} = q + q_{induced} , so q_{induced} = - q . Then if the conductor as a whole is electrically neutral, there must be a charge +q on its outer surface. Example 2.10 An uncharged spherical conductor centered at the origin has a cavity of some weird shape carved out of it (Fig. 2.46). Somewhere within the cavity is a charge q. Question: What is the field outside the sphere? Solution At first glance, it would appear that the answer depends on the shape of the cavity and the location of the charge. But that's wrong: the answer is \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\hat{r} regardless. The conductor conceals from us all information concerning the nature of the cavity, revealing only the total charge it contains. How can this be? Well, the charge +q induces an opposite charge -q on the wall of the cavity, which distributes itself in such a way that its field cancels that of q, for all points exterior to the cavity. Since the conductor carries no net charge, this leaves +q to distribute itself uniformly over the surface of the sphere. (It's uniform because the asymmetrical influence of the point charge +q is negated by that of the induced charge -q on the inner surface.) For points outside the sphere, then, the only thing that survives is the field of the leftover +q, uniformly distributed over the outer surface. It may occur to you that in one respect this argument is open to challenge: There are actually three fields at work here: \\vec{E_q}, \\vec{E_{induced}} , and \\vec{E_{leftover}} . All we know for certain is that the sum of the three is zero inside the conductor, yet I claimed that the first two alone cancel, while the third is separately zero there. Moreover, even if the first two cancel within the conductor, who is to say they still cancel for points outside? They do not, after all, cancel for points inside the cavity. I cannot give you a completely satisfactory answer at the moment, but this much at least is true: there exists a way of distributing -q over the inner surface so as to cancel the field of q at all exterior points. For that same cavity could have been carved out of a huge spherical conductor with a radius of 27 miles or light years or whatever. In that case, the leftover +q on the outer surface is simply too far away to produce a significant field, and the other two fields would have to accomplish the cancellation by themselves. So we know they can do it... but are we sure they choose to? Perhaps for small spheres nature prefers some complicated three-way cancellation? Nope: As we'll see in the uniqueness theorems of Chapter 3, electrostatics is very stingy with its options; there is always precisely one way - no more - of distributing the charge on a conductor so as to make the field inside zero. Having found a possible way, we are guaranteed that no alternative exists, even in principle. If a cavity surrounded by conducting material is itself empty of charge, then the field within the cavity is zero. For any field line would have to begin and end on the cavity wall, going from a plus charge to a minus charge (Fig 2.47). Letting that field line be part of a closed loop, the rest of which is entirely inside the conductor (where E = 0), the integral \\oint \\vec{E} \\cdot \\dd{\\vec{l}} is distinctly positive , in violation of Eq. 2.19. It follows that E = 0 within an empty cavity, and there is in vact no charge on the surface of the cavity. (This is why you are relatively safe inside a metal car during a thunderstorm - you may get cooked, if lightning strikes, but you will not be electrocuted. The same principle applies to the placement of sensitive apparatus inside a grounded Faraday cage , to shield out stray electric fields. In practice, the enclosure doesn't even have to be solid conductor - chicken wire will often suffice.) 2.5.3: Surface Charge and the Force on a Conductor Because the field inside a conductor is zero, boundary condition Eq. 2.33 requires that the field immediately outside is \\vec{E} = \\frac{\\sigma}{\\epsilon_0} \\hat{n} \\label{2.48} \\tag{2.48} consistent with our earlier conclusion that the field is normal to the surface. In terms of potential, Eq. 2.36 yields \\sigma = - \\epsilon_0 \\pdv{V}{n} \\label{2.49} \\tag{2.49} These equations enable you to calculate the surface charge on a conductor, if you can determine \\vec{E} or V ; we shall use them frequently in the next chapter. In the presence of an electric field, a surface charge will experience a force; the force per unit area, \\vec{f} , is \\sigma \\vec{E} . But there's a problem here, for the electric field is discontinuous at a surface charge, so what are we supposed to use: \\vec{E}_{above}, \\vec{E}_{below} , or something in between? The answer is that we should use the average of the two \\vec{f} = \\sigma \\vec{E}_{average} = \\frac{1}{2} \\sigma (\\vec{E}_{above} + \\vec{E}_{below}) \\label{2.50} \\tag{2.50} Why the average? The reason is very simple, thought the telling makes it sound complicated: Let's focus our attention on a tiny patch of surface surrounding the point in question (Fig. 2.50). Make it small enough so it is essentially flat and the surface in question is essentially constant. The total field consists of two parts - that attributable to the patch itself, and that due to everything else (other regions of the surface, as well as any external sources that may be present) \\vec{E} = \\vec{E}_{patch} + \\vec{E}_{other} Now, the patch cannot exert a force on itself, any more than you can lift yourself by standing in a basket and pulling up on the handles. The force on the patch, then, is exclusively due to \\vec{E}_{other} , and this suffers no discontinuity (if we removed the patch, the field in the \"hole\" would be perfectly smooth). The discontinuity is due entirely to the charge on the patch, which puts out a field (\\sigma / 2 \\epsilon_0) on either side, pointing away from the surface. Thus, \\vec{E}_{above} = \\vec{E}_{other} + \\frac{\\sigma}{2 \\epsilon_0} \\hat{n} \\\\ \\vec{E}_{below} = \\vec{E}_{other} - \\frac{\\sigma}{2 \\epsilon_0} \\hat{n} \\\\ and hence \\vec{E}_{other} = \\frac{1}{2} (\\vec{E}_{above} + \\vec{E}_{below}) = \\vec{E}_{average} Averaging is really just a device for removing the contribution of the patch itself. That argument applies to any surface charge; in the particular case of a conductor, the field is zero inside and (\\sigma / \\epsilon_0)\\hat{n} outside ( \\eqref{2.48} , so the average is (\\sigma / 2 \\epsilon_0) \\hat{n} , and the force per unit area is f = \\frac{1}{2 \\epsilon_0} \\sigma ^2 \\hat{n} \\label{2.51} \\tag{2.51} This amounts to an outward electrostatic pressure on the surface, tending to draw the conductor into the field, regardless of the sign of \\sigma . Expressing the pressure in terms of the field just outside the surface P = \\frac{\\epsilon_0}{2} E^2 2.5.4: Capacitors Suppose we have two conductors, and we put charge +Q on one and -Q on the other (Fig 2.51). Since V is constant over a conductor, we can speak unambiguously of the potential difference between them: V = V_{+} - V_{-} = - \\int_{(-)}^{(+)} \\vec{E} \\cdot \\dd{\\vec{l}} We don't know how the charge distributes itself over the two conductors, and calculating the field would be a nightmare, if their shapes are complicated, but this much we do know: \\vec{E} is proportional to Q . For \\vec{E} is given by Coulomb's law: \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho}{\\gr^2} \\hat{\\gr} \\dd{\\tau} so if you double \\rho , you double \\vec{E} . Wait a minute! How do we know that doubling Q (and also -Q) simply doubles \\rho ? Maybe the charge moves around into a completely different configuration, quadrupling \\rho in some places and halving it in others, just so the total charge on the conductor is doubled. The fact is that this concern is unwarranted - doubling Q does double \\rho everywhere; it doesn't shift charge around. The proof will come in Chapter 3; for now you'll have to trust me. Since \\vec{E} is proportional to Q, so also is V. The constant of proportionality is called the capacitance of the arrangement C \\equiv \\frac{Q}{V} \\label{2.53} \\tag{2.53} Capacitance is a purely geometrical quantity, determined by the sizes, shapes, and separation of the two conductors. In SI units, C is measured in farads (F); a farad is a coulomb-per-volt. Actually this turns out to be inconveniently large; more practical units are the microfarad ( 10^{-6} F ) and the picofarad ( 10^{-12} F ) Notice that V is, by definition, the potential of the positive conductor less that of the negative one; likewise, Q is the charge of the positive conductor. Accordingly, capacitance is an intrinsically positive quantity. By the way, you will occasionally hear someone speak of the capacitance of a single conductor. In this case the \"second conductor\" is an imaginary spherical shell of infinite radius surrounding the one conductor. It contributes nothing to the field, so the capacitance is given by \\eqref{2.53} , where V is the potential with infinity as the reference point. Example 2.11 Find the capacitance of a parallel-plate capacitor consisting of two metal surfaces of area A held a distance d apart (Fig. 2.52) Solution If we put +Q on the top and -Q on the bottom, they will spread out uniformly over the two surfaces, provided the area is reasonably large and the separation small. The surface charge density, then, is \\sigma = Q / A on the top plate, and so the field, according to Ex. 2.6, is (1 / \\epsilon_0) Q / A . The potential difference between the plates is therefore V = \\frac{Q}{A \\epsilon_0} d and hence C = \\frac{A \\epsilon_0}{d} \\label{2.54} \\tag{2.54} If, for instance, the plates are square with sides 1 cm long, and they are held 1 mm apart, then the capacitance is 9 \\times 10^{-13} F Example 2.12 Find the capacitance of two concentric spherical metal shells, with radii a and b. Solution Place charge +Q on the inner sphere, and -Q on the outer one. The field between the spheres is \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q}{r^2} \\vec{r} so the potential difference between them is V = - \\int_{b}^{a} \\vec{E} \\cdot \\dd{\\vec{l}} = - \\frac{Q}{4 \\pi \\epsilon_0} \\int_{b}^a \\frac{1}{r^2} \\dd{r} = \\frac{Q}{4 \\pi \\epsilon_0} \\left( \\frac{1}{a} - \\frac{1}{b} \\right) As promised, V is proportional to Q; the capacitance is C = \\frac{Q}{V} = 4 \\pi \\epsilon_0 \\frac{ab}{(b - a)} To \"charge up\" a capacitor, you have to remove electrons from the positive plate and carry them to the negative plate. In doing so, you fight against the electric field, which is pulling them back toward the positive conductor and pushing them away from the negative one. How much work does it take, then, to charge the capacitor up to a final amount Q ? Suppose that at some intermediate stage in the process the charge on the positive plate is q , so that the potential difference is q / C . According to Eq. 2.38, the work you must do to transport the next piece of charge dq is \\dd{W} = \\left( \\frac{q}{C} \\right) \\dd{q} The total work necessary, then, to go from q = 0 to q = Q , is W = \\int_{0} ^Q \\left( \\frac{q}{C} \\dd{q} \\right) = \\frac{1}{2} \\frac{Q^2}{C} or, since Q = CV , W = \\frac{1}{2} C V^2 \\label{2.55} \\tag{2.55} where V is the final potential of the capacitor.","title":"2.5 - Conductors"},{"location":"ch2-5/#25-conductors","text":"","title":"2.5: Conductors"},{"location":"ch2-5/#251-basic-properties","text":"In an insulator , such as glass or rubber, each electron is on a short leash, attached to a particular atom. In a metallic conductor , by contrast, one or more electrons per atom are free to roam. (In liquid conductors such as salt water, it is ions that do the moving). A perfect conductor would contain an unlimited supply of free charges. In real life there are no perfect conductors, but metals come pretty close, for most purposes. From this definition, the basic electrostatic properties of ideal conductors immediately follow: (i) E = 0 inside a conductor . Why? Because if there were any field, those free charges would move, and it wouldn't be electrostatics any more. Hmm... that's hardly a satisfactory explanation; maybe all it proves is that you can't have electrostatics when conductors are present. We had better examine what happens when you put a conductor into an external electric field \\vec{E_0} (Fig. 2.42). Initially, the field will drive any free positive charges to the right, and negative ones to the left. (In practice, it's the negative charges - electrons - that do the moving, but when they depart, the right side is left with a net positive charge - the stationary nuclei - so it doesn't really matter which charges move; the effect is the same). When they come to the edge of the material, the charges pile up: plus on the right side, minus on the left. Now, these induced charges produce a field of their own, \\vec{E_1} , which, as you can see from the figure, is in the opposite direction to \\vec{E_0} . That's the crucial point, for it means that the field of the induced charges tends to cancel the original field. Charge will continue to flow until this cancellation is complete, and the resultant field inside the conductor is precisely zero. The whole process is practically instantaneous. (ii) \\rho = 0 inside a conductor. This follows from Gauss's law: if E is zero, so also is \\rho . There is still charge around, but exactly as much plus as minus, so the net charge density in the interior is zero. (iii) Any net charge resides on the surface . That's the only place left. (iv) A conductor is an equipotential . For if a and b are any two points within (or at the surface of) a given conductor, V(b) - V(a) = - \\int _{a} ^{b} \\vec{E} \\cdot \\dd{\\vec{l}} = 0 , and hence V(a) = V(b) . (v) E is perpendicular to the surface, just outside a conductor. Otherwise, as in (i), charge will immediately flow around the surface until it kills off the tangential component (Fig. 2.43). (Perpendicular to the surface, charge cannot flow, of course, since it is confined to the conducting object.) I think it is astonishing that the charge on a conductor flows to the surface. Because of their mutual repulsion, the charges naturally spread out as much as possible, but for all of them to go to the surface seems like a waste of the interior space. Surely we could do better, from the point of view of making each charge as possible from its neighbors, to sprinkle some of them throughout the volume. Well, it simply is not so. You do best to put all the charge on the surface, and this is true regardless of the size or shape of the conductor. The problem can also be phrased in terms of energy. Like any other free dynamical system, the charge on a conductor will seek the configuration that minimizes its potential energy. What property (iii) asserts is that the electrostatic energy of a solid object (with specified shape and total charge) is a minimum when that charge is spread over the surface. For instance, the energy of a sphere is (1 / 8 \\pi \\epsilon_0)(q^2 / R) if the charge is uniformly distributed over the surface, as we found in Ex 2.9, but it is greater (3/20 \\pi \\epsilon_0)(q^2 / R) if the charge is uniformly distributed throughout the volume (Prob. 2.34).","title":"2.5.1: Basic Properties"},{"location":"ch2-5/#252-induced-charges","text":"If you hold a charge +q near an uncharged conductor (Fig 2.44), the two will attract one another. The reason for this is that q will pull minus charges over to the near side and repel plus charges to the far side (Another way to think of it is that the charge moves around in such a way as to kill off the field of q for points inside the conductor, where the total field must be zero.) Since the negative induced charge is closer to q, there is a net force of attraction. (In chapter 3 we will calculate this force explicitly, for the case of a spherical conductor.) When I speak of the field, charge, or potential \"inside\" a conductor, I mean in the \"meat\" of the conductor. If there is some hollow cavity in the conductor, and within that cavity you put some charge, then the field in the cavity will not be zero. But in a remarkable way the cavity and its contents are electrically isolated from the outside world by the surrounding conductor (Fig. 2.45). No external fields penetrate the conductor; they are canceled at the outer surface by the induced charge there. Similarly, the field due to charges within the cavity is canceled, for all exterior points, by the induced charge on the inner surface. However, the compensating charge left over on the outer surface of the conductor effectively \"communicates\" the presence of q to the outside world. The total charge induced on the cavity wall is equal and opposite to the charge inside, for if we surround the cavity with a Gaussian surface, all points of which are in the conductor (Fig 2.45), \\oint \\vec{E} \\cdot \\dd{\\vec{a}} = 0 , and hence (by Gauss's law) the net enclosed charge must be zero. But Q_{enc} = q + q_{induced} , so q_{induced} = - q . Then if the conductor as a whole is electrically neutral, there must be a charge +q on its outer surface.","title":"2.5.2: Induced Charges"},{"location":"ch2-5/#example-210","text":"An uncharged spherical conductor centered at the origin has a cavity of some weird shape carved out of it (Fig. 2.46). Somewhere within the cavity is a charge q. Question: What is the field outside the sphere? Solution At first glance, it would appear that the answer depends on the shape of the cavity and the location of the charge. But that's wrong: the answer is \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{r^2} \\hat{r} regardless. The conductor conceals from us all information concerning the nature of the cavity, revealing only the total charge it contains. How can this be? Well, the charge +q induces an opposite charge -q on the wall of the cavity, which distributes itself in such a way that its field cancels that of q, for all points exterior to the cavity. Since the conductor carries no net charge, this leaves +q to distribute itself uniformly over the surface of the sphere. (It's uniform because the asymmetrical influence of the point charge +q is negated by that of the induced charge -q on the inner surface.) For points outside the sphere, then, the only thing that survives is the field of the leftover +q, uniformly distributed over the outer surface. It may occur to you that in one respect this argument is open to challenge: There are actually three fields at work here: \\vec{E_q}, \\vec{E_{induced}} , and \\vec{E_{leftover}} . All we know for certain is that the sum of the three is zero inside the conductor, yet I claimed that the first two alone cancel, while the third is separately zero there. Moreover, even if the first two cancel within the conductor, who is to say they still cancel for points outside? They do not, after all, cancel for points inside the cavity. I cannot give you a completely satisfactory answer at the moment, but this much at least is true: there exists a way of distributing -q over the inner surface so as to cancel the field of q at all exterior points. For that same cavity could have been carved out of a huge spherical conductor with a radius of 27 miles or light years or whatever. In that case, the leftover +q on the outer surface is simply too far away to produce a significant field, and the other two fields would have to accomplish the cancellation by themselves. So we know they can do it... but are we sure they choose to? Perhaps for small spheres nature prefers some complicated three-way cancellation? Nope: As we'll see in the uniqueness theorems of Chapter 3, electrostatics is very stingy with its options; there is always precisely one way - no more - of distributing the charge on a conductor so as to make the field inside zero. Having found a possible way, we are guaranteed that no alternative exists, even in principle. If a cavity surrounded by conducting material is itself empty of charge, then the field within the cavity is zero. For any field line would have to begin and end on the cavity wall, going from a plus charge to a minus charge (Fig 2.47). Letting that field line be part of a closed loop, the rest of which is entirely inside the conductor (where E = 0), the integral \\oint \\vec{E} \\cdot \\dd{\\vec{l}} is distinctly positive , in violation of Eq. 2.19. It follows that E = 0 within an empty cavity, and there is in vact no charge on the surface of the cavity. (This is why you are relatively safe inside a metal car during a thunderstorm - you may get cooked, if lightning strikes, but you will not be electrocuted. The same principle applies to the placement of sensitive apparatus inside a grounded Faraday cage , to shield out stray electric fields. In practice, the enclosure doesn't even have to be solid conductor - chicken wire will often suffice.)","title":"Example 2.10"},{"location":"ch2-5/#253-surface-charge-and-the-force-on-a-conductor","text":"Because the field inside a conductor is zero, boundary condition Eq. 2.33 requires that the field immediately outside is \\vec{E} = \\frac{\\sigma}{\\epsilon_0} \\hat{n} \\label{2.48} \\tag{2.48} consistent with our earlier conclusion that the field is normal to the surface. In terms of potential, Eq. 2.36 yields \\sigma = - \\epsilon_0 \\pdv{V}{n} \\label{2.49} \\tag{2.49} These equations enable you to calculate the surface charge on a conductor, if you can determine \\vec{E} or V ; we shall use them frequently in the next chapter. In the presence of an electric field, a surface charge will experience a force; the force per unit area, \\vec{f} , is \\sigma \\vec{E} . But there's a problem here, for the electric field is discontinuous at a surface charge, so what are we supposed to use: \\vec{E}_{above}, \\vec{E}_{below} , or something in between? The answer is that we should use the average of the two \\vec{f} = \\sigma \\vec{E}_{average} = \\frac{1}{2} \\sigma (\\vec{E}_{above} + \\vec{E}_{below}) \\label{2.50} \\tag{2.50} Why the average? The reason is very simple, thought the telling makes it sound complicated: Let's focus our attention on a tiny patch of surface surrounding the point in question (Fig. 2.50). Make it small enough so it is essentially flat and the surface in question is essentially constant. The total field consists of two parts - that attributable to the patch itself, and that due to everything else (other regions of the surface, as well as any external sources that may be present) \\vec{E} = \\vec{E}_{patch} + \\vec{E}_{other} Now, the patch cannot exert a force on itself, any more than you can lift yourself by standing in a basket and pulling up on the handles. The force on the patch, then, is exclusively due to \\vec{E}_{other} , and this suffers no discontinuity (if we removed the patch, the field in the \"hole\" would be perfectly smooth). The discontinuity is due entirely to the charge on the patch, which puts out a field (\\sigma / 2 \\epsilon_0) on either side, pointing away from the surface. Thus, \\vec{E}_{above} = \\vec{E}_{other} + \\frac{\\sigma}{2 \\epsilon_0} \\hat{n} \\\\ \\vec{E}_{below} = \\vec{E}_{other} - \\frac{\\sigma}{2 \\epsilon_0} \\hat{n} \\\\ and hence \\vec{E}_{other} = \\frac{1}{2} (\\vec{E}_{above} + \\vec{E}_{below}) = \\vec{E}_{average} Averaging is really just a device for removing the contribution of the patch itself. That argument applies to any surface charge; in the particular case of a conductor, the field is zero inside and (\\sigma / \\epsilon_0)\\hat{n} outside ( \\eqref{2.48} , so the average is (\\sigma / 2 \\epsilon_0) \\hat{n} , and the force per unit area is f = \\frac{1}{2 \\epsilon_0} \\sigma ^2 \\hat{n} \\label{2.51} \\tag{2.51} This amounts to an outward electrostatic pressure on the surface, tending to draw the conductor into the field, regardless of the sign of \\sigma . Expressing the pressure in terms of the field just outside the surface P = \\frac{\\epsilon_0}{2} E^2","title":"2.5.3: Surface Charge and the Force on a Conductor"},{"location":"ch2-5/#254-capacitors","text":"Suppose we have two conductors, and we put charge +Q on one and -Q on the other (Fig 2.51). Since V is constant over a conductor, we can speak unambiguously of the potential difference between them: V = V_{+} - V_{-} = - \\int_{(-)}^{(+)} \\vec{E} \\cdot \\dd{\\vec{l}} We don't know how the charge distributes itself over the two conductors, and calculating the field would be a nightmare, if their shapes are complicated, but this much we do know: \\vec{E} is proportional to Q . For \\vec{E} is given by Coulomb's law: \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho}{\\gr^2} \\hat{\\gr} \\dd{\\tau} so if you double \\rho , you double \\vec{E} . Wait a minute! How do we know that doubling Q (and also -Q) simply doubles \\rho ? Maybe the charge moves around into a completely different configuration, quadrupling \\rho in some places and halving it in others, just so the total charge on the conductor is doubled. The fact is that this concern is unwarranted - doubling Q does double \\rho everywhere; it doesn't shift charge around. The proof will come in Chapter 3; for now you'll have to trust me. Since \\vec{E} is proportional to Q, so also is V. The constant of proportionality is called the capacitance of the arrangement C \\equiv \\frac{Q}{V} \\label{2.53} \\tag{2.53} Capacitance is a purely geometrical quantity, determined by the sizes, shapes, and separation of the two conductors. In SI units, C is measured in farads (F); a farad is a coulomb-per-volt. Actually this turns out to be inconveniently large; more practical units are the microfarad ( 10^{-6} F ) and the picofarad ( 10^{-12} F ) Notice that V is, by definition, the potential of the positive conductor less that of the negative one; likewise, Q is the charge of the positive conductor. Accordingly, capacitance is an intrinsically positive quantity. By the way, you will occasionally hear someone speak of the capacitance of a single conductor. In this case the \"second conductor\" is an imaginary spherical shell of infinite radius surrounding the one conductor. It contributes nothing to the field, so the capacitance is given by \\eqref{2.53} , where V is the potential with infinity as the reference point.","title":"2.5.4: Capacitors"},{"location":"ch2-5/#example-211","text":"Find the capacitance of a parallel-plate capacitor consisting of two metal surfaces of area A held a distance d apart (Fig. 2.52) Solution If we put +Q on the top and -Q on the bottom, they will spread out uniformly over the two surfaces, provided the area is reasonably large and the separation small. The surface charge density, then, is \\sigma = Q / A on the top plate, and so the field, according to Ex. 2.6, is (1 / \\epsilon_0) Q / A . The potential difference between the plates is therefore V = \\frac{Q}{A \\epsilon_0} d and hence C = \\frac{A \\epsilon_0}{d} \\label{2.54} \\tag{2.54} If, for instance, the plates are square with sides 1 cm long, and they are held 1 mm apart, then the capacitance is 9 \\times 10^{-13} F","title":"Example 2.11"},{"location":"ch2-5/#example-212","text":"Find the capacitance of two concentric spherical metal shells, with radii a and b. Solution Place charge +Q on the inner sphere, and -Q on the outer one. The field between the spheres is \\vec{E} = \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q}{r^2} \\vec{r} so the potential difference between them is V = - \\int_{b}^{a} \\vec{E} \\cdot \\dd{\\vec{l}} = - \\frac{Q}{4 \\pi \\epsilon_0} \\int_{b}^a \\frac{1}{r^2} \\dd{r} = \\frac{Q}{4 \\pi \\epsilon_0} \\left( \\frac{1}{a} - \\frac{1}{b} \\right) As promised, V is proportional to Q; the capacitance is C = \\frac{Q}{V} = 4 \\pi \\epsilon_0 \\frac{ab}{(b - a)} To \"charge up\" a capacitor, you have to remove electrons from the positive plate and carry them to the negative plate. In doing so, you fight against the electric field, which is pulling them back toward the positive conductor and pushing them away from the negative one. How much work does it take, then, to charge the capacitor up to a final amount Q ? Suppose that at some intermediate stage in the process the charge on the positive plate is q , so that the potential difference is q / C . According to Eq. 2.38, the work you must do to transport the next piece of charge dq is \\dd{W} = \\left( \\frac{q}{C} \\right) \\dd{q} The total work necessary, then, to go from q = 0 to q = Q , is W = \\int_{0} ^Q \\left( \\frac{q}{C} \\dd{q} \\right) = \\frac{1}{2} \\frac{Q^2}{C} or, since Q = CV , W = \\frac{1}{2} C V^2 \\label{2.55} \\tag{2.55} where V is the final potential of the capacitor.","title":"Example 2.12"},{"location":"ch3-1/","text":"3.1: Laplace's Equation 3.1.1: Introduction The primary task of electrostatics is to find the electric field of a given stationary charge distribution. In principle, this purpose is accomplished by Coulomb's law, in the form of \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r'})}{\\gr ^2} \\hat{\\gr} \\dd{\\tau'} \\label{3.1} Unfortunately, integrals of this type can be difficult to calculate for any but the simplest charge configurations. Occasionally we can get around this by exploiting symmetry and using Gauss's law, but ordinarily the best strategy is first to calculate the potential , V, which is given by the somewhat more tractable V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r}')}{\\gr} \\dd{\\tau'} \\label{3.2} Still, even this integral is often too tough to handle analytically. Moreover, in problems involving conductors \\rho itself may not be known in advance; since charge is free to move around, the only thing we control directly is the total charge (or perhaps the potential) of each conductor. In such cases, it is fruitful to recast the problem in differential form, using Poisson's equation \\laplacian{V} = - \\frac{1}{\\epsilon_0} \\rho \\label{3.3} which, together with appropriate boundary conditions, is equivalent to \\eqref{3.2} . Very often, in fact, we are interested in finding the potential in a region where \\rho = 0 . (If \\rho = 0 everywhere, of course, then V = 0 , and there is nothing further to say - that's not what I mean. There may be plenty of charge elsewhere, but we're confining our attention to places where there is no charge.) In this case, Poisson's equation reduces to Laplace's equation \\laplacian{V} = 0 \\label{3.4} or, written out in Cartesian coordinates, \\frac{\\partial^2{V}}{\\partial{x^2}} + \\frac{\\partial^2 V}{\\partial{y^2}} + \\frac{\\partial^2{V}}{\\partial{z^2}} = 0 \\label{3.5} This formula is so fundamental to the subject that one might almost say electrostatics is the study of Laplace's equation. At the same time, it is a ubiquitous equation, appearing in such diverse branches of physics as gravitation and magnetism, the theory of heat, and the study of soap bubbles. In mathematics, it plays a major role in analytic function theory. To get a feel for Laplace's equation and its solutions (which are called harmonic functions ), we shall begin with the one- and two-dimensional versions, which are easier to picture, and illustrate all the essential properties of the three-dimensional case. 3.1.2: Laplace's Equation in One Dimension Suppose V depends on only one variable, x. Then Laplace's equation becomes \\frac{d^2 V}{dx^2} = 0 The general solution is V(x) = mx + b \\label{3.6} \\tag{3.6} the equation for a straight line. It contains two undetermined constants (m and b), as is appropriate for a second-order (ordinary) differential equation. They are fixed, in any particular case, by the boundary conditions of that problem. For instance, it might be specified that V = 4 at x = 1 and V = 0 at x = 5 . In that case, m = -1 and b = 5 , so V = - x + 5 (See Fig. 3.1) I want to call your attention to two features of this result; they may seem silly and obvious in one dimension, where I can write down the general solution explicitly, but the analogs in two and three dimensions are powerful and by no means obvious: V(x) is the average of V(x + a) and V(x - a) for any a: V(x) = \\frac{1}{2} [V(x + a) + V(x-a)] Laplace's equation is a kind of averaging instruction; it tells you to assign to the point x the average of the values to the left and to the right of x. Solutions to Laplace's equation are, in this sense, as boring as they could possibly be, and yet fit the end points properly. Laplace's equation tolerates no local maxima or minima ; extreme values of V must occur at the end points. Actually, this is a consequence of (1), for if there were a local maximum, V would be greater at that point than on either side, and therefore could not be the average. (Ordinarily, you expect the second derivative to be negative at a maximum and positive at a minimum. Since Laplace's equation requires, on the contrary, that the second derivative is zero, it seems reasonable that solutions should exhibit no extrema. However, this is not a proof, since there exist functions that have maxima and minima at points where the second derivative vanishes: x^4 for example, has such a minimum point at x=0 ). 3.1.3: Laplace's Equation in Two Dimensions If V depends on two variables, Laplace's equation becomes \\frac{\\partial^2{V}}{\\partial{x^2}} + \\frac{\\partial^2{V}}{\\partial{y^2}} = 0 This is no longer an ordinary differential equation (that is, one involving ordinary derivatives only); it is a partial differential equation. As a consequence, some of the simple rules you may be familiar with do not apply. For instance, the general solution to this equation doesn't contain just two arbitrary constants - or, for that matter, any finite number - despite the fact that it's a second order equation. Indeed, one cannot write down a \"general solution\" (at least, not in a closed form like \\eqref{3.6} ). Nevertheless, it is possible to deduce certain properties common to all solutions. It may help to have a physical example in mind. Picture a thin rubber sheet (or a soap film) stretched over some support. For definiteness, suppose you take a cardboard box, cut a wavy line all the way around, and remove the top part (Fig. 3.2). Now glue a tightly stretched rubber membrane over the box, so that it fits like a drum head (it won't be a flat drumhead, of course, unless you choose to cut the edges off straight). Now, if you lay out the coordinates (x, y) on the bottom of the box, the height V(x, y) of the sheet above the point (x, y) will satisfy Laplace's equation. (The one-dimensional analog would be a rubber band stretched between two points. Of course, it would form a straight line.) Actually, the equation satisfied by a rubber sheet is \\frac{\\partial}{\\partial{x}} \\left( g \\pdv{V}{y} \\right) + \\frac{\\partial}{\\partial{y}} \\left( g \\pdv{V}{y} \\right) = 0, \\\\ \\qquad \\text{ where } g = \\left[ 1 + \\left( \\pdv{V}{x} \\right)^2 + \\left( \\pdv{V}{y} \\right)^2 \\right]^{-1/2} Harmonic functions in two dimensions have the same properties we noted in one dimension: The value of V at a point (x, y) is the average of those around the point. More precisely, if you draw a circle of any radius R about the point (x, y), the average value of V on the circle is equal to the value at the center: V(x, y) = \\frac{1}{2 \\pi R} \\oint_{\\text{circle}} = V \\dd{l} (This, incidentally, suggests the method of relaxation, on which computer solutions to Laplace's equation are based: Starting with specified values for V at the boundary, and reasonable guesses for V on a grid of interior points, the first pass reassigns to each point the average of its nearest neighbors. The second pass repeats this process, using the corrected values, and so on. After a few iterations, the numbers begin to settle down, so that subsequent passes produce negligible changes, and a numerical solution to Laplace's equation, with the given boundary values, has been achieved.) V has no local maxima or minima; all extrema occur at the boundaries. (As before, this follows from (1)). Again, Laplace's equation picks the most featureless function possible, consistent with the boundary conditions: no hills, no valleys, just the smoothest conceivable surface. For instance, if you put a ping-pong ball on the stretched rubber sheet of Fig 3.2, it will roll over to one side and fall off - it will not find a \"pocket\" somewhere to settle into, for Laplace's equation allows no such dents in the surface. From a geometrical point of view, just as a straight line is the shortest distance between two points, so a harmonic function in two dimensions minimizes the surface area spanning the given boundary line. 3.1.4: Laplace's Equation in Three Dimensions In three dimensions I can neither provide you with an explicit solution (as in one dimension) nor offer a suggestive physical example to guide your intuition (as I did in two dimensions). Nevertheless, the same two properties remain true, and this time I will sketch a proof. For a proof that does not rely on Coulomb's law (only on Laplace's equation), see Prob. 3.37 The value of V at point r is the average value of V over a spherical surface of radius R centered at r : V(\\vec{r}) = \\frac{1}{4 \\pi R^2} \\oint_{\\text{sphere}} V \\dd{a} As a consequence, V can have no local maxima or minima; the extreme values of V must occur at the boundaries (For if V had a local maximum at r , then by the very nature of the maximum I could draw a sphere around r over which all the values of V - and a fortiori the average - would be less than at r .) Proof: V is a solution to the three-dimensional Laplace's equation. Then the value of V at point r is the average value of V over a spherical surface of radius R centered at r Let's begin by calculating the average potential over a spherical surface of radius R due to a single point charge q located outside the sphere. We may as well center the sphere at the origin and choose coordinates so that q lies on the z-axis (Fig 3.3). The potential at a point on the surface is V = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{\\gr} where \\gr ^2 = z^2 + R^2 - 2z R \\cos \\theta so V_{\\text{ave}} = \\frac{1}{4\\pi R^2} \\frac{1}{4 \\pi \\epsilon_0} \\int [z^2 + R^2 - 2 z R \\cos \\theta]^{-1/2} R^2 \\sin \\theta \\dd{\\theta} \\dd{\\phi} \\\\ = \\left. \\frac{q}{4 \\pi \\epsilon_0} \\frac{1}{2 z R} \\sqrt{z^2 + R^2 - 2 z R \\cos\\theta} \\right|_{0} ^{\\pi} \\\\ = \\frac{q}{4 \\pi \\epsilon_0} \\frac{1}{2zR} [(z + R) - (z-R)] = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{z} But this is precisely the potential due to q at the center of the sphere! By the superposition principle, the same goes for any collection of charges outside the sphere: their average potential over the sphere is equal to the net potential they produce at the center 3.1.5: Boundary Conditions and Uniqueness Theorems Laplace's equation does not by itself determine V; in addition, suitable boundary conditions must be supplied. This raises a delicate question: What are appropriate boundary conditions, sufficient to determine the answer and yet not so strong as to generate inconsistencies? The one-dimensional case is easy, for here the general solution V = mx + b contains two arbitrary constants, and we therefore require two boundary conditions. We might, for instance, specify the value of the function at each end, or we might give the value of the function and its derivative at one end, or the value at one end and the derivative at the other, and so on. But we cannot get away with just the value or just the derivative at one end - this is insufficient information. Nor would it do to specify the derivatives at both ends - this would be either redundant (if the two are equal) or inconsistent (if they are not). In two or three dimensions we are confronted by a partial differential equation, and it is not so obvious what would constitute acceptable boundary conditions. Is the shape of a taut rubber membrane, for instance, uniquely determined by the frame over which it is stretched, or, like a canning jar lid, can it snap from one stable configuration to another? The answer, as I think your intuition would suggest, that V is uniquely determined by its value at the boundary (canning jars evidently do not obey Laplace's equation). However, other boundary conditions can also be used (see Prob. 3.5). The proof that a proposed set of boundary conditions will suffice is usually presented in the form of a uniqueness theorem. There are many such theorems for electrostatics, all sharing the same basic format - I'll show you the two most useful ones: First Uniqueness Theorem: The solution to Laplace's equation in some volume \\mathscr{V} is uniquely determined if V is specified on the boundary surface \\mathscr{S} . In Fig. 3.5 I have drawn such a region and its boundary. (There could also be \"islands\" inside, so long as V is given on all their surfaces; also, the outer boundary could be at infinity, where V is ordinarily taken to be zero.) Proof : Suppose there were two solutions to Laplace's equation: \\laplacian{V_1} = 0 \\quad \\text{and} \\quad \\laplacian{V_2} = 0 both of which assume the specified value on the surface. I want to prove that they must be equal. The trick is to look at their difference : V_3 \\equiv V_1 - V_2 This obeys Laplace's equation (obviously) \\laplacian{V_3} = \\laplacian{V_1} - \\laplacian{V_2} = 0 and it takes the value zero on all boundaries (since V_1 and V_2 are equal there). But Laplace's equation allows no local maxima or minima - all extrema occur on the boundaries. So the maximum and minimum of V_3 are both zero. Therefore V_3 must be zero everywhere, and hence V_1 = V_2 Example 3.1 Show that the potential is constant inside an enclosure completely surrounded by conducting material, provided there is no charge within the enclosure. Can I have an equation in here? Solution The potential on the cavity wall is some constant V_0 (that's item (iv) in Sect. 2.5.1), so the potential inside is a function that satisfies Laplace's equation and has the constant value V_0 at the boundary. It doesn't take a genius to think of one solution to this problem: V = V_0 everywhere. The uniqueness theorem guarantees that this is the only solution. (It follows that the field inside an empty cavity is zero - the same result we found in Sect. 2.5.2 on rather different grounds.) The uniqueness theorem is a license to your imagination. It doesn't matter how you come by your solution; if (a) it satisfies Laplace's equation and (b) it has the correct value on the boundaries, then it's right . You'll see the power of this argument when we come to the method of images. Incidentally, it is easy to improve on the first uniqueness theorem: I assumed there was no charge inside the region in question, so the potential obeyed Laplace's equation, but we may as well throw in some charge (in which case V obeys Poisson's equation). Corollary: The potential in a volume \\mathscr{V} is uniquely determined if (a) the charge density throughout the region, and (b) the value of V on all boundaries, are specified The argument is the same, only this time \\laplacian{V_1} = -\\frac{1}{\\epsilon_0} \\rho, \\qquad \\laplacian{V_2} = - \\frac{1}{\\epsilon_0} \\rho so \\laplacian{V_3} = \\laplacian{V_1} - \\laplacian{V_2} = - \\frac{1}{\\epsilon_0} \\rho + \\frac{1}{\\epsilon_0} \\rho = 0 Once again the difference (V_3 \\equiv V_1 - V_2) satisfies Laplace's equation and has the value zero on all boundaries, so V_3 = 0 and hence V_1 = V_2 3.1.6: Conductors and the Second Uniqueness Theorem The simplest way to set the boundary conditions for an electrostatic problem is to specify the value of V on all surfaces surrounding the region of interest. And this situation often occurs in practice: In the laboratory, we have conductors connected to batteries, which maintain a given potential, or to ground , which is the experimentalist's word for V = 0 . However, there are other circumstances in which we do not know the potential at the boundary, but rather the charges on various conducting surfaces. Suppose I put Q_a on the first conductor, Q_b on the second conductor, and so on - I'm not telling you how the charge distributes itself over each conducting surface, because as soon as I put it on, it moves around in a way I do not control. And for good measure, let's say there is some specified charge density \\rho in the region between the conductors. Is the electric field now uniquely determined? Or are there perhaps a number of different ways the charges could arrange themselves on their respective conductors, each leading to a different field? Second uniqueness theorem : In a volume \\mathscr{V} surrounded by conductors and containing a specified charge density \\rho , the electric field is uniquely determined if the total charge on each conductor is given (Fig. 3.6). (The region as a whole can be bounded by another conductor, or else unbounded.) Proof : Suppose there are two fields satisfying the conditions of the problem. Both obey Gauss's law in differential form in the space betwen the conductors: \\div{\\vec{E_1}} = \\frac{1}{\\epsilon_0} \\rho, \\qquad \\div{\\vec{E_2}} = \\frac{1}{\\epsilon_0} \\rho And both obey Gauss's law in integral form for a Gaussian surface enclosing each conductor \\oint_{i_{th} \\text{ conducting surface}} \\vec{E_1} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_i \\quad \\text{ and } \\\\ \\oint_{i_{th} \\text{ conducting surface}} \\vec{E_2} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_i \\quad \\text{ and } \\\\ Likewise, for the outer boundary (whether this is just inside an enclosing conductor at infinity), \\oint_{\\text{outer boundary}} \\vec{E_1} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{tot} \\\\ \\oint_{\\text{outer boundary}} \\vec{E_2} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{tot} As before, we examine the difference \\vec{E_3} \\equiv \\vec{E_1} - \\vec{E_2} which obeys \\div{\\vec{E_3}} = 0 \\label{3.7} \\tag{3.7} in the region between the conductors, and \\oint \\vec{E_3} \\cdot \\dd{\\vec{a}} = 0 \\label{3.8} \\tag{3.8} over each boundary surface. Now there is one final piece of information we must exploit: Although we do not know how the charge Q_i distributes itself over the _i_th conductor, we do know that each conductor is an equipotential, and hence V_3 is a constant (not necessarily the same constant) over each conducting surface. (It need not be zero, for the potentials V_1 and V_2 may not be equal - all we know for sure is that both are constant over any given conductor.) Next comes a trick. Invoking the product rule \\div{(V_3 \\vec{E_3})} = V_3 (\\div{\\vec{E_3}}) + \\vec{E_3} \\cdot (\\grad{V_3}) = - (E_3)^2 Here I have used \\eqref{3.7} and \\vec{E_3} = - \\grad{V_3} . Integrating this over \\mathscr{V} and applying the divergence theorem to the left side: \\int_{\\mathscr{V}} \\div{(V_3 \\vec{E_3})} \\dd{\\tau} = \\oint_{S} V_3 \\vec{E_3} \\cdot \\dd{\\vec{a}} = - \\int_{\\mathscr{V}} (E_3)^2 \\dd{\\tau} The surface integral covers all boundaries of the region in question - the conductors and outer boundary. Now V_3 is a constant over each surface (if the outer boundary is invinity, V_3 = 0 there), so it comes outside each integral, and what remains is zero, according to \\eqref{3.8} . Therefore \\int_{\\mathscr{V}}(E_3)^2 \\dd{\\tau} = 0 The integrand is never negative, so the only way the integral can vanish is if E_3 = 0 everywhere. Consequently, \\vec{E_1} = \\vec{E_2} and the theorem is proved. This proof was not easy, and there is a real danger that the theorem itself will seem more plausible to you than the proof. In case you think the second uniqueness theorem is \"obvious,\" consider this example of Purcell's: Figure 3.7 shows a simple electrostatic configuration, consisting of four conductors with charges \\pm Q , situated so that the plusses are near the minuses. It all looks very comfortable. Now, what happens if we join them in pairs, by tiny wires, as indicated in Fig. 3.8? Since the positive charges are very near negative charges (which is where they like to be) you might well guess that nothing will happen - the configuration looks stable. Well, that sounds reasonable, but it's wrong. The configuration in Fig. 3.8 is impossible. For there are now effectively two conductors, and the total charge on each is zero. One possible way to distribute zero charge over these conductors is to have no accumulation of charge anywhere, and hence zero field everywhere (Fig. 3.9). By the second uniqueness theorem, this must be the solution: The charge will flow down the tiny wires, canceling itself off.","title":"3.1 - Laplace's Equation"},{"location":"ch3-1/#31-laplaces-equation","text":"","title":"3.1: Laplace's Equation"},{"location":"ch3-1/#311-introduction","text":"The primary task of electrostatics is to find the electric field of a given stationary charge distribution. In principle, this purpose is accomplished by Coulomb's law, in the form of \\vec{E}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r'})}{\\gr ^2} \\hat{\\gr} \\dd{\\tau'} \\label{3.1} Unfortunately, integrals of this type can be difficult to calculate for any but the simplest charge configurations. Occasionally we can get around this by exploiting symmetry and using Gauss's law, but ordinarily the best strategy is first to calculate the potential , V, which is given by the somewhat more tractable V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\rho(\\vec{r}')}{\\gr} \\dd{\\tau'} \\label{3.2} Still, even this integral is often too tough to handle analytically. Moreover, in problems involving conductors \\rho itself may not be known in advance; since charge is free to move around, the only thing we control directly is the total charge (or perhaps the potential) of each conductor. In such cases, it is fruitful to recast the problem in differential form, using Poisson's equation \\laplacian{V} = - \\frac{1}{\\epsilon_0} \\rho \\label{3.3} which, together with appropriate boundary conditions, is equivalent to \\eqref{3.2} . Very often, in fact, we are interested in finding the potential in a region where \\rho = 0 . (If \\rho = 0 everywhere, of course, then V = 0 , and there is nothing further to say - that's not what I mean. There may be plenty of charge elsewhere, but we're confining our attention to places where there is no charge.) In this case, Poisson's equation reduces to Laplace's equation \\laplacian{V} = 0 \\label{3.4} or, written out in Cartesian coordinates, \\frac{\\partial^2{V}}{\\partial{x^2}} + \\frac{\\partial^2 V}{\\partial{y^2}} + \\frac{\\partial^2{V}}{\\partial{z^2}} = 0 \\label{3.5} This formula is so fundamental to the subject that one might almost say electrostatics is the study of Laplace's equation. At the same time, it is a ubiquitous equation, appearing in such diverse branches of physics as gravitation and magnetism, the theory of heat, and the study of soap bubbles. In mathematics, it plays a major role in analytic function theory. To get a feel for Laplace's equation and its solutions (which are called harmonic functions ), we shall begin with the one- and two-dimensional versions, which are easier to picture, and illustrate all the essential properties of the three-dimensional case.","title":"3.1.1: Introduction"},{"location":"ch3-1/#312-laplaces-equation-in-one-dimension","text":"Suppose V depends on only one variable, x. Then Laplace's equation becomes \\frac{d^2 V}{dx^2} = 0 The general solution is V(x) = mx + b \\label{3.6} \\tag{3.6} the equation for a straight line. It contains two undetermined constants (m and b), as is appropriate for a second-order (ordinary) differential equation. They are fixed, in any particular case, by the boundary conditions of that problem. For instance, it might be specified that V = 4 at x = 1 and V = 0 at x = 5 . In that case, m = -1 and b = 5 , so V = - x + 5 (See Fig. 3.1) I want to call your attention to two features of this result; they may seem silly and obvious in one dimension, where I can write down the general solution explicitly, but the analogs in two and three dimensions are powerful and by no means obvious: V(x) is the average of V(x + a) and V(x - a) for any a: V(x) = \\frac{1}{2} [V(x + a) + V(x-a)] Laplace's equation is a kind of averaging instruction; it tells you to assign to the point x the average of the values to the left and to the right of x. Solutions to Laplace's equation are, in this sense, as boring as they could possibly be, and yet fit the end points properly. Laplace's equation tolerates no local maxima or minima ; extreme values of V must occur at the end points. Actually, this is a consequence of (1), for if there were a local maximum, V would be greater at that point than on either side, and therefore could not be the average. (Ordinarily, you expect the second derivative to be negative at a maximum and positive at a minimum. Since Laplace's equation requires, on the contrary, that the second derivative is zero, it seems reasonable that solutions should exhibit no extrema. However, this is not a proof, since there exist functions that have maxima and minima at points where the second derivative vanishes: x^4 for example, has such a minimum point at x=0 ).","title":"3.1.2: Laplace's Equation in One Dimension"},{"location":"ch3-1/#313-laplaces-equation-in-two-dimensions","text":"If V depends on two variables, Laplace's equation becomes \\frac{\\partial^2{V}}{\\partial{x^2}} + \\frac{\\partial^2{V}}{\\partial{y^2}} = 0 This is no longer an ordinary differential equation (that is, one involving ordinary derivatives only); it is a partial differential equation. As a consequence, some of the simple rules you may be familiar with do not apply. For instance, the general solution to this equation doesn't contain just two arbitrary constants - or, for that matter, any finite number - despite the fact that it's a second order equation. Indeed, one cannot write down a \"general solution\" (at least, not in a closed form like \\eqref{3.6} ). Nevertheless, it is possible to deduce certain properties common to all solutions. It may help to have a physical example in mind. Picture a thin rubber sheet (or a soap film) stretched over some support. For definiteness, suppose you take a cardboard box, cut a wavy line all the way around, and remove the top part (Fig. 3.2). Now glue a tightly stretched rubber membrane over the box, so that it fits like a drum head (it won't be a flat drumhead, of course, unless you choose to cut the edges off straight). Now, if you lay out the coordinates (x, y) on the bottom of the box, the height V(x, y) of the sheet above the point (x, y) will satisfy Laplace's equation. (The one-dimensional analog would be a rubber band stretched between two points. Of course, it would form a straight line.) Actually, the equation satisfied by a rubber sheet is \\frac{\\partial}{\\partial{x}} \\left( g \\pdv{V}{y} \\right) + \\frac{\\partial}{\\partial{y}} \\left( g \\pdv{V}{y} \\right) = 0, \\\\ \\qquad \\text{ where } g = \\left[ 1 + \\left( \\pdv{V}{x} \\right)^2 + \\left( \\pdv{V}{y} \\right)^2 \\right]^{-1/2} Harmonic functions in two dimensions have the same properties we noted in one dimension: The value of V at a point (x, y) is the average of those around the point. More precisely, if you draw a circle of any radius R about the point (x, y), the average value of V on the circle is equal to the value at the center: V(x, y) = \\frac{1}{2 \\pi R} \\oint_{\\text{circle}} = V \\dd{l} (This, incidentally, suggests the method of relaxation, on which computer solutions to Laplace's equation are based: Starting with specified values for V at the boundary, and reasonable guesses for V on a grid of interior points, the first pass reassigns to each point the average of its nearest neighbors. The second pass repeats this process, using the corrected values, and so on. After a few iterations, the numbers begin to settle down, so that subsequent passes produce negligible changes, and a numerical solution to Laplace's equation, with the given boundary values, has been achieved.) V has no local maxima or minima; all extrema occur at the boundaries. (As before, this follows from (1)). Again, Laplace's equation picks the most featureless function possible, consistent with the boundary conditions: no hills, no valleys, just the smoothest conceivable surface. For instance, if you put a ping-pong ball on the stretched rubber sheet of Fig 3.2, it will roll over to one side and fall off - it will not find a \"pocket\" somewhere to settle into, for Laplace's equation allows no such dents in the surface. From a geometrical point of view, just as a straight line is the shortest distance between two points, so a harmonic function in two dimensions minimizes the surface area spanning the given boundary line.","title":"3.1.3: Laplace's Equation in Two Dimensions"},{"location":"ch3-1/#314-laplaces-equation-in-three-dimensions","text":"In three dimensions I can neither provide you with an explicit solution (as in one dimension) nor offer a suggestive physical example to guide your intuition (as I did in two dimensions). Nevertheless, the same two properties remain true, and this time I will sketch a proof. For a proof that does not rely on Coulomb's law (only on Laplace's equation), see Prob. 3.37 The value of V at point r is the average value of V over a spherical surface of radius R centered at r : V(\\vec{r}) = \\frac{1}{4 \\pi R^2} \\oint_{\\text{sphere}} V \\dd{a} As a consequence, V can have no local maxima or minima; the extreme values of V must occur at the boundaries (For if V had a local maximum at r , then by the very nature of the maximum I could draw a sphere around r over which all the values of V - and a fortiori the average - would be less than at r .) Proof: V is a solution to the three-dimensional Laplace's equation. Then the value of V at point r is the average value of V over a spherical surface of radius R centered at r Let's begin by calculating the average potential over a spherical surface of radius R due to a single point charge q located outside the sphere. We may as well center the sphere at the origin and choose coordinates so that q lies on the z-axis (Fig 3.3). The potential at a point on the surface is V = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{\\gr} where \\gr ^2 = z^2 + R^2 - 2z R \\cos \\theta so V_{\\text{ave}} = \\frac{1}{4\\pi R^2} \\frac{1}{4 \\pi \\epsilon_0} \\int [z^2 + R^2 - 2 z R \\cos \\theta]^{-1/2} R^2 \\sin \\theta \\dd{\\theta} \\dd{\\phi} \\\\ = \\left. \\frac{q}{4 \\pi \\epsilon_0} \\frac{1}{2 z R} \\sqrt{z^2 + R^2 - 2 z R \\cos\\theta} \\right|_{0} ^{\\pi} \\\\ = \\frac{q}{4 \\pi \\epsilon_0} \\frac{1}{2zR} [(z + R) - (z-R)] = \\frac{1}{4 \\pi \\epsilon_0} \\frac{q}{z} But this is precisely the potential due to q at the center of the sphere! By the superposition principle, the same goes for any collection of charges outside the sphere: their average potential over the sphere is equal to the net potential they produce at the center","title":"3.1.4: Laplace's Equation in Three Dimensions"},{"location":"ch3-1/#315-boundary-conditions-and-uniqueness-theorems","text":"Laplace's equation does not by itself determine V; in addition, suitable boundary conditions must be supplied. This raises a delicate question: What are appropriate boundary conditions, sufficient to determine the answer and yet not so strong as to generate inconsistencies? The one-dimensional case is easy, for here the general solution V = mx + b contains two arbitrary constants, and we therefore require two boundary conditions. We might, for instance, specify the value of the function at each end, or we might give the value of the function and its derivative at one end, or the value at one end and the derivative at the other, and so on. But we cannot get away with just the value or just the derivative at one end - this is insufficient information. Nor would it do to specify the derivatives at both ends - this would be either redundant (if the two are equal) or inconsistent (if they are not). In two or three dimensions we are confronted by a partial differential equation, and it is not so obvious what would constitute acceptable boundary conditions. Is the shape of a taut rubber membrane, for instance, uniquely determined by the frame over which it is stretched, or, like a canning jar lid, can it snap from one stable configuration to another? The answer, as I think your intuition would suggest, that V is uniquely determined by its value at the boundary (canning jars evidently do not obey Laplace's equation). However, other boundary conditions can also be used (see Prob. 3.5). The proof that a proposed set of boundary conditions will suffice is usually presented in the form of a uniqueness theorem. There are many such theorems for electrostatics, all sharing the same basic format - I'll show you the two most useful ones: First Uniqueness Theorem: The solution to Laplace's equation in some volume \\mathscr{V} is uniquely determined if V is specified on the boundary surface \\mathscr{S} . In Fig. 3.5 I have drawn such a region and its boundary. (There could also be \"islands\" inside, so long as V is given on all their surfaces; also, the outer boundary could be at infinity, where V is ordinarily taken to be zero.) Proof : Suppose there were two solutions to Laplace's equation: \\laplacian{V_1} = 0 \\quad \\text{and} \\quad \\laplacian{V_2} = 0 both of which assume the specified value on the surface. I want to prove that they must be equal. The trick is to look at their difference : V_3 \\equiv V_1 - V_2 This obeys Laplace's equation (obviously) \\laplacian{V_3} = \\laplacian{V_1} - \\laplacian{V_2} = 0 and it takes the value zero on all boundaries (since V_1 and V_2 are equal there). But Laplace's equation allows no local maxima or minima - all extrema occur on the boundaries. So the maximum and minimum of V_3 are both zero. Therefore V_3 must be zero everywhere, and hence V_1 = V_2","title":"3.1.5: Boundary Conditions and Uniqueness Theorems"},{"location":"ch3-1/#example-31","text":"Show that the potential is constant inside an enclosure completely surrounded by conducting material, provided there is no charge within the enclosure. Can I have an equation in here? Solution The potential on the cavity wall is some constant V_0 (that's item (iv) in Sect. 2.5.1), so the potential inside is a function that satisfies Laplace's equation and has the constant value V_0 at the boundary. It doesn't take a genius to think of one solution to this problem: V = V_0 everywhere. The uniqueness theorem guarantees that this is the only solution. (It follows that the field inside an empty cavity is zero - the same result we found in Sect. 2.5.2 on rather different grounds.) The uniqueness theorem is a license to your imagination. It doesn't matter how you come by your solution; if (a) it satisfies Laplace's equation and (b) it has the correct value on the boundaries, then it's right . You'll see the power of this argument when we come to the method of images. Incidentally, it is easy to improve on the first uniqueness theorem: I assumed there was no charge inside the region in question, so the potential obeyed Laplace's equation, but we may as well throw in some charge (in which case V obeys Poisson's equation). Corollary: The potential in a volume \\mathscr{V} is uniquely determined if (a) the charge density throughout the region, and (b) the value of V on all boundaries, are specified The argument is the same, only this time \\laplacian{V_1} = -\\frac{1}{\\epsilon_0} \\rho, \\qquad \\laplacian{V_2} = - \\frac{1}{\\epsilon_0} \\rho so \\laplacian{V_3} = \\laplacian{V_1} - \\laplacian{V_2} = - \\frac{1}{\\epsilon_0} \\rho + \\frac{1}{\\epsilon_0} \\rho = 0 Once again the difference (V_3 \\equiv V_1 - V_2) satisfies Laplace's equation and has the value zero on all boundaries, so V_3 = 0 and hence V_1 = V_2","title":"Example 3.1"},{"location":"ch3-1/#316-conductors-and-the-second-uniqueness-theorem","text":"The simplest way to set the boundary conditions for an electrostatic problem is to specify the value of V on all surfaces surrounding the region of interest. And this situation often occurs in practice: In the laboratory, we have conductors connected to batteries, which maintain a given potential, or to ground , which is the experimentalist's word for V = 0 . However, there are other circumstances in which we do not know the potential at the boundary, but rather the charges on various conducting surfaces. Suppose I put Q_a on the first conductor, Q_b on the second conductor, and so on - I'm not telling you how the charge distributes itself over each conducting surface, because as soon as I put it on, it moves around in a way I do not control. And for good measure, let's say there is some specified charge density \\rho in the region between the conductors. Is the electric field now uniquely determined? Or are there perhaps a number of different ways the charges could arrange themselves on their respective conductors, each leading to a different field? Second uniqueness theorem : In a volume \\mathscr{V} surrounded by conductors and containing a specified charge density \\rho , the electric field is uniquely determined if the total charge on each conductor is given (Fig. 3.6). (The region as a whole can be bounded by another conductor, or else unbounded.) Proof : Suppose there are two fields satisfying the conditions of the problem. Both obey Gauss's law in differential form in the space betwen the conductors: \\div{\\vec{E_1}} = \\frac{1}{\\epsilon_0} \\rho, \\qquad \\div{\\vec{E_2}} = \\frac{1}{\\epsilon_0} \\rho And both obey Gauss's law in integral form for a Gaussian surface enclosing each conductor \\oint_{i_{th} \\text{ conducting surface}} \\vec{E_1} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_i \\quad \\text{ and } \\\\ \\oint_{i_{th} \\text{ conducting surface}} \\vec{E_2} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_i \\quad \\text{ and } \\\\ Likewise, for the outer boundary (whether this is just inside an enclosing conductor at infinity), \\oint_{\\text{outer boundary}} \\vec{E_1} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{tot} \\\\ \\oint_{\\text{outer boundary}} \\vec{E_2} \\cdot \\dd{\\vec{a}} = \\frac{1}{\\epsilon_0} Q_{tot} As before, we examine the difference \\vec{E_3} \\equiv \\vec{E_1} - \\vec{E_2} which obeys \\div{\\vec{E_3}} = 0 \\label{3.7} \\tag{3.7} in the region between the conductors, and \\oint \\vec{E_3} \\cdot \\dd{\\vec{a}} = 0 \\label{3.8} \\tag{3.8} over each boundary surface. Now there is one final piece of information we must exploit: Although we do not know how the charge Q_i distributes itself over the _i_th conductor, we do know that each conductor is an equipotential, and hence V_3 is a constant (not necessarily the same constant) over each conducting surface. (It need not be zero, for the potentials V_1 and V_2 may not be equal - all we know for sure is that both are constant over any given conductor.) Next comes a trick. Invoking the product rule \\div{(V_3 \\vec{E_3})} = V_3 (\\div{\\vec{E_3}}) + \\vec{E_3} \\cdot (\\grad{V_3}) = - (E_3)^2 Here I have used \\eqref{3.7} and \\vec{E_3} = - \\grad{V_3} . Integrating this over \\mathscr{V} and applying the divergence theorem to the left side: \\int_{\\mathscr{V}} \\div{(V_3 \\vec{E_3})} \\dd{\\tau} = \\oint_{S} V_3 \\vec{E_3} \\cdot \\dd{\\vec{a}} = - \\int_{\\mathscr{V}} (E_3)^2 \\dd{\\tau} The surface integral covers all boundaries of the region in question - the conductors and outer boundary. Now V_3 is a constant over each surface (if the outer boundary is invinity, V_3 = 0 there), so it comes outside each integral, and what remains is zero, according to \\eqref{3.8} . Therefore \\int_{\\mathscr{V}}(E_3)^2 \\dd{\\tau} = 0 The integrand is never negative, so the only way the integral can vanish is if E_3 = 0 everywhere. Consequently, \\vec{E_1} = \\vec{E_2} and the theorem is proved. This proof was not easy, and there is a real danger that the theorem itself will seem more plausible to you than the proof. In case you think the second uniqueness theorem is \"obvious,\" consider this example of Purcell's: Figure 3.7 shows a simple electrostatic configuration, consisting of four conductors with charges \\pm Q , situated so that the plusses are near the minuses. It all looks very comfortable. Now, what happens if we join them in pairs, by tiny wires, as indicated in Fig. 3.8? Since the positive charges are very near negative charges (which is where they like to be) you might well guess that nothing will happen - the configuration looks stable. Well, that sounds reasonable, but it's wrong. The configuration in Fig. 3.8 is impossible. For there are now effectively two conductors, and the total charge on each is zero. One possible way to distribute zero charge over these conductors is to have no accumulation of charge anywhere, and hence zero field everywhere (Fig. 3.9). By the second uniqueness theorem, this must be the solution: The charge will flow down the tiny wires, canceling itself off.","title":"3.1.6: Conductors and the Second Uniqueness Theorem"},{"location":"ch3-2/","text":"3.2: The Method of Images 3.1.1: The Classic Image Problem Suppose a point charge q is held a distance d above an infinite grounded conducting plane (Fig. 3.10). Question : what is the potential in the region above the plane? It's not just (1/4 \\pi \\epsilon_0) q / \\gr , for q will induce a certain amount of negative charge on the nearby surface of the conductor; the total potential is due in part to q directly, and in part to this induced charge. But how can we possibly calculate the potential, when we don't know how much charge is induced or how it is distributed? From a mathematical point of view, our problem is to solve Poisson's equation in the region z > 0 , with a single point charge q at (0, 0, d) , subject to the boundary conditions V = 0 when z = 0 (since the conducting plane is grounded) V \\rightarrow 0 far from the charge (that is, for x^2 + y^2 + z^2 \\gg d^2 The first uniqueness theorem (actually, its corollary) guarantees that there is only one function that meets these requirements. If by trick or clever guess we can discover such a function, it's got to be the answer. Trick: Forget about the actual problem; we're going to study a completely different situation. This new configuration consists of two point charges, +q at (0, 0, d) and -q at (0, 0, -d) , and no conducting plane (Fig. 3.11). For this configuration I can easily write down the potential: V(x, y, z) = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{q}{\\sqrt{x^2 + y^2 + (z - d)^2 }} - \\frac{q}{\\sqrt{x^2 + y^2 + (z + d)^2}} \\right] \\label{3.9} \\tag{3.9} It follows that V = 0 when z = 0 V \\rightarrow 0 for x^2 + y^2 + z^2 \\gg d^2 and the only charge in the region z > 0 is the point charge +q at (0, 0, d) . But these are precisely the conditions of the original problem! Evidently the second configuration happens to produce exactly the same potential as the first configuration, in the \"upper\" region z \\geq 0 . (The \"lower\" region, z < 0 , is completely different, but who cares? The upper part is all we need.) Conclusion : The potential of a point charge above an infinite grounded conductor is given by \\eqref{3.9} , for z > 0 . Notice the crucial role played by the uniqueness theorem in this argument: without it, no one would believe this solution, since it was obtained for a completely different charge distribution. But the uniqueness theorem certifies it: If it satisfies Poisson's equation in the region of interest, and assumes the correct value at the boundaries, then it must be right. 3.2.2: Induced Surface Charge Now that we know the potential, it is a straightforward matter to compute the surface charge \\sigma induced on the conductor. According to Eq. 2.49, \\sigma = - \\epsilon_0 \\pdv{V}{n} where \\partial V / \\partial n is the normal derivative of V at the surface. In this case the normal direction is the z direction, so \\sigma = \\left. - \\epsilon_0 \\pdv{V}{z} \\right|_{z = 0} From Eq. 3.9 \\pdv{V}{z} = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{-q (z - d)}{[x^2 + y^2 + (z - d)^2]^{3/2}} + \\frac{q(z + d)}{[x^2 + y^2 + (z + d)^2 ]^{3/2}} \\right] so \\sigma(x, y) = \\frac{-qd}{2 \\pi (x^2 + y^2 + d^2)^{3/2}} \\label{3.10} \\tag{3.10} As expected, the induced charge is negative (assuming q is positive) and greatest at x = y = 0 . While we're at it, let's compute the total induced charge Q = \\int \\sigma \\dd{a} This integral, over the xy plane, could be done in Cartesian coordinates, with \\dd{a} = \\dd{x} \\dd{y} , but it's easier to use polar coordinates (r, \\phi) , with r^2 = x^2 + y^2 and \\dd{a} = r \\dd{r} \\dd{\\phi} . Then \\sigma(r) = \\frac{-qd}{2 \\pi (r^2 + d^2)^{3/2}} and Q = \\int_{0} ^{2\\pi} \\int_{0} ^{\\infty} \\frac{-qd}{2 \\pi (r^2 + d^2)^{3/2}} r \\dd{r} \\dd{\\phi} = \\left. \\frac{qd}{\\sqrt{r^2 + d^2}} \\right|_{0} ^{\\infty} = -q \\label{3.11} \\tag{3.11} The total charge induced on the plane is -q , as (with benefit of hindsight) you can perhaps convince yourself it had to be. 3.2.3: Force and Energy The charge q is attracted toward the plane, because of the negative induced charge. Let's calculate the force of attraction. Since the potential in the vicinity of q is the same as in the analog problem (the one with +q and -q but no conductor), so also is the field and, therefore, the force \\vec{F} = -\\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{(2d)^2} \\hat{z} \\label{3.12} \\tag{3.12} Beware : It is easy to get carried away, and assume that everything is the same in the two problems. Energy, however, is not the same. With the two point charges and no conductor, Eq. 2.42 gives W = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{2d} But for a single charge and conducting plane, the energy is half this W = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{4d} \\label{3.14} \\tag{3.14} Why half? Think of the energy stored in the fields (Eq. 2.45): W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} In the first case, both the upper region (z > 0) and the lower region (z < 0) contribute, and by symmetry they contribute equally. But in the second case, only the upper region contains a nonzero field, and hence the energy is half as great. Of course, one could also determine the energy by calculating the work required to bring q in from infinity. The force required (to oppose the electrical force in \\eqref{3.12} is (1 / 4 \\pi \\epsilon_0)(q^2/4z^2) \\hat{z} , so \\begin{align} W & = & \\int _{\\infty} ^{d} \\vec{F} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{\\infty} ^d \\frac{q^2}{4z^2} \\dd{z} \\\\ & = & \\frac{1}{4 \\pi \\epsilon_0} \\left. \\left( - \\frac{q^2}{4z} \\right) \\right|_{\\infty} ^d = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{4d} \\end{align} As I move q toward the conductor, I do work only on q. It is true that induced charge is moving in over the conductor, but this costs me nothing, since the whole conductor is at potential zero. By contrast, if I simultaneously bring in two point charges (with no conductor), I do work on both of them, and the total is (again) twice as great. 3.2.4: Other Image Problems The method just described is not limited to a single point charge; any stationary charge distribution near a grounded conducting plane can be treated in the same way, by introducing its mirror image - hence the name method of images. (Remember that the image charges have the opposite sign; this is what guarantees that the xy plane will be at potential zero.) There are also some exotic problems that can be handled in similar fashion; the nicest of these is the following. Example 3.2 A point charge q is situated a distance a from the center of a grounded conducting sphere of radius R (Fig. 3.12). Find the potential outside the sphere Solution Examine the completely different configuration, consisting of the point charge q together with another point charge q' = - \\frac{R}{a} q \\label{3.15} \\tag{3.15} placed a distance b = \\frac{R^2}{a} \\label{3.16} \\tag{3.16} to the right of the center of the sphere (Fig 3.13). No conductor, now - just the two point charges. The potential of this configuration is V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{\\gr} + \\frac{q'}{\\gr '} \\right) \\label{3.17} \\tag{3.17} where \\gr and \\gr' are the distances from q and q' , respectively. Now, it happens (see Prob. 3.8) that this potential vanishes at all points on the sphere, and therefore fits the boundary conditions for our original problem, in the exterior region. Conclusion : \\eqref{3.17} is the potential of a point charge near a grounded conducting sphere. (Notice that b is less than R , so the \"image\" charge q' is safely inside the sphere - you cannot put image charges in the region where you are calculating V ; that would change \\rho , and you'd be solving Poisson's equation with the wrong source.) In particular, the force of attraction between the charge and the sphere is F = \\frac{1}{4\\pi \\epsilon_0} \\frac{q q'}{(a - b)^2} = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2 R a}{(a^2 - R^2)^2} \\label{3.18} \\tag{3.18} The method of images is delightfully simple... when it works. But it is as much an art as a science, for you must somehow think up just the right \"auxiliary\" configuration, and for most shapes this is forbiddingly complicated, if not impossible.","title":"3.2 - The Method of Images"},{"location":"ch3-2/#32-the-method-of-images","text":"","title":"3.2: The Method of Images"},{"location":"ch3-2/#311-the-classic-image-problem","text":"Suppose a point charge q is held a distance d above an infinite grounded conducting plane (Fig. 3.10). Question : what is the potential in the region above the plane? It's not just (1/4 \\pi \\epsilon_0) q / \\gr , for q will induce a certain amount of negative charge on the nearby surface of the conductor; the total potential is due in part to q directly, and in part to this induced charge. But how can we possibly calculate the potential, when we don't know how much charge is induced or how it is distributed? From a mathematical point of view, our problem is to solve Poisson's equation in the region z > 0 , with a single point charge q at (0, 0, d) , subject to the boundary conditions V = 0 when z = 0 (since the conducting plane is grounded) V \\rightarrow 0 far from the charge (that is, for x^2 + y^2 + z^2 \\gg d^2 The first uniqueness theorem (actually, its corollary) guarantees that there is only one function that meets these requirements. If by trick or clever guess we can discover such a function, it's got to be the answer. Trick: Forget about the actual problem; we're going to study a completely different situation. This new configuration consists of two point charges, +q at (0, 0, d) and -q at (0, 0, -d) , and no conducting plane (Fig. 3.11). For this configuration I can easily write down the potential: V(x, y, z) = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{q}{\\sqrt{x^2 + y^2 + (z - d)^2 }} - \\frac{q}{\\sqrt{x^2 + y^2 + (z + d)^2}} \\right] \\label{3.9} \\tag{3.9} It follows that V = 0 when z = 0 V \\rightarrow 0 for x^2 + y^2 + z^2 \\gg d^2 and the only charge in the region z > 0 is the point charge +q at (0, 0, d) . But these are precisely the conditions of the original problem! Evidently the second configuration happens to produce exactly the same potential as the first configuration, in the \"upper\" region z \\geq 0 . (The \"lower\" region, z < 0 , is completely different, but who cares? The upper part is all we need.) Conclusion : The potential of a point charge above an infinite grounded conductor is given by \\eqref{3.9} , for z > 0 . Notice the crucial role played by the uniqueness theorem in this argument: without it, no one would believe this solution, since it was obtained for a completely different charge distribution. But the uniqueness theorem certifies it: If it satisfies Poisson's equation in the region of interest, and assumes the correct value at the boundaries, then it must be right.","title":"3.1.1: The Classic Image Problem"},{"location":"ch3-2/#322-induced-surface-charge","text":"Now that we know the potential, it is a straightforward matter to compute the surface charge \\sigma induced on the conductor. According to Eq. 2.49, \\sigma = - \\epsilon_0 \\pdv{V}{n} where \\partial V / \\partial n is the normal derivative of V at the surface. In this case the normal direction is the z direction, so \\sigma = \\left. - \\epsilon_0 \\pdv{V}{z} \\right|_{z = 0} From Eq. 3.9 \\pdv{V}{z} = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{-q (z - d)}{[x^2 + y^2 + (z - d)^2]^{3/2}} + \\frac{q(z + d)}{[x^2 + y^2 + (z + d)^2 ]^{3/2}} \\right] so \\sigma(x, y) = \\frac{-qd}{2 \\pi (x^2 + y^2 + d^2)^{3/2}} \\label{3.10} \\tag{3.10} As expected, the induced charge is negative (assuming q is positive) and greatest at x = y = 0 . While we're at it, let's compute the total induced charge Q = \\int \\sigma \\dd{a} This integral, over the xy plane, could be done in Cartesian coordinates, with \\dd{a} = \\dd{x} \\dd{y} , but it's easier to use polar coordinates (r, \\phi) , with r^2 = x^2 + y^2 and \\dd{a} = r \\dd{r} \\dd{\\phi} . Then \\sigma(r) = \\frac{-qd}{2 \\pi (r^2 + d^2)^{3/2}} and Q = \\int_{0} ^{2\\pi} \\int_{0} ^{\\infty} \\frac{-qd}{2 \\pi (r^2 + d^2)^{3/2}} r \\dd{r} \\dd{\\phi} = \\left. \\frac{qd}{\\sqrt{r^2 + d^2}} \\right|_{0} ^{\\infty} = -q \\label{3.11} \\tag{3.11} The total charge induced on the plane is -q , as (with benefit of hindsight) you can perhaps convince yourself it had to be.","title":"3.2.2: Induced Surface Charge"},{"location":"ch3-2/#323-force-and-energy","text":"The charge q is attracted toward the plane, because of the negative induced charge. Let's calculate the force of attraction. Since the potential in the vicinity of q is the same as in the analog problem (the one with +q and -q but no conductor), so also is the field and, therefore, the force \\vec{F} = -\\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{(2d)^2} \\hat{z} \\label{3.12} \\tag{3.12} Beware : It is easy to get carried away, and assume that everything is the same in the two problems. Energy, however, is not the same. With the two point charges and no conductor, Eq. 2.42 gives W = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{2d} But for a single charge and conducting plane, the energy is half this W = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{4d} \\label{3.14} \\tag{3.14} Why half? Think of the energy stored in the fields (Eq. 2.45): W = \\frac{\\epsilon_0}{2} \\int E^2 \\dd{\\tau} In the first case, both the upper region (z > 0) and the lower region (z < 0) contribute, and by symmetry they contribute equally. But in the second case, only the upper region contains a nonzero field, and hence the energy is half as great. Of course, one could also determine the energy by calculating the work required to bring q in from infinity. The force required (to oppose the electrical force in \\eqref{3.12} is (1 / 4 \\pi \\epsilon_0)(q^2/4z^2) \\hat{z} , so \\begin{align} W & = & \\int _{\\infty} ^{d} \\vec{F} \\cdot \\dd{\\vec{l}} = \\frac{1}{4 \\pi \\epsilon_0} \\int_{\\infty} ^d \\frac{q^2}{4z^2} \\dd{z} \\\\ & = & \\frac{1}{4 \\pi \\epsilon_0} \\left. \\left( - \\frac{q^2}{4z} \\right) \\right|_{\\infty} ^d = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2}{4d} \\end{align} As I move q toward the conductor, I do work only on q. It is true that induced charge is moving in over the conductor, but this costs me nothing, since the whole conductor is at potential zero. By contrast, if I simultaneously bring in two point charges (with no conductor), I do work on both of them, and the total is (again) twice as great.","title":"3.2.3: Force and Energy"},{"location":"ch3-2/#324-other-image-problems","text":"The method just described is not limited to a single point charge; any stationary charge distribution near a grounded conducting plane can be treated in the same way, by introducing its mirror image - hence the name method of images. (Remember that the image charges have the opposite sign; this is what guarantees that the xy plane will be at potential zero.) There are also some exotic problems that can be handled in similar fashion; the nicest of these is the following.","title":"3.2.4: Other Image Problems"},{"location":"ch3-2/#example-32","text":"A point charge q is situated a distance a from the center of a grounded conducting sphere of radius R (Fig. 3.12). Find the potential outside the sphere Solution Examine the completely different configuration, consisting of the point charge q together with another point charge q' = - \\frac{R}{a} q \\label{3.15} \\tag{3.15} placed a distance b = \\frac{R^2}{a} \\label{3.16} \\tag{3.16} to the right of the center of the sphere (Fig 3.13). No conductor, now - just the two point charges. The potential of this configuration is V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{\\gr} + \\frac{q'}{\\gr '} \\right) \\label{3.17} \\tag{3.17} where \\gr and \\gr' are the distances from q and q' , respectively. Now, it happens (see Prob. 3.8) that this potential vanishes at all points on the sphere, and therefore fits the boundary conditions for our original problem, in the exterior region. Conclusion : \\eqref{3.17} is the potential of a point charge near a grounded conducting sphere. (Notice that b is less than R , so the \"image\" charge q' is safely inside the sphere - you cannot put image charges in the region where you are calculating V ; that would change \\rho , and you'd be solving Poisson's equation with the wrong source.) In particular, the force of attraction between the charge and the sphere is F = \\frac{1}{4\\pi \\epsilon_0} \\frac{q q'}{(a - b)^2} = - \\frac{1}{4 \\pi \\epsilon_0} \\frac{q^2 R a}{(a^2 - R^2)^2} \\label{3.18} \\tag{3.18} The method of images is delightfully simple... when it works. But it is as much an art as a science, for you must somehow think up just the right \"auxiliary\" configuration, and for most shapes this is forbiddingly complicated, if not impossible.","title":"Example 3.2"},{"location":"ch3-3/","text":"3.3: Separation of Variables In this section we shall attack Laplace's equation directly, using the method of separation of variables , which is the physicist's favorite tool for solving partial differential equations. The method is applicable in circumstances where the potential (V) or the charge density (\\sigma) is specified on the boundaries of some region, and we are asked to find the potential in the interior. The basic strategy is very simple: We look for solutions that are products of functions, each of which depends on only one of the coordinates. The algebraic details, however, can be formidable, so I'm going to develop the method through a sequence of examples. We'll start with Cartesian coordinates and then do spherical coordinates (I'll leave the cylindrical case for you to tackle on your own, in Prob 3.24). 3.3.1: Cartesian Coordinates Example 3.3 Two infinite grounded metal plates lie parallel to the xz plane, one at y = 0 , the other at y = a (Fig. 3.17). The left end, at x = 0 , is closed off with an infinite strip insulated from the two plates, and maintained at a specific potential V_0(y) . Find the potential inside this 'slot.' Solution The configuration is independent of z, so this is really a two-dimensional problem. In mathematical terms, we must solve Laplace's equation, \\frac{\\partial ^2 V}{\\partial{x^2}} + \\frac{\\partial ^2 V}{\\partial{y^2}} = 0 \\label{3.20} \\tag{3.20} subject to the boundary conditions (i) V = 0 when y = 0 (ii) V = 0 when y = a (iii) V = V_0(y) when x = 0 (iv) V \\rightarrow 0 as x \\rightarrow \\infty (The latter, although not explicitly stated in the problem, is necessary on physical grounds: as you get farther and farther away from the \"hot\" strip at x = 0 , the potential should drop to zero.) Since the potential is specified on all boundaries, the answer is uniquely determined. The first step is to look for solutions in the form of products: V(x, y) = X(x)Y(y) \\tagl{3.22} On the face of it, this is an absurd restriction - the overwhelming majority of solutions to Laplace's equation do not have such a form. For example, V(x, y) = (5x + 6y) satisfies the equation, but you can't express it as the product of a function of x times a function of y. Obviously, we're only going to get a tiny subset of all possible solutions by this means, and it would be a miracle if one of them happened to fit the boundary conditions of our problem... But hang on, because the solutions we do get are very special, and it turns out that by pasting them together we can construct the general solution. Anyway, putting \\eqref{3.22} into \\eqref{3.20} we obtain Y \\frac{d^2X}{dx^2} + X \\frac{d^2 Y}{dy^2} = 0 The next step is to \"separate the variables\" (that is, collect all the x-dependence into one term and all the y-dependence into another). Typically, this is accomplished by dividing through by V: \\frac{1}{X} \\frac{d^2 X}{dx^2} + \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = 0 \\tagl{3.23} Here the first term depends only on x and the second term only on y; in other words, we have an equation of the form f(x) + g(y) = 0 \\label{3.24} \\tag{3.24} Now, there's only one way this could possibly be true: f and g must both be constant . For what if f(x) changed, as you vary x - then if we held y fixed and fiddled with x, the sum f(x) + g(y) would change, in violation of \\eqref{3.24} , which says it's always zero. (That's a simple but somehow rather elusive argument; don't accept it without due thought, because the whole method rides on it.) It follows from \\eqref{3.23} , then, that \\frac{1}{X} \\frac{d^2 X}{dx^2} = C_1 \\quad \\text{ and } \\quad \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = C_2, \\quad \\text{ with } C_1 + C_2 = 0 \\tagl{3.25} One of these constants is positive, the other negative (or perhaps both are zero). In general, one must investigate all possibilities; however in our particular problem we need C_1 positive and C_2 negative, for reasons that will appear in a moment. Thus \\frac{d^2X}{dx^2} = k^2 X, \\qquad \\frac{d^2 Y}{dy^2} = - k^2 Y \\tagl{3.26} Notice what has happened: A partial differential equation has been converted into two ordinary differential equations. The advantage of this is obvious - ordinary differential equations are a lot easier to solve. Indeed: X(x) = A e^{kt} + B e^{-kt}, \\qquad Y(y) = C \\sin ky + D \\cos ky so V(x, y) = (A e^{kt} + B e^{-kt})(C \\sin ky + D \\cos ky) \\tagl{3.27} This is the appropriate separable solution to Laplace's equation; it remains to impose the boundary conditions, and see what they tell us about the constants. To begin at the end, condition (iv) requires tha A equal zero. Absorbing B into C and D, we are left with V(x, y) = e^{-kx} (C\\sin ky + D \\cos ky) Condition (i) now demands that D equal zero V(x, y) = C ^{-kx} \\sin ky \\tagl{3.28} Meanwhile (ii) yields \\sin ka = 0 , from which it follows that k = \\frac{n \\pi}{a} \\quad (n = 1, 2, 3, \\ldots) \\tagl{3.29} (At this point you can see why I chose C_1 positive and C_2 negative: If X were sinusoidal, we could never manage for it to go to zero at infinity, and if Y were exponential we could not make it vanish at both 0 and a . Incidentally, n = 0 is no good, for in that case the potential vanishes everywhere. And we have already excluded negative n's) That's as far as we can go, using separable solutions, and unless V_0(y) just happens to have the form \\sin(n \\pi / a) for some integer n , we simply can't fit the final boundary condition at x = 0 . But now comes the crucial step that redeems the method: Separation of variables has given us an infinite family of solutions (one for each n), and whereas none of them by itself satisfies the final boundary condition, it is possible to combine them in a way that does . Laplace's equation is linear, in the sense that if V_1, V_2, V_3, \\ldots satisfy it, so does any linear combination, for \\laplacian{V} = \\alpha_1 \\laplacian{V_1} + \\alpha_2 \\laplacian{V_2} + \\ldots = 0 \\alpha_1 + 0 \\alpha_2 + \\ldots = 0 Exploiting this fact, we can patch together the separable solutions \\eqref{3.28} to construct a much more general solution: V(x, y) = \\sum_{n=1} ^{\\infty} C_n e^{-n \\pi x / a} \\sin (n \\pi y / a) \\tagl{3.30} This still satisfies three of the boundary conditions; the question is, can we (by astute choice of the coefficients C_n ) fit the final boundary condition (iii)? V(0, y) = \\sum_{n=1} ^{\\infty} C_n \\sin (n \\pi y / a) = V_0(y) \\tagl{3.31} Well, you may recognize this sum - it's a Fourier sine series. And Dirichlet's theorem guarantees that virtually any function V_0(y) - it can even have a finite number of discontinuities - can be expanded in such a series. But how do we actually determine the coefficients C_n , buried as they are in that infinite sum? The device for accomplishing this is so lovely it deserves a name - I call it Fourier's trick , though it seems Euler had used essentially the same idea somewhat earlier. Here's how it goes: Multiply \\eqref{3.31} by \\sin(n' \\pi y /a) (where n' is a positive integer), and integrate from 0 to a: \\sum_{n=1} ^{\\infty} C_n \\int_{0} ^{a} \\sin(n \\pi y / a) \\sin(n' \\pi y/a) \\dd{y} = \\int_{0} ^a V_0(y) \\sin (n' \\pi /a) \\dd{y} \\tagl{3.32} You can work out the integral on the left yourself; the answer is \\int_{0} ^a \\sin (n \\pi y /a) \\sin (n' \\pi y / a) \\dd{y} = \\begin{cases} 0 & \\quad \\text{if } n' \\neq n \\\\ \\frac{a}{2} & \\quad \\text{if } n' = n \\end{cases} \\tagl{3.33} Thus all the terms in the series drop out, save only the one where n = n' , and the left side of \\eqref{3.32} reduces to (a/2)C_{n'} . Conclusion : C_n = \\frac{2}{a} \\int_{0}^a V_0(y) \\sin (n \\pi y /a) \\dd{y} \\tagl{3.34} That does it: \\eqref{3.30} is the solution, with coefficients given by eqref{3.34} . As a concrete example, suppose the strip at x = 0 is a metal plate with constant potential V_0 (remember, it's insulated from the grounded plates at y = 0 and y = a . Then C_n = \\frac{2V_0}{a} \\int_0 ^a \\sin (n \\pi y / a) \\dd y \\\\ = \\frac{2 V_0}{n \\pi} (1 - \\cos n \\pi) = \\begin{cases} 0 & \\quad \\text{if n is even } \\\\ \\frac{4 V_0}{n \\pi} & \\quad \\text{if n is odd} \\end{cases} \\tagl{3.35} Thus V(x, y) = \\frac{4 V_0}{\\pi} \\sum_{n = 1, 3, 5, \\ldots} \\frac{1}{n} e^{- n \\pi x / a} \\sin (n \\pi y / a) \\tagl{3.36} Figure 3.18 is a plot of this potential; Fig. 3.10 shows how the first few terms in the Fourier series combine to make a better and better approximation to the constant V_0 : (a) is the n=1 term only, (b) includes n up to 5, (c) is the sum of the first 10 terms, and (d) is the sum of the first 100 terms. Incidentally, the infinite series in Eq. 3.36 can be summed explicitly (try your hand at it if you like); the result is V(x, y) = \\frac{2V_0}{\\pi} \\tan^{-1} \\left( \\frac{\\sin(\\pi y / a)}{\\sinh(\\pi x /a )} \\right) \\tagl{3.37} In this form, it is easy to check that Laplace's equation is obeyed and the four boundary conditions are satisfied The success of this method hinged on two extraordinary properties of the separable solutions \\eqref{3.28} and \\eqref{3.29} : completeness and orthogonality . A set of functions f_n(y) is said to be complete if any other function f(y) can be expressed as a linear combination of them: f(y) = \\sum_{n=1} ^{\\infty} C_n f_n(y) \\tagl{3.38} The functions \\sin (n \\pi y/a) are complete on the interval 0 \\leq y \\leq a . It was this fact, guaranteed by Dirichlet's theorem, that assured us \\eqref{3.31} could be satisfied, given the proper choice of the coefficients C_n . (The proof of completeness, for a particular set of functions, is an extremely difficult business, and I'm afraid physicists tend to assume it's true and leave the checking to others.) A set of functions is orthogonal if the integral of the product of any two different members of the set is zero: \\int_0 ^a f_n(y) f_{n'} (y) \\dd{y} = 0 \\quad \\text{for } n' \\neq n The sine functions are orthogonal \\eqref{3.33} ; that is the property on which Fourier's trick is based, allowing us to kill off all terms but one in the infinite series and thereby solve for the coefficients C_n (Proof of orthogonality is generally quite simple, either by direct integration or by analysis of the differential equation from which the functions came.) Example 3.4 Two infinitely-long grounded metal plates, again at y=0 and y=a are connected at x= \\pm b by metal strips maintained at a constant potential V_0 , as shown in Fig. 3.20 (a thin layer of insulation at each corner prevents them from shorting out). Find the potential inside the resulting rectangular pipe. Solution Once again, the configuration is independent of z. Our problem is to solve Laplace's equation \\frac{\\partial ^2 V}{\\partial{x^2}} + \\frac{\\partial ^2 V}{\\partial{y^2}} = 0 subject to the boundary conditions (i) V = 0 when y = 0 (ii) V = 0 when y = a (iii) V = V_0 when x = b (iv) V = V_0 when x = -b The argument runs as before, up to \\eqref{3.27} : V(x, y) = (A e^{kt} + B e^{-kt})(C \\sin ky + D \\cos ky) This time, however, we cannot set A = 0 ; the region in question does not extend to x = \\infty , so e^{kx} is perfectly acceptable. On the other hand, the situation is symmetric with respect to x, so V(-x, y) = V(x, y) , and it follows that A = B . Using e^{kx} + e^{-kx} = 2 \\cosh kx and absorbing 2A into C and D , we have V(x, y) = \\cosh kx (C \\sin ky + D\\cos ky) Boundary conditions (i) and (ii) require, as before, that D = 0 and k = n\\pi /a , so V(x, y) = C \\cosh (n \\pi x /a )\\sin(n \\pi y/a) \\tagl{3.41} Because V(x, y) is even in x, it will automatically meet conditions (iv) if it fits (iii). It remains, therefore, to construct the general linear combination V(x, y) = \\sum _{n=1}^{\\infty} C_n \\cosh (n \\pi x / a) \\sin(n \\pi y /a) and pick the coefficients C_n in such a way as to satisfy condition (iii): V(b, y) = \\sum_{n=1}^{\\infty} C_n \\cosh (n \\pi b /a) \\sin(n \\pi y/a) = V_0 This is the same problem in Fourier analysis that we faced before; I quote the result from \\eqref{3.35} ; C_n \\cosh (n \\pi b/a) = \\begin{cases} 0 & \\quad \\text {if n is even} \\\\ \\frac{4 V_0}{n \\pi} & \\quad \\text{if n is odd} \\end{cases} Conclusion : The potential in this case is given by V(x, y) = \\frac{4 V_0}{\\pi} \\sum_{n=1, 3, 5\\ldots} \\frac{1}{n} \\frac{\\cosh(n \\pi x/a)}{\\cosh(n \\pi b/a)} \\sin(n \\pi y/a) \\tagl{3.42} This function is shown in Fig. 3.21 Example 3.5 An infinitely long rectangular metal pipe (sides a and b) is grounded, but one end, at x = 0 , is maintained at a specified potential V_0(y, z) , as indicated in Fig. 3.22. Find the potential inside the pipe. Solution This is genuinely a three-dimensional problem, \\frac{\\partial ^2 V}{\\partial{x^2}} + \\frac{\\partial ^2 V}{\\partial{y^2}} + \\frac{\\partial ^2 V}{\\partial{z^2}} = 0 \\tagl{3.43} subject to the boundary conditions - (i) V = 0 when y = 0 - (ii) V = 0 when y = a - (iii) V = 0 when z = 0 - (iv) V = 0 when z = b - (v) V \\rightarrow 0 as x \\rightarrow \\infty - (vi) V = V_0(y, z) whem x = 0 As always, we look for solutions that are products: V(x, y, z) = X(x)Y(y)Z(z) \\tagl{3.45} Putting this into \\eqref{3.43} and dividing by V, we find \\frac{1}{X} \\frac{d^2 X}{dx^2} + \\frac{1}{Y} \\frac{d^2 Y}{dy^2} + \\frac{1}{Z} \\frac{d^2 Z}{dz^2} = 0 It follows that \\frac{1}{X} \\frac{d^2 X}{dx^2} = C_1 , \\quad \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = C_2 , \\quad \\frac{1}{Z} \\frac{d^2 Z}{dz^2} = C_3 , \\text{ with } C_1 + C_2 + C_3 = 0 Our previous experience in Ex. 3.3 suggests that C_1 must be positive, C_2 and C_3 negative. Setting C_2 = -k^2 and C_3 = -l^2 , we have C_1 = k^2 + l^2 , and hence \\frac{d^2 X}{dx^2} = (k^2 + l^2)X, \\quad \\frac{d^2 Y}{dy^2} = -k^2 Y, \\quad \\frac{d^2 Z}{dz^2} = -l^2 Z \\tagl{3.46} Once again, separation of variables has turned a partial differential equation into ordinary differential equations. The solutions are \\begin{align*} X(x) & = A e^{\\sqrt{k^2 + l^2} x} + B e^{- \\sqrt{k^2 + l^2} x} \\\\ Y(y) & = C \\sin ky + D \\cos ky \\\\ Z(z) & = E \\sin lz + F \\cos lz \\end{align*} Boundary condition (v) implies A =0 , (i) gives D = 0 , and (iii) yields F = 0 whereas (ii) and (iv) require that k = n\\pi /a and l =m \\pi /b , where n and m are positive integers. Combining the remaining constants, we are left with V(x, y, z) = C e^{-\\pi \\sqrt{(n/a)^2 + (m/b)^2}x} \\sin (n \\pi y / a) \\sin(m \\pi z /b) \\tagl{3.47} This solution meets all the boundary conditions except (vi). It contains two unspecified integers (n and m), and the most general linear combination is a double sum V(x, y, z) = \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C e^{-\\pi \\sqrt{(n/a)^2 + (m/b)^2}x} \\sin (n \\pi y / a) \\sin(m \\pi z /b) = V_0(y, z) \\tagl{3.48} We hope to fit the remaining boundary condition, V(0, y, z) = \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C \\sin (n \\pi y / a) \\sin(m \\pi z /b) = V_0(y, z) \\tagl{3.49} by appropriate choice of the coefficients C_{n, m} . To determine these constants, we multiply by \\sin(n' n \\pi y/a) \\sin(m' \\pi z / b) , where n' and m' are arbitrary positive integers, and integrate \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C_{n, m} \\int_0 ^a \\sin (n \\pi y/a) \\sin(n' \\pi y/a) \\dd{y} \\int_0 ^b \\sin(m \\pi z/b) \\sin (m' \\pi z/b) \\dd{z} \\\\ = \\int_0 ^a \\int_0 ^b V_0(y, z) \\sin(n' \\pi y/a) \\sin(m' \\pi z/b) \\dd{y} \\dd{z} Quoting \\eqref{3.33} , the left side is (ab/4) C_{n', m'} , so C_{n, m} = \\frac{4}{ab} \\int_0 ^a \\int_0 ^b V_0(y, z) \\sin (n \\pi y/a) \\sin(m\\pi z/b) \\dd{y} \\dd{z} \\tagl{3.50} Equation \\eqref{3.48} , with the coefficients given by \\eqref{3.50} , is the solution to our problem. For instance, if the end of the tube is a conductor at constant potential V_0 , C_{n, m} = \\frac{4}{ab} \\int_0 ^a \\sin(n \\pi y/a) \\dd{y} \\int_0 ^b \\sin(m \\pi z/b) \\dd{z} \\\\ = \\begin{cases} 0 & \\qquad \\text{if n or m is even} \\\\ \\frac{16 V_0}{\\pi^2 nm} & \\qquad \\text{if n and m are odd} \\end{cases} \\tagl{3.51} In this case, V(x, y, z) = \\frac{16V_0}{\\pi^2} \\sum_{n,m=1,3,5,\\ldots} ^{\\infty} \\frac{1}{nm} e^{-\\pi \\sqrt{(n/a)^2 + (m/b)^2}x} \\sin(n \\pi y/a) \\sin(m \\pi z/b) \\tagl{3.52} Notice that successive terms decrease rapidly; a reasonable approximation would be obtained by keeping only the first few. 3.3.2: Spherical Coordinates In the examples considered so far, Cartesian coordinates were clearly appropriate, since the boundaries were planes. For round objects, spherical coordinates are more natural. In the spherical system, Laplace's equation reads: \\frac{1}{r^2} \\pdv{}{r} \\left( r^2 \\pdv{V}{r} \\right) + \\frac{1}{r^2 \\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{V}{\\theta} \\right) + \\frac{1}{r^2\\sin ^2 \\theta} \\frac{\\partial ^2 V}{\\partial \\phi ^2} = 0 \\tagl{3.53} I shall assume the problem has azimuthal symmetry , so that V is independent of \\phi ; In that case, \\eqref{3.53} reduces to \\pdv{}{r} \\left( r^2 \\pdv{V}{r} \\right) + \\frac{1}{\\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{V}{\\theta} \\right) = 0 \\tagl{3.54} As before, we look for solutions that are products: V(r, \\theta) = R(r) \\Theta (\\theta) \\tagl{3.55} Putting this into \\eqref{3.54} , and dividing by V , \\frac{1}{R} \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) + \\frac{1}{\\Theta \\sin \\theta} \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = 0 \\tagl{3.56} Since the first term depends only on r , and the second only on \\theta , it follows that each must be a constant: \\frac{1}{R} \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) = l(l+1), \\quad \\frac{1}{\\Theta \\sin \\theta} \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = -l(l+1) \\tagl{3.57} Here l(l+1) is just a fancy way of writing the separation constant, whose convenience will appear shortly. As always, separation of variables has converted a partial differential equation into ordinary differential equations. The radial equation, \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) = l(l+1)R \\tagl{3.58} has the general solution R(r) = A r^l + \\frac{B}{r^{l+1}} \\tagl{3.59} as you can easily check; A and B are the two arbitrary constants to be expected in the solution of a second-order differential equation. But the angular equation, \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = -l(l+1) \\sin \\theta \\Theta \\tagl{3.60} is not so simple. The solutions are Legendre polynomials in the variable \\cos \\theta : \\Theta (\\theta ) = P_l (\\cos \\theta ) \\tagl{3.61} P_l (x) is most conveniently defined by the Rodrigues formula : P_l(x) \\equiv \\frac{1}{2^l l!}\\left( \\dv{}{x} \\right)^l (x^2 - 1)^l \\tagl{3.62} The first few Legendre polynomials are listed: Legendre Polynomials P_0 - P_5 \\begin{align*} P_0(x) & = 1 \\\\ P_1(x) & = x \\\\ P_2(x) & = (3x^2 - 1)/2 \\\\ P_3(x) & = (5x^3 - 3x)/2 \\\\ P_4(x) & = (35x^4 - 30x^2 + 3)/8 \\\\ P_5(x) & = (63x^5 - 70x^3 + 15x)/8 \\end{align*} Notice that P_l(x) is (as the name suggests) an _l_th-order polynomial in x; it contains only even powers if l is even, and only odd powers if l is odd. The factor in front (1/2^l l! was chosen in order that P_l(1) = 1 \\tagl{3.63} The Rodrigues formula obviously only works for nonnegative integer values of l. Moreover, it provides us with only one solution. But \\eqref{3.60} is second-order, and it should possess two independent solutions for every value of l . It turns out that these \"other solutions\" blow up at \\theta = 0 and/or \\theta = \\pi , and are therefore unacceptable on physical grounds. For instance, the second solution for l=0 is \\Theta(\\theta) = \\ln \\left( \\tan \\frac{\\theta}{2} \\right) \\tagl{3.64} You might want to check for yourself that this satisfies \\eqref{3.60} . In the case of azimuthal symmetry, then, the most general separable solution to Laplace's equation, consistent with minimal physical requirements, is V(r, \\theta) = \\left( A r^l + \\frac{B}{r^{l+1}} \\right) P_l(\\cos \\theta) (There was no need to include an overall constant in \\eqref{3.61} because it can be absorbed into A and B at this stage.) As before, separation of variables yields an infinite set of solutions, one for each l . The general solution is the linear combination of separable solutions: V(r, \\theta) = \\sum_{l=0} ^{\\infty} \\left( A r^l + \\frac{B}{r^{l+1}} \\right) P_l(\\cos \\theta) \\tagl{3.65} The following examples illustrate the power of this important result. Example 3.6 The potential V_0(\\theta) is specified on the surface of a hollow sphere, of radius R . Find the potential inside the sphere. Solution In this case, B_l = 0 for all l , otherwise the potential would blow up at the origin. Thus, V(r, \\theta) = \\sum_{l = 0} ^{\\infty} A_l r^l P_l(\\cos \\theta) \\tagl{3.66} At r = R this must match the specified function V_0(\\theta) : V(R, \\theta) = \\sum_{l=0}^\\infty A_l R^l P_l(\\cos \\theta) = V_0(\\theta) \\tagl{3.67} Can this equation be satisfied, for an appropriate choice of coefficients A_l ? Yes: The Legendre polynomials (like the sines) constitute a complete set of functions, on the interval -1 \\leq x \\leq 1 (0 \\leq \\theta \\leq \\pi) . How do we determine the constants? Again, by Fourier's trick, for the Legendre polynomials (like the sines) are orthogonal functions: \\begin{align*} \\int_{-1}^1 P_l(x) P_{l'}(x) \\dd{x} & = \\int_0 ^\\pi P_l(\\cos \\theta) P_{l'}(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\\\ & = \\begin{cases} 0, & \\quad \\text{if } l' \\neq l \\\\ \\frac{2}{2l +1} , & \\quad \\text{if } l' = l \\end{cases} \\end{align*} \\tagl{3.68} Thus, multiplying \\eqref{3.67} by P_{l'}(\\cos \\theta) \\sin \\theta and integrating, we have A_{l'} R^{l'} \\frac{2}{2l' + 1} = \\int_{0} ^\\pi V_0(\\theta) P_{l'}(\\cos \\theta) \\sin \\theta \\dd{\\theta} or A_l = \\frac{2l+1}{2R^l} \\int_0 ^\\pi V_0(\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.69} \\eqref{3.66} is the solution to our problem, with the coefficients given by \\eqref{3.69} . It can be difficult to evaluate integrals of the form \\eqref{3.69} analytically, and in practice it is often easier to solve \\eqref{3.67} \"by eyeball.\" For instance, suppose we are told that the potential on the sphere is V_0(\\theta) = k \\sin^2 (\\theta/2) \\tagl{3.70} where k is constant. Using the half-angle formula, we rewrite this as V_0(\\theta) = \\frac{k}{2}(1 - \\cos \\theta) = \\frac{k}{2} [P_0(\\cos \\theta) - P_1 (\\cos \\theta)] Putting this into \\eqref{3.67} , we read off immediately that A_0 = k/2 , A_1 = -k/(2R) , and all other A_l 's vanish. Therefore V(r, \\theta) = \\frac{k}{2} \\left[ r^0 P_{0}(\\cos \\theta) - \\frac{r^1}{R} P_1 (\\cos \\theta) \\right] = \\frac{k}{2} \\left( 1 - \\frac{r}{R} \\cos \\theta \\right) \\tagl{3.71} Example 3.7 The potential V_0(\\theta) is again specified on the surface of a sphere of radius R , but this time we are asked to find the potential outside , assuming there is no charge there. Solution In this case it's the A_l 's that must be zero (or else V would not go to zero at \\infty ), so V(r, \\theta) = \\sum_{l=0} ^\\infty \\frac{B_l}{r^{l+1}} P_l(\\cos \\theta) = V_0(\\theta) \\tagl{3.72} Multiplying by P_{l'}(\\cos \\theta) \\sin \\theta and integrating - exploiting, again, the orthogonality relation 3.68 - we have \\frac{B_{l'}}{R^{l'+1}} \\frac{2}{2l' + 1} = \\int_0 ^\\pi V_0(\\theta) P_{l'}(\\cos \\theta) \\sin \\theta \\dd{\\theta} or B_l = \\frac{2l + 1}{2} R^{l+1} \\int_0 ^\\pi V_0(\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.73} \\eqref{3.72} , with the coefficients given by \\eqref{3.73} , is the solution to our problem. Example 3.8 An uncharged metal sphere of radius R is placed in an otherwise uniform electric field \\vec{E} = E_0 \\hat{z} . The field will push positive charge to the 'northern' surface of the sphere, and - symmetrically - negative charge to the 'southern' surface (Fig. 3.24). This induced charge, in turn, distorts the field in the neighborhood of the sphere. Find the potential in the region outside the sphere. Solution The sphere is an equipotential - we may as well set it to zero. Then by symmetry the entire xy plane is at potential zero. This time, however, V does not go to zero at large z . In fact, far from the sphere the field is E_0 \\hat{z} and hence V \\rightarrow - E_0 z + C Since V = 0 in the equatorial plane, the constant C must be zero. Accordingly, the boundary conditions for this problem are - (i) V = 0 when r = R - (ii) V \\rightarrow - E_0 r \\cos \\theta for r \\gg R We must fit these boundary conditions with a function of the form \\eqref{3.65} . The first condition yields A_l R^l + \\frac{B_l}{R^{l+1}} = 0 or B_l = -A_l R^{2l+1} \\tagl{3.75} so V(r, \\theta) = \\sum_{l = 0} ^{\\infty} A_l \\left( r^l - \\frac{R^{2l+1}}{r^{l+1}} \\right) P_l(\\cos \\theta) For r \\gg R , the second term in parentheses is negligible, and therefore the condition (ii) requires that \\sum_{l=0}^\\infty A_l R^{l} P_l (\\cos \\theta) = - E_0 r \\cos \\theta Evidently only one term is present: l = 1 . In fact, since P_1(\\cos \\theta) = \\cos \\theta we can read off immediately A_1 = - E_0, \\qquad \\text{ all other }A_l's \\text{ zero} Conclusion : V(r, \\theta) = - E_0 \\left( r - \\frac{R^3}{r^2} \\right) \\cos \\theta \\tagl{3.76} The first term (-E_0 r \\cos \\theta) is due to the external field; the contribution attributable to the induced charge is E_0 \\frac{R^3}{r^2} \\cos \\theta If you want to know the induced charge density, it can be calculated in the usual way: \\sigma(\\theta) = - \\epsilon_0 \\left. \\pdv{V}{r} \\right|_{r = R} = \\epsilon_0 E_0 \\left. \\left( 1 + 2 \\frac{R^3}{r^3} \\right) \\cos \\theta \\right|_{r = R} = 3 \\epsilon_0 E_0 \\cos \\theta \\tagl{3.77} As expected, it is positive in the 'northern' hemisphere 0 \\leq \\theta \\leq \\pi /2 and negative in the 'southern' \\pi/2 \\leq \\theta \\leq \\pi . Example 3.9 A specified charge density \\sigma_0(\\theta) is glued over the surface of a spherical shell of radius R . Find the resulting potential inside and outside the sphere. Solution You could, of course, do this by direct integration: V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma_0}{\\gr} \\dd{a} but separation of variables is often easier. For the interior region, we have V(r, \\theta) = \\sum_{l = 0}^\\infty A_l r^l P_l (\\cos \\theta) \\quad (r \\leq R) \\tagl{3.78} (no B_l terms - they blow up at the origin); in the exterior region V(r, \\theta) = \\sum_{l=0}^\\infty \\frac{B_l}{r^{l+1}} P_l(\\cos \\theta) \\quad (r \\geq R) \\tagl{3.79} (no A_l terms - they don't go to zero at infinity). These two functions must be joined together by the appropriate boundary conditions at the surface itself. First, the potential is continuous at r = R (Eq. 2.34): \\sum_{l=0}^\\infty A_l R^l P_l(\\cos \\theta) = \\sum_{l=0} ^\\infty \\frac{B_l}{R^{l+1}} P_l(\\cos \\theta) \\tagl{3.80} It follows that the coefficients of like Legendre polynomial are equal: B_l = A_l R^{2l+1} \\tagl{3.81} (To prove that formally, multiply both sides of \\eqref{3.80} by P_{l'} (\\cos \\theta)\\sin \\theta and integrate from 0 to \\pi , using the orthogonality relation \\eqref{3.68} .) Second, the radial derivative of V suffers a discontinuity at the surface (Eq. 2.36): \\left. \\left( \\pdv{V_{out}}{r} - \\pdv{V_{in}}{r} \\right) \\right|_{r = R} = - \\frac{1}{\\epsilon_0} \\sigma_0(\\theta) \\tagl{3.82} Thus, - \\sum_{l=0}^\\infty (l+1) \\frac{B_l}{R^{l+2}} P_l(\\cos \\theta) - \\sum_{l=0}^\\infty l A_l R^{l-1} P_l(\\cos \\theta) = - \\frac{1}{\\epsilon_0} \\sigma_0 (\\theta) or, using \\eqref{3.81} , \\sum_{l=0}^\\infty (2l+1) A_l R^{l-1} P_l(\\cos \\theta) = \\frac{1}{\\epsilon_0} \\sigma_0(\\theta) \\tagl{3.83} From here, the coefficients can be determined using Fourier's trick A_l = \\frac{1}{2 \\epsilon_0 R^{l-1}} \\int_0 ^\\pi \\sigma_0 (\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.84} Equations 3.78 and 3.79 constitute the solution to our problem, with the coefficients given by \\eqref{3.81} and \\eqref{3.84} . For instance, if \\sigma_0(\\theta) = k \\cos \\theta = k P_1 (\\cos \\theta) \\tagl{3.85} for some constant k , then all the A_l 's are zero except for l = 1 , and A_1 = \\frac{k}{2 \\epsilon_0} \\int_0 ^\\pi [P_1(\\cos \\theta)]^2 \\sin \\theta \\dd{\\theta} = \\frac{k}{3\\epsilon_0} The potential inside the sphere is therefore V(r, \\theta) = \\frac{k}{3 \\epsilon_0} r \\cos \\theta \\quad (r \\leq R) \\tagl{3.86} whereas outside the sphere V(r, \\theta) = \\frac{kR^3}{3 \\epsilon_0} \\frac{1}{r^2} \\cos \\theta \\quad (r \\geq R) \\tagl{3.87} In particular, if \\sigma_0(\\theta) is the induced charge on a metal sphere in an external field E_0(\\hat{z}) , so that k = 3 \\epsilon_0 E_0 \\eqref{3.77} , then the potential inside is E_0 r \\cos \\theta = E_0 z , and the field is -E_0 \\hat{z} - exactly right to cancel off the external field, as of course it should be. Outside the sphere the potential due to this surface charge is E_0 \\frac{R^3}{r^2} \\cos \\theta consistent with our conclusion in Example 3.8.","title":"3.3 - Separation of Variables"},{"location":"ch3-3/#33-separation-of-variables","text":"In this section we shall attack Laplace's equation directly, using the method of separation of variables , which is the physicist's favorite tool for solving partial differential equations. The method is applicable in circumstances where the potential (V) or the charge density (\\sigma) is specified on the boundaries of some region, and we are asked to find the potential in the interior. The basic strategy is very simple: We look for solutions that are products of functions, each of which depends on only one of the coordinates. The algebraic details, however, can be formidable, so I'm going to develop the method through a sequence of examples. We'll start with Cartesian coordinates and then do spherical coordinates (I'll leave the cylindrical case for you to tackle on your own, in Prob 3.24).","title":"3.3: Separation of Variables"},{"location":"ch3-3/#331-cartesian-coordinates","text":"","title":"3.3.1: Cartesian Coordinates"},{"location":"ch3-3/#example-33","text":"Two infinite grounded metal plates lie parallel to the xz plane, one at y = 0 , the other at y = a (Fig. 3.17). The left end, at x = 0 , is closed off with an infinite strip insulated from the two plates, and maintained at a specific potential V_0(y) . Find the potential inside this 'slot.' Solution The configuration is independent of z, so this is really a two-dimensional problem. In mathematical terms, we must solve Laplace's equation, \\frac{\\partial ^2 V}{\\partial{x^2}} + \\frac{\\partial ^2 V}{\\partial{y^2}} = 0 \\label{3.20} \\tag{3.20} subject to the boundary conditions (i) V = 0 when y = 0 (ii) V = 0 when y = a (iii) V = V_0(y) when x = 0 (iv) V \\rightarrow 0 as x \\rightarrow \\infty (The latter, although not explicitly stated in the problem, is necessary on physical grounds: as you get farther and farther away from the \"hot\" strip at x = 0 , the potential should drop to zero.) Since the potential is specified on all boundaries, the answer is uniquely determined. The first step is to look for solutions in the form of products: V(x, y) = X(x)Y(y) \\tagl{3.22} On the face of it, this is an absurd restriction - the overwhelming majority of solutions to Laplace's equation do not have such a form. For example, V(x, y) = (5x + 6y) satisfies the equation, but you can't express it as the product of a function of x times a function of y. Obviously, we're only going to get a tiny subset of all possible solutions by this means, and it would be a miracle if one of them happened to fit the boundary conditions of our problem... But hang on, because the solutions we do get are very special, and it turns out that by pasting them together we can construct the general solution. Anyway, putting \\eqref{3.22} into \\eqref{3.20} we obtain Y \\frac{d^2X}{dx^2} + X \\frac{d^2 Y}{dy^2} = 0 The next step is to \"separate the variables\" (that is, collect all the x-dependence into one term and all the y-dependence into another). Typically, this is accomplished by dividing through by V: \\frac{1}{X} \\frac{d^2 X}{dx^2} + \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = 0 \\tagl{3.23} Here the first term depends only on x and the second term only on y; in other words, we have an equation of the form f(x) + g(y) = 0 \\label{3.24} \\tag{3.24} Now, there's only one way this could possibly be true: f and g must both be constant . For what if f(x) changed, as you vary x - then if we held y fixed and fiddled with x, the sum f(x) + g(y) would change, in violation of \\eqref{3.24} , which says it's always zero. (That's a simple but somehow rather elusive argument; don't accept it without due thought, because the whole method rides on it.) It follows from \\eqref{3.23} , then, that \\frac{1}{X} \\frac{d^2 X}{dx^2} = C_1 \\quad \\text{ and } \\quad \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = C_2, \\quad \\text{ with } C_1 + C_2 = 0 \\tagl{3.25} One of these constants is positive, the other negative (or perhaps both are zero). In general, one must investigate all possibilities; however in our particular problem we need C_1 positive and C_2 negative, for reasons that will appear in a moment. Thus \\frac{d^2X}{dx^2} = k^2 X, \\qquad \\frac{d^2 Y}{dy^2} = - k^2 Y \\tagl{3.26} Notice what has happened: A partial differential equation has been converted into two ordinary differential equations. The advantage of this is obvious - ordinary differential equations are a lot easier to solve. Indeed: X(x) = A e^{kt} + B e^{-kt}, \\qquad Y(y) = C \\sin ky + D \\cos ky so V(x, y) = (A e^{kt} + B e^{-kt})(C \\sin ky + D \\cos ky) \\tagl{3.27} This is the appropriate separable solution to Laplace's equation; it remains to impose the boundary conditions, and see what they tell us about the constants. To begin at the end, condition (iv) requires tha A equal zero. Absorbing B into C and D, we are left with V(x, y) = e^{-kx} (C\\sin ky + D \\cos ky) Condition (i) now demands that D equal zero V(x, y) = C ^{-kx} \\sin ky \\tagl{3.28} Meanwhile (ii) yields \\sin ka = 0 , from which it follows that k = \\frac{n \\pi}{a} \\quad (n = 1, 2, 3, \\ldots) \\tagl{3.29} (At this point you can see why I chose C_1 positive and C_2 negative: If X were sinusoidal, we could never manage for it to go to zero at infinity, and if Y were exponential we could not make it vanish at both 0 and a . Incidentally, n = 0 is no good, for in that case the potential vanishes everywhere. And we have already excluded negative n's) That's as far as we can go, using separable solutions, and unless V_0(y) just happens to have the form \\sin(n \\pi / a) for some integer n , we simply can't fit the final boundary condition at x = 0 . But now comes the crucial step that redeems the method: Separation of variables has given us an infinite family of solutions (one for each n), and whereas none of them by itself satisfies the final boundary condition, it is possible to combine them in a way that does . Laplace's equation is linear, in the sense that if V_1, V_2, V_3, \\ldots satisfy it, so does any linear combination, for \\laplacian{V} = \\alpha_1 \\laplacian{V_1} + \\alpha_2 \\laplacian{V_2} + \\ldots = 0 \\alpha_1 + 0 \\alpha_2 + \\ldots = 0 Exploiting this fact, we can patch together the separable solutions \\eqref{3.28} to construct a much more general solution: V(x, y) = \\sum_{n=1} ^{\\infty} C_n e^{-n \\pi x / a} \\sin (n \\pi y / a) \\tagl{3.30} This still satisfies three of the boundary conditions; the question is, can we (by astute choice of the coefficients C_n ) fit the final boundary condition (iii)? V(0, y) = \\sum_{n=1} ^{\\infty} C_n \\sin (n \\pi y / a) = V_0(y) \\tagl{3.31} Well, you may recognize this sum - it's a Fourier sine series. And Dirichlet's theorem guarantees that virtually any function V_0(y) - it can even have a finite number of discontinuities - can be expanded in such a series. But how do we actually determine the coefficients C_n , buried as they are in that infinite sum? The device for accomplishing this is so lovely it deserves a name - I call it Fourier's trick , though it seems Euler had used essentially the same idea somewhat earlier. Here's how it goes: Multiply \\eqref{3.31} by \\sin(n' \\pi y /a) (where n' is a positive integer), and integrate from 0 to a: \\sum_{n=1} ^{\\infty} C_n \\int_{0} ^{a} \\sin(n \\pi y / a) \\sin(n' \\pi y/a) \\dd{y} = \\int_{0} ^a V_0(y) \\sin (n' \\pi /a) \\dd{y} \\tagl{3.32} You can work out the integral on the left yourself; the answer is \\int_{0} ^a \\sin (n \\pi y /a) \\sin (n' \\pi y / a) \\dd{y} = \\begin{cases} 0 & \\quad \\text{if } n' \\neq n \\\\ \\frac{a}{2} & \\quad \\text{if } n' = n \\end{cases} \\tagl{3.33} Thus all the terms in the series drop out, save only the one where n = n' , and the left side of \\eqref{3.32} reduces to (a/2)C_{n'} . Conclusion : C_n = \\frac{2}{a} \\int_{0}^a V_0(y) \\sin (n \\pi y /a) \\dd{y} \\tagl{3.34} That does it: \\eqref{3.30} is the solution, with coefficients given by eqref{3.34} . As a concrete example, suppose the strip at x = 0 is a metal plate with constant potential V_0 (remember, it's insulated from the grounded plates at y = 0 and y = a . Then C_n = \\frac{2V_0}{a} \\int_0 ^a \\sin (n \\pi y / a) \\dd y \\\\ = \\frac{2 V_0}{n \\pi} (1 - \\cos n \\pi) = \\begin{cases} 0 & \\quad \\text{if n is even } \\\\ \\frac{4 V_0}{n \\pi} & \\quad \\text{if n is odd} \\end{cases} \\tagl{3.35} Thus V(x, y) = \\frac{4 V_0}{\\pi} \\sum_{n = 1, 3, 5, \\ldots} \\frac{1}{n} e^{- n \\pi x / a} \\sin (n \\pi y / a) \\tagl{3.36} Figure 3.18 is a plot of this potential; Fig. 3.10 shows how the first few terms in the Fourier series combine to make a better and better approximation to the constant V_0 : (a) is the n=1 term only, (b) includes n up to 5, (c) is the sum of the first 10 terms, and (d) is the sum of the first 100 terms. Incidentally, the infinite series in Eq. 3.36 can be summed explicitly (try your hand at it if you like); the result is V(x, y) = \\frac{2V_0}{\\pi} \\tan^{-1} \\left( \\frac{\\sin(\\pi y / a)}{\\sinh(\\pi x /a )} \\right) \\tagl{3.37} In this form, it is easy to check that Laplace's equation is obeyed and the four boundary conditions are satisfied The success of this method hinged on two extraordinary properties of the separable solutions \\eqref{3.28} and \\eqref{3.29} : completeness and orthogonality . A set of functions f_n(y) is said to be complete if any other function f(y) can be expressed as a linear combination of them: f(y) = \\sum_{n=1} ^{\\infty} C_n f_n(y) \\tagl{3.38} The functions \\sin (n \\pi y/a) are complete on the interval 0 \\leq y \\leq a . It was this fact, guaranteed by Dirichlet's theorem, that assured us \\eqref{3.31} could be satisfied, given the proper choice of the coefficients C_n . (The proof of completeness, for a particular set of functions, is an extremely difficult business, and I'm afraid physicists tend to assume it's true and leave the checking to others.) A set of functions is orthogonal if the integral of the product of any two different members of the set is zero: \\int_0 ^a f_n(y) f_{n'} (y) \\dd{y} = 0 \\quad \\text{for } n' \\neq n The sine functions are orthogonal \\eqref{3.33} ; that is the property on which Fourier's trick is based, allowing us to kill off all terms but one in the infinite series and thereby solve for the coefficients C_n (Proof of orthogonality is generally quite simple, either by direct integration or by analysis of the differential equation from which the functions came.)","title":"Example 3.3"},{"location":"ch3-3/#example-34","text":"Two infinitely-long grounded metal plates, again at y=0 and y=a are connected at x= \\pm b by metal strips maintained at a constant potential V_0 , as shown in Fig. 3.20 (a thin layer of insulation at each corner prevents them from shorting out). Find the potential inside the resulting rectangular pipe. Solution Once again, the configuration is independent of z. Our problem is to solve Laplace's equation \\frac{\\partial ^2 V}{\\partial{x^2}} + \\frac{\\partial ^2 V}{\\partial{y^2}} = 0 subject to the boundary conditions (i) V = 0 when y = 0 (ii) V = 0 when y = a (iii) V = V_0 when x = b (iv) V = V_0 when x = -b The argument runs as before, up to \\eqref{3.27} : V(x, y) = (A e^{kt} + B e^{-kt})(C \\sin ky + D \\cos ky) This time, however, we cannot set A = 0 ; the region in question does not extend to x = \\infty , so e^{kx} is perfectly acceptable. On the other hand, the situation is symmetric with respect to x, so V(-x, y) = V(x, y) , and it follows that A = B . Using e^{kx} + e^{-kx} = 2 \\cosh kx and absorbing 2A into C and D , we have V(x, y) = \\cosh kx (C \\sin ky + D\\cos ky) Boundary conditions (i) and (ii) require, as before, that D = 0 and k = n\\pi /a , so V(x, y) = C \\cosh (n \\pi x /a )\\sin(n \\pi y/a) \\tagl{3.41} Because V(x, y) is even in x, it will automatically meet conditions (iv) if it fits (iii). It remains, therefore, to construct the general linear combination V(x, y) = \\sum _{n=1}^{\\infty} C_n \\cosh (n \\pi x / a) \\sin(n \\pi y /a) and pick the coefficients C_n in such a way as to satisfy condition (iii): V(b, y) = \\sum_{n=1}^{\\infty} C_n \\cosh (n \\pi b /a) \\sin(n \\pi y/a) = V_0 This is the same problem in Fourier analysis that we faced before; I quote the result from \\eqref{3.35} ; C_n \\cosh (n \\pi b/a) = \\begin{cases} 0 & \\quad \\text {if n is even} \\\\ \\frac{4 V_0}{n \\pi} & \\quad \\text{if n is odd} \\end{cases} Conclusion : The potential in this case is given by V(x, y) = \\frac{4 V_0}{\\pi} \\sum_{n=1, 3, 5\\ldots} \\frac{1}{n} \\frac{\\cosh(n \\pi x/a)}{\\cosh(n \\pi b/a)} \\sin(n \\pi y/a) \\tagl{3.42} This function is shown in Fig. 3.21","title":"Example 3.4"},{"location":"ch3-3/#example-35","text":"An infinitely long rectangular metal pipe (sides a and b) is grounded, but one end, at x = 0 , is maintained at a specified potential V_0(y, z) , as indicated in Fig. 3.22. Find the potential inside the pipe. Solution This is genuinely a three-dimensional problem, \\frac{\\partial ^2 V}{\\partial{x^2}} + \\frac{\\partial ^2 V}{\\partial{y^2}} + \\frac{\\partial ^2 V}{\\partial{z^2}} = 0 \\tagl{3.43} subject to the boundary conditions - (i) V = 0 when y = 0 - (ii) V = 0 when y = a - (iii) V = 0 when z = 0 - (iv) V = 0 when z = b - (v) V \\rightarrow 0 as x \\rightarrow \\infty - (vi) V = V_0(y, z) whem x = 0 As always, we look for solutions that are products: V(x, y, z) = X(x)Y(y)Z(z) \\tagl{3.45} Putting this into \\eqref{3.43} and dividing by V, we find \\frac{1}{X} \\frac{d^2 X}{dx^2} + \\frac{1}{Y} \\frac{d^2 Y}{dy^2} + \\frac{1}{Z} \\frac{d^2 Z}{dz^2} = 0 It follows that \\frac{1}{X} \\frac{d^2 X}{dx^2} = C_1 , \\quad \\frac{1}{Y} \\frac{d^2 Y}{dy^2} = C_2 , \\quad \\frac{1}{Z} \\frac{d^2 Z}{dz^2} = C_3 , \\text{ with } C_1 + C_2 + C_3 = 0 Our previous experience in Ex. 3.3 suggests that C_1 must be positive, C_2 and C_3 negative. Setting C_2 = -k^2 and C_3 = -l^2 , we have C_1 = k^2 + l^2 , and hence \\frac{d^2 X}{dx^2} = (k^2 + l^2)X, \\quad \\frac{d^2 Y}{dy^2} = -k^2 Y, \\quad \\frac{d^2 Z}{dz^2} = -l^2 Z \\tagl{3.46} Once again, separation of variables has turned a partial differential equation into ordinary differential equations. The solutions are \\begin{align*} X(x) & = A e^{\\sqrt{k^2 + l^2} x} + B e^{- \\sqrt{k^2 + l^2} x} \\\\ Y(y) & = C \\sin ky + D \\cos ky \\\\ Z(z) & = E \\sin lz + F \\cos lz \\end{align*} Boundary condition (v) implies A =0 , (i) gives D = 0 , and (iii) yields F = 0 whereas (ii) and (iv) require that k = n\\pi /a and l =m \\pi /b , where n and m are positive integers. Combining the remaining constants, we are left with V(x, y, z) = C e^{-\\pi \\sqrt{(n/a)^2 + (m/b)^2}x} \\sin (n \\pi y / a) \\sin(m \\pi z /b) \\tagl{3.47} This solution meets all the boundary conditions except (vi). It contains two unspecified integers (n and m), and the most general linear combination is a double sum V(x, y, z) = \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C e^{-\\pi \\sqrt{(n/a)^2 + (m/b)^2}x} \\sin (n \\pi y / a) \\sin(m \\pi z /b) = V_0(y, z) \\tagl{3.48} We hope to fit the remaining boundary condition, V(0, y, z) = \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C \\sin (n \\pi y / a) \\sin(m \\pi z /b) = V_0(y, z) \\tagl{3.49} by appropriate choice of the coefficients C_{n, m} . To determine these constants, we multiply by \\sin(n' n \\pi y/a) \\sin(m' \\pi z / b) , where n' and m' are arbitrary positive integers, and integrate \\sum_{n=1} ^{\\infty} \\sum_{m = 1} ^{\\infty} C_{n, m} \\int_0 ^a \\sin (n \\pi y/a) \\sin(n' \\pi y/a) \\dd{y} \\int_0 ^b \\sin(m \\pi z/b) \\sin (m' \\pi z/b) \\dd{z} \\\\ = \\int_0 ^a \\int_0 ^b V_0(y, z) \\sin(n' \\pi y/a) \\sin(m' \\pi z/b) \\dd{y} \\dd{z} Quoting \\eqref{3.33} , the left side is (ab/4) C_{n', m'} , so C_{n, m} = \\frac{4}{ab} \\int_0 ^a \\int_0 ^b V_0(y, z) \\sin (n \\pi y/a) \\sin(m\\pi z/b) \\dd{y} \\dd{z} \\tagl{3.50} Equation \\eqref{3.48} , with the coefficients given by \\eqref{3.50} , is the solution to our problem. For instance, if the end of the tube is a conductor at constant potential V_0 , C_{n, m} = \\frac{4}{ab} \\int_0 ^a \\sin(n \\pi y/a) \\dd{y} \\int_0 ^b \\sin(m \\pi z/b) \\dd{z} \\\\ = \\begin{cases} 0 & \\qquad \\text{if n or m is even} \\\\ \\frac{16 V_0}{\\pi^2 nm} & \\qquad \\text{if n and m are odd} \\end{cases} \\tagl{3.51} In this case, V(x, y, z) = \\frac{16V_0}{\\pi^2} \\sum_{n,m=1,3,5,\\ldots} ^{\\infty} \\frac{1}{nm} e^{-\\pi \\sqrt{(n/a)^2 + (m/b)^2}x} \\sin(n \\pi y/a) \\sin(m \\pi z/b) \\tagl{3.52} Notice that successive terms decrease rapidly; a reasonable approximation would be obtained by keeping only the first few.","title":"Example 3.5"},{"location":"ch3-3/#332-spherical-coordinates","text":"In the examples considered so far, Cartesian coordinates were clearly appropriate, since the boundaries were planes. For round objects, spherical coordinates are more natural. In the spherical system, Laplace's equation reads: \\frac{1}{r^2} \\pdv{}{r} \\left( r^2 \\pdv{V}{r} \\right) + \\frac{1}{r^2 \\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{V}{\\theta} \\right) + \\frac{1}{r^2\\sin ^2 \\theta} \\frac{\\partial ^2 V}{\\partial \\phi ^2} = 0 \\tagl{3.53} I shall assume the problem has azimuthal symmetry , so that V is independent of \\phi ; In that case, \\eqref{3.53} reduces to \\pdv{}{r} \\left( r^2 \\pdv{V}{r} \\right) + \\frac{1}{\\sin \\theta} \\pdv{}{\\theta} \\left( \\sin \\theta \\pdv{V}{\\theta} \\right) = 0 \\tagl{3.54} As before, we look for solutions that are products: V(r, \\theta) = R(r) \\Theta (\\theta) \\tagl{3.55} Putting this into \\eqref{3.54} , and dividing by V , \\frac{1}{R} \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) + \\frac{1}{\\Theta \\sin \\theta} \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = 0 \\tagl{3.56} Since the first term depends only on r , and the second only on \\theta , it follows that each must be a constant: \\frac{1}{R} \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) = l(l+1), \\quad \\frac{1}{\\Theta \\sin \\theta} \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = -l(l+1) \\tagl{3.57} Here l(l+1) is just a fancy way of writing the separation constant, whose convenience will appear shortly. As always, separation of variables has converted a partial differential equation into ordinary differential equations. The radial equation, \\dv{}{r} \\left( r^2 \\dv{R}{r} \\right) = l(l+1)R \\tagl{3.58} has the general solution R(r) = A r^l + \\frac{B}{r^{l+1}} \\tagl{3.59} as you can easily check; A and B are the two arbitrary constants to be expected in the solution of a second-order differential equation. But the angular equation, \\dv{}{\\theta} \\left( \\sin \\theta \\dv{\\Theta}{\\theta} \\right) = -l(l+1) \\sin \\theta \\Theta \\tagl{3.60} is not so simple. The solutions are Legendre polynomials in the variable \\cos \\theta : \\Theta (\\theta ) = P_l (\\cos \\theta ) \\tagl{3.61} P_l (x) is most conveniently defined by the Rodrigues formula : P_l(x) \\equiv \\frac{1}{2^l l!}\\left( \\dv{}{x} \\right)^l (x^2 - 1)^l \\tagl{3.62} The first few Legendre polynomials are listed: Legendre Polynomials P_0 - P_5 \\begin{align*} P_0(x) & = 1 \\\\ P_1(x) & = x \\\\ P_2(x) & = (3x^2 - 1)/2 \\\\ P_3(x) & = (5x^3 - 3x)/2 \\\\ P_4(x) & = (35x^4 - 30x^2 + 3)/8 \\\\ P_5(x) & = (63x^5 - 70x^3 + 15x)/8 \\end{align*} Notice that P_l(x) is (as the name suggests) an _l_th-order polynomial in x; it contains only even powers if l is even, and only odd powers if l is odd. The factor in front (1/2^l l! was chosen in order that P_l(1) = 1 \\tagl{3.63} The Rodrigues formula obviously only works for nonnegative integer values of l. Moreover, it provides us with only one solution. But \\eqref{3.60} is second-order, and it should possess two independent solutions for every value of l . It turns out that these \"other solutions\" blow up at \\theta = 0 and/or \\theta = \\pi , and are therefore unacceptable on physical grounds. For instance, the second solution for l=0 is \\Theta(\\theta) = \\ln \\left( \\tan \\frac{\\theta}{2} \\right) \\tagl{3.64} You might want to check for yourself that this satisfies \\eqref{3.60} . In the case of azimuthal symmetry, then, the most general separable solution to Laplace's equation, consistent with minimal physical requirements, is V(r, \\theta) = \\left( A r^l + \\frac{B}{r^{l+1}} \\right) P_l(\\cos \\theta) (There was no need to include an overall constant in \\eqref{3.61} because it can be absorbed into A and B at this stage.) As before, separation of variables yields an infinite set of solutions, one for each l . The general solution is the linear combination of separable solutions: V(r, \\theta) = \\sum_{l=0} ^{\\infty} \\left( A r^l + \\frac{B}{r^{l+1}} \\right) P_l(\\cos \\theta) \\tagl{3.65} The following examples illustrate the power of this important result.","title":"3.3.2: Spherical Coordinates"},{"location":"ch3-3/#example-36","text":"The potential V_0(\\theta) is specified on the surface of a hollow sphere, of radius R . Find the potential inside the sphere. Solution In this case, B_l = 0 for all l , otherwise the potential would blow up at the origin. Thus, V(r, \\theta) = \\sum_{l = 0} ^{\\infty} A_l r^l P_l(\\cos \\theta) \\tagl{3.66} At r = R this must match the specified function V_0(\\theta) : V(R, \\theta) = \\sum_{l=0}^\\infty A_l R^l P_l(\\cos \\theta) = V_0(\\theta) \\tagl{3.67} Can this equation be satisfied, for an appropriate choice of coefficients A_l ? Yes: The Legendre polynomials (like the sines) constitute a complete set of functions, on the interval -1 \\leq x \\leq 1 (0 \\leq \\theta \\leq \\pi) . How do we determine the constants? Again, by Fourier's trick, for the Legendre polynomials (like the sines) are orthogonal functions: \\begin{align*} \\int_{-1}^1 P_l(x) P_{l'}(x) \\dd{x} & = \\int_0 ^\\pi P_l(\\cos \\theta) P_{l'}(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\\\ & = \\begin{cases} 0, & \\quad \\text{if } l' \\neq l \\\\ \\frac{2}{2l +1} , & \\quad \\text{if } l' = l \\end{cases} \\end{align*} \\tagl{3.68} Thus, multiplying \\eqref{3.67} by P_{l'}(\\cos \\theta) \\sin \\theta and integrating, we have A_{l'} R^{l'} \\frac{2}{2l' + 1} = \\int_{0} ^\\pi V_0(\\theta) P_{l'}(\\cos \\theta) \\sin \\theta \\dd{\\theta} or A_l = \\frac{2l+1}{2R^l} \\int_0 ^\\pi V_0(\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.69} \\eqref{3.66} is the solution to our problem, with the coefficients given by \\eqref{3.69} . It can be difficult to evaluate integrals of the form \\eqref{3.69} analytically, and in practice it is often easier to solve \\eqref{3.67} \"by eyeball.\" For instance, suppose we are told that the potential on the sphere is V_0(\\theta) = k \\sin^2 (\\theta/2) \\tagl{3.70} where k is constant. Using the half-angle formula, we rewrite this as V_0(\\theta) = \\frac{k}{2}(1 - \\cos \\theta) = \\frac{k}{2} [P_0(\\cos \\theta) - P_1 (\\cos \\theta)] Putting this into \\eqref{3.67} , we read off immediately that A_0 = k/2 , A_1 = -k/(2R) , and all other A_l 's vanish. Therefore V(r, \\theta) = \\frac{k}{2} \\left[ r^0 P_{0}(\\cos \\theta) - \\frac{r^1}{R} P_1 (\\cos \\theta) \\right] = \\frac{k}{2} \\left( 1 - \\frac{r}{R} \\cos \\theta \\right) \\tagl{3.71}","title":"Example 3.6"},{"location":"ch3-3/#example-37","text":"The potential V_0(\\theta) is again specified on the surface of a sphere of radius R , but this time we are asked to find the potential outside , assuming there is no charge there. Solution In this case it's the A_l 's that must be zero (or else V would not go to zero at \\infty ), so V(r, \\theta) = \\sum_{l=0} ^\\infty \\frac{B_l}{r^{l+1}} P_l(\\cos \\theta) = V_0(\\theta) \\tagl{3.72} Multiplying by P_{l'}(\\cos \\theta) \\sin \\theta and integrating - exploiting, again, the orthogonality relation 3.68 - we have \\frac{B_{l'}}{R^{l'+1}} \\frac{2}{2l' + 1} = \\int_0 ^\\pi V_0(\\theta) P_{l'}(\\cos \\theta) \\sin \\theta \\dd{\\theta} or B_l = \\frac{2l + 1}{2} R^{l+1} \\int_0 ^\\pi V_0(\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.73} \\eqref{3.72} , with the coefficients given by \\eqref{3.73} , is the solution to our problem.","title":"Example 3.7"},{"location":"ch3-3/#example-38","text":"An uncharged metal sphere of radius R is placed in an otherwise uniform electric field \\vec{E} = E_0 \\hat{z} . The field will push positive charge to the 'northern' surface of the sphere, and - symmetrically - negative charge to the 'southern' surface (Fig. 3.24). This induced charge, in turn, distorts the field in the neighborhood of the sphere. Find the potential in the region outside the sphere. Solution The sphere is an equipotential - we may as well set it to zero. Then by symmetry the entire xy plane is at potential zero. This time, however, V does not go to zero at large z . In fact, far from the sphere the field is E_0 \\hat{z} and hence V \\rightarrow - E_0 z + C Since V = 0 in the equatorial plane, the constant C must be zero. Accordingly, the boundary conditions for this problem are - (i) V = 0 when r = R - (ii) V \\rightarrow - E_0 r \\cos \\theta for r \\gg R We must fit these boundary conditions with a function of the form \\eqref{3.65} . The first condition yields A_l R^l + \\frac{B_l}{R^{l+1}} = 0 or B_l = -A_l R^{2l+1} \\tagl{3.75} so V(r, \\theta) = \\sum_{l = 0} ^{\\infty} A_l \\left( r^l - \\frac{R^{2l+1}}{r^{l+1}} \\right) P_l(\\cos \\theta) For r \\gg R , the second term in parentheses is negligible, and therefore the condition (ii) requires that \\sum_{l=0}^\\infty A_l R^{l} P_l (\\cos \\theta) = - E_0 r \\cos \\theta Evidently only one term is present: l = 1 . In fact, since P_1(\\cos \\theta) = \\cos \\theta we can read off immediately A_1 = - E_0, \\qquad \\text{ all other }A_l's \\text{ zero} Conclusion : V(r, \\theta) = - E_0 \\left( r - \\frac{R^3}{r^2} \\right) \\cos \\theta \\tagl{3.76} The first term (-E_0 r \\cos \\theta) is due to the external field; the contribution attributable to the induced charge is E_0 \\frac{R^3}{r^2} \\cos \\theta If you want to know the induced charge density, it can be calculated in the usual way: \\sigma(\\theta) = - \\epsilon_0 \\left. \\pdv{V}{r} \\right|_{r = R} = \\epsilon_0 E_0 \\left. \\left( 1 + 2 \\frac{R^3}{r^3} \\right) \\cos \\theta \\right|_{r = R} = 3 \\epsilon_0 E_0 \\cos \\theta \\tagl{3.77} As expected, it is positive in the 'northern' hemisphere 0 \\leq \\theta \\leq \\pi /2 and negative in the 'southern' \\pi/2 \\leq \\theta \\leq \\pi .","title":"Example 3.8"},{"location":"ch3-3/#example-39","text":"A specified charge density \\sigma_0(\\theta) is glued over the surface of a spherical shell of radius R . Find the resulting potential inside and outside the sphere. Solution You could, of course, do this by direct integration: V = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{\\sigma_0}{\\gr} \\dd{a} but separation of variables is often easier. For the interior region, we have V(r, \\theta) = \\sum_{l = 0}^\\infty A_l r^l P_l (\\cos \\theta) \\quad (r \\leq R) \\tagl{3.78} (no B_l terms - they blow up at the origin); in the exterior region V(r, \\theta) = \\sum_{l=0}^\\infty \\frac{B_l}{r^{l+1}} P_l(\\cos \\theta) \\quad (r \\geq R) \\tagl{3.79} (no A_l terms - they don't go to zero at infinity). These two functions must be joined together by the appropriate boundary conditions at the surface itself. First, the potential is continuous at r = R (Eq. 2.34): \\sum_{l=0}^\\infty A_l R^l P_l(\\cos \\theta) = \\sum_{l=0} ^\\infty \\frac{B_l}{R^{l+1}} P_l(\\cos \\theta) \\tagl{3.80} It follows that the coefficients of like Legendre polynomial are equal: B_l = A_l R^{2l+1} \\tagl{3.81} (To prove that formally, multiply both sides of \\eqref{3.80} by P_{l'} (\\cos \\theta)\\sin \\theta and integrate from 0 to \\pi , using the orthogonality relation \\eqref{3.68} .) Second, the radial derivative of V suffers a discontinuity at the surface (Eq. 2.36): \\left. \\left( \\pdv{V_{out}}{r} - \\pdv{V_{in}}{r} \\right) \\right|_{r = R} = - \\frac{1}{\\epsilon_0} \\sigma_0(\\theta) \\tagl{3.82} Thus, - \\sum_{l=0}^\\infty (l+1) \\frac{B_l}{R^{l+2}} P_l(\\cos \\theta) - \\sum_{l=0}^\\infty l A_l R^{l-1} P_l(\\cos \\theta) = - \\frac{1}{\\epsilon_0} \\sigma_0 (\\theta) or, using \\eqref{3.81} , \\sum_{l=0}^\\infty (2l+1) A_l R^{l-1} P_l(\\cos \\theta) = \\frac{1}{\\epsilon_0} \\sigma_0(\\theta) \\tagl{3.83} From here, the coefficients can be determined using Fourier's trick A_l = \\frac{1}{2 \\epsilon_0 R^{l-1}} \\int_0 ^\\pi \\sigma_0 (\\theta) P_l(\\cos \\theta) \\sin \\theta \\dd{\\theta} \\tagl{3.84} Equations 3.78 and 3.79 constitute the solution to our problem, with the coefficients given by \\eqref{3.81} and \\eqref{3.84} . For instance, if \\sigma_0(\\theta) = k \\cos \\theta = k P_1 (\\cos \\theta) \\tagl{3.85} for some constant k , then all the A_l 's are zero except for l = 1 , and A_1 = \\frac{k}{2 \\epsilon_0} \\int_0 ^\\pi [P_1(\\cos \\theta)]^2 \\sin \\theta \\dd{\\theta} = \\frac{k}{3\\epsilon_0} The potential inside the sphere is therefore V(r, \\theta) = \\frac{k}{3 \\epsilon_0} r \\cos \\theta \\quad (r \\leq R) \\tagl{3.86} whereas outside the sphere V(r, \\theta) = \\frac{kR^3}{3 \\epsilon_0} \\frac{1}{r^2} \\cos \\theta \\quad (r \\geq R) \\tagl{3.87} In particular, if \\sigma_0(\\theta) is the induced charge on a metal sphere in an external field E_0(\\hat{z}) , so that k = 3 \\epsilon_0 E_0 \\eqref{3.77} , then the potential inside is E_0 r \\cos \\theta = E_0 z , and the field is -E_0 \\hat{z} - exactly right to cancel off the external field, as of course it should be. Outside the sphere the potential due to this surface charge is E_0 \\frac{R^3}{r^2} \\cos \\theta consistent with our conclusion in Example 3.8.","title":"Example 3.9"},{"location":"ch3-4/","text":"3.4: Multipole Expansion 3.4.1: Approximate Potentials at Large Distances If you are very far away from a localized charge distribution, it \"looks\" like a point charge, and the potential is - to good approximation - (1/4 \\pi \\epsilon_0) Q/r , where Q is the total charge. We have often used this as a check on formulas for V . But what if Q is zero? You might reply that the potential is then approximately zero, and of course, you're right in a sense (indeed, the potential at large r is pretty small even if Q is not zero). But we're looking for something a bit more informative than that. Example 3.10 A (physical) electric dipole consists of two equal and opposite charges (\\pm q) separated by a distance d . Find the approximate potential at points far from the dipole Solution Let \\gr_- be the distance from -q and \\gr_{+} be the distance from +q (Fig. 3.26). Then V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{\\gr_{+}} - \\frac{1}{\\gr_{-}} \\right) and (from the law of cosines), \\gr_{\\pm} ^2 = r^2 + (d/2)^2 \\mp r d \\cos \\theta = r^2 \\left( 1 \\mp \\frac{d}{r} \\cos \\theta + \\frac{d^2}{4r^2} \\right) We're interested in the regime r \\gg d , so the third term is negligible, and the binomial expansion yields \\frac{1}{\\gr_{\\pm}} \\approx \\frac{1}{r} \\left( 1 \\mp \\frac{d}{r} \\cos \\theta \\right) ^{-1/2} \\approx \\frac{1}{r} \\left( 1 \\pm \\frac{d}{2r} \\cos \\theta \\right) Thus \\frac{1}{\\gr_+} - \\frac{1}{\\gr_{-}} \\approx \\frac{d}{r^2} \\cos \\theta and hence V(r) \\approx \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd \\cos \\theta}{r^2} \\tagl{3.90} The potential of a dipole goes like 1/r^2 at large r ; as we might have anticipated, it falls off more rapidly than the potential of a point charge. If we put together a pair of equal and opposite dipoles to make a quadrupole , the potential goes like 1/r^3 ; for back-to-back quadrupoles (an octopole ), it goes like 1/r^4 , and so on. Figure 3.27 summarizes the hierarchy; for completeness I have included the electric monopole (point charge), whose potential, of course, goes like 1/r Example 3.10 pertains to a very special charge configuration. I propose now to develop a systematic expansion for the potential of any localized charge distribution, in powers of 1/r . Figure 3.28 defines the relevant variables; the potential at r is given by V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{1}{\\gr} \\rho(\\vec{r'}) \\dd{\\tau'} \\tagl{3.91} Using the law of cosines, \\gr ^2 = r^2 + (r')^2 - 2r r' \\cos \\alpha = r^2 \\left[ 1 + \\left( \\frac{r'}{r} \\right)^2 - 2 \\left( \\frac{r'}{r} \\right)\\cos \\alpha \\right] where \\alpha is the angle between \\vec{r} and \\vec{r'} . Thus, \\gr = r \\sqrt{1 + \\epsilon} \\tagl{3.92} with \\epsilon \\equiv \\left( \\frac{r'}{r} \\right)\\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right) For points well outside the charge distribution, \\epsilon is much less than 1, and this invites a binomial expansion: \\frac{1}{\\gr} = \\frac{1}{r} (1 + \\epsilon)^{-1/2} = \\frac{1}{r} \\left( 1 - \\frac{1}{2} \\epsilon + \\frac{3}{8} \\epsilon^2 - \\frac{5}{16} \\epsilon^3 + \\ldots \\right) \\tagl{3.93} or, in terms of r, r' , and \\alpha , \\begin{align*} \\frac{1}{\\gr} & = \\frac{1}{r} \\left[ 1 - \\frac{1}{2} \\left( \\frac{r'}{r} \\right) \\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right) + \\frac{3}{8} \\left( \\frac{r'}{r} \\right)^2 \\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right)^2 \\right. \\\\ & \\qquad \\left. - \\frac{5}{16} \\left( \\frac{r'}{r} \\right)^3 \\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right)^3 + \\ldots \\right] \\\\ & = \\frac{1}{r} \\left[ 1 + \\left( \\frac{r'}{r} \\right)(\\cos \\alpha) + \\left( \\frac{r'}{r} \\right) \\left( \\frac{3 \\cos ^2 \\alpha - 1}{2} \\right) \\right. \\\\ & \\qquad \\left. \\left( \\frac{r'}{r} \\right)^3 \\left( \\frac{5\\cos ^3 \\alpha - 3 \\cos \\alpha}{2} \\right) + \\ldots \\right] \\end{align*} In the last step, I have collected together like powers of (r'/r) ; surprisingly, their coefficients (the terms in parentheses) are Legendre polynomials! The remarkable result is that \\frac{1}{\\gr} = \\frac{1}{r} \\sum_{n=0}^\\infty \\left( \\frac{r'}{r} \\right)^n P_n (\\cos \\alpha) \\tagl{3.94} Substituting this back into \\eqref{3.91} , and noting that r is constant, as far as the integration is concerned, I conclude that V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{n=0} ^\\infty \\frac{1}{r^{(n+1)}} \\int (r') P_n(\\cos \\alpha) \\rho(\\vec{r'}) \\dd{\\tau'} \\tagl{3.95} or, more explicitly, V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{1}{r} \\int \\rho(\\vec{r'}) \\dd{\\tau'} + \\frac{1}{r^2} \\int r' \\cos \\alpha \\rho(\\vec{r'})\\dd{\\tau'} \\right. \\\\ + \\left. \\frac{1}{r^3} \\int (r')^2 \\left( \\frac{3}{2} \\cos^2 \\alpha - \\frac{1}{2} \\right) \\rho(\\vec{r'}) \\dd{\\tau'} + \\ldots \\right] \\tagl{3.96} This is the desired result - the multipole expansion of V in powers of 1/r . The first term (n=0) is the monopole contribution (it goes like 1/r ); the second (n=1) is the dipole (it goes like 1/r^2 ); the third is quadrupole; the fourth octopole, and so on. Remember that \\alpha is the angle between \\vec{r} and \\vec{r'} , so the integrals depend on the direction to the field point. If you are interested in the potential along the z' axis (or - putting it the other way round - if you orient your \\vec{r'} coordinates so the z' axis lies along \\vec{r} ), then \\alpha is the usual polar angle \\theta' . As it stands, \\eqref{3.95} is exact , but it is useful primarily as an approximation scheme: the lowest nonzero term in the expansion provides the approximate potential at large r , and the successive terms tell us how to improve the approximation if greater precision is required. 3.4.2: The Monopole and Dipole Terms Ordinarily, the multipole expansion is dominated (at large r) by the monopole term: V_{mon} (\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q}{r} where Q =\\int \\rho \\dd{\\tau} is the total charge of the configuration. This is just what we expect for the approximate potential at large distances from the charge. For a point charge at the origin, V_{mon} is the exact potential, not merely a first approximation at large r; in this case, all the higher multipoles vanish. If the total charge is zero, the dominant term in the potential will be the dipole (unless, of course, it also vanishes): V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\int r' \\cos \\alpha \\rho(\\vec{r'}) \\dd{\\tau'} Since \\alpha is the angle between r' and r (Fig 2.38), r' \\cos \\alpha = \\hat{r} \\cdot \\vec{r'} and the dipole potential can be written more succinctly: V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\hat{r} \\cdot \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} This integral (which does not depend on \\vec{r} ) is called the dipole moment of the distribution: \\vec{p} \\equiv \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} \\tagl{3.98} and the dipole contribution to the potential simplifies to V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\hat{r}}{r^2} \\tagl{3.99} The dipole moment is determined by the geometry (size, shape, and density) of the charge distribution. \\eqref{3.98} translates in the usual way (Sect 2.1.4) for point, line, and surface charges. Thus, the dipole moment for a collection of point charges is \\vec{p} = \\sum_{i=1} ^n q_i \\vec{r'}_i \\tagl{3.100} For a physical dipole (equal and opposite charges \\pm q ), \\vec{p} = q\\vec{r'_+} - q \\vec{r_- ' } = q(\\vec{r' _+} - \\vec{r'_-}) = q \\vec{d} \\tagl{3.101} where \\vec{d} is the vector from the negative charge to the positive one (Fig. 3.29). Is this consistent with what we got in Example 3.10? Yes: If you put \\eqref{3.101} into \\eqref{3.99} , you recover \\eqref{3.90} . Notice, however, that this is only the approximate potential of the physical dipole - evidently there are higher multipole contributions. Of course, as you go farther and farther away, V_{dip} becomes a better and better approximation, since the higher terms die off more rapidly with increasing r . By the same token, at a fixed r the dipole approximation improves as you shrink the separation distance d . To construct a perfect dipole whose potential is given exactly by \\eqref{3.99} , you'd have to let d approach zero. Unfortunately you then lose the dipole term too, unless you simultaneously arrange for q to go to infinity! A physical dipole becomes a pure dipole, then, in the rather artificial limit d \\rightarrow 0. q \\rightarrow \\infty , with the product qd = p held fixed. When someone uses the word \"dipole,\" you can't always tell whether they mean a physical dipole (with finite separation between the charges) or an ideal dipole. If in doubt, assume that d is small enough that you can safely apply \\eqref{3.99} . Dipole moments are vectors , and they add accordingly: if you have two dipoles \\vec{p_1} and \\vec{p_2} , the total dipole moment is \\vec{p_1} + \\vec{p_2} . For instance, with four charges at the corners of a square, as shown in Fig. 3.30, the net dipole moment is zero. You can see this by combining the charges in pairs or by adding up the four contributions individually using \\eqref{3.100} . This is a quadrupole , as I indicated earlier, and its potential is dominated by the quadrupole term in the multipole expansion. 3.4.3: Origin of Coordinates in Multipole Expansions I mentioned earlier that a point charge at the origin constitutes a \"pure\" monopole. If it is not at the origin, it's no longer a pure monopole. For instance, the charge in Fig. 3.32 has a dipole moment \\vec{p} = q d \\hat{y} , and a corresponding dipole term in its potential. The monopole potential (1/4 \\pi \\epsilon_0) q/r is not quite correct for this configuration; rather, the exact potential is (1/4 \\pi \\epsilon_0) q/\\gr . The multipole expansion is, remember, a series in inverse powers of r (the distance to the origin), and when we expand 1/\\gr , we get all powers, not just the first. So moving the origin (or, what amounts to the same thing, moving the charge) can radically alter a multipole expansion. The monopole moment Q does not change, since the total charge is obviously independent of the coordinate system. (In Fig. 3.32, the monopole term was unaffected when we moved q away from the origin - it's just that it was no longer the whole story: a dipole term - and for that matter all higher poles - appeared as well.) Ordinarily, the dipole moment does change when you shift the origin, but there is an important exception: If the total charge is zero, then the dipole moment is independent of the choice of origin. For suppose we displace the origin by an amount \\vec{a} (Fig. 3.33). The new dipole moment is then \\begin{align*} \\vec{p_2} & = \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} = \\int (\\vec{r'} - \\vec{a} ) \\rho (\\vec{r'}) \\dd{\\tau'} \\\\ & = \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} - \\vec{a} \\int \\rho(\\vec{r'}) \\dd{\\tau'} = \\vec{p} - Q \\vec{a} \\end{align*} In particular, if Q = 0 , the \\vec{p_2} = \\vec{p} . So if someone asks for the dipole moment in Fig 3.34(a), you can answer with confidence \" q \\vec{d} ,\" but if you're asked for the dipole moment in Fig 3.34(b), the appropriate response would be \"With respect to what origin?\" 3.4.4: The Electric Field of a Dipole So far we have only worked with potentials . Now I would like to calculate the electric field of a (perfect) dipole. If we choose coordinates so that \\vec{p} is at the origin and points in the z direction (Fig. 3.36), then the potential at r, \\theta is \\eqref{3.99} : V_{dip} (r, \\theta) = \\frac{\\hat{r} \\cdot \\vec{p}}{4 \\pi \\epsilon_0 r^2} = \\frac{p \\cos \\theta}{4 \\pi \\epsilon_0 r^2} \\tagl{3.102} To get the field, we take the negative gradient of V : \\begin{align*} E_r & = - \\pdv{V}{r} = \\frac{2 p \\cos \\theta}{4 \\pi \\epsilon_0 r^3} \\\\ E_\\theta & = - \\frac{1}{r} \\pdv{V}{\\theta} = \\frac{p \\sin \\theta}{4 \\pi \\epsilon_0 r^3} \\\\ E_\\phi & = - \\frac{1}{r \\sin \\theta} \\pdv{V}{\\phi} = 0 \\end{align*} Thus, \\vec{E_{dip}} (r, \\theta) = \\frac{p}{4 \\pi \\epsilon_0 r^3}(2 \\cos \\theta \\hat{r} + \\sin \\theta \\hat{\\theta}) \\tagl{3.103} This formula makes explicit reference to a particular coordinate system (spherical) and assumes a particular orientation for \\vec{p} (along z). It can be recast in a coordinate-free form, analogous to the potential in \\eqref{3.99} - See problem 3.36. Notice that the dipole falls off as the inverse cube of r; the monopole field (Q / 4 \\pi \\epsilon_0 r^2) \\hat{r} goes as the inverse square, of course. Quadrupole fields go like 1/r^4 , octopole like 1/r^5 , and so on. (This merely reflects how the respective potentials fall off - the gradient introduces another factor of 1/r ). Figure 3.37(a) shows the field lines of a \"pure\" dipole \\eqref{3.103} . For comparison, I have also sketched the field lines for a \"physical\" dipole, in Fig 3.37(b). Notice how similar the two pictures become if you blot out the central region; up close, however, they are entirely different. Only for points r \\gg d does \\eqref{3.103} represent a valid approximation to the field of a physical dipole. As I mentioned earlier, this regime can be reached either by going to large r or by squeezing the charges very close together.","title":"3.4 - Multipole Expansion"},{"location":"ch3-4/#34-multipole-expansion","text":"","title":"3.4: Multipole Expansion"},{"location":"ch3-4/#341-approximate-potentials-at-large-distances","text":"If you are very far away from a localized charge distribution, it \"looks\" like a point charge, and the potential is - to good approximation - (1/4 \\pi \\epsilon_0) Q/r , where Q is the total charge. We have often used this as a check on formulas for V . But what if Q is zero? You might reply that the potential is then approximately zero, and of course, you're right in a sense (indeed, the potential at large r is pretty small even if Q is not zero). But we're looking for something a bit more informative than that.","title":"3.4.1: Approximate Potentials at Large Distances"},{"location":"ch3-4/#example-310","text":"A (physical) electric dipole consists of two equal and opposite charges (\\pm q) separated by a distance d . Find the approximate potential at points far from the dipole Solution Let \\gr_- be the distance from -q and \\gr_{+} be the distance from +q (Fig. 3.26). Then V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\left( \\frac{q}{\\gr_{+}} - \\frac{1}{\\gr_{-}} \\right) and (from the law of cosines), \\gr_{\\pm} ^2 = r^2 + (d/2)^2 \\mp r d \\cos \\theta = r^2 \\left( 1 \\mp \\frac{d}{r} \\cos \\theta + \\frac{d^2}{4r^2} \\right) We're interested in the regime r \\gg d , so the third term is negligible, and the binomial expansion yields \\frac{1}{\\gr_{\\pm}} \\approx \\frac{1}{r} \\left( 1 \\mp \\frac{d}{r} \\cos \\theta \\right) ^{-1/2} \\approx \\frac{1}{r} \\left( 1 \\pm \\frac{d}{2r} \\cos \\theta \\right) Thus \\frac{1}{\\gr_+} - \\frac{1}{\\gr_{-}} \\approx \\frac{d}{r^2} \\cos \\theta and hence V(r) \\approx \\frac{1}{4 \\pi \\epsilon_0} \\frac{qd \\cos \\theta}{r^2} \\tagl{3.90} The potential of a dipole goes like 1/r^2 at large r ; as we might have anticipated, it falls off more rapidly than the potential of a point charge. If we put together a pair of equal and opposite dipoles to make a quadrupole , the potential goes like 1/r^3 ; for back-to-back quadrupoles (an octopole ), it goes like 1/r^4 , and so on. Figure 3.27 summarizes the hierarchy; for completeness I have included the electric monopole (point charge), whose potential, of course, goes like 1/r Example 3.10 pertains to a very special charge configuration. I propose now to develop a systematic expansion for the potential of any localized charge distribution, in powers of 1/r . Figure 3.28 defines the relevant variables; the potential at r is given by V(r) = \\frac{1}{4 \\pi \\epsilon_0} \\int \\frac{1}{\\gr} \\rho(\\vec{r'}) \\dd{\\tau'} \\tagl{3.91} Using the law of cosines, \\gr ^2 = r^2 + (r')^2 - 2r r' \\cos \\alpha = r^2 \\left[ 1 + \\left( \\frac{r'}{r} \\right)^2 - 2 \\left( \\frac{r'}{r} \\right)\\cos \\alpha \\right] where \\alpha is the angle between \\vec{r} and \\vec{r'} . Thus, \\gr = r \\sqrt{1 + \\epsilon} \\tagl{3.92} with \\epsilon \\equiv \\left( \\frac{r'}{r} \\right)\\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right) For points well outside the charge distribution, \\epsilon is much less than 1, and this invites a binomial expansion: \\frac{1}{\\gr} = \\frac{1}{r} (1 + \\epsilon)^{-1/2} = \\frac{1}{r} \\left( 1 - \\frac{1}{2} \\epsilon + \\frac{3}{8} \\epsilon^2 - \\frac{5}{16} \\epsilon^3 + \\ldots \\right) \\tagl{3.93} or, in terms of r, r' , and \\alpha , \\begin{align*} \\frac{1}{\\gr} & = \\frac{1}{r} \\left[ 1 - \\frac{1}{2} \\left( \\frac{r'}{r} \\right) \\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right) + \\frac{3}{8} \\left( \\frac{r'}{r} \\right)^2 \\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right)^2 \\right. \\\\ & \\qquad \\left. - \\frac{5}{16} \\left( \\frac{r'}{r} \\right)^3 \\left( \\frac{r'}{r} - 2 \\cos \\alpha \\right)^3 + \\ldots \\right] \\\\ & = \\frac{1}{r} \\left[ 1 + \\left( \\frac{r'}{r} \\right)(\\cos \\alpha) + \\left( \\frac{r'}{r} \\right) \\left( \\frac{3 \\cos ^2 \\alpha - 1}{2} \\right) \\right. \\\\ & \\qquad \\left. \\left( \\frac{r'}{r} \\right)^3 \\left( \\frac{5\\cos ^3 \\alpha - 3 \\cos \\alpha}{2} \\right) + \\ldots \\right] \\end{align*} In the last step, I have collected together like powers of (r'/r) ; surprisingly, their coefficients (the terms in parentheses) are Legendre polynomials! The remarkable result is that \\frac{1}{\\gr} = \\frac{1}{r} \\sum_{n=0}^\\infty \\left( \\frac{r'}{r} \\right)^n P_n (\\cos \\alpha) \\tagl{3.94} Substituting this back into \\eqref{3.91} , and noting that r is constant, as far as the integration is concerned, I conclude that V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\sum_{n=0} ^\\infty \\frac{1}{r^{(n+1)}} \\int (r') P_n(\\cos \\alpha) \\rho(\\vec{r'}) \\dd{\\tau'} \\tagl{3.95} or, more explicitly, V(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\left[ \\frac{1}{r} \\int \\rho(\\vec{r'}) \\dd{\\tau'} + \\frac{1}{r^2} \\int r' \\cos \\alpha \\rho(\\vec{r'})\\dd{\\tau'} \\right. \\\\ + \\left. \\frac{1}{r^3} \\int (r')^2 \\left( \\frac{3}{2} \\cos^2 \\alpha - \\frac{1}{2} \\right) \\rho(\\vec{r'}) \\dd{\\tau'} + \\ldots \\right] \\tagl{3.96} This is the desired result - the multipole expansion of V in powers of 1/r . The first term (n=0) is the monopole contribution (it goes like 1/r ); the second (n=1) is the dipole (it goes like 1/r^2 ); the third is quadrupole; the fourth octopole, and so on. Remember that \\alpha is the angle between \\vec{r} and \\vec{r'} , so the integrals depend on the direction to the field point. If you are interested in the potential along the z' axis (or - putting it the other way round - if you orient your \\vec{r'} coordinates so the z' axis lies along \\vec{r} ), then \\alpha is the usual polar angle \\theta' . As it stands, \\eqref{3.95} is exact , but it is useful primarily as an approximation scheme: the lowest nonzero term in the expansion provides the approximate potential at large r , and the successive terms tell us how to improve the approximation if greater precision is required.","title":"Example 3.10"},{"location":"ch3-4/#342-the-monopole-and-dipole-terms","text":"Ordinarily, the multipole expansion is dominated (at large r) by the monopole term: V_{mon} (\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{Q}{r} where Q =\\int \\rho \\dd{\\tau} is the total charge of the configuration. This is just what we expect for the approximate potential at large distances from the charge. For a point charge at the origin, V_{mon} is the exact potential, not merely a first approximation at large r; in this case, all the higher multipoles vanish. If the total charge is zero, the dominant term in the potential will be the dipole (unless, of course, it also vanishes): V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\int r' \\cos \\alpha \\rho(\\vec{r'}) \\dd{\\tau'} Since \\alpha is the angle between r' and r (Fig 2.38), r' \\cos \\alpha = \\hat{r} \\cdot \\vec{r'} and the dipole potential can be written more succinctly: V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{1}{r^2} \\hat{r} \\cdot \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} This integral (which does not depend on \\vec{r} ) is called the dipole moment of the distribution: \\vec{p} \\equiv \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} \\tagl{3.98} and the dipole contribution to the potential simplifies to V_{dip}(\\vec{r}) = \\frac{1}{4 \\pi \\epsilon_0} \\frac{\\vec{p} \\cdot \\hat{r}}{r^2} \\tagl{3.99} The dipole moment is determined by the geometry (size, shape, and density) of the charge distribution. \\eqref{3.98} translates in the usual way (Sect 2.1.4) for point, line, and surface charges. Thus, the dipole moment for a collection of point charges is \\vec{p} = \\sum_{i=1} ^n q_i \\vec{r'}_i \\tagl{3.100} For a physical dipole (equal and opposite charges \\pm q ), \\vec{p} = q\\vec{r'_+} - q \\vec{r_- ' } = q(\\vec{r' _+} - \\vec{r'_-}) = q \\vec{d} \\tagl{3.101} where \\vec{d} is the vector from the negative charge to the positive one (Fig. 3.29). Is this consistent with what we got in Example 3.10? Yes: If you put \\eqref{3.101} into \\eqref{3.99} , you recover \\eqref{3.90} . Notice, however, that this is only the approximate potential of the physical dipole - evidently there are higher multipole contributions. Of course, as you go farther and farther away, V_{dip} becomes a better and better approximation, since the higher terms die off more rapidly with increasing r . By the same token, at a fixed r the dipole approximation improves as you shrink the separation distance d . To construct a perfect dipole whose potential is given exactly by \\eqref{3.99} , you'd have to let d approach zero. Unfortunately you then lose the dipole term too, unless you simultaneously arrange for q to go to infinity! A physical dipole becomes a pure dipole, then, in the rather artificial limit d \\rightarrow 0. q \\rightarrow \\infty , with the product qd = p held fixed. When someone uses the word \"dipole,\" you can't always tell whether they mean a physical dipole (with finite separation between the charges) or an ideal dipole. If in doubt, assume that d is small enough that you can safely apply \\eqref{3.99} . Dipole moments are vectors , and they add accordingly: if you have two dipoles \\vec{p_1} and \\vec{p_2} , the total dipole moment is \\vec{p_1} + \\vec{p_2} . For instance, with four charges at the corners of a square, as shown in Fig. 3.30, the net dipole moment is zero. You can see this by combining the charges in pairs or by adding up the four contributions individually using \\eqref{3.100} . This is a quadrupole , as I indicated earlier, and its potential is dominated by the quadrupole term in the multipole expansion.","title":"3.4.2: The Monopole and Dipole Terms"},{"location":"ch3-4/#343-origin-of-coordinates-in-multipole-expansions","text":"I mentioned earlier that a point charge at the origin constitutes a \"pure\" monopole. If it is not at the origin, it's no longer a pure monopole. For instance, the charge in Fig. 3.32 has a dipole moment \\vec{p} = q d \\hat{y} , and a corresponding dipole term in its potential. The monopole potential (1/4 \\pi \\epsilon_0) q/r is not quite correct for this configuration; rather, the exact potential is (1/4 \\pi \\epsilon_0) q/\\gr . The multipole expansion is, remember, a series in inverse powers of r (the distance to the origin), and when we expand 1/\\gr , we get all powers, not just the first. So moving the origin (or, what amounts to the same thing, moving the charge) can radically alter a multipole expansion. The monopole moment Q does not change, since the total charge is obviously independent of the coordinate system. (In Fig. 3.32, the monopole term was unaffected when we moved q away from the origin - it's just that it was no longer the whole story: a dipole term - and for that matter all higher poles - appeared as well.) Ordinarily, the dipole moment does change when you shift the origin, but there is an important exception: If the total charge is zero, then the dipole moment is independent of the choice of origin. For suppose we displace the origin by an amount \\vec{a} (Fig. 3.33). The new dipole moment is then \\begin{align*} \\vec{p_2} & = \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} = \\int (\\vec{r'} - \\vec{a} ) \\rho (\\vec{r'}) \\dd{\\tau'} \\\\ & = \\int \\vec{r'} \\rho(\\vec{r'}) \\dd{\\tau'} - \\vec{a} \\int \\rho(\\vec{r'}) \\dd{\\tau'} = \\vec{p} - Q \\vec{a} \\end{align*} In particular, if Q = 0 , the \\vec{p_2} = \\vec{p} . So if someone asks for the dipole moment in Fig 3.34(a), you can answer with confidence \" q \\vec{d} ,\" but if you're asked for the dipole moment in Fig 3.34(b), the appropriate response would be \"With respect to what origin?\"","title":"3.4.3: Origin of Coordinates in Multipole Expansions"},{"location":"ch3-4/#344-the-electric-field-of-a-dipole","text":"So far we have only worked with potentials . Now I would like to calculate the electric field of a (perfect) dipole. If we choose coordinates so that \\vec{p} is at the origin and points in the z direction (Fig. 3.36), then the potential at r, \\theta is \\eqref{3.99} : V_{dip} (r, \\theta) = \\frac{\\hat{r} \\cdot \\vec{p}}{4 \\pi \\epsilon_0 r^2} = \\frac{p \\cos \\theta}{4 \\pi \\epsilon_0 r^2} \\tagl{3.102} To get the field, we take the negative gradient of V : \\begin{align*} E_r & = - \\pdv{V}{r} = \\frac{2 p \\cos \\theta}{4 \\pi \\epsilon_0 r^3} \\\\ E_\\theta & = - \\frac{1}{r} \\pdv{V}{\\theta} = \\frac{p \\sin \\theta}{4 \\pi \\epsilon_0 r^3} \\\\ E_\\phi & = - \\frac{1}{r \\sin \\theta} \\pdv{V}{\\phi} = 0 \\end{align*} Thus, \\vec{E_{dip}} (r, \\theta) = \\frac{p}{4 \\pi \\epsilon_0 r^3}(2 \\cos \\theta \\hat{r} + \\sin \\theta \\hat{\\theta}) \\tagl{3.103} This formula makes explicit reference to a particular coordinate system (spherical) and assumes a particular orientation for \\vec{p} (along z). It can be recast in a coordinate-free form, analogous to the potential in \\eqref{3.99} - See problem 3.36. Notice that the dipole falls off as the inverse cube of r; the monopole field (Q / 4 \\pi \\epsilon_0 r^2) \\hat{r} goes as the inverse square, of course. Quadrupole fields go like 1/r^4 , octopole like 1/r^5 , and so on. (This merely reflects how the respective potentials fall off - the gradient introduces another factor of 1/r ). Figure 3.37(a) shows the field lines of a \"pure\" dipole \\eqref{3.103} . For comparison, I have also sketched the field lines for a \"physical\" dipole, in Fig 3.37(b). Notice how similar the two pictures become if you blot out the central region; up close, however, they are entirely different. Only for points r \\gg d does \\eqref{3.103} represent a valid approximation to the field of a physical dipole. As I mentioned earlier, this regime can be reached either by going to large r or by squeezing the charges very close together.","title":"3.4.4: The Electric Field of a Dipole"}]}